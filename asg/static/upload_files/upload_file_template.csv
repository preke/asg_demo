reference paper title,reference paper citation information (can be collected from Google scholar/DBLP),reference paper abstract (Please copy the text AND paste here),reference paper introduction (Please copy the text AND paste here),reference paper doi link (optional),reference paper category id (optional)
[Example 1] Evaluation measures for ordinal regression,"Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Evaluation measures for ordinal regression. Ninth International Conference on Intelligent Systems Design and Applications, 2009. ISDA鈥?9. IEEE, 283鈥?87.","Ordinal regression (OR 鈥?also known as ordinal classification) has received increasing attention in recent times, due to its importance in IR applications such as learning to rank and product review rating. However, research has not paid attention to the fact that typical applications of OR often involve datasets that are highly imbalanced. An imbalanced dataset has the consequence that, when testing a system with an evaluation measure conceived for balanced datasets, a trivial system assigning all items to a single class (typically, the majority class) may even outperform genuinely engineered systems. Moreover, if this evaluation measure is used for parameter optimization, a parameter choice may result that makes the system behave very much like a trivial system. In order to avoid this, evaluation measures that can handle imbalance must be used. We propose a simple way to turn standard measures for OR into ones robust to imbalance. We also show that, once used on balanced datasets, the two versions of each measure coincide, and therefore argue that our measures should become the standard choice for OR. ","Are the standard evaluation measures for OR robust to imbalance? The most commonly used such measures are 1) Mean Absolute Error (here denoted MAE碌, and also called ranking loss 鈥?see e.g., <NO>), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>. MAE碌 is defined as the average deviation of the predicted class from the true class, i.e., MAE碌(桅虃, T e) = 1 |Te| 鈭?xi鈭圱e |桅虃(xi)鈭?桅(xi)| (1) where Te denotes the test set and the n classes in Y are assumed to be real numbers, so that |桅虃(xi)鈭捨?xi)| exactly quantifies the distance between the true and the predicted rank (the meaning of the 碌 superscript will be clarified later). 2) Mean Squared Error (MSE碌 鈥?also called Squared Error Loss), as used e.g., in <NO>, defined as MSE碌(桅虃, T e) = 1 |Te| 鈭?xi鈭圱e (桅虃(xi)鈭?桅(xi))2 (2) A variant is Root Mean Square Error, as used e.g., in <NO>, which corresponds to the square root of MSE碌. 3) Mean Zero-One Error (more frequently known as Error Rate), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, and simply defined as the fraction of incorrect predictions, i.e., MZOE碌(桅虃, T e) = |{xi 鈭?Te : 桅虃(xi) 6= 桅(xi}| |Te| (3) Unlike MSE碌 and MAE碌, MZOE碌 has the disadvantage that all errors are treated alike, and thus insufficiently penalizes algorithms that incur into blatant errors. MSE碌 penalizes blatant mistakes (e.g., misplacing an item into a rank faraway from the correct one) more than MAE碌, due 4 Here we assume that the positives are the minority and the negatives are the majority, which is usually the case in binary classification. to the presence of squaring; as such, it has been argued (see e.g., <NO>) that MSE碌 is more adequate for measuring systems that classify product reviews, since different reviewers might attribute identical reviews to different but neighbouring classes. It is quite evident that none of these measures is robust to imbalance, since they are all based on a sum of the classification errors across documents. Since the majorityclass classifier incurs in zero error for all the documents whose true class is the majority class, and since in an imbalanced dataset these documents are many, this trivial policy tends to be fairly 鈥渆rror-free鈥? To make this problem even worse, it is easy to show that for all these error measures the 鈥渢rivial class鈥?桅虄k need not be the majority class; in other words, there may exist trivial classifiers that are even more 鈥渆rror-free鈥?than the majority-class classifier. For instance, in the TripAdvisor15763 dataset mentioned above, assuming that the class distribution in the test set is the same as that in the training set, by assigning all test documents 4 stars we obtain lower MAE碌 than by assigning all of them 5 stars, which is the majority class. This is because 4 stars is only marginally less frequent than 5 stars, but in misclassifying all of the documents belonging to the lower classes (1 stars to 3 stars) as 4 stars we make a smaller mistake than in misclassifying them as 5 stars. Little research has been performed in order to identify evaluation measures that overcome the shortcomings of measures (1)-(3). Gaudette and Japkovicz <NO> acknowledge that these and other measures are somehow problematic but do not concretely propose alternatives. Waegeman et al. <NO> instead propose an evaluation method based on ROC analysis. The problem with their method is that, like all methods based on ROC analysis, it is more apt to evaluate the ability of a classifier at correctly ranking the objects (i.e., at placing 5 stars reviews higher than 4 stars reviews) than to evaluate the ability of the classifier to classify an object into its true (or into a nearby) class. In other words, the ROC measure of <NO> does not reward the ability of a learning device to correctly identify the thresholds 蟿j that separate a class yj from its successor class yj+1, for all j = 1, . . . , (n鈭?1).",https://doi.org/10.1109/ISDA.2009.230,feature selection
[Example 2] An experimental design to evaluate class imbalance treatment methods,"Guilherme Batista, Danilo Silva, and Ronaldo Prati. 2012. An experimental design to evaluate class imbalance treatment methods. 2012 11th International Conference on Machine Learning and Applications (ICMLA), Vol. 2. IEEE, 95鈥?01.","In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition applications; therefore, these research communities have responded to such interest with literally dozens of methods and techniques. Surprisingly, there are still many fundamental open-ended questions such as 鈥淎re all learning paradigms equally affected by class imbalance?鈥? 鈥淲hat is the expected performance loss for different imbalance degrees?鈥?and 鈥淗ow much of the performance losses can be recovered by the treatment methods?鈥? In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods. This experimental setup uses real data sets with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We employ such experimental design in a large-scale experimental evaluation with twenty-two data sets and seven learning algorithms from different paradigms. Our results indicate that the expected performance loss, as a percentage of the performance obtained with the balanced distribution, is quite modest (below 5%) for the most balanced distributions up to 10% of minority examples. However, the loss tends to increase quickly for higher degrees of class imbalance, reaching 20% for 1% of minority class examples. Support Vector Machine is the classifier paradigm that is less affected by class imbalance, being almost insensitive to all but the most imbalanced distributions. Finally, we show that the sampling algorithms only partially recover the performance losses. On average, typically about 30% or less of the performance that was lost due to class imbalance was recovered by random oversampling and SMOTE. ","Due to lack of space we assume the reader is familiar with the class imbalance problem and methods. In the case this assumption does not hold, the literature has several comprehensible surveys, we recommend <NO>. For the same reason, we are not able to include here all numerical results obtained in our experiments. Therefore, we decided to create a paper website <NO> that has detailed results, including tables and data; however, we note this paper is totally selfcontained. Our experimental design is inspired by the design used in <NO>. The central idea is to generate several training set distributions with increasing degrees of class imbalance while maintaining the training sets with a fixed number of examples. Our training set distributions range from the balanced distribution (denoted as 50/501) to the imbalanced distributions of 1/99 and 99/1. In order to generate these class distributions, we restricted the training sets to have a total number of instances equals to the number of instances of the minority class. We avoid using extremely imbalanced data sets; in particular, the combination of a small data set with large class imbalance would result in a training set with too few examples. We return to this discussion in Section V, where we comment possible limitations of this work. The test sets have the naturally occurring class distributions. We reserved 25% of the original data as test set using a stratified sample, and did not touch this portion of the data until the final evaluation; the remaining 75% was used as training set, with class distributions manipulated as previously described. In order to reduce variance and increase statistical significance, we repeated the whole process 100 times using different train and test sample partitions. For this specific study we assembled a database with twenty data sets. Since we want to promote reproducibility, most of them are public domain benchmark data sets available in repositories such as UCI Machine Learning Repository <NO> or used in projects like Statlog <NO>. A few data sets are not public domain, but we decided to include them since they are frequently used in studies involving class imbalance. These data sets include tumour identification in mammography images <NO>, <NO>. We also included data sets obtained in past research, and we make them publicly available for the first time in the paper website. We use the area under the ROC curve (AUC) <NO> as the main measure to assess our results. Since we chose AUC and also because we want to control the degree of class imbalance, we converted all non-binary class data sets into binary class, associating a positive label to one of the classes (usually the minority one) and aggregating all remaining classes into a negative class. Table I presents a summarised description of the data sets included in our study. The table lists the data sets full names, as well as a short identifier used in this paper, the number of instances and attributes, labels associated with the positive and negative classes and attribute class distribution. The data sets are listed in increasing order of class imbalance. Two data sets resulted in two entries each in Table I, because different classes were used as positive class. For the Letter dataset, Letter-a is the variation in which the positive class is the original letter 鈥渁鈥?class; and Letter-vowel has the positive class composed by all classes that represent vowels. For the Splice-junction Gene Sequences dataset, one entry has the intron-exon (鈥渋e鈥? boundaries as positive class 1 We use the notation X/Y , with X + Y = 100 to denote that for a set of 100 instances, X belongs to the positive class and Y belongs to the negative class. and the other has the exon-intron (鈥渆i鈥? boundaries. The final number of data sets is twenty-two, considering the four entries generated from these two data sets. We use this experimental design to answer the questions raised in abstract and introduction. We start discussing the expected performance loss for different learning paradigms in the next section.",https://doi.org/10.1109/ICMLA.2012.162,imbalanced data
Please delete above examples and fill in your reference papers. No need to add [Example 1 / 2] before your input title.   ,,,,,
