reference paper title,reference paper citation information (can be collected from Google scholar/DBLP),reference paper abstract (Please copy the text AND paste here),reference paper introduction (Please copy the text AND paste here),reference paper doi link (optional),reference paper category label (optional)
Evaluation measures for ordinal regression,"Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Evaluation measures for ordinal regression. Ninth International Conference on Intelligent Systems Design and Applications, 2009. ISDAÔø?9. IEEE, 283Ôø?87.","Ordinal regression (OR Ôø?also known as ordinal classification) has received increasing attention in recent times, due to its importance in IR applications such as learning to rank and product review rating. However, research has not paid attention to the fact that typical applications of OR often involve datasets that are highly imbalanced. An imbalanced dataset has the consequence that, when testing a system with an evaluation measure conceived for balanced datasets, a trivial system assigning all items to a single class (typically, the majority class) may even outperform genuinely engineered systems. Moreover, if this evaluation measure is used for parameter optimization, a parameter choice may result that makes the system behave very much like a trivial system. In order to avoid this, evaluation measures that can handle imbalance must be used. We propose a simple way to turn standard measures for OR into ones robust to imbalance. We also show that, once used on balanced datasets, the two versions of each measure coincide, and therefore argue that our measures should become the standard choice for OR. ","Are the standard evaluation measures for OR robust to imbalance? The most commonly used such measures are
1) Mean Absolute Error (here denoted MAE¬µ, and also called ranking loss Ôø?see e.g., <NO>), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>. MAE¬µ is defined as the average deviation of the predicted class from the true class, i.e.,
MAE¬µ(Œ¶ÃÇ, T e) = 1 |Te| Ôø?xi‚ààTe |Œ¶ÃÇ(xi)Ôø?Œ¶(xi)| (1)
where Te denotes the test set and the n classes in Y are assumed to be real numbers, so that |Œ¶ÃÇ(xi)‚àíÔøΩ?xi)| exactly quantifies the distance between the true and the predicted rank (the meaning of the ¬µ superscript will be clarified later). 2) Mean Squared Error (MSE¬µ Ôø?also called Squared Error Loss), as used e.g., in <NO>, defined as
MSE¬µ(Œ¶ÃÇ, T e) = 1 |Te| Ôø?xi‚ààTe (Œ¶ÃÇ(xi)Ôø?Œ¶(xi))2 (2)
A variant is Root Mean Square Error, as used e.g., in <NO>, which corresponds to the square root of MSE¬µ. 3) Mean Zero-One Error (more frequently known as Error Rate), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, and simply defined as the fraction of incorrect predictions, i.e.,
MZOE¬µ(Œ¶ÃÇ, T e) = |{xi Ôø?Te : Œ¶ÃÇ(xi) 6= Œ¶(xi}|
|Te| (3)
Unlike MSE¬µ and MAE¬µ, MZOE¬µ has the disadvantage that all errors are treated alike, and thus insufficiently penalizes algorithms that incur into blatant errors. MSE¬µ penalizes blatant mistakes (e.g., misplacing an item into a rank faraway from the correct one) more than MAE¬µ, due
4Here we assume that the positives are the minority and the negatives are the majority, which is usually the case in binary classification.
to the presence of squaring; as such, it has been argued (see e.g., <NO>) that MSE¬µ is more adequate for measuring systems that classify product reviews, since different reviewers might attribute identical reviews to different but neighbouring classes.
It is quite evident that none of these measures is robust to imbalance, since they are all based on a sum of the classification errors across documents. Since the majorityclass classifier incurs in zero error for all the documents whose true class is the majority class, and since in an imbalanced dataset these documents are many, this trivial policy tends to be fairly ‚Äúerror-freeÔø?
To make this problem even worse, it is easy to show that for all these error measures the ‚Äútrivial classÔø?Œ¶ÃÉk need not be the majority class; in other words, there may exist trivial classifiers that are even more ‚Äúerror-freeÔø?than the majority-class classifier. For instance, in the TripAdvisor15763 dataset mentioned above, assuming that the class distribution in the test set is the same as that in the training set, by assigning all test documents 4 stars we obtain lower MAE¬µ than by assigning all of them 5 stars, which is the majority class. This is because 4 stars is only marginally less frequent than 5 stars, but in misclassifying all of the documents belonging to the lower classes (1 stars to 3 stars) as 4 stars we make a smaller mistake than in misclassifying them as 5 stars.
Little research has been performed in order to identify evaluation measures that overcome the shortcomings of measures (1)-(3). Gaudette and Japkovicz <NO> acknowledge that these and other measures are somehow problematic but do not concretely propose alternatives. Waegeman et al. <NO> instead propose an evaluation method based on ROC analysis. The problem with their method is that, like all methods based on ROC analysis, it is more apt to evaluate the ability of a classifier at correctly ranking the objects (i.e., at placing 5 stars reviews higher than 4 stars reviews) than to evaluate the ability of the classifier to classify an object into its true (or into a nearby) class. In other words, the ROC measure of <NO> does not reward the ability of a learning device to correctly identify the thresholds œÑj that separate a class yj from its successor class yj+1, for all j = 1, . . . , (nÔø?1).",https://doi.org/10.1109/ISDA.2009.230,feature selection
An experimental design to evaluate class imbalance treatment methods,"Guilherme Batista, Danilo Silva, and Ronaldo Prati. 2012. An experimental design to evaluate class imbalance treatment methods. 2012 11th International Conference on Machine Learning and Applications (ICMLA), Vol. 2. IEEE, 95Ôø?01.","In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition applications; therefore, these research communities have responded to such interest with literally dozens of methods and techniques. Surprisingly, there are still many fundamental open-ended questions such as ‚ÄúAre all learning paradigms equally affected by class imbalance?Ôø? ‚ÄúWhat is the expected performance loss for different imbalance degrees?Ôø?and ‚ÄúHow much of the performance losses can be recovered by the treatment methods?Ôø? In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods. This experimental setup uses real data sets with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We employ such experimental design in a large-scale experimental evaluation with twenty-two data sets and seven learning algorithms from different paradigms. Our results indicate that the expected performance loss, as a percentage of the performance obtained with the balanced distribution, is quite modest (below 5%) for the most balanced distributions up to 10% of minority examples. However, the loss tends to increase quickly for higher degrees of class imbalance, reaching 20% for 1% of minority class examples. Support Vector Machine is the classifier paradigm that is less affected by class imbalance, being almost insensitive to all but the most imbalanced distributions. Finally, we show that the sampling algorithms only partially recover the performance losses. On average, typically about 30% or less of the performance that was lost due to class imbalance was recovered by random oversampling and SMOTE. ","Due to lack of space we assume the reader is familiar with the class imbalance problem and methods. In the case this assumption does not hold, the literature has several comprehensible surveys, we recommend <NO>. For the same reason, we are not able to include here all numerical results obtained in our experiments. Therefore, we decided to create a paper website <NO> that has detailed results, including tables and data; however, we note this paper is totally selfcontained.
Our experimental design is inspired by the design used in <NO>. The central idea is to generate several training set
distributions with increasing degrees of class imbalance while maintaining the training sets with a fixed number of examples. Our training set distributions range from the balanced distribution (denoted as 50/501) to the imbalanced distributions of 1/99 and 99/1. In order to generate these class distributions, we restricted the training sets to have a total number of instances equals to the number of instances of the minority class. We avoid using extremely imbalanced data sets; in particular, the combination of a small data set with large class imbalance would result in a training set with too few examples. We return to this discussion in Section V, where we comment possible limitations of this work.
The test sets have the naturally occurring class distributions. We reserved 25% of the original data as test set using a stratified sample, and did not touch this portion of the data until the final evaluation; the remaining 75% was used as training set, with class distributions manipulated as previously described. In order to reduce variance and increase statistical significance, we repeated the whole process 100 times using different train and test sample partitions.
For this specific study we assembled a database with twenty data sets. Since we want to promote reproducibility, most of them are public domain benchmark data sets available in repositories such as UCI Machine Learning Repository <NO> or used in projects like Statlog <NO>. A few data sets are not public domain, but we decided to include them since they are frequently used in studies involving class imbalance. These data sets include tumour identification in mammography images <NO>, <NO>. We also included data sets obtained in past research, and we make them publicly available for the first time in the paper website.
We use the area under the ROC curve (AUC) <NO> as the main measure to assess our results. Since we chose AUC and also because we want to control the degree of class imbalance, we converted all non-binary class data sets into binary class, associating a positive label to one of the classes (usually the minority one) and aggregating all remaining classes into a negative class. Table I presents a summarised description of the data sets included in our study. The table lists the data sets full names, as well as a short identifier used in this paper, the number of instances and attributes, labels associated with the positive and negative classes and attribute class distribution. The data sets are listed in increasing order of class imbalance.
Two data sets resulted in two entries each in Table I, because different classes were used as positive class. For the Letter dataset, Letter-a is the variation in which the positive class is the original letter ‚ÄúaÔø?class; and Letter-vowel has the positive class composed by all classes that represent vowels. For the Splice-junction Gene Sequences dataset, one entry has the intron-exon (‚ÄúieÔø? boundaries as positive class
1We use the notation X/Y , with X + Y = 100 to denote that for a set of 100 instances, X belongs to the positive class and Y belongs to the negative class.
and the other has the exon-intron (‚ÄúeiÔø? boundaries. The final number of data sets is twenty-two, considering the four entries generated from these two data sets.
We use this experimental design to answer the questions raised in abstract and introduction. We start discussing the expected performance loss for different learning paradigms in the next section.",https://doi.org/10.1109/ICMLA.2012.162,imbalanced data
A study of the behavior of several methods for balancing machine learning training data,"Gustavo E.A.P.A. Batista, Ronaldo C. Prati, and Maria Carolina Monard. 2004. A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 20Ôø?9.","There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.","Learning from imbalanced data sets is often reported as being a difficult task. In order to better understand this problem, imagine the situation illustrated in Figure 1. In Fig. 1(a) there is a large imbalance between the majority class (-) and the minority class (+), and the data set presents some degree of class overlapping. A much more comfortable situation for learning is represented in Fig. 1(b), where the classes are balanced with well-defined clusters.
In a situation similar to the one illustrated in Fig. 1(a), spare cases from the minority class may confuse a classifier like k-Nearest Neighbor (k-NN). For instance, 1-NN may incorrectly classify many cases from the minority class because the nearest neighbors of these cases are examples belonging to the majority class. In a situation where the imbalance is very high, the probability of the nearest neighbor of a minority class case is a case of the majority class is likely to be high, and the minority class error rate will tend to have high values, which is unacceptable.
Figure 1: Many negative cases against some spare positive cases (a) balanced data set with well-defined clusters (b).
Decision trees also experience a similar problem. In the presence of class overlapping, decision trees may need to create many tests to distinguish the minority class cases from majority class cases. Pruning the decision tree might not necessarily alleviate the problem. This is due to the fact that pruning removes some branches considered too specialized, labelling new leaf nodes with the dominant class on this node. Thus, there is a high probability that the majority class will also be the dominant class of those leaf nodes.",https://doi.org/10.1145/1007730.1007735,sampling method
FSVM-CIL: Fuzzy support vector machines for class imbalance learning,"Rukshan Batuwita, and Vasile Palade. 2010. FSVM-CIL: Fuzzy support vector machines for class imbalance learning. IEEE Trans. Fuzzy Syst. 18, 3 (2010), 558Ôø?71.","Support vector machines (SVMs) is a popular machine learning technique, which works effectively with balanced datasets. However, when it comes to imbalanced datasets, SVMs produce suboptimal classification models. On the other hand, the SVM algorithm is sensitive to outliers and noise present in the datasets. Therefore, although the existing class imbalance learning (CIL) methods can make SVMs less sensitive to class imbalance, they can still suffer from the problem of outliers and noise. Fuzzy SVMs (FSVMs) is a variant of the SVM algorithm, which has been proposed to handle the problem of outliers and noise. In FSVMs, training examples are assigned different fuzzy-membership values based on their importance, and these membership values are incorporated into the SVM learning algorithm to make it less sensitive to outliers and noise. However, like the normal SVM algorithm, FSVMs can also suffer from the problem of class imbalance. In this paper, we present a method to improve FSVMs for CIL (called FSVM-CIL), which can be used to handle the class imbalance problem in the presence of outliers and noise. We thoroughly evaluated the proposed FSVM-CIL method on ten real-world imbalanced datasets and compared its performance with five existing CIL methods, which are available for normal SVM training. Based on the overall results, we can conclude that the proposed FSVMCIL method is a very effective method for CIL, especially in the presence of outliers and noise in datasets.","In this section, we briefly review the learning algorithm of SVMs, which has been initially proposed in <NO>, <NO>. Consider we have a binary classification problem, which is represented by a dataset {(x1 , y1), (x2 , y2), . . . , (xl,yl)}, where xi Ôø?n represents an n-dimensional data point, and yi Ôø?{Ôø?, 1} represents the class of that data point, for i = 1, . . . , l. The goal of the SVM learning algorithm is to find a separating hyperplane that separates these data points into two classes. In order to find a better separation of classes, the data are first transformed into a higher dimensional feature space by a mapping function Œ¶. Then, a possible separating hyperplane, which resides in the higher dimensional feature space, can be represented by
w ¬∑ Œ¶(x) + b = 0. (1) If the dataset is completely linearly separable, the separating hyperplane with the maximum margin can be found by solving the following maximal-margin optimization problem:
Min (
1 2 w ¬∑ w
)
s.t. yi(w ¬∑ Œ¶(xi) + b) Ôø?1 i = 1, . . . , l. (2)
However, in most real-world problems, the datasets are not completely linearly separable, although they are mapped into a higher dimensional feature space. Therefore, the constrains in the aforementioned optimization problem in (2) are relaxed by introducing a slack variable Œµi Ôø?0, and then, the soft-margin optimization problem is formulated as follows:
Min (
1 2 w ¬∑ w + C lÔø?i=1 Œµi
)
s.t. yi(w ¬∑ Œ¶(xi) + b) Ôø?1 Ôø?Œµi Œµi Ôø?0, i = 1, . . . , l. (3)
The slack variables Œµi > 0 hold for misclassified examples, and therefore, ‚àël i=1 Œµi can be thought of as a measure of the amount of misclassifications. This new objective function in (3) has two goals. One is to maximize the margin, and the other one is to minimize the number of misclassifications. The parameter C controls the tradeoff between these two goals, and it can also be treated as the misclassification cost of a training
example. This quadratic-optimization problem can be solved by constructing a Lagrangian representation and transforming it into the following dual problem:
Max W (Œ±) = lÔø?i=1
Œ±i Ôø?1 2 lÔø?i=1 lÔø?j=1 Œ±iŒ±jyiyjŒ¶(xi) ¬∑ Œ¶(xj )
s.t. lÔø?i=1
yiŒ±i = 0, 0 Ôø?Œ±i Ôø?C, i = 1, . . . , l (4)
where Œ±is are Lagrange multipliers, which should satisfy the following Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions:
Œ±i(yi(w ¬∑ Œ¶(xi) + b) Ôø?1 + Œµi) = 0, i = 1, . . . , l (5) (C Ôø?Œ±i)Œæi = 0, i = 1, . . . , l. (6)
An important property of SVMs is that it is not necessary to know the mapping function Œ¶(x) explicitly. By applying a kernel function, such that K(xi, xj ) = Œ¶(xi) ¬∑ Œ¶(xj ), we would be able to transform the dual-optimization problem in (4) into
MaxW (Œ±) = lÔø?i=1
Œ±i Ôø?1 2 lÔø?i=1 lÔø?j=1 Œ±iŒ±jyiyjK(xi, xj )
s.t. lÔø?i=1
yiŒ±i = 0, 0 Ôø?Œ±i Ôø?C, i = 1, . . . , l. (7)
By solving (7) and finding the optimal values for Œ±i , w can be recovered as follows:
w = lÔø?i=1
Œ±iyiŒ¶(xi) (8)
and b can be determined from the KKT conditions in (5). The data points having nonzero Œ±i values are called support vectors. Finally, the SVM decision function is given by
f(x) = sign(w ¬∑ Œ¶(x) + b) = sign ( lÔø?i=1
Œ±iyiK(xi, x) + b ) .
(9)",https://doi.org/10.1109/TFUZZ.2010.2042721,algorithm sensitive outliers
Wrapper-based computation and evaluation of sampling methods for imbalanced datasets,"Nitesh V. Chawla, Lawrence O. Hall, and Ajay Joshi. 2005. Wrapper-based computation and evaluation of sampling methods for imbalanced datasets. Proceedings of the 1st International Workshop on UtilityBased Data Mining. ACM, New York, NY, 24Ôø?3.","Learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. When the imbalance is large, classification accuracy on the smaller class(es) tends to be lower. In particular, when a class is of great interest but occurs relatively rarely such as cases of fraud, instances of disease, and regions of interest in largescale simulations, it is important to accurately identify it. It then becomes more costly to misclassify the interesting class. In this paper, we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (SMOTE) to improve minority class accuracy. The f-value serves as the evaluation function. Experimental results show the wrapper approach is effective in optimization of the composite f-value, and reduces the average cost per test example for the datasets considered. We report both average cost per test example and the cost curves in the paper. The true positive rate of the minority class increases significantly without causing a significant change in the f-value. We also obtain the lowest cost per test example, compared to any result we are aware of for the KDD Cup-99 intrusion detection data set.","Researchers in the machine learning community have dealt with the problem of class imbalance by using various approaches like over-sampling the minority classes, undersampling the majority classes, assigning different costs for different misclassification errors, learning by recognition as opposed to discrimination, etc <NO>. There is a significant body of research comparing the various sampling methods <NO>. Sampling strategies have almost become the de facto standard for countering the imbalance in datasets <NO>. With all this there is still no answer on how to do the sampling required for obtaining good classifier accuracies on minority classes.
There are a number of different approaches that can be applied to build classifiers on imbalanced data sets. In this work, we examined under sampling and over-sampling by creating synthetic examples of minority classes. Under-
sampling the majority class can reduce the bias of the learned classifier towards it and thus improve the accuracy on the minority classes.
Some studies <NO> have been done which combined under-sampling of majority classes with over sampling by replication of minority classes. While Japkowicz <NO> found this approach very effective, Ling and Li <NO> were not able to get significant improvement in their performance measures. Japkowicz experimented with only one-dimensional artificial data of varying complexity whereas Ling and Li used real data from a Direct Marketing problem. This might have been the reason for the discrepancy between their results. On the whole, from the body of literature, it was found that under-sampling of majority classes was better than over-sampling with replication of minority classes <NO> and that the combination of the two did not significantly improve the performance over under sampling alone.
Chawla et al. <NO> introduced a new over-sampling approach for two class problems that over-sampled the minority class by creating synthetic examples rather than replicating examples. They pointed out the limitation of oversampling with replication in terms of the decision regions in feature space for decision trees. They showed that as the minority class was over sampled by increasing amounts, for decision trees, the result was to identify similar but more specific regions in the feature space. A preferable approach is to build generalized regions around minority class examples.
The synthetic minority over-sampling technique (SMOTE) was introduced to provide synthetic minority class examples which were not identical but came from the same region in feature space. The over-sampling was done by selecting each minority class example and creating a synthetic example along the line segment joining the selected example and any/all of the k minority class nearest neighbors. In the calculations of the nearest neighbors for the minority class examples a Euclidean distance for continuous features and the value Distance Metric (with the Euclidean assumption) for nominal features was used. For examples with continuous features, the synthetic examples are generated by taking the difference between the feature vectors of selected examples under consideration and their nearest neighbors. The difference between the feature vectors is multiplied by a random number between 0 and 1 and then added to the feature vector of the example under consideration to get a new synthetic example. For nominal valued features, a majority vote for the feature value is taken between the example under consideration and its k nearest neighbors. This approach effectively selects a random point along the line segment between the two feature vectors. This strategy forces the decision regions of the minority class learned by the classifier to become more general and effectively provides better generalization performance on unseen data.
However, an investigation into how to choose the number of examples to be added was not done. In addition, the amount of under-sampling also needs to be determined. Given the various costs of making errors, it is important to identify potentially optimal values for both SMOTE and under-sampling. This is equivalent to discovering the operating point in the ROC space giving the best trade-off between True Positives and False Positives. In this paper, we develop an approach to automatically set the parameters. We discuss a wrapper framework using cross-validation that
performs a step-wise and greedy search for the parameters. Note that while the computational aspects of the automated approach induces certain costs, we do not incorporate that into our framework. We optimize based on the different types of errors made. However, we do try to restrict our search space. We show that this approach works on three highly skewed datasets. We also utilized a cost-matrix to indicate the costs per test example based on the different kinds of errors.",https://doi.org/10.1145/1089827.1089830,imbalanced data
Ramoboost: Ranked minority oversampling in boosting,"Sheng Chen, Haibo He, and Edwardo A. Garcia. 2010. Ramoboost: Ranked minority oversampling in boosting. IEEE Trans. Neural Networks 21, 10 (2010), 1624Ôø?642.","In recent years, learning from imbalanced data has attracted growing attention from both academia and industry due to the explosive growth of applications that use and produce imbalanced data. However, because of the complex characteristics of imbalanced data, many real-world solutions struggle to provide robust efficiency in learning-based applications. In an effort to address this problem, this paper presents Ranked Minority Oversampling in Boosting (RAMOBoost), which is a RAMO technique based on the idea of adaptive synthetic data generation in an ensemble learning system. Briefly, RAMOBoost adaptively ranks minority class instances at each learning iteration according to a sampling probability distribution that is based on the underlying data distribution, and can adaptively shift the decision boundary toward difficult-to-learn minority and majority class instances by using a hypothesis assessment procedure. Simulation analysis on 19 real-world datasets assessed over various metrics‚Äîincluding overall accuracy, precision, recall, F-measure, G-mean, and receiver operation characteristic analysis‚Äîis used to illustrate the effectiveness of this method.","A comprehensive review of the development of research in learning from imbalanced data, including the nature of the problem, the state-of-the-art approaches, the assessment metrics, and the major opportunities and challenges, has been presented in <NO>. Interested readers can refer to that article for details. In this section, we provide a focused review of four major categories of research activity in imbalanced learning.",https://doi.org/10.1109/TNN.2010.2066988,imbalanced data
Fast: A roc-based feature selection metric for small samples and imbalanced data classification problems,"Xue-wen Chen, and Michael Wasikowski. 2008. Fast: A roc-based feature selection metric for small samples and imbalanced data classification problems. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, 124Ôø?32.","The class imbalance problem is encountered in a large number of practical applications of machine learning and data mining, for example, information retrieval and filtering, and the detection of credit card fraud. It has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifier‚Äôs suboptimal performance. This is even more true when the imbalanced data are also high dimensional. In such cases, feature selection methods are critical to achieve optimal performance. In this paper, we propose a new feature selection method, Feature Assessment by Sliding Thresholds (FAST), which is based on the area under a ROC curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. FAST is compared to two commonly-used feature selection methods, correlation coefficient and RELevance In Estimating Features (RELIEF), for imbalanced data classification. The experimental results obtained on text mining, mass spectrometry, and microarray data sets showed that the proposed method outperformed both RELIEF and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred, the classification performance of the proposed method was significantly improved compared to correlation and RELIEF-based methods.","In this section, we briefly review two commonly-used feature selection methods, CC and RELIEF.",https://doi.org/10.1145/1401890.1401910,feature selection
The relationship between Precision-Recall and ROC curves,"Jesse Davis, and Mark Goadrich. 2006. The relationship between Precision-Recall and ROC curves. ICMLÔø?6: Proc. of the 23rd Int. Conf. on Machine Learning (ACM ICPS). ACM, New York, NY, 233Ôø?240.","Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm‚Äôs performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.","In a binary decision problem, a classifier labels examples as either positive or negative. The decision made by the classifier can be represented in a structure known as a confusion matrix or contingency table. The confusion matrix has four categories: True positives (TP) are examples correctly labeled as positives. False positives (FP) refer to negative examples incorrectly labeled as positive. True negatives (TN) correspond to negatives correctly labeled as negative. Finally, false negatives (FN) refer to positive examples incorrectly labeled as negative.
A confusion matrix is shown in Figure 2(a). The confusion matrix can be used to construct a point in either ROC space or PR space. Given the confusion matrix, we are able to define the metrics used in each space as in Figure 2(b). In ROC space, one plots the False Positive Rate (FPR) on the x-axis and the True Positive Rate (TPR) on the y-axis. The FPR measures the fraction of negative examples that are misclassified as positive. The TPR measures the fraction of positive examples that are correctly labeled. In PR space, one plots Recall on the x-axis and Precision on the y-axis. Recall is the same as TPR, whereas Precision measures that fraction of examples classified as positive that are truly positive. Figure 2(b) gives the definitions for each metric. We will treat the metrics as functions that act on the underlying confusion matrix which defines a point in either ROC space or PR space. Thus, given a confusion matrix A, RECALL(A) returns the Recall associated with A.",https://doi.org/10.1145/1143844.1143874,sampling method
A multistrategy approach for digital text categorization from imbalanced documents,"Marƒ±ÃÅa Dolores Del Castillo, and Jos√© Ignacio Serrano 2004. A multistrategy approach for digital text categorization from imbalanced documents. ACM SIGKDD Explor. Newslett","The goal of the research described here is to develop a multistrategy classifier system that can be used for document categorization. The system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample. The learners work in a parallel manner, where each learner carries out its own feature selection based on evolutionary techniques and then obtains a classification model. In classifying documents, the system combines the predictions of the learners by applying evolutionary techniques as well. The system relies on a modular, flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain.","HYCLA operates in two stages, learning and integration. In the learning stage, learners apply an evolutionary technique to obtain their own feature set, and then they are trained to obtain their classification model. In the integration stage, individual learned models are evaluated on a test set, and the predictions made are combined in order to achieve the best classification of test documents. The subsections below describe the modules and procedures of this system.
The underlying architecture of HYCLA can be instantiated to approach a different text mining task by upgrading its modules.",https://doi.org/10.1145/1007730.1007740,imbalanced data
Explicitly representing expected cost: An alternative to ROC representation,"Chris Drummond, and Robert C. Holte. 2000. Explicitly representing expected cost: An alternative to ROC representation. Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, 198Ôø?07.","This paper proposes an alternative to ROC representation, in which the expected cost of a classi er is represented explicitly . This expected cost representation maintains many of the adv an tagesof R OCrepresen tation, but is easier to understand. It allows the experimenter to immediately see the range of costs and class frequencies where a particular classi er is the best and quantitatively how much better it is than other classi ers. This paper demonstrates there is a point/line duality between the tw o represen tations.A point in ROC space representing a classi er becomes a line segment spanning the full range of costs and class frequencies. This duality produces equivalen t operations in the tw o spaces, allowing most techniques used in ROC analysis to be readily reproduced in the cost space.","In this section we brie y review ROC analysis and how it is used in evaluating or comparing a classi er's performance. We then introduce our alternative dual representation, which maintains these advantages but by making explicit the expected cost is much easier to understand. In both representations, the analysis is restricted to two class problems which are referred to as the positive and negative class.",https://doi.org/10.1145/347090.347126,sampling method
Learning on the border: Active learning in imbalanced data classification,"≈ûeyda Ertekin, Jian Huang, Leon Bottou, and Lee Giles. 2007. Learning on the border: Active learning in imbalanced data classification. Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management. ACM, New York, NY, 127Ôø?36.","This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly less number of observations of the target concept. Various real-world classification tasks, such as medical diagnosis, text categorization and fraud detection suffer from this phenomenon. The standard machine learning algorithms yield better prediction performance with balanced datasets. In this paper, we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes. We also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire dataset. The proposed method yields an efficient querying system and allows active learning to be applied to very large datasets. Our experimental results show that with an early stopping criteria, active learning achieves a fast solution with competitive prediction performance in imbalanced data classification.","Recent research on class imbalance problem has focused on several major groups of techniques. One is to assign distinct costs to the classification errors <NO>. In this method, the misclassification penalty for the positive class is assigned a higher value than that of the negative class. This method requires tuning to come up with good penalty parameters for the misclassified examples. The second is to resample the original training dataset, either by over-sampling the minority class and/or under-sampling the majority class until the classes are approximately equally represented <NO>. Both resampling methods introduce additional computational costs of data preprocessing and oversampling can be overwhelming in the case of very large scale training data. Undersampling has been proposed as a good means of increasing the sensitivity of a classifier. However this method may discard potentially useful data that could be important for the learning process therefore significant decrease in the prediction performance may be observed. Discarding the redundant examples in undersampling has been discussed in <NO> but since it is an adaptive method for ensemble learning and does not involve an external preprocessing step it can not be applied to other types of algorithms. Oversampling has been proposed to create synthetic positive instances from the existing positive samples to increase the representation of the class. Nevertheless, oversampling may suffer from overfitting and due to the increase in the number of
samples, the training time of the learning process gets longer. If a complex oversampling method is used, it also suffers from high computational costs during preprocessing data. In addition to those, oversampling methods demand more memory space for the storage of newly created instances and the data structures based on the learning algorithm (i.e., extended kernel matrix in kernel classification algorithms). Deciding on the oversampling and undersampling rate is also another issue of those methods. Another technique suggested for class imbalance problem is to use a recognition-based, instead of discrimination-based inductive learning <NO>. These methods attempt to measure the amount of similarity between a query object and the target class, where classification is accomplished by imposing a threshold on the similarity measure. The major drawback of those methods is the need for tuning the similarity threshold of which the success of the method mostly relies on. On the other hand, discrimination-based learning algorithms have been proved to give better prediction performance in most domains.
In <NO> the behavior of Support Vector Machines (SVM) with imbalanced data is investigated. They applied <NO>‚Äôs SMOTE algorithm to oversample the data and trained SVM with different error costs. SMOTE is an oversampling approach in which the minority class is oversampled by creating synthetic examples rather than with replacement. The k nearest positive neighbors of all positive instances are identified and synthetic positive examples are created and placed randomly along the line segments joining the k minority class nearest neighbors. Preprocessing the data with SMOTE may lead to improved prediction performance at the classifiers, however it also brings more computational cost to the system for preprocessing and yet the increased number of training data makes the SVM training very costly since the training time at SVMs scales quadratically with the number of training instances. In order to cope with today‚Äôs tremendously growing dataset sizes, we believe that there is a need for more computationally efficient and scalable algorithms. We show that such a solution can be achieved by using active learning strategy.",https://doi.org/10.1145/1321440.1321461,imbalanced data
Active learning for class imbalance problem,"≈ûeyda Ertekin, Jian Huang, and C. Lee Giles. 2007. Active learning for class imbalance problem. Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New York, NY, 823Ôø?24.",The class imbalance problem has been known to hinder the learning performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem.,"The basic SVM based active learning selects the closest instance to the current hyperplane from the unseen training data and adds it to the training set to retrain the model. In classical active learning <NO>, the search for the most informative (closest) instance is done through the entire unseen dataset. Each iteration of active learning involves the recomputation of the distances of each instance to the new hyperplane. Thus, for large datasets, searching the entire training set is very time-consuming and computationally expensive.
We propose a selection method which will not necessitate a full search through the entire dataset but locates an approximate most informative sample by examining a small constant number of randomly chosen samples. The method picks L (L # training instances) random training samples in each iteration and selects the best (closest to the hyperplane) among them. Suppose, instead of picking the closest instance among all the training samples XN = (x1, x2, ¬∑ ¬∑ ¬∑ , xN) at each iteration, we first pick a random subset XL, L N and select the closest sample xi from XL based on the condition that xi is among the top p% closest instances in XN with probability (1Ôø?Œ∑). Any numerical modification to these constraints can be met by varying the size of L, and is independent of N . To demonstrate, the probability that at least one of the L instances is among the closest p% is 1 Ôø?(1 Ôø?p%)L. Due to the requirement of (1 Ôø?Œ∑) probability, we have
1 Ôø?(1 Ôø?p%)L = 1 Ôø?Œ∑ (1) which follows the solution of L in terms of Œ∑ and p
L = log Œ∑ / log(1 Ôø?p%) (2) For example, the active learner will pick one instance, with 95% probability, that is among the top 5% closest instances to the hyperplane, by randomly sampling only log(.05)/ log(.95) = 59 instances regardless of the training set size. This approach scales well since the size of the subset L is independent of the training set size N , requires significantly less training time and does not have an adverse effect on the classification performance of the learner. In our experiments, we set L = 59.
Early Stopping: In SVM learning the classification boundary (hyperplane) is only determined by support vectors. This means that there is no point of adding new instances to the model after the
number of support vectors saturates. A practical implementation of this idea is to count the number of support vectors during the active learning training process. If the number of the support vectors stabilizes, it implies that all possible support vectors have been selected by the active learning method and the rest of the training instances are redundant. Therefore, we choose our stopping point where the number of support vectors saturates.",https://doi.org/10.1145/1277741.1277927,feature selection
Theoretical analysis of a performance measure for imbalanced data,"Vicente Garcƒ±ÃÅa, Ram√≥n Alberto Mollineda, and Jos√© Salvador S√°nchez 2010. Theoretical analysis of a performance measure for imbalanced data. In 2010 20th International Conference on Pattern Recognition (ICPR)","This paper analyzes a generalization of a new metric to evaluate the classification performance in imbalanced domains, combining some estimate of the overall accuracy with a plain index about how dominant the class with the highest individual accuracy is. A theoretical analysis shows the merits of this metric when compared to other well-known measures.","Traditionally, classification accuracy (Acc) and/or error rates have been the standard metrics used to estimate the performance of learning systems. For a twoclass problem, they can be easily derived from a 2 √ó 2 confusion matrix as that given in Table 1.
However, empirical and theoretical evidences show that these measures are biased with respect to data imbalance and proportions of correct and incorrect classifications. These shortcomings have motivated a search for new metrics based on simple indexes, such as the true positive rate (TPrate) and the true negative rate (TNrate). The TPrate (TNrate) is the percentage of positive (negative) examples correctly classified.
One of the most widely-used evaluation methods in the context of class imbalance is the ROC curve, which is a tool for visualizing and selecting classifiers based on their trade-offs between benefits (true positives) and costs (false positives). A quantitative representation of a ROC curve is the area under it (AUC) <NO>. For just one run of a classifier, the AUC can be computed as <NO> AUC = (TPrate+ TNrate)/2.
1051-4651/10 $26.00 ¬© 2010 IEEE DOI 10.1109/ICPR.2010.156
62117
Kubat et al. <NO> use the geometric mean of accuracies measured separately on each class, with the aim of maximizing the accuracies of both classes while keeping them balanced, Gmean = Ôø?TPrate ¬∑ TNrate.
Both AUC and Gmean minimize the negative influence of skewed distributions of classes, but they do not show up the contribution of each class to the overall performance, nor which is the prevalent class. This means that different combinations of TPrate and TNrate may produce the same result for those metrics.
Recently, Ranawana and Palade <NO> introduced the optimized precision, which can be computed as,
OP = AccÔø?|TNrateÔø?TPrate| TNrate+ TPrate
(1)
This represents the difference between the global accuracy and a second term that computes how balanced both class accuracies are. High OP values require high global accuracy and well-balanced class accuracies. However, OP can be strongly affected by the biased influence of the global accuracy.",https://doi.org/10.1109/ICPR.2010.156,feature selection
Active learning from positive and unlabeled data,"Alireza Ghasemi, Hamid R. Rabiee, Mohsen Fadaee, Mohammad T. Manzuri, and Mohammad H. Rohban. 2011. Active learning from positive and unlabeled data. 2011 IEEE 11th International Conference on Data Mining Workshops (ICDMW). IEEE, 244Ôø?50.","During recent years, active learning has evolved into a popular paradigm for utilizing user‚Äôs feedback to improve accuracy of learning algorithms. Active learning works by selecting the most informative sample among unlabeled data and querying the label of that point from user. Many different methods such as uncertainty sampling and minimum risk sampling have been utilized to select the most informative sample in active learning. Although many active learning algorithms have been proposed so far, most of them work with binary or multi-class classification problems and therefore can not be applied to problems in which only samples from one class as well as a set of unlabeled data are available. Such problems arise in many real-world situations and are known as the problem of learning from positive and unlabeled data. In this paper we propose an active learning algorithm that can work when only samples of one class as well as a set of unlabeled data are available. Our method works by separately estimating probability density of positive and unlabeled points and then computing expected value of informativeness to get rid of a hyper-parameter and have a better measure of informativeness. Experiments and empirical analysis show promising results compared to other similar methods. ","Many active learning algorithms have been proposed in the literature so far. <NO> is a comprehensive survey of recent works in this field. Among the earliest and most popular active learning paradigms is the uncertainty sampling approach which is based on selecting the least confident sample for querying. The definition of confidence depends on the base classifier in use. For example, <NO> proposes an active learning approach for SVM which selects for querying the sample which is closest to the separating hyperplane. selecting the sample with minimum margin <NO> and the data sample with maximum entropy <NO> are other approaches which have been applied to active learning problems.
For classifiers that are unable to define a similarity measure over their predictions, committee-based active learning methods have been proposed. these methods form an ensemble or committee of diverse classifiers and measure uncertainty by the amount of disagreement between committee membersÔø?votes for a data sample <NO>.
In problems where samples of only one class are available, traditional uncertainty assessment methods can not work since they require information about at least two of the classes or their separating hyperplane . Therefore, specific active learning methods are required for one-class problems. One of the earlier works is <NO> which uses active learning for outlier detection. This methods works in two stages: First, a number of unlabeled samples are selected as negative samples by means of statical methods. Then a traditional committee based active learning algorithm is used to perform active learning on the rest of samples. The main advantage of this approach is that it‚Äôs flexible and can utilize a wide range of traditional active learning algorithms. However, <NO> approaches the problem of one-class learning by a traditional binary classification method. This causes degradation in accuracy of the resulting classifier since the two classes have very different characteristics and should not be treated equally. moreover, because of using two learning algorithms, the runtime complexity of this approach is much higher than other similar methods.
Another method for active learning from positive and unlabeled data has been proposed by <NO>. This paper suggests that the best choice for active learning is selecting the most relevant data sample. the justification behind this claim comes from the nature of relevance feedback in image retrieval applications. In other words, the most informative data will be chosen by the following rule:
xÔø?= argmaxx‚ààU f(x), (1)
in which f(.) is the scoring function of one-class learning which is used to rank data samples by their likelihood to the target (positive) class. The main advantages of this method lie in its simplicity and speed. However, since this method does not consider uncertainty in choosing samples, the selected data point may lack informativeness.
A more recent approach has been proposed in <NO>, which tries to apply active learning to the well-known SVDD method. <NO> considers likelihood as well as local density of data point to assess their uncertainty. First, the algorithm constructs a neighborhood graph over all data samples. Then, the most informative sample is selected using the following rule:
xÔø?=
argminxi‚ààU œÉ ||d(xi,C)‚àíR|| c + 1‚àíÔøΩ?2k Œ£xj‚ààL‚à™U (yj + 1)aij
(2)
In (2), parameters c and œÉ are used to manipulate the significance of any of two factors in the final decision measure, d(xi, C) is the distance between xi and center of sphere formed by the SVDD approach. R is radius of that sphere. y is 0 for unlabeled data, +1 for positive and Ôø? for negative samples. a is the adjacency matrix of the data neighborhood graph. aij = 1 if there is an edge between xi and xj , and 0 otherwise.
The main advantage of <NO> is that it considers both selection based on uncertainty of data, and exploring unknown regions of the feature space. This fact can be easily inferred from the two terms of equation 2. However, this methods is biased toward exploring regions containing negative data in the feature space. This causes algorithm to be biased toward selecting data which are more likely negative samples. Due to the nature of one-class learning, positive data are much more valuable than negative data samples and therefore selecting negative samples may not be much helpful in improving classification accuracy. Moreover, constructing the neighborhood graph is a time consuming task and makes the algorithm infeasible for real-time applications.",https://doi.org/10.1109/ICDMW.2011.20,imbalanced data
Learning from imbalanced data,"Haibo He, and Edwardo A. Garcia. 2009. Learning from imbalanced data. IEEE Knowl. Data Eng. 21, 9 (2009), 1263Ôø?284.","With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.","Technically speaking, any data set that exhibits an unequal distribution between its classes can be considered imbalanced. However, the common understanding in the community is that imbalanced data correspond to data sets exhibiting significant, and in some cases extreme, imbalances. Specifically, this form of imbalance is referred to as a between-class imbalance; not uncommon are betweenclass imbalances on the order of 100:1, 1,000:1, and 10,000:1, where in each case, one class severely outrepresents another <NO>, <NO>, <NO>. Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. In this paper, we only briefly touch upon the multiclass imbalanced learning problem, focusing instead on the two-class imbalanced learning problem for space considerations.
In order to highlight the implications of the imbalanced learning problem in the real world, we present an example from biomedical applications. Consider the ‚ÄúMammography Data Set,Ôø?a collection of images acquired from a series of mammography exams performed on a set of distinct patients, which has been widely used in the analysis of algorithms addressing the imbalanced learning problem <NO>, <NO>, <NO>. Analyzing the images in a binary sense, the natural classes (labels) that arise are ‚ÄúPositiveÔø?or ‚ÄúNegativeÔø?for an image representative of a ‚ÄúcancerousÔø?or ‚ÄúhealthyÔø?patient, respectively. From experience, one would expect the number of noncancerous patients to exceed greatly the number of cancerous patients; indeed, this data
set contains 10,923 ‚ÄúNegativeÔø?(majority class) samples and 260 ‚ÄúPositiveÔø?(minority class) samples. Preferably, we require a classifier that provides a balanced degree of predictive accuracy (ideally 100 percent) for both the minority and majority classes on the data set. In reality, we find that classifiers tend to provide a severely imbalanced degree of accuracy, with the majority class having close to 100 percent accuracy and the minority class having accuracies of 0-10 percent, for instance <NO>, <NO>. Suppose a classifier achieves 10 percent accuracy on the minority class of the mammography data set. Analytically, this would suggest that 234 minority samples are misclassified as majority samples. The consequence of this is equivalent to 234 cancerous patients classified (diagnosed) as noncancerous. In the medical industry, the ramifications of such a consequence can be overwhelmingly costly, more so than classifying a noncancerous patient as cancerous <NO>. Therefore, it is evident that for this domain, we require a classifier that will provide high accuracy for the minority class without severely jeopardizing the accuracy of the majority class. Furthermore, this also suggests that the conventional evaluation practice of using singular assessment criteria, such as the overall accuracy or error rate, does not provide adequate information in the case of imbalanced learning. Therefore, more informative assessment metrics, such as the receiver operating characteristics curves, precision-recall curves, and cost curves, are necessary for conclusive evaluations of performance in the presence of imbalanced data. These topics will be discussed in detail in Section 4 of this paper. In addition to biomedical applications, further speculation will yield similar consequences for domains such as fraud detection, network intrusion, and oil-spill detection, to name a few <NO>, <NO>, <NO>, <NO>, <NO>.
Imbalances of this form are commonly referred to as intrinsic, i.e., the imbalance is a direct result of the nature of the dataspace. However, imbalanced data are not solely restricted to the intrinsic variety. Variable factors such as time and storage also give rise to data sets that are imbalanced. Imbalances of this type are considered extrinsic, i.e., the imbalance is not directly related to the nature of the dataspace. Extrinsic imbalances are equally as interesting as their intrinsic counterparts since it may very well occur that the dataspace from which an extrinsic imbalanced data set is attained may not be imbalanced at all. For instance, suppose a data set is procured from a continuous data stream of balanced data over a specific interval of time, and if during this interval, the transmission has sporadic interruptions where data are not transmitted, then it is possible that the acquired data set can be imbalanced in which case the data set would be an extrinsic imbalanced data set attained from a balanced dataspace.
In addition to intrinsic and extrinsic imbalance, it is important to understand the difference between relative imbalance and imbalance due to rare instances (or ‚Äúabsolute rarityÔø? <NO>, <NO>. Consider a mammography data set with 100,000 examples and a 100:1 between-class imbalance. We would expect this data set to contain 1,000 minority class examples; clearly, the majority class dominates the minority class. Suppose we then double the sample space by testing more patients, and suppose further that the distribution
does not change, i.e., the minority class now contains 2,000 examples. Clearly, the minority class is still outnumbered; however, with 2,000 examples, the minority class is not necessarily rare in its own right but rather relative to the majority class. This example is representative of a relative imbalance. Relative imbalances arise frequently in real-world applications and are often the focus of many knowledge discovery and data engineering research efforts. Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>. These results are particularly suggestive because they show that the degree of imbalance is not the only factor that hinders learning. As it turns out, data set complexity is the primary determining factor of classification deterioration, which, in turn, is amplified by the addition of a relative imbalance.
Data complexity is a broad term that comprises issues such as overlapping, lack of representative data, small disjuncts, and others. In a simple example, consider the depicted distributions in Fig. 2. In this figure, the stars and circles represent the minority and majority classes, respectively. By inspection, we see that both distributions in Figs. 2a and 2b exhibit relative imbalances. However, notice how Fig. 2a has no overlapping examples between its classes and has only one concept pertaining to each class, whereas Fig. 2b has both multiple concepts and severe overlapping. Also of interest is subconcept C in the distribution of Fig. 2b. This concept might go unlearned by some inducers due to its lack of representative data; this issue embodies imbalances due to rare instances, which we proceed to explore.
Imbalance due to rare instances is representative of domains where minority class examples are very limited, i.e., where the target concept is rare. In this situation, the lack of representative data will make learning difficult regardless of the between-class imbalance <NO>. Furthermore, the minority concept may additionally contain a subconcept with limited instances, amounting to diverging degrees of classification difficulty <NO>, <NO>. This, in fact, is the result of another form of imbalance, a within-class imbalance, which concerns itself with the distribution of representative data for subconcepts within a class <NO>, <NO>, <NO>. These ideas are again highlighted in our simplified example in Fig. 2. In Fig. 2b, cluster B represents the dominant minority class concept and cluster C represents a subconcept of the minority class. Cluster D represents two subconcepts of the majority class and cluster A (anything
not enclosed) represents the dominant majority class concept. For both classes, the number of examples in the dominant clusters significantly outnumber the examples in their respective subconcept clusters, so that this dataspace exhibits both within-class and between-class imbalances. Moreover, if we completely remove the examples in cluster B, the dataspace would then have a homogeneous minority class concept that is easily identified (cluster C), but can go unlearned due to its severe underrepresentation.
The existence of within-class imbalances is closely intertwined with the problem of small disjuncts, which has been shown to greatly depreciate classification performance <NO>, <NO>, <NO>, <NO>. Briefly, the problem of small disjuncts can be understood as follows: A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept <NO>, <NO>, <NO>. In the case of homogeneous concepts, the classifier will generally create large disjuncts, i.e., rules that cover a large portion (cluster) of examples pertaining to the main concept. However, in the case of heterogeneous concepts, small disjuncts, i.e., rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts <NO>, <NO>, <NO>. Moreover, since classifiers attempt to learn both majority and minority concepts, the problem of small disjuncts is not only restricted to the minority concept. On the contrary, small disjuncts of the majority class can arise from noisy misclassified minority class examples or underrepresented subconcepts. However, because of the vast representation of majority class data, this occurrence is infrequent. A more common scenario is that noise may influence disjuncts in the minority class. In this case, the validity of the clusters corresponding to the small disjuncts becomes an important issue, i.e., whether these examples represent an actual subconcept or are merely attributed to noise. For example, in Fig. 2b, suppose a classifier generates disjuncts for each of the two noisy minority samples in cluster A, then these would be illegitimate disjuncts attributed to noise compared to cluster C, for example, which is a legitimate cluster formed from a severely underrepresented subconcept.
The last issue we would like to discuss is the combination of imbalanced data and the small sample size problem <NO>, <NO>. In many of today‚Äôs data analysis and knowledge discovery applications, it is often unavoidable to have data with high dimensionality and small sample size; some specific examples include face recognition and gene expression data analysis, among others. Traditionally, the small sample size problem has been studied extensively in the pattern recognition community <NO>. Dimensionality reduction methods have been widely adopted to handle this issue, e.g., principal component analysis (PCA) and various extension methods <NO>. However, when the representative data setsÔø?concepts exhibit imbalances of the forms described earlier, the combination of imbalanced data and small sample size presents a new challenge to the community <NO>. In this situation, there are two critical issues that arise simultaneously <NO>. First, since the sample size is small, all of the issues related to absolute rarity and within-class imbalances are applicable. Second and more importantly, learning algorithms often fail to
Fig. 2. (a) A data set with a between-class imbalance. (b) A highcomplexity data set with both between-class and within-class imbalances, multiple concepts, overlapping, noise, and lack of representative data.
generalize inductive rules over the sample space when presented with this form of imbalance. In this case, the combination of small sample size and high dimensionality hinders learning because of difficultly involved in forming conjunctions over the high degree of features with limited samples. If the sample space is sufficiently large enough, a set of general (albeit complex) inductive rules can be defined for the dataspace. However, when samples are limited, the rules formed can become too specific, leading to overfitting. In regards to learning from such data sets, this is a relatively new research topic that requires much needed attention in the community. As a result, we will touch upon this topic again later in our discussions.",https://doi.org/10.1145/1007730.1007733,imbalanced data
An improved SMOTE imbalanced data classification method based on support degree,"Kewen Li, Wenrong Zhang, Qinghua Lu, and Xianghua Fang. 2014. An improved SMOTE imbalanced data classification method based on support degree. 2014 International Conference on Identification, Information and Knowledge in the Internet of Things (IIKI). IEEE, 34Ôø?8.","Imbalanced data-set Classification has become a hotspot problem in Data Mining. The essential assumption of the traditional classification algorithms is that the distribution of the classes is balanced, therefore the algorithms used in Imbalanced data-set Classification cannot achieve an ideal effect. In view of imbalance date-set classification, we propose an oversampling method based on support degree in order to guide people to select minority class samples and generate new minority class samples. In the light of support degree, it is now possible to identify minority class boundary samples, then produce a number of new samples between the boundary samples and their neighbors, finally add the synthetic samples to the original data-set to participate in training and testing. Experimental results show that the method has an obvious advantage in dealing with imbalanced data-set. ","In data level, people mainly use sampling techniques to deal with imbalanced data. The basic idea of sampling is that we change the distribution of training samples to overcome the imbalance of data-set. Data sampling techniques include three types: under-sampling, over-sampling, mixed sampling. Under-sampling removes some majority class samples in order to achieve balanced data-set; oversampling increases the number of minority class samples to change the distribution of data-set; mix sampling uses both over-sampling and undersampling techniques to deal with data-set.
1) Under-sampling technique Random under-sampling <NO> is the most simple and common method in under-sampling technique, it changes the distribution of data-set by removing some negative class samples randomly, but this method also exists shortcomings, such as deleting samples artificially may lose the samples with important information and reduce the performance of classifiers.
NCR(Neighborhood Cleaning Rule, NCR) proposed by J.Laurikkala <NO> is an under-sampling method. It uses the nearest neighbor thought to remove negative class samples. Its basic idea is as follows: select a sample iX from data-set randomly, then find its three nearest neighbors and their categories, compare iX with the three neighbors: if iX is a negative class sample, at least two of the three samples are positive class samples, then remove iX from the data-set; if
iX is a positive class sample, at least two of the three samples are negative class samples, then remove the three neighbors from the data-set. So you can use this method to under-sample negative class samples.
978-1-4799-8003-1/14 $31.00 ¬© 2014 IEEE DOI 10.1109/IIKI.2014.14
34
2) Over-sampling technique Random over-sampling <NO> is the most simple and common method in oversampling technique. It increases the number of positive class by copying positive class samples randomly. This method really changes the distribution of dataset, but it has some shortcomings: copying too many positive class samples may cause classifier over-fitting , the time required for building classifiers becomes longer.
SMOTE algorithm <NO> is a classic oversampling algorithm. The basic idea of SMOTE is that new positive class samples are synthesized through linear interpolation between two near positive class samples, then add them to the original data-set. The two classes could be balanced by increasing new minority class samples. The specific approach is: for a positive class sample iX , calculate its distance from other samples of positive class, then select a sample jX from the k-nearest neighbor samples of positive class randomly, finally generate new samples as the following manner:
(0,1) ( )new i j iX X rand X X
According to Eq. 1, newX is added to participate in the training and testing. The method can prevent the occurrence of over-fitting effectively because it is not just copying positive class samples, but it cannot provide a scalar control of the number of new samples, and it cannot select positive class samples and synthesize new samples with guidance, so the quality of the new samples is not very good.
3) Mix sampling technique Both over-sampling and under-sampling are able to reduce the imbalance of data-set, but they have some drawbacks inevitably. C.Drummond <NO> proposed that the performance of classifiers which are built based on under-sampling technology is superior to the performance of classifiers which are built based on over-sampling technology, Chris Seiffert <NO> put forward a similar view from the model training complexity and training time, GEBatista <NO> thought that over-sampling technique was better than under-sampling techniques when there are overlaps in the data-set. There is not a uniform conclusion about which is better method. Therefore, combination of the two techniques is a common approach to imbalanced data classification.",https://doi.org/10.1109/IIKI.2014.14,imbalanced data
A hybrid re-sampling method for SVM learning from imbalanced data sets,"Peng Li, Pei-Li Qiao, and Yuan-Chao Liu. 2008. A hybrid re-sampling method for SVM learning from imbalanced data sets. Fifth International Conference on Fuzzy Systems and Knowledge Discovery, 2008. FSKDÔø?8. Vol. 2. IEEE, 65Ôø?9.","Support Vector Machine (SVM) has been widely studied and shown success in many application fields. However, the performance of SVM drops significantly when it is applied to the problem of learning from imbalanced data sets in which negative instances greatly outnumber the positive instances. This paper analyzes the intrinsic factors behind this failure and proposes a suitable re-sampling method. We re-sample the imbalance data by using variable SOM clustering so as to overcome the flaws of the traditional re-sampling methods, such as serious randomness, subjective interference and information loss. Then we prune the training set by means of K-NN rule to solve the problem of data confusion, which improves the generalization ability of SVM. Experiment results show that our method obviously improves the performance of the SVM on imbalanced data sets.","Imbalanced datasets have two inner factors, namely, imbalance ratio (IR) and lack of information (LI). Imbalance ratio is the value of Number of Majority/ Number of Minority and LI is the lack of information for the monority class. For a data set consisting of 100:10 majority : minority examples the imbalance factor IR is the same as in a data set of 1000:100, but the intuition implicate us there are several defference in them. In the first case the minority class is poorly respresented and suffers more from the LI factor than in the second case. Both the above inherent factors are present in every IDS learning problem, in combination with other external factors, such as overlap, complexity, size of the data and high dimension etc.
We theoretically analyze Influencing factors by means of linear separable imbalanced datasets. In figure 1(a), SVM learning from an imbanlanced data set, the result show that the learning hyperplane has almost the same orientation as the ideal hyperplane, but the distance of the learning hyperplane is far away from the ideal hyperplane. Furthermore, learning hyperplane is too close to the positive support vectors. In figure 1(b), the learning hyperplane will lean toward the negative instances and mis-identify the positive instances to negative ones in the process of testing. The phenomemon is data-whelming, when the training data gets more imbalanced, the ratio between the positive and negative support vectors also becomes more skewed. They dicide the learning hyperplane far off the ideal hyperplane, the neighborhood of a testing instances close to the boundary is more likely to be dominated by negative support vectors and hence the decision function is more likely to classify a boundary point negative and led to the majority whelm the minority.
If we randomly under-sampe the majority instances of imbalance training data, until their numbers are equal to the minority instances in gross. In figure 2(a), we can
see that the learning hyperplane is close to ideal hyperplane but the orientation of the learning hyperplane is no longer accurate. In figure 2(b), the learning hyperplane will lead to severely wrong classification result in the process of testing. This phenomemon is information loss. The reason is that mass negative instances are cut down randomly lead to many valuable information is lossing, the remainder negative instances can no longer give good cues about the orientation of the hyperplane and there is a greater degree of freedom for the orientation to vary.
From above analysis, we get the guiding principle to our re-sampling method that is to find a strategy to filter large number of majority instances, which are far away from the target boundary, without losing too many minority instances. This allows us to concentrate on distinguishing the more difficult boundary instances and reduce the imbablance ratio which makes the learning task more tractable.",https://doi.org/10.1109/FSKD.2008.407,imbalanced data
Fuzzy support vector machines,"Chun-Fu Lin, and Sheng-De Wang. 2002. Fuzzy support vector machines. IEEE Trans. Neur. Network. 13, 2 (2002), 464Ôø?71.","A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different constributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).",In this section we briefly review the basis of the theory of SVM in classification problems <NO>Ôø?NO>.,https://doi.org/10.1109/TFUZZ.2010.2042721,algorithm sensitive outliers
Exploratory undersampling for class-imbalance learning,"Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. 2009. Exploratory undersampling for class-imbalance learning. IEEE Trans. Syst. Man Cybernet. B 39, 2 (2009), 539Ôø?50.","Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. EasyEnsemble samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. BalanceCascade trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the ROC Curve, F-measure, and G-mean values than many existing classimbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods.","As mentioned in the previous section, many existing classimbalance learning methods manipulate the following four components: training set size, class prior, cost matrix, and placement of decision boundary. Here, we pay special attention to two classes of methods that are most widely used: sampling
1083-4419/$25.00 ¬© 2008 IEEE
and cost-sensitive learning. For other methods, we refer the readers to <NO> for a more complete and detailed review.
Sampling is a class of methods that alters the size of training sets. Undersampling and oversampling change the training sets by sampling a smaller majority training set and repeating instances in the minority training set, respectively <NO>. The level of imbalance is reduced in both methods, with the hope that a more balanced training set can give better results. Both sampling methods are easy to implement and have been shown to be helpful in imbalanced problems <NO>, <NO>. Undersampling requires shorter training time, at the cost of ignoring potentially useful data. Oversampling increases the training set size and thus requires longer training time. Furthermore, it tends to lead to overfitting since it repeats minority class examples <NO>, <NO>. Aside from the basic undersampling and oversampling methods, there are also methods that sample in more complex ways. SMOTE <NO> added new synthetic minority class examples by randomly interpolating pairs of closest neighbors in the minority class. The one-sided selection procedures <NO> tried to find a representative subset of majority class examples by only removing ‚ÄúborderlineÔø?and ‚ÄúnoisyÔø?majority examples. Some other methods combine different sampling strategies to achieve further improvement <NO>. In addition, researchers have studied the effect of varying the level of imbalance and how to find the best ratio when a C4.5 tree classifier was used <NO>.
Cost-sensitive learning <NO>, <NO> is another important class of class-imbalance learning methods. Although many learning algorithms have been adapted to accommodate class-imbalance and cost-sensitive problems, variants of AdaBoost appear to be the most popular ones. Many cost-sensitive boosting algorithms have been proposed <NO>. A common strategy of these variants was to intentionally increase the weights of examples with higher misclassification cost in the boosting process. In <NO>, the initial weights of high cost examples were increased. It was reported that, however, the weight differences between examples in different classes disappear quickly when the boosting process proceeds <NO>. Thus, many algorithms raised high cost examplesÔø?weights in every iteration of the boosting process, for example, AsymBoost <NO>, AdaCost <NO>, CSB <NO>, DataBoost <NO>, and AdaUBoost <NO>, just to name a few. Another way to adapt a boosting algorithm to cost-sensitive problems is to change the weights of the weak classifiers in forming the final ensemble classifier, such as BMPM <NO> and LAC <NO>. Unlike the heuristic methods mentioned earlier, Asymmetric Boosting <NO> directly minimized a cost-sensitive loss function in the statistical interpretation of boosting.
SMOTEBoost <NO> is designed for class-imbalance learning, which is very similar to AsymBoost. Both methods alter the distribution for the minority class and majority class in separate ways. The only difference is how these distributions are altered. AsymBoost directly updates instance weights for the majority class and minority class differently in each iteration, while SMOTEBoost alters distribution by first updating instance weights for majority class and minority class equally and then using SMOTE to get new minority class instances.
Chan and Stolfo <NO> introduced an approach to explore majority class examples. They split the majority class into several nonoverlapping subsets, with each subset having ap-
proximately the same number of examples as the minority class. One classifier was trained from each of these subsets and the minority class. The final classifier ensembled these classifiers using stacking <NO>. However, when a data set is highly imbalanced, this approach requires a much longer training time than undersampling. Moreover, since the minority class examples are used by every classifier, stacking these classifiers will have a high probability of suffering from overfitting when the number of minority class examples is limited.
III. EasyEnsemble AND BalanceCascade
As was shown by Drummond and Holte <NO>, undersampling is an efficient strategy to deal with class imbalance. However, the drawback of undersampling is that it throws away many potentially useful data. In this section, we propose two strategies to explore the majority class examples ignored by undersampling: EasyEnsemble and BalanceCascade.",https://doi.org/10.1109/TSMCB.2008.2007853,imbalanced data
Disturbing neighbors ensembles of trees for imbalanced data,"Juan J. Rodrƒ±ÃÅguez, Jos√©-Francisco Dƒ±ÃÅez-Pastor, Jes√∫s Maudes, and C√©sar Garcƒ±ÃÅa-Osorio 2012. Disturbing neighbors ensembles of trees for imbalanced data. In 2012 11th International Conference on Machine Learning and Applications (ICMLA),","Disturbing Neighbors (DN ) is a method for generating classifier ensembles. Moreover, it can be combined with any other ensemble method, generally improving the results. This paper considers the application of these ensembles to imbalanced data: classification problems where the class proportions are significantly different. DN ensembles are compared and combined with Bagging, using three tree methods as base classifiers: conventional decision trees (C4.5), Hellinger distance decision trees (HDDT) ‚Äîa method designed for imbalance dataÔø?and model trees (M5P) ‚Äîtrees with linear models at the leavesÔø? The methods are compared using two collections of imbalanced datasets, with 20 and 66 datasets, respectively. The best results are obtained combining Bagging and DN , using conventional decision trees. ","As many other ensemble methods (e.g., Bagging, Random Subspaces) the Disturbing Neighbors (DN ) method is based on training each base classifier with a random transformation of the dataset.
In this method, the transformation adds new attributes. These attributes are obtained from a small sample of randomly selected training examples. These examples are called the Disturbing Neighbors.
The additional attributes are based on which one of the Disturbing Neighbors is the nearest. The first added attribute is nominal, the possible values are the class labels. For a binary class data set, the attribute will be binary. For a given example, the value of this attribute is the class label of the nearest Disturbing Neighbor.
For each Disturbing Neighbor an additional boolean attribute is included. The value of the attribute is true for a given example if the corresponding Disturbing Neighbor is the nearest one.
As an additional source of diversity (a necessary ingredient of successful ensembles) for each base classifier the distances are calculated in a random subspace. This random subspace is only used for determining the nearest Disturbing Neighbor, the transformed dataset contains all the original attributes.
It is possible to obtain ensembles using only the DN method, but usually is possible to obtain better results combining DN with other ensemble methods. The combination generally is better than the other ensemble method and the DN ensemble <NO>.
In Figure 1 the tree at the right was obtained with the same method and from the same dataset (glass4) as the other trees but with the additional attributes from the Disturbing Neighbors. One of this attributes, Nearest 8, was selected as the root of the tree. This attribute indicates if the nearest DN is the eighth.
From the considered sources of diversity, in <NO> it was determined that for non particularly imbalanced data sets the most important source was the use of the boolean features, the impact of the nominal feature and the calculation of the distances in a random subspace were less important.",https://doi.org/10.1109/ICMLA.2012.181,imbalanced data
RUSBoost: A hybrid approach to alleviating class imbalance,"Chris Seiffert, Taghi M. Khoshgoftaar, Jason Van Hulse, and Amri Napolitano. 2010. RUSBoost: A hybrid approach to alleviating class imbalance. IEEE Trans.Syst. Man Cybernet. A 40, 1 (2010), 185Ôø?97.","Class imbalance is a problem that is common to many application domains. When examples of one class in a training data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classification models. Several techniques have been used to alleviate the problem of class imbalance, including data sampling and boosting. In this paper, we present a new hybrid sampling/boosting algorithm, called RUSBoost, for learning from skewed training data. This algorithm provides a simpler and faster alternative to SMOTEBoost, which is another algorithm that combines boosting and data sampling. This paper evaluates the performances of RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using 15 data sets from various application domains, four base learners, and four evaluation metrics. RUSBoost and SMOTEBoost both outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a simpler and faster technique. Given these experimental results, we highly recommend RUSBoost as an attractive alternative for improving the classification performance of learners built using imbalanced data.","Much research has been performed with respect to the class imbalance problem. Weiss <NO> provides a survey of the class imbalance problem and techniques for reducing the negative impact imbalance that has on classification performance. The study identifies many methods for alleviating the problem of class imbalance, including data sampling and boosting, which are the two techniques investigated in this paper. Japkowicz <NO> presents another study addressing the issue of class imbalance, including an investigation of the types of imbalance that most negatively impact classification performance, and a small case study comparing several techniques for alleviating the problem.
Data sampling has received much attention in research related to class imbalance. Data sampling attempts to overcome imbalanced class distributions by adding examples to (oversampling) or removing examples from (undersampling) the data set. The simplest form of undersampling is RUS. RUS randomly removes examples from the majority class until a desired class distribution is found. While there is no universally accepted optimal class distribution, a balanced (50 : 50) distribution is often considered to be near optimal <NO>. However, when
examples from the minority class are very rare, a ratio closer to 35 : 65 (minority:majority) may result in better classification performance <NO>.
In addition to random data sampling techniques, several more ‚ÄúintelligentÔø?algorithms for resampling data have been proposed. Barandela et al. <NO> and Han et al. <NO> examine the performance of some of these ‚ÄúintelligentÔø?data sampling techniques, such as SMOTE, borderline SMOTE, and Wilson‚Äôs editing. Van Hulse et al. <NO> examine the performance of seven different data sampling techniques (both ‚ÄúintelligentÔø?and random) using a large number of different learning algorithms and experimental data sets, finding both RUS and SMOTE to be very effective data sampling techniques.
Another technique for dealing with class imbalance is boosting. While boosting is not specifically designed to handle the class imbalance problem, it has been shown to be very effective in this regard <NO>. The most commonly used boosting algorithm is AdaBoost <NO>, which has been shown to improve the performance of any weak classifier, provided that the classifier results in better performance than random guessing. Several variations have been proposed to make AdaBoost cost sensitive <NO>Ôø?NO> or to improve its performance on imbalanced data <NO>Ôø?NO>. One of the most promising of these techniques is SMOTEBoost <NO>. SMOTEBoost combines an intelligent oversampling technique (SMOTE) with AdaBoost, resulting in a highly effective hybrid approach to learning from imbalanced data. Our proposed technique, which is RUSBoost, is based on the SMOTEBoost algorithm but provides a faster and simpler alternative for learning from imbalanced data with performance that is usually as good (and often better) than that of SMOTEBoost.
Closely related to the issue of class imbalance is costsensitive learning. Weiss et al. <NO> compare the performances of oversampling, undersampling, and cost-sensitive learning when dealing with data that have both an imbalanced class distribution and unequal error costs. Sun et al. <NO> present an in-depth examination of cost-sensitive boosting. Chawla et al. <NO> perform a detailed evaluation of a wrapper-based sampling approach to minimize misclassification cost. A detailed evaluation of RUSBoost as a cost-sensitive learning technique, including a comparison to existing methodologies, is left as a future work.",https://doi.org/10.1109/TSMCA.2009.2029559,imbalanced data
An improved AdaBoost algorithm for unbalanced classification data,"Jie Song, Xiaoling Lu, and Xizhi Wu. 2009. An improved AdaBoost algorithm for unbalanced classification data. Sixth International Conference on Fuzzy Systems and Knowledge Discovery, 2009. FSKDÔø?9. Vol. 1. IEEE, 109Ôø?13.","AdaBoost algorithm is proved to be a very efficient classification method for the balanced dataset with all classes having similar proportions. However, in real application, it is quite common to have unbalanced dataset with a certain class of interest having very small size. It will be problematic since the algorithm might predict all the cases into majority classes without loss of overall accuracy. This paper proposes an improved AdaBoost algorithm called BABoost (Balanced AdaBoost), which gives higher weights to the misclassified examples from the minority class. Empirical results show that the new method decreases the prediction error of minority class significantly with increasing the prediction error of majority class a little bit. It can also produce higher values of margin which indicates a better classification method","In this paper, we propose a new algorithm called BABoost based on dividing the overall misclassification error into several parts. We want to utilize BABoost to improve the accuracy over the minority class without much sacrificing the accuracy over the majority class.
Adaboost algorithm gives equal weight to each misclassified example. But the misclassification error of each class is not same. Generally, the misclassification error of the minority class will larger than the majority‚Äôs. So Adaboost algorithm will lead to higher bias and smaller margin when encountering skew distribution. Our goal is to reduce the bias of Adaboost algorithm and increase the margin between each two classes. The proposed BABoost algorithm (Figure 1) in each round of boosting assigns more weights to the misclassified examples, especially those in the minority class. More generally, we focus on multiclass (J-class) problems in which in this paper. Following Schapire and Singer‚Äôs <NO>, <NO> approach to multiclass problems, we change the multiclass problem into two-class problems. That is, if equals , we set it equal to a 1 by vector with the element as +1 and -1 for others; if equals , then it equals a 1 by J-1 vector with all elements as -1. D denotes the resampling probabilities for each case and H is the matrix for the ensemble predictor.
The differences between BABoost and Adaboost are the calculation of , , and the update of the H. indicates the prediction error for the predictor when predicting the cases into the class. is the weight for the
predictor and class. For example, instead of the overall error in the AdaBoost, it will be two within group prediction errors: and for binary problems.
Except that and will lead to different weights to majority class and minority class, also give a strong impact to reweight the distribution of samples. Different class corresponds to its own . The value of should be positive and larger for majority class, smaller for minority class in order to emphasize more on minority class: The larger the is, the smaller the is. So the corresponding misclassified objects of the class get lower weights.",https://doi.org/10.1109/FSKD.2009.608,imbalanced data
Boosting for learning multiple classes with imbalanced class distribution,"Yanmin Sun, Mohamed S. Kamel, and Yang Wang. 2006. Boosting for learning multiple classes with imbalanced class distribution. Sixth International Conference on Data Mining, 2006. ICDMÔø?6. IEEE, 592Ôø?02.","Classification of data with imbalanced class distribution has posed a significant drawback of the performance attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. This learning difficulty attracts a lot of research interests. Most efforts concentrate on bi-class problems. However, bi-class is not the only scenario where the class imbalance problem prevails. Reported solutions for bi-class applications are not applicable to multi-class problems. In this paper, we develop a cost-sensitive boosting algorithm to improve the classification performance of imbalanced data involving multiple classes. One barrier of applying the cost-sensitive boosting algorithm to the imbalanced data is that the cost matrix is often unavailable for a problem domain. To solve this problem, we apply Genetic Algorithm to search the optimum cost setup of each class. Empirical tests show that the proposed cost-sensitive boosting algorithm improves the classification performances of imbalanced data sets significantly.","The original AdaBoost algorithm reported in <NO> takes as input a training set {(x1, y1), ¬∑ ¬∑ ¬∑, (xm, ym)} where each xi is an n-tuple of attribute values belonging to a certain domain or instance space X , and yi is a label in a label set Y = {Ôø?, +1} in the context of bi-class applications. The Pseudocode for AdaBoost is given in Figure 1.
It has been shown in <NO> that the training error of the final classifier is bounded as
1 m |{i : H(xi) 6= yi}| Ôø?Ôø?t Zt (3)
Let
f(x) = TÔø?t=1 Œ±tht(x)
By unraveling the update rule of Equation 1, we have that
Dt+1(i) = exp(Ôø?Ôø?t Œ±tht(xi)yi)
m Ôø?t Zt
= exp(‚àíyif(xi))
m Ôø?t Zt
(4)
By the definition of the final hypothesis of Equation 2, if H(xi) 6= yi, the yif(xi) Ôø?0 implying that exp(‚àíyif(xi)) Ôø?1. Thus,
<NO> Ôø?exp(‚àíyif(xi)). (5)
where for any predicate œÄ,
<NO> = { 1 if œÄ holds 0 otherwise
(6)
Combining Equation 4 and 5 gives the error upper bound of Equation 3 since
1
m Ôø?i <NO> Ôø?1 m Ôø?i exp(‚àíyif(xi)) (7)
= Ôø?i
( Ôø?t
Zt)D t+1(i) = Ôø?t Zt (8)
To minimize the error upper-bound, on each boosting round, the learning objective is to minimize
Zt = Ôø?i
Dt(i)exp(‚àíŒ±tyiht(xi)) (9)
= Ôø?i
Dt(i)( 1 + yiht(xi) 2 e‚àíÔøΩ?+ 1Ôø?yiht(xi) 2 eŒ±) (10)
Then, by minimizing Zt on each round, Œ±t is induced as
Œ±t = 1
2 log(
Ôø?i,yi=ht(xi) Dt(i)
Ôø?i,yi 6=ht(xi) Dt(i) ) (11)
The sample weight updating goal of AdaBoost is to decrease the weight of training samples which are correctly classified and increase the weights of the opposite part. Therefore, Œ±t should be a positive value demanding that the training error should be less than randomly guessing (0.5) based on the current data distribution. That is
Ôø?i,yi=ht(xi) Dt(i) > Ôø?i,yi 6=ht(xi) Dt(i) (12)",https://doi.org/10.1109/ICDM.2006.29,imbalanced data
Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval,"Dacheng Tao, Xiaoou Tang, Xuelong Li, and Xindong Wu. 2006. Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval. IEEE Trans. Pattern Anal. Mach. Intell. 28, 7 (2006), 1088Ôø?099.","Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM‚Äôs optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance.","FEEDBACK IN CONTENT-BASED IMAGE RETRIEVAL
SVM <NO>, <NO> is a very effective binary classification
algorithm. Consider a linearly separable binary classifica-
tion problem (as shown in Fig. 1):
f√∞xi; yi√ûgNi¬º1 and yi ¬º √æ1; 1f g; √∞1√û
where xi is an n-dimension vector and yi is the label of the class that the vector belongs to. SVM separates the two
classes of points by a hyperplane,
wTx√æ b ¬º 0; √∞2√û
where w is an input vector, x is an adaptive weight vector,
and b is a bias. SVM finds the parameters w and b for the
optimal hyperplane to maximize the geometric margin 2= k w k , subject to
yi w Txi √æ b
√æ1: √∞3√û
The solution can be found through a Wolfe dual problem
with the Lagrangian multiplied by i:
Q√∞ √û ¬º Xm i¬º1 i Xm i;j¬º1 i jyiyj√∞xi xj√û=2; √∞4√û
subject to i 0 and Pm
i¬º1 iyi ¬º 0. In the dual format, data points only appear in the inner
product. To get a potentially better representation of the
data, the data points are mapped into a Hilbert Inner
Product space through a replacement:
xi xj ! √∞xi√û √∞xj√û ¬º K√∞xi;xj√û; √∞5√û
where K√∞:√û is a kernel function. We then get the kernel version of the Wolfe dual problem:
Q√∞ √û ¬º Xm i¬º1 i Xm i;j¬º1 i jyiyjK√∞xi;xj√û=2: √∞6√û
Thus, for a given kernel function, the SVM classifier is
given by
F x√∞ √û ¬º sgn f x√∞ √û√∞ √û; √∞7√û
where f x√∞ √û ¬º Pl
i¬º1 iyiK xi;x√∞ √û √æ b is the output hyperplane decision function of the SVM.
In general, when f x√∞ √ûj j for a given pattern is high, the corresponding prediction confidence will be high. Meanwhile, a low f x√∞ √ûj j of a given pattern means that the pattern is close to the decision boundary and its corresponding
prediction confidence will be low. Consequently, the output of SVM, f x√∞ √û, has been used to measure the dissimilarity <NO>, <NO> between a given pattern and the query image, in
traditional SVM-based CBIR RF.",https://doi.org/10.1109/TPAMI.2006.134,imbalanced data
Regression error characteristic surfaces,"Luƒ±ÃÅs Torgo 2005. Regression error characteristic surfaces. In KDDÔø?5: Proc. of the 11th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining. ACM Press,","This paper presents a generalization of Regression Error Characteristic (REC) curves. REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems. REC curves provide useful information for analyzing the performance of models, particularly when compared to error statistics like for instance the Mean Squared Error. In this paper we present Regression Error Characteristic (REC) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable, i.e. the joint cumulative distribution function of the errors and the target variable. This provides a more detailed analysis of the performance of models when compared to REC curves. This extra detail is particularly relevant in applications with non-uniform error costs, where it is important to study the performance of models for specific ranges of the target variable. In this paper we present the notion of REC surfaces, describe how to use them to compare the performance of models, and illustrate their use with an important practical class of applications: the prediction of rare extreme values.","Bi and Bennet <NO> have presented REC curves. These curves play a role similar to ROC curves (e.g. <NO>) in classification tasks, but for regression problems. They provide a graphical description of the cumulative distribution function of the error of a model, i.e. D( ) = P (Œµ Ôø?). The authors describe a simple algorithm for plotting these curves based on estimating the probabilities using the observed frequencies of the errors.
REC curves provide a better description of a model predictive performance when compared to prediction error statistics because they illustrate its performance across the range of possible errors. It is thus possible to extract more information by comparing the REC curves of two alternative models than with the two respective error statistics. Moreover, the interpretation of REC curves is quite appealing to non-experts and it is possible to obtain the same quantitative information given by prediction error statistics by calculating the Area Over the Curve (AOC), which Bi and Bennet <NO> have proved to be a biased estimate of the expected error of a model.
Figure 1 shows an example of the REC curves of three models. This example shows a model (model A) clearly
dominating the others over all range of possible errors. On the contrary models B and C have performances that are harder to compare. For smaller errors model C dominates model B, but as we move towards larger errors we see model B overcoming model C. The decision on which of these two is preferable may be domain dependent, provided their area over the curve (i.e. expected error) is similar.
In spite of the above mentioned advantages there are some specific domain requirements that are difficult to check using REC curves. These have to do with domains where the cost of errors is non-uniform, i.e. where the importance of an error with an amplitude of say 1.2, can be different depending on the true target variable value. For this type of applications, it may be important to inspect the distribution of the errors across the distribution of the target variable. In effect, it is possible to have two different models with exactly the same REC curves but still one being preferable to the other just because smaller errors occur for target values that are more relevant (e.g. have higher cost) for the application being studied. Distinguishing these two models and checking that one behaves more favorably than the other is not possible with the information provided by REC curves. This is the objective of our work.",https://doi.org/10.1145/1081870.1081959,sampling method
Experimental perspectives on learning from imbalanced data,"Jason Van Hulse, Taghi M. Khoshgoftaar, and Amri Napolitano. 2007. Experimental perspectives on learning from imbalanced data. Proceedings of the 24th International Conference on Machine Learning. ACM, 935Ôø?42.","We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the performance of learners built from imbalanced data? Is the effectiveness of sampling related to the type of learner? Do the results change if the objective is to optimize different performance metrics? We address these and other issues in this work, showing that sampling in many cases will improve classifier performance.","The 35 datasets utilized in our empirical study are listed in Table 1. The percentage of minority examples varies from 1.33% (highly imbalanced) to almost 35% (only slightly imbalanced). The datasets also come from a wide variety of application domains, and 19 are from the UCI repository (Blake & Merz, 1998). The Mammography dataset was generously provided by Dr. Nitesh Chawla (Chawla et al., 2002). Fifteen datasets (some of which are proprietary) are from the domain of software engineering measurements. We have also considered datasets with diversity in the number of attributes, and datasets with both continuous and categorical attributes. The smallest dataset had 214 total examples (Glass-3), while the two largest datasets each contain 20,000 observations. Note that all datasets have, or were transformed to have, a binary class. We only consider binary classification problems in this work.",https://doi.org/10.1145/1273496.1273614,sampling method
Class probability estimates are unreliable for imbalanced data (and how to fix them),"Byron C. Wallace, and Issa J. Dahabreh. 2012. Class probability estimates are unreliable for imbalanced data (and how to fix them). 2012 IEEE 12th International Conference on Data Mining (ICDM). IEEE, 695Ôø?04.","Obtaining good probability estimates is imperative for many applications. The increased uncertainty and typically asymmetric costs surrounding rare events increases this need. Experts (and classification systems) often rely on probabilities to inform decisions. However, we demonstrate that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. To our knowledge, this problem has not previously been explored. Motivated by our exposition of this issue, we propose a simple, effective and theoretically motivated method to mitigate the bias of probability estimates for imbalanced data that bags estimators calibrated over balanced bootstrap samples. This approach drastically improves performance on the minority instances without greatly affecting overall calibration. We show that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates. ","The standard method for estimating probabilities in the supervised learning framework is to regress measurements correlated with (predicted) class labels output by a trained
1We note that Cieslak and Chawla have investigated the specific case of Probability Estimation Trees (PETs) for imbalanced data <NO>, and that Foster and Stine have considered the related task of variable selection for prediction under imbalance <NO>.
classifier against the true target labels <NO>, <NO>. This process is called calibration. By convention these measurements are denoted by fi, where i indexes instances. This calibration squashes the (arbitrarily scaled) fi‚Äôs into the <NO> range permissible for probabilities. When the sigmoid form is used (Equation 1), this method is referred to as Platt scaling <NO>. It is perhaps the most popular way of obtaining probability estimates from classification models.
P (yi|fi) = 1 1 + exp{‚àíÔøΩ? Ôø?Œ≤1fi} (1)
The fi‚Äôs may be any scalar that is predictive of class membership. We focus on two specific post-training calibration strategies; Platt calibration with SVMs and with boosted decision trees. We selected these methods because they have been shown to out-perform other supervised learning algorithms with respect to class probability estimation <NO>, <NO>.
In the case of SVMs, the fi is taken as the signed distance from the hyperplane w, i.e., fi = wTxi. This was the method originally proposed by Platt <NO>, and is now widely used <NO>. Niculescu-Mizil and Caruana, meanwhile, have proposed attaining probabilities via calibrated boosted decision trees <NO>. More precisely, recall that in boosting one induces a sequence of learners h0, h1, ... , hk over different distributions of the training set. These are in turn associated with a set of weights Œ±0, Œ±1, ... , Œ±k reflecting the their estimated performance. A prediction is then taken as a function over these, i.e., as sign( Ôø?j Œ±jhj(x)). The natural value for fi is then the sum of the weighted class predictions over the ensemble, i.e., Ôø?j Œ±jhj(x).
Figure 2 displays the overall and stratified residual errors of probability estimates (obtained via Platt‚Äôs method) for the instances comprising a particular imbalanced dataset.2 Specifically, each sub-plot shows histograms of the absolute differences between the true (observed) labels and corresponding probability estimates, i.e., |yi‚àíPÃÇ{yi|xi}|. Density to the left therefore suggests good calibration, as this implies probability estimates largely agree with the observed labels. For example, if yi = 1 and PÃÇ{yi|xi} = .99, the difference would be .01. Were the estimate .01, on the other hand, the difference would be .99.
The left-hand side of Figure 2 shows this histogram for all instances, corresponding to overall calibration. Over 80% of instances are in the left-most bin, implying that the estimator is well-calibrated, i.e., its estimates do not much diverge from the observed labels. But this ostensibly good calibration belies the unreliability of the probability estimates for the instances comprising the rare class. One can see this by looking at the middle plot, which is the same figure but includes only minority instances. In this case, the estimates diverge strikingly from the observed
2The proton beam dataset in Table I.
labels; indeed the model assigned a probability (of belonging to the minority class) of less than 20% to most of the minority instances. In other words, the probability estimates for instances comprising the minority class are completely unreliable (we demonstrate this on 16 datasets in Section V).",https://doi.org/10.1109/ICDM.2012.115,imbalanced data
Combating the small sample class imbalance problem using feature selection,"Mike Wasikowski, and Xue-wen Chen. 2010. Combating the small sample class imbalance problem using feature selection. IEEE Trans. Knowl. Data Eng. 22, 10 (2010), 1388Ôø?400.","The class imbalance problem is encountered in real-world applications of machine learning and results in a classifier‚Äôs suboptimal performance. Researchers have rigorously studied the resampling, algorithms, and feature selection approaches to this problem. No systematic studies have been conducted to understand how well these methods combat the class imbalance problem and which of these methods best manage the different challenges posed by imbalanced data sets. In particular, feature selection has rarely been studied outside of text classification problems. Additionally, no studies have looked at the additional problem of learning from small samples. This paper presents a first systematic comparison of the three types of methods developed for imbalanced data classification problems and of seven feature selection metrics evaluated on small sample data sets from different applications. We evaluated the performance of these metrics using area under the receiver operating characteristic (AUC) and area under the precisionrecall curve (PRC). We compared each metric on the average performance across all problems and on the likelihood of a metric yielding the best performance on a specific problem. We examined the performance of these metrics inside each problem domain. Finally, we evaluated the efficacy of these metrics to see which perform best across algorithms. Our results showed that signal-tonoise correlation coefficient (S2N) and Feature Assessment by Sliding Thresholds (FAST) are great candidates for feature selection in most applications, especially when selecting very small numbers of features.","The two Learning from Imbalanced Data Sets workshops thoroughly explored the approaches to combating the class imbalance problem: sampling, new algorithms, and feature selection. The first was held at the AAAI conference in 2000 <NO>, and the second was held at the ICML conference in 2003 <NO>. Also, Weiss reviewed these approaches in SIGKDD Explorations <NO>.",https://doi.org/10.1109/TKDE.2009.187,feature selection
An active under-sampling approach for imbalanced data classification,"Zeping Yang, and Daqi Gao. 2012. An active under-sampling approach for imbalanced data classification. 2012 Fifth International Symposium on Computational Intelligence and Design (ISCID). Vol. 2. IEEE, 270Ôø?73.","An active under-sampling approach is proposed for handling the imbalanced problem in this paper. Traditional classifiers usually assume that training examples are evenly distributed among different classes, so they are often biased to the majority class and tend to ignore the minority class. In this case, it is important to select the suitable training dataset for learning from imbalanced data. The samples of the majority class which are far away from the decision boundary should be got rid of the training dataset automatically in our algorithm, and this process doesn‚Äôt change the density distribution of the whole training dataset. As a result, the ratio of majority class is decreased significantly, and the final balance training dataset is more suitable for the traditional classification algorithms. Compared with other under-sampling methods, our approach can effectively improve the classification accuracy of minority classes while maintaining the overall classification performance by the experimental results. ","V. CONCLUSION Learning from imbalanced dataset is a challenging problem, because traditional classifiers are designed on the assumption that training examples are evenly distributed among different classes. Sampling methods can change data distribution by adding or deleting samples from the original training dataset. An active under-sampling approach has been presented on imbalanced data in this paper. Instead of getting rid of majority class samples randomly, our algorithm
actively selected the samples of majority class near the decision boundary, and, at the same time, maintained the original density distribution. The experimental results show that the proposed algorithm can achieve better performance compared to other methods on imbalanced datasets. For the future, the proposed method and suitable data cleaning technique were combined to handle highly imbalanced and overlapping datasets.",https://doi.org/10.1109/ISCID.2012.219,sampling method
An unsupervised learning approach to resolving the data imbalanced issue in supervised learning problems in functional genomics,"Kihoon Yoon, and Stephen Kwek. 2005. An unsupervised learning approach to resolving the data imbalanced issue in supervised learning problems in functional genomics. Fifth International Conference on Hybrid Intelligent Systems, 2005. HISÔø?5. IEEE, 6‚Äìpp.","Learning from imbalanced data occurs very frequently in functional genomic applications. One positive example to thousands of negative instances is common in scientific applications. Unfortunately, traditional machine learning treats the extremely small instances as noise. The standard approach for this difficulty is balancing training data by resampling them. However, this results in high false positive predictions. Hence, we propose preprocessing majority instances by partitioning them into clusters. This greatly reduces the ambiguity between minority instances and instances in each cluster. For moderately high imbalance ratio and low in-class complexity, our technique gives better prediction accuracy than undersampling method. For extreme imbalance ratio like splice site prediction problem, we demonstrate that this technique serves as a good filter with almost perfect recall that reduces the amount of imbalance so that traditional classification techniques can be deployed and yield significant improvements over previous predictor. We also show that the technique works for subcellular localization and post-translational modification site prediction problems.","Most supervised learning algorithms tend to focus on obtaining high accuracy on the observed labeled training data. To further aggravate this difficulty, almost all algorithms tend to follow the Occam‚Äôs razor principle (or related minimum description length MDL principle) where there is a preference toward simple
hypothesis <NO>. Short decision trees and neural networks with small weights are preferred. The underlying assumption here is that events (instances) that occur infrequently are considered as noise. This further discriminates against the minority class so as to achieve high overall prediction accuracy. For highly imbalance data, the classifiers constructed using these algorithms would simply predict negative all the time and achieve almost 100% accuracy! This is nonsensical for applications in functional genomic (and computer security) where the aims are to detect minority instances within a certain reasonable tolerance of false positive mistakes.
Various approaches <NO> have been proposed to tackle the challenge posed by the imbalance ratio problem. These approaches fall into two different categories, namely weighting or resampling based methods. Weighting methods either assign heavier weights to the minority training instances or penalties for misclassifications of minority instances <NO>. The other way is to preprocess training data to minimize discrepancy between the classes. Oversampling <NO> the minority class and undersampling <NO> the majority class are the data level approaches. Ling and Li <NO> combining oversampling and undersampling methods but did not achieve significant improvement in the ""lift index"" metric that they used. Both methods effectively change the training distribution to one that no longer resemble the original (highly imbalance) distribution, resulting in overfitting. Other important related works similar to resampling approaches are to focus on solving small disjuncts problem within each class. Japkowicz <NO> discussed about the cause for lower performance in standard classifiers is actually small disjuncts of within-class. These works agree with what we observed from our experiments.",https://doi.org/10.1109/ICHIS.2005.23,imbalanced data
Feature selection for text categorization on imbalanced data,"Zhaohui Zheng, Xiaoyun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced data. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 80Ôø?9.","A number of feature selection metrics have been explored in text categorization, among which information gain (IG), chi-square (CHI), correlation coefficient (CC) and odds ratios (OR) are considered most effective. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and nonmembership (e.g. negative features) by ignoring the signs of features. The former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data. In this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework. Using multinomial n√§ƒ±ve Bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.","In this section, we present six feature selection metrics (four known measures and two proposed variants), which are functions of the following four dependency tuples:
1. (t, ci): presence of t and membership in ci.
2. (t, ci): presence of t and non-membership in ci.
3. (t, ci): absence of t and membership in ci.
4. (t, ci): absence of t and non-membership in ci.
where: t and ci represent a term and a category respectively. The frequencies of the four tuples in the collection are denoted by A, B, C and D respectively. The first and last tuples represent the positive dependency between t and ci, while the other two represent the negative dependency.
Information gain (IG) Information gain <NO> measures the number of bits of information obtained for category prediction by knowing the presence or absence of a term in a document. The information gain of term t and category ci is defined to be:
IG(t, ci) = Ôø?c‚àà{ci,ci}
Ôø?t‚Ä≤‚àà{t,t}
P (tÔø? c) ¬∑ log P (t Ôø? c)
P (tÔø? ¬∑ P (c)
Information gain is also known as Expected Mutual Information. The Expected Likelihood Estimation (ELE) smoothing technique was used in this paper to handle singularities when estimating those probabilities.
Chi-square (CHI) Chi-square measures the lack of independence between a term t and a category ci and can be compared to the chi-square distribution with one degree of freedom to judge extremeness <NO>. It is defined as:
œá2(t, ci) = N <NO>2
P (t)P (t)P (ci)P (ci)
where: N is the total number of documents.
Correlation coefficient (CC) Correlation coefficient of a word t with a category ci was defined by Ng et al. as <NO>
CC(t, ci) =
Ôø?N <NO>
Ôø?P (t)P (t)P (ci)P (ci)
It is a variant of the CHI metric, where CC2 = œá2. CC can be viewed as a ‚Äúone-sidedÔø?chi-square metric.
Odds ratio (OR) Odds ratio measures the odds of the word occurring in the positive class normalized by that of the negative class. The basic idea is that the distribution of features on the relevant documents is different from the distribution of features on the nonrelevant documents. It has been used by MladenicÃÅ for selecting terms in text categorization <NO>. It is defined as follows:
OR(t, ci) = log P (t|ci)<NO> <NO>P (t|ci)
Similar to IG, ELE smoothing was used when estimating those conditional probabilities.
According to the definitions, OR considers the first two dependency tuples, and IG, CHI, and CC consider all the four tuples. CC and OR are one-sided metrics, whose positive and negative values correspond to the positive and negative features respectively. On the other hand, IG and CHI are two-sided, whose values are non-negative. We can easily obtain that the sign for a one-sided metric, e.g. CC or OR, is sign(AD ‚àíBC).
A one-sided metric could be converted to its two-sided counterpart by ignoring the sign, while a two-sided metric could be converted to its one-sided counterpart by recovering the sign, e.g. CHI vs. CC.
We propose the two-sided counterpart of OR, namely ORsquare, and the one-sided counterpart of IG, namely signed IG as follows.
OR-square (ORS) and Signed IG (SIG)
ORS(t, ci) = OR 2(t, ci),
SIG(t, ci) = sign(AD ‚àíBC) ¬∑ IG(t, ci)
The overall feature selection procedure is to score each potential feature according to a particular feature selection metric, and then take the best features. Feature selection using one-sided metrics like SIG, CC, and OR pick out the terms most indicative of membership only. The basic idea behind this is the features coming from non-relevant documents are useless. They will never consider negative features unless all the positive features have already been selected. Feature selection using two-sided metrics like IG, CHI, and ORS, however, do not differentiate between the positive and negative features. They implicitly combine the two.",https://doi.org/10.1145/1007730.1007741,feature selection
Training cost-sensitive neural networks with methods addressing the class imbalance problem,"Zhi-Hua Zhou, and Xu-Ying Liu. 2006. Training cost-sensitive neural networks with methods addressing the class imbalance problem. IEEE Trans. Knowl. Data Eng. 18, 1 (2006), 63Ôø?7.","This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and softensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that costsensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training costsensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.","Suppose there areC classes and the ith class hasNi number of training examples. LetCost¬Ωi; c √∞i; c 2 f1::Cg√ûdenote the cost of misclassifying an example of the ith class to the cth class (Cost¬Ωi; i ¬º 0) and Cost¬Ωi √∞i 2 f1::Cg√û denote the cost of the ith class. Moreover, suppose the classes are ordered such that, for the ith class and the jth class, if i < j, then √∞Cost¬Ωi < Cost¬Ωj √ûor √∞Cost¬Ωi ¬º Cost¬Ωj and Ni Nj√û.Cost¬Ωi is usually derived from Cost¬Ωi; c . There are many possible rules for the derivation, among which a popular one is Cost¬Ωi ¬ºPC
c¬º1 Cost¬Ωi; c <NO>, <NO>.",https://doi.org/10.1109/TKDE.2006.17,feature selection
