Virtual Machine Scheduling for Multicores Considering Effects of Shared On-chip Last Level Cache Interference
Shin-gyu Kim,Hyeonsang Eom,Heon Y. Yeom
yeom}@dcslab.snu.ac.kr

Keywords-Virtual machine consolidation; Shared resource interference; Multicore processor
I. INTRODUCTION
Nowadays multicore processors are prevailing in PCs and servers, even in mobile devices such as smartphones and tablet computers. A 100-core processor is already introduced to market, and the number of cores in one CPU continues to grow. [1], [2] Cloud computing platforms consolidate multiple applications in a server by exploiting the increased computational capacity of multicore CPUs. [3], [4] It results in increased overall server utilization and reduced infrastructure costs. The cloud computing platforms also make use of virtualization technologies such as Xen [5], VMware [6] and Hyper-V [7] for server consolidation. These virtualization solutions provide virtual machine (VM) migration, fault isolation and resource isolation on some of the server resources.
As the cloud market grows, the cloud service providers are faced with new challenges such as reduction of power consumption and guaranteeing service level agreements (SLAs). One reason for these problems is the server consolidation policy. The consolidation implies that multiple VMs contend
978-1-4673-2154-9/12/$31.00 c©2012 IEEE
for shared resources, such as last-level cache (LLC), memory bus, network and disk bandwidth, on each compute node. It can result in performance interference among VMs and negative impact on the quality of service (QoS) of the cloud service. Effective performance isolation among co-located VMs is the key factor of successful consolidation policy of the cloud computing platforms.
Figure 1 shows performance degradation due to the colocation of two VMs on a server with Intel Core2Quad Q9550 processor. In this experiment, we use KVM [8]. Each VM runs only one SPECcpu 2006 benchmark application [9] in its entirety and is pinned to a dedicated core. Performance degradation is measured as the ratio of the increased execution time compared to its running time when there is no co-located VM. The amount of reserved memory for all VMs is the same in both cases. In Figure 1, the performance degradation ranges from 1.96% to 42.41%. Since the benchmark applications assigned to dedicated cores and are not I/O intensive, the performance degradation is due to interference in shared last level cache and memory bus. Frank et al. [10] also showed that application’s performance can be degraded significantly by other co-located applications on the same compute node. If cloud service providers can identify the VM arrangement that minimizes the performance interferences, it is possible to reduce energy
consumption by turning off redundant servers. Therefore, it is required to consider shared resource usage patterns of active virtual machines for server consolidation in the cloud computing platforms.
In order to provide performance isolation among colocated VMs, there has been a lot of research work performed on network bandwidth [11], [12] and disk I/O [13], [14]. It is more hard to ensure performance isolation between VMs regarding some kinds of resources such as LLC and memory bus. Because such resources are almost entirely managed by the hardware in a best effort fashion, system softwares and operating systems have very little control over them. Existing research work to address the performance interference due to on-chip shared resources requires modification of hardware to expose cache usage information [15], [16], [17] or changes at several layers of the software stack for cache partitioning by page-coloring. [18], [19] Recent work [20], [21] proposed performance degradation prediction methods based on workload profiling by varying pressure on the cache memory. While these methods have high prediction accuracy, they cannot be adopted directly to the public cloud service due to the prior profiling phase before execution. Public cloud service providers do not know about customers’ applications beforehand, and customers also do not want to report their work to the cloud providers.
In this work, we propose a Performance-Maximizing VM scheduler (PMV) for multicore machines by harnessing a LLC reference ratio per VM. The multicore machine that PMV deals with has multiple cores per socket and multiple sockets per machine. We don’t care about single socket server which has only one shared last level cache. PMV improves overall performance by arranging VMs to minimize the last level cache interference among them. If a VM that has large cache demand is co-located with a VM that has small cache demand, overall performance can be improved compared to a case in which two VMs that have large cache demand are co-located together. The LLC reference ratio informs the cache demand of each VM. Therefore, we can determine which VM requires more cache by comparing the LLC reference ratio of each VM. Moreover, the LLC reference ratio has a distinct characteristic that its value does not change depending on co-located workloads. Therefore, we can measure the cache demand of a VM even after it is launched. In other words, prior profiling of workloads is not necessary for our PMV. This makes a difference between PMV and other approaches.
The rest of this paper is organized as follows: Section 2 describes characterization of cache demand by LLC reference ratio. Section 3 explains our performance-maximizing VM scheduler and PMV scheduling algorithm in detail, and experimental results are presented in Section 4. Section 5 explains background and related work, and Section 6 concludes our work.

II. EFFECTS OF SHARED ON-CHIP LAST LEVEL CACHE INTERFERENCE
In this section, we describe a cache hierachy in modern multicore processors and how to characterize cache demand of a workload by LLC reference ratio.

A. Cache Hierachy in Modern Multicore Processors
The reason of cache interference is that the cache is shared among multiple cores. Most of modern multicore CPUs, such as Intel’s Core, Nehalem and AMD’s Barcelona, have the same cache hierachy that LLC is shared among several cores and other caches are private to each core (Figure 2). In Intel’s Core processor, L2 cache is the LLC (Figure 2(a)) and L3 is the LLC in Intel’s Nehalem and AMD’s Barcelona (Figure 2(b)). There is no cache interference in core-private caches when multiple VMs are co-located with the assumption that the number of VMs is not larger than the number of cores and each core is dedicated to only one VM. Cache interference among VMs occurs only in LLC, and the increased LLC miss ratio results in performance degradation of VMs. The LLC miss ratio (RLLCmiss) is defined as follows.
RLLCmiss = number of LLC misses
number of LLC references × 100 (1)
Thus, the LLC miss ratio can be a candidate for an indicator of cache demand of a VM. However, because the LLC miss ratio can be varied by other co-located VMs, the ratio should be measured while running alone. This also makes it difficult to consider temporal locality of VMs. Instead of the LLC miss ratio, we use a LLC reference ratio as the indicator of cache demand for our PMV scheduler. The LLC reference ratio (RLLCref ) is defined as follows.
RLLCref = number of LLC references
number of instructions × 100 (2)
The number of LLC references is the same as the number of misses in the last core-private cache. Because the number of misses in core-private caches is unaffected by the other co-located VMs, the number of LLC references is also unchanged. This enables to measure the LLC reference ratio at any time while a VM is co-located with other VMs. The following section explains the relation between the LLC reference ratio and the performance impact due to cache interference in detail.

B. Performance Impact and LLC Reference Ratio
[In order to identify a relationship between the LLC reference ratio and performance degradation, we conducted experiments with SPECcpu2006 benchmark 
[9]. First, execution time (Tsolo) of applications are measured when running alone. Tsolo refers the best running time of each application. Second, execution time (Tduo) of applications are measured again while two identical applications are colocated at cores that share a same LLC. In this case, each application is encapsulated by a VM, and each VM occupies one core. Tduo refers the running time when cache interference is occurred. Performance degradation ratio (Rperf ) is calculated as follows.Rperf = Tduo − TsoloTduo × 100 (3)Additionally, the LLC reference ratio is also measured. KVM is used for this experiment, and the LLC referenceratio is measured by the perf tool included in the Linux kernel release. Host OS is Ubuntu 11.10 of Linux kernel 3.0.0 and guest OS is the same. Same experiment is repeated with Intel Q9550 quad-core processor (Core architecture) and Intel Xeon E7-4807 Hexa-core processors (Nehalem architecture). Q9550 has two L2 caches (each 6 MB, total 12 MB) and E7-4807 has one L3 cache (18 MB). All VMs are configured to have 2GB of main memory, and the size of physical memory is 8 GB and 512 GB in the Q9550 and the E7-4807 machines respectively. Figure 3 shows the experimental results for the two processors. In both graphs, the performance degradation ratio grows with increasing LLC reference ratio. It means that an application with large LLC reference ratio has higher chance of performance degradation by LLC interference.We performed another experiment with the same environments. In this experiment, lbm and omnetpp of the SPECcpu 2006 benchmark suite are co-located with another applications. As shown in Figure 3, lbm and omnetpp have the highest LLC reference ratio among our test applications in each processor. Experimental results are shown in Figure 4(a) and 4(b). As shown in the graphs, the performance of lbm and omnetpp are degraded when an application with high LLC reference ratio is co-located in both processors. We performed the same experiment with sjeng that has the smallest LLC reference ratio among our test applications. Figure 4(c) and 4(d) show the results. The performance degradation ratio of sjeng is relatively small regardless of co-located applications. According to these experimental results, the best overall performance can be achieved when a VM with a large LLC reference ratio is co-located with a VM that has small LLC reference ratio. Our PMV scheduling algorithm is built upon this observation, and is introduced in the following section.
