Energy-aware Scheduling for Infrastructure Clouds
Thomas Knauth,Christof Fetzer
firstname.lastname@tu-dresden.de

978-1-4673-4510-1/12/$31.00 ©2012 IEEE
I. INTRODUCTION
Cloud computing and energy efficiency are two important trends in the IT industry at the moment. Economies of scale make cloud computing attractive to corporations and universities alike. Instead of investing in on-premise IT infrastructure, customers rent off-premise computing equipment from infrastructure providers. As multiple corporations share the same centralized infrastructure, all benefit from the reduced overhead. If the organization’s computing demands change, capital is not tied up in unnecessary IT infrastructure.
IT services are outsourced and rented at three different abstraction levels: the infrastructure, platform, or software level; referred to as IaaS, PaaS, and SaaS, respectively. In this study, we focus on Infrastructure as a Service (IaaS) “clouds”. In IaaS offerings, customers rent compute capacity in the form of virtual machines by the hour. The onus of configuring and adapting the virtual machine is on the customer. The IaaS provider only supplies the resources and assigns the virtual machine (VM) instance to a physical machine (PM).
Although centralized computing benefits from economies of scale, the rising need for compute resources leads to a rising number of data centers – in-house and centralized. An increasing fraction of the cost for running a data center is energy. As energy is getting more expensive, people started to investigate measures of reducing the energy consumption within the data center. An overall measure of efficiency for data centers is power usage effectiveness (PUE). PUE measures how much energy the non-computing equipment (e.g., power distribution and cooling) in the data center consumes. For an ideal PUE of 1, the computing equipment consumes all the power, i.e., no energy is “wasted”. A modern data center has a PUE value
of 1.1, whereas the average data center’s PUE is around 1.6 1. Out of every watt reaching the IT infrastructure, servers consume the largest share with around 60% [3], making them the single highest consuming infrastructure element. Servers are also known for their low energy efficiency at light utilization levels. A typical server consumes 50% of peak power at 0% utilization. For example, Barroso and Hölzle [1] reported on the overall low server utilization levels in Google’s data centers. To increase server utilization, the same computation is performed on fewer servers. However, factors such as workload spikes, system instability at high utilization levels, and fault tolerance place a limit on workload consolidation. If consolidation is not an option, transitioning quickly between full-power active and low-power idle states may be an alternative [6]. If idle times are too short for lowpower idle states, only full-system active low-power states will reduce the energy bill [7]. We observe, however, that servicing requests for virtual machines in IaaS clouds, suits the on/off model of low-power idle states perfectly. Because the infrastructure is exclusively used to host customer’s virtual machines, as soon as all virtual machines are off, the physical machine can transition into a low-power idle mode.
Besides energy concerns and the move to server-side computing, the growing popularity of infrastructure management software also motivates our work. OpenStack, OpenNebula, and Eucalyptus are three popular frameworks that have lowered the bar to deploy in-house private clouds significantly. Multiple departments within an organization can share the previously fragmented IT resources. The same reasoning that motivates customers to use, for example, Amazon Web Services, also applies for private infrastructure clouds.
There exists little work on how the virtual machine schedulers affects the overall system. Load balancing or first fit schedulers are the norm but they achieve poor results with respect to energy efficiency. Research from other areas of scheduling, such as scheduling operating system threads, I/O scheduling, or job scheduling, is not directly applicable to IaaS clouds. The optimization targets and constraints between these related problems and the problem considered here are indeed different. For example, scheduling operating system threads assumes that tasks can be interrupted and resumed at will. This assumption does not translate to virtual machines. Any interruption, e.g., to migrate the VM, will result in a service interruption.
1http://www.google.com/about/datacenters/inside/efficiency/power-usage. html
978-1-4673-4510-1/12/$31.00 ©2012 IEEE
To evaluate the behavior of different virtual machine schedulers we developed a workload generator together with a data center simulator. By varying parameters such as data center size and request distribution, we quantify the energy saving potential of our virtual machine scheduler. For each model parameter we draw conclusions about how it affects energy savings.

A. Summary of Contributions
In this paper, we make the following contributions: • Building on our previous initial study [5] on the use-
fulness of timed instances, we extend the evaluation to include more varied settings such at heterogeneous hosts, heterogeneous instance sizes, run time distributions, and batch requests. • We provide an extensive (re)-evaluation of the energy saving potential of timed instances. • Based on the changed simulation parameters, we adapt the original optimizing scheduler to achieve even better results in the new environment. • We share with the scientific community our simulator and workload generator. Other researchers can thus repeat and verify our results easily. The code and other artifacts can be found at http://bitbucket.org/tknauth/cloudcom2012/

II. PROBLEM
We consider the problem of scheduling virtual machine instances in data centers serving Infrastructure as a Service (IaaS) workloads. The provider must decide on which physical machine to place the virtual machine. Energy consumption is the most important factor driving the placement decision. The scheduling algorithm shall use a possibly minimal number of servers to service a given workload. The objective function is cumulative machine uptime (CMU). CMU captures the total up time of all machines in the data center. The goal is to minimize the cumulative machine uptime. By minimizing CMU, we directly reduce the data center energy consumption. Because lowly utilized servers have the worst energy efficiency, driving server utilization to 0 and switching them off is particularly effective.
One of the challenges we face while conducting this study is the lack of publicly available workloads. None of the popular infrastructure providers has released any data useful for our analysis. To overcome this challenge, we developed our own synthetic workload generator [5]. The addition of some parameters, e.g., heterogeneous VM instances, required adapting the workload generator as well as the scheduler. Where necessary, we detail these modifications.
By pursuing this work, we advocate the use of low-power idle states to increase the data center energy efficiency. Energy efficient and energy proportional data centers have received a lot of attention lately. Meisner et al. [7] argued against low-power idle states to achieve energy efficiency. While this is certainly true for the workload class they examined, called online data intensive workloads, there exist workload classes, that can benefit from low-power idle states. In fact, the
emerging popularity of on-demand infrastructure provisioning, fits the on/off semantics of low-power idle states particularly well. Because the server’s sole purpose is to host virtual machine instances, it can be switched off as soon as all virtual machine instances have been shut down.
Popular choices for infrastructure management frameworks are Eucalyptus, OpenNebula, and OpenStack. In their most recent releases 2, each framework utilizes a load balancing scheduler by default. Although the default scheduler is a safe choice, it is not optimized in any way if energy consumption is a concern. By spreading resource requests between servers, every server is minimally utilized but ineligible for shut down.

A. Simulation parameters
For every simulation, there is the question of which entities to model and which entities to leave out. We model and vary the following parameters:
1) Number of hosts: to simulate variable size data centers we varied the number of hosts. Exploring different data center sizes allows us to see whether savings are correlated to the number of machines. Data center size has an effect on the total number of handled requests as well as their inter-arrival time. According to the EPA [11], the majority of energy is consumed by small to medium sized data centers. Possibly smaller relative savings in small to medium sized data centers will thus be offset by their total number.
2) Server heterogeneity: A homogeneous server population eases management and service of the data center. However, over time, due to machine failures or novel features in next generation servers, the population will become heterogeneous. Each server in our simulation has a specific number of resources. We use resources as an abstract measure to quantify a server’s hardware capabilities. Using an abstract measure simplifies resource allocation because the scheduler must not match each resource (e.g., CPUs, RAM, and local disk) individually. Virtual servers, as rented out to customers, then have fractional sizes of the original server hardware, e.g., 1/8, 1/4, 1/2, or 1/1. Individual server resources, such as CPU, RAM, and local disk, double between virtual machine “sizes”. For example, a small instance may have one CPU, 1 GB RAM, and 100 GB local disk. The next instance size has 2 CPUs, 2 GB RAM, and 200 GB local disk. Popular providers, such as Amazon and Google, have a small, finite set of instance “sizes” following this pattern. We believe that abstractly quantifying a server’s compute power with “resources” is justified.
We assume that less powerful servers have a worse performance per watt than more powerful machines. In our context, a more powerful machine can host more VM instances simultaneously than a less powerful machine. The increased performance per watt is due to more recent and more energyefficient hardware. New server generations may even consume less peak power than previous generation servers. Energywise, it makes sense to prefer more recent, powerful machines, reserving the use of old machines for peak load.
2Eucalyptus 3.1, OpenNebula 3.6, and OpenStack Essex
978-1-4673-4510-1/12/$31.00 ©2012 IEEE
3) Instance size: As with homogeneous servers, homogeneous instance sizes make scheduling easier because a fixed number of instances will fit on each server. However, customers have diverse resource needs. As such, they want to mix and match the resource sizes to their computing demands. Low-frequency web servers will run on “small” instances, whereas compute intensive scientific applications will run on “extra large” instances. We consider 4 different instance sizes: small, normal, large, and extra large. The instance sizes consume 1, 2, 4, and 8 resources, respectively.
4) Purchase type: Amazon offers two different purchase types: on-demand and spot instances. On-demand instances are more expensive and reliable, but have a fixed price. Spot instances are, in the common case, cheaper than on-demand instances. However, because the spot instance price varies and customers specify a maximum price they are willing to pay, the provider may terminate the instance prematurely depending on how the spot price changes. In our previous work [5], we argued for a third pricing option, called timed instances. Timed instances have an a priori specified reservation time of fixed length. The scheduling algorithm uses the reservation time to co-locate instances with similar expiration times. We based our energy-aware scheduler on this purchase type to reduce the overall energy consumption.
5) Run time distribution: The instance run times follow a normal or log-normal distribution. The mean μ and shape σ parameters varied between runs. For μ we used two values: 2 and 4 hours. The generator used values of 0.5, 0.75, and 1.0 for σ. Instance run times are rounded to full hours. This is the accounting granularity used by most providers these days.
6) Batch instances: Our previous work only looked at one instance allocation per request. Some workloads, e.g., MapReduce, require the concerted instantiation of more than one instance. Each request specifies the number of instances to allocate. Instances belonging to the same request have identical resource sizes. They also have the same run time. This is true regardless of the purchase type. Even on-demand instances, where the run time is unknown to the scheduler, expire at the same time. This is a reasonable assumption given the fact that all instances cooperatively solve the same problem.

B. Workload
A synthetic sample workload is shown in Figure 1. The three plots capture the relationship between virtual machine instances, occupied resources, and occupied servers. Each of the three sub-plots covers a time frame of one day (1440 minutes). Plot (a) shows the number of virtual machine instances over time. The instance number varies over the course of the day following a typical diurnal pattern [1, 7, 9]. The peak is around noon; the trough at night. The peak-to-trough ratio is around 2.5.
The number of resources occupied by the instances is shown in Figure 1 (b). Each request is for one or more virtual machines. Each VM instance, in turn, utilizes one or more resources. This is evident by the larger y-axis range and the steeper drops and rises.
The number of utilized host machines is shown in Figure 1 (c). The shape of the curve resembles the other two. However, the shape is smoother because each server hosts multiple instances. Even if the number of instances changes, the number of utilized host machines may remain constant. The scheduler may place additional instances on already running hosts. On the other hand, if instances expire, the server may remain powered, because other instances are still using it. While the curves (a) and (b) are independent of the scheduler, curve (c) is not.

III. SCHEDULER
Before we present our energy-aware virtual machine scheduler, OptSched, we briefly present the baseline schedulers first fit and round robin. The three most popular “cloud” frameworks OpenStack, OpenNebula, and Eucalyptus use either first fit or round robin as their default scheduler. It is thus reasonable to use them for comparison. Among the two reference schedulers, first fit approximates an energy and resource conserving scheduler. The evaluation will show that OptSched is able to conserve even more resources than first fit.

A. Round Robin
Round robin assigns the first virtual machine (VM) to the first physical machine (PM), the second VM to the second
978-1-4673-4510-1/12/$31.00 ©2012 IEEE
PM, and so on. In many setups, this is the default scheduler as it load balances requests between physical machines. For example, the most recent version (Essex) of the popular OpenStack framework load balances according to each host’s available RAM 3. The popularity and wide-spread use of round robin is best explained by its ease of implementation and low complexity. If energy awareness is no concern, round robin is a reasonable default.

B. First Fit
The second reference scheduler is first fit. First fit assigns all VMs to a single PM until the PM’s resources are depleted. At this point, first fit picks another PM and continues to pack as many VMs as possible on the new host. In the absence of any run time information, first fit is already a good resource conserving strategy. We chose first fit as our second reference scheduler because it represents a more challenging baseline. Like round robin, first fit is easy to implement and requires no additional knowledge about the scheduled entities. The latter can also be seen as a drawback because more detailed knowledge may lead to better scheduling decisions.

C. Optimized scheduler (OptSched)
The scheduling algorithm implemented in Python is shown in Listing 1. Among the schedulers evaluated in the previous study [5], this performed consistently better than the others. The algorithm has knowledge about the duration of timed instances to optimize the virtual to physical machine assignment. We detail the algorithm’s high level ideas here, referencing to corresponding lines in the code: (1) Timed instances and on-demand instances occupy non-intersecting sets of hosts. Each host only runs either timed or on-demand instances. This is evident in the algorithm by the variables ondemand hosts and timed hosts (Line 5 and Line 13). OptSched derives its benefits from knowing the run time of (a subset of) VM instances. Mixing instances with known and unknown run times onto the same hosts potentially cancels this advantage. (2) If multiple generations of servers are present (i.e., powerful and less powerful hosts), the scheduler uses the powerful machines first. The code shown in Listing 1 is part of a loop over all capacity classes. The hosts belonging to the current capacity class are defined in the first line. Eligible hosts are filter by intersecting with the hosts in the current capacity class (Line 4 and 13). Only if an instance does not fit within a capacity class anymore, are the less powerful hosts considered. (3) OptSched co-locates timed instances with similar expiration times. For each host (Line 13), the algorithm calculates the expiration time delta between the instance to be scheduled and the instance with the longest remaining run time (Line 19-21). OptSched schedules the new instance on the host with the smallest delta (Line 39-41). If the new instance’s run time is longer than the remaining run time of all running instances, the new instance will be scheduled on the host where it extends the host’s up time minimally (Line 42-46).
3http://docs.openstack.org/essex/openstack-compute/admin/content/ch scheduling.html
If the already running servers have insufficient resources, a previously unoccupied machine is used (Line 30-38).
Listing 1. Optimized scheduler 1 c c h o s t s = s e t ( r a n g e ( m i n hos t i d , max hos t id ) ) 2 i f vm . p u r c h a s e t y p e == ON DEMAND : 3 # & o p e r a t o r i s s e t i n t e r s e c t i o n 4 f o r i in c c h o s t s & 5 ( s e l f . ondemand hos ts | 6 s e l f . e m p t y h o s t s ) : 7 i f v m s l o t s [ i ] . f r e e r e s o u r c e s >= vm . s i z e : 8 v m s l o t s [ i ] . add (vm ) 9 vm . h o s t i d = i 10 break 11 e l s e : 12 # & o p e r a t o r i s s e t i n t e r s e c t i o n 13 f o r i in c c h o s t s & t i m e d h o s t s : 14 i f vm . s i z e == m a x h o s t c a p a c i t y : 15 break 16 i f h o s t . f r e e r e s o u r c e s < vm . s i z e : 17 c o n t i n u e 18 19 max elem = max ( v m s l o t s [ i ] . vms , 20 key=lambda x : x . f i n i s h m i n ) 21 d e l t a = max elem . f i n i s h m i n − vm . f i n i s h m i n 22 23 i f d e l t a >= 0 and d e l t a < m i n d e l t a : 24 m i n d e l t a = d e l t a 25 m i n d e l t a i n d e x = i 26 e l i f d e l t a < 0 and d e l t a > max de l t a : 27 max de l t a = d e l t a 28 m a x d e l t a i n d e x = i 29 30 i f m a x d e l t a i n d e x == None and 31 m i n d e l t a i n d e x == None : 32 # A l l PMs e i t h e r empty or f u l l . 33 # Pick f i r s t empty PM 34 f o r i in c c h o s t s & s e l f . e m p t y h o s t s : 35 i f v m s l o t s [ i ] . c a p a c i t y >= vm . s i z e : 36 v m s l o t s [ i ] . add (vm ) 37 vm . h o s t i d = i 38 break 39 e l i f m i n d e l t a i n d e x != None : 40 v m s l o t s [ m i n d e l t a i n d e x ] . add (vm ) 41 vm . h o s t i d = m i n d e l t a i n d e x 42 e l s e : 43 # I n s t a n c e run t i m e l o n g e r tha n any o t h e r 44 # i n s t a n c e ’ s run t i m e . 45 v m s l o t s [ m a x d e l t a i n d e x ] . add (vm ) 46 vm . h o s t i d = m a x d e l t a i n d e x 47 48 m i n h o s t i d += n r h o s t s

IV. EVALUATION
Our previous work [5] evaluated the overall viability of timed instances. We compared multiple algorithms for incorporating virtual machine lease time information into the scheduling decision. The goal of each algorithm was to reduce the cumulative machine uptime (CMU). Compared to round robin, OptSched reduces the CMU by 14.7% to 44.7%. OptSched also achieves a reduction of 3.3% to 16.7% when compared to first fit. This evaluation focuses on the following previously unanswered questions:
• How does the run time distribution’s shape parameter σ influence CMU reductions? • What is the effect of heterogeneous hosts and instances? • How do multiple purchase options affect the scheduler? • Do batch requests increase or decrease savings?
978-1-4673-4510-1/12/$31.00 ©2012 IEEE
To answer these questions we performed 100 runs for each of the 576 parameter combinations. When talking about savings we refer to the average reduction in cumulative machine uptime achieved by OptSched over first fit. Each simulation run covers three days. The change in cumulative machine uptime is only calculated for the last two days. The first day is for the system to stabilize.
A. Impact of sigma
Previously, we used only a single value for the run time distribution’s shape parameter σ. The shape parameter determines the likelihood of events far away from the distribution’s mean. If sigma is small, events cluster around the distribution’s mean. Conversely, events further away from the mean are more likely if sigma is large.
We looked at sigma values of 0.5, 0.75, and 1.0. In all but 7 cases (out of 244) did a higher sigma increase the effectiveness of OptSched. The reduction in CMU stayed constant in the other 7 cases. Changing sigma from 0.5 to 1.0, CMU increases by an average of 1.9 percentage points, with a maximum increase of 10.5 percentage points. The highest increase in percentage points is for scenarios that already have high CMU reductions. For example, with a single instance size, only timed instances, a log-normal run time distribution, 240 min mean run time, and 8 instances per host, the savings increase from 10.7% to 21.2%. Conversely, savings were unaffected by a change in sigma for an already difficult optimization scenario: multiple host and instance sizes, multiple purchase types, and a normal run time distribution with mean of 120 min.
Diverse run times improve the efficacy of OptSched. Because the entropy in run times is higher, the additional knowledge is more advantageous. If the majority of instances have identical run times, knowing the run time is less valuable. The flatter the run time distribution, the more savings in CMU and energy OptSched will achieve.

B. Heterogeneous hosts
We contrast a homogeneous infrastructure, with a 25/75 split between brawny and wimpy nodes. Brawny nodes are twice as powerful as wimpy nodes. The make up of Google’s infrastructure provides a third configuration. Table I summarizes the number of hosts as well as their normalized processor and memory resources for an exemplary Google cluster. The majority of machines (53%) belong to a single configuration class with 0.5 normalized CPU and RAM. If we add to this class the machines with 0.5/0.25 and 0.5/0.75 normalized CPU/RAM configurations 4, the class covers 92% of all machines. 6% of the machines have a 1.0/1.0 CPU/RAM configurations. All other configuration types make up one percent or less. Even though the data center has heterogeneous servers, the number of different configurations is small (less than ten in the Google example).
We denote used host configurations with {(hosts, size)}: (1) {(526, 8)}, (2) {(460, 8), (32, 16), (5, 4)},
4 Our model does not capture individual resources such as CPU and RAM.
(3) {(263, 8), (526, 4)}. For example, the third configuration has 263 servers with resource size 8 and 526 servers with resource size 4. The total number of resources for each configuration is as close to identical as possible.
After introducing heterogeneous hosts into our simulation, OptSched performed consistently worse than first fit. Whether OptSched would perform better or worse than first fit, depended on the order in which servers were filled. Utilizing wimpy machines first, OptSched performed better than first fit. If brawny machines were used first, first fit had a lower CMU than OptSched. Clearly, the order in which machines are filled should have no impact on the results. Also, brawny machines should be preferred over wimpy machines because of their more favorable energy-performance characteristics. The overall energy consumption will be worse if wimpy machines are used first, even though the relative savings may be high.
Figure 2 illustrates the behavior. Each plot shows the delta in used hosts for the first fit scheduler and OptSched. For example, −40 means OptSched used 40 hosts less than first fit. A positive value means that first fit is better than OptSched. Sub-plot (a) of Figure 2 shows the behavior for filling brawny machines first. Even though OptSched uses fewer hosts for the period t = 2200 until t = 2600, first fit uses fewer hosts during the rest of the time. Overall, first fit has a lower CMU than OptSched in this example.
The effect of filling wimpy nodes first is illustrated by Figure 2 (b). The shape of the curve is completely different, even though the workload is the same. At no point in time does first fit occupy fewer hosts than OptSched.
Up to this point, the scheduler was unaware of the distinction between “brawny” and “wimpy” servers. This episode showed, that a successful energy-aware scheduler has to be aware of different host capacities. We modified the scheduler to partition the servers by capacity. The algorithm optimizes the assignment only within a partition, beginning with the partition consisting of highest capacity machines. Only if there are no more resources left in the current partition, does OptSched assign instances to partitions consisting of lower capacity servers. Figure 2 (c) shows the effectiveness of this change. The capacity-aware scheduler utilizes a similar number of hosts as first fit until t = 2300. For the period from t = 2300 until t = 2700 the benefit of OptSched is evident.
Comparing a homogeneous data center of 526 hosts with 8 resources (configuration (1)) with a data center where the majority of machines are identical (configuration (2)) reveals the following: in 69 out of 192 cases did the savings
978-1-4673-4510-1/12/$31.00 ©2012 IEEE
of OptSched decrease. Savings are unaffected in 85 cases. Savings actually increased slightly in 38 cases. The average change is a slight decrease of 0.1 percentage points. We recorded a maximum decrease of 1.9 percentage points. If we compare the homogeneous configuration (1) with the 25/75 split configuration (3), we observe that relative CMU savings decrease in 167 cases. In one cases, there was no change, and in 24 cases did the savings increase. The average decrease is 2.1 percentage points, with a maximum of 11.6. Parameter combinations that were favorable to OptSched before introducing heterogeneous hosts, had the highest absolute decrease in savings. For example, the configuration with a maximum decrease of 11.6 percentage points was: no batches, single host and instance size, only timed instances, sigma of 1.0, and a log-normal run time distribution with a mean of 120 min.
In the common case, OptSched achieves higher reductions in CMU and energy consumption if the host population is homogeneous. Because optimization is only performed within a capacity class, optimization potential is lost if multiple capacity classes exist. Skewed host populations, like in configuration (2), are less of a problem than 50/50 splits resourcewise. The more homogeneous the better.

C. Heterogeneous instances
Considering only a single instance size simplifies the problem of resource allocation. Each host accommodates a fixed number of virtual machines depending on its capacity. If virtual machines differ in size, resource allocations of already running instances must be considered. With multiple resource sizes the problem of fragmentation arises. Even though the system’s total resources may be sufficient to host an instance, the resources may be scattered across multiple hosts. Individually, no single server has enough resources to host the instance.
The simulated instances occupy 1, 2, 4, or 8 resources. We expect savings to decrease, because the scheduling algorithm has fewer hosts to chose from. Considering different resource sizes means the scheduler has to apply an additional filter, before ranking hosts. All hosts with insufficient available resources for the current instance are discarded. Hosts with
sufficient resources are ranked according to their expiration time difference.
Having multiple instance sizes decreases the overall number of instances in the system. This is a result of two factors: (a) fragmentation lead us to increase the inter-arrival time by a factor equivalent to the mean resource size. For example, using resource sizes of 1, 2, 4, and 8, and assuming a uniform resource size distribution the mean resource size is
1 + 2 + 4 + 8
4 = 3.75 (1)
Factoring in resource fragmentation, we adapt the original inter-arrival distribution’s λ as follows:
λmod = λ
avg resource size ∗ 1.5 (2) We call 1.5 the fragmentation factor. The fragmentation factor was determined experimentally by observing the number of rejected instances. If no host has sufficient resources to accommodate an instance, the instance is rejected. The simulator records the number of rejected instances.
Our baseline is a single instance size. Each instance occupies one resource. We compare this to instance size mixes of {1, 2}, {1, 2, 4}, and {1, 2, 4, 8}. Instance sizes are assigned randomly with equal probability.
For each instance size mix, there are 144 possible combinations of other parameters. Moving to four different instance sizes, increased CMU savings in 12/144, and decreased them in 132/144 cases. The mean and maximum increase are 1.2 and 1.9 percentage points. More pronounced, however, is the decrease in savings with an average and maximum drop of 4.6 and 17.3 percentage points, respectively. The maximum decrease is, again, from a setting favorable to OptSched: no batches, homogeneous instances and hosts, only timed instances, sigma of 1.0, and a log-normal run time distribution with a mean of 120 min. Reductions happen more often and include more percentage points for the other instance size mixes as well. An environment consisting of a single instance size is better for OptSched, as the realized savings are generally higher. Resource fragmentation counteracts OptSched’s attempts to reduce the total number of hosts.
978-1-4673-4510-1/12/$31.00 ©2012 IEEE

D. Multiple purchase types
Intuitively, savings should decrease if the workload contains multiple purchase options. After all, we can only optimize the assignment of instances if the instance run time is known. For on-demand instances, the scheduler does not have this knowledge.
Two scenarios are compared: only timed instances with a 50/50 mix of on-demand and timed instances. Across the entire parameter space (288 combinations), savings decrease with multiple purchase types. Because of the equal split between on-demand and timed instances, we expect savings to decrease by 50%. The simulations confirm our expectation: on-demand instances reduce CMU savings by 3.9 percentage points on average. For some settings, the savings decrease to 0, i.e., OptSched does not have any effect. For example, CMU reductions of 1.8% dropped to 0% for no-batches, 25/75 host split, four instance sizes, only timed instances, sigma 0.5, and normal run time distribution of 240 min.
The higher the fraction of on-demand instances is in the total workload, the less beneficial OptSched will be. Savings tend toward 0, as the fraction of on-demand instances tends toward 100%.

E. Batch requests
When allocating resources on-demand, customers do not only request a single instance at a time. For example, when using map-reduce-style computations, the workload is split across multiple (virtual) machines. To simulate this behavior we include batch requests. With batch requests enabled, a certain percentage of requests allocate multiple instances. Batch instances are identical in their configuration, i.e., each occupies the same number of resources. The number of instances per batch, the batch size, is uniformly distributed between 1 and 10.
With only one instance per request the scheduler treats each instance independently. However, with batch instances, the scheduler must assign multiple instances at once. A viable solution is to treat every instance of a batch requests independently. This requires no change to the scheduler, but sacrifices optimization potential. Irrespective of the purchase type (ondemand, spot, or timed) knowing that certain instances belong to the same request may be beneficial. The probability of batch instances to finish at the same time is higher than for instances from independent requests. It makes intuitive sense to co-locate batch instances.
We consider the following strategy to handle batch requests: completely fill as many unoccupied hosts as possible with batch instances. Only the “remainder”, i.e., instances that do not fill up a host, are scheduled using the algorithm outlined in Listing 1. In case of multiple instance and host sizes this strategy works best, if multiples of each instance size fill each host size with no left over resources. Stated differently, each instance size evenly divides every host size. Otherwise, the introduced fragmentation may cancel the savings.
We adapted both our optimizing scheduler and first fit with the batch-awareness logic. Although first fit behaves similar
to the batch-awareness modification, it still benefits from batch-awareness. By making first fit batch-aware, the CMU was reduced by 2.9 percentage points on average compared to a batch-unaware first fit. OptSched benefits from batchawareness too. On average, CMU reductions are 1.8 percentage points higher. Averaged over all outcomes, batch requests reduce CMU savings by 1.0 percentage points. 104/288 increases, 173/288 decreases, and no change in 11/288 cases. The highest decrease in absolute terms was by 10.9 percentage points, from 21.4% to 10.5%. Decreases are more frequent and on average more pronounced: on average 2.4 percentage points are lost. An increase on average means a rise by 1.3 percentage points.

F. Summary
Table II gives an overview of the results. For each parameter the average change (positive or negative) in percentage points of CMU savings is shown. As an example take heterogeneous hosts: going from a homogeneous server landscape to a 25/75 split between brawny/wimpy servers reduces the CMU savings by 4.4 percentage points on average. The table can be used as a reference to quickly assess how a change in one of the parameters will affect the cumulative machine uptime. Averaged over all parameter combinations, OptSched reduces the CMU compared to round robin by 26.5% on average; in favorable settings by as much as 60.1%. On average 4.9% are saved over first fit, sometimes as much as 21.4%.

V. RELATED WORK
We briefly touch related work in two adjacent areas of research:

A. Energy-efficient computing
Research into energy efficient servers have focused on two aspects: low-power idle vs. low-power active states. Proponents of low-power idle states argue for rapidly transitioning between active and idle states in order to save energy [2, 7]. Idle states consume significantly less power than the active states. However, depending on the actual workload, idle times may be too few or short to be useful. In these cases, only low-power active states can increase the energy efficiency [7].

B. Scheduling
Scheduling is an important aspect of many computing systems and at many abstraction levels. Operating systems schedule threads onto multi-core CPUs. One important underlying assumption in the real-time multi-core scheduling
978-1-4673-4510-1/12/$31.00 ©2012 IEEE
community concerns zero-cost migrations. Threads can be migrated between cores for free. Also, processes are assumed to be interruptible. Both assumptions do not apply to virtual machine scheduling. Though migrations are possible, they incur non-negligible bandwidth and processing costs, along with multi-second instance unavailability [4]. Further, virtual machines are interruptible (e.g., offline migrations use this feature), but the interruption is visible to the customer.
Batch scheduling systems like Condor and LSF must map a set of jobs onto a heterogeneous set of resources. It is common to assume that jobs can be stopped, moved, and restarted at will in this environment. Infrastructure clouds do not have this freedom, because workloads consist of interactive services. Migrating virtual machine instances between physical servers may result in unacceptable service unavailability. Though explicitly allowing to preempt certain virtual machines for the sake of energy efficiency may be an option [10].
Third, data processing frameworks, such as MapReduce, also have a scheduling component. The MapReduce scheduler assigns jobs to compute nodes in a fashion that optimizes for locality. Because cross-rack bandwidth is limited in a data center, the focus is on minimizing data transfer. In infrastructure clouds, data access patterns are unknown at the time of scheduling, precluding locality-aware placement. Depending on how stable storage is attached (e.g., directly vs. networked), optimizing for locality [8] may be superfluous.

VI. CONCLUSION
We presented our energy-aware scheduler for infrastructure clouds called OptSched. OptSched reduces the cumulative machine uptime by up to 60.1% compared to round robin, and 16.7% for first fit. OptSched utilizes the reservation length of timed instances to optimize the virtual to physical machine mapping. For each of the simulation parameters, we have shown how expected energy savings change. The parameters covered in this study are data center and virtual machine heterogeneity, the long-tailedness of run time distributions, sensitivity to batch requests, and mixed purchase types. We can conclude that the environment where OptSched yields the most savings consists of homogeneous instance sizes and hosts, has no batch instances, and only offers a single purchase type (i.e., timed instances). Even though CMU savings are reduced by heterogeneity, batching, and mixed purchase types, OptSched was still able to reduce CMU.
We conclude with a back-of-the-envelope calculation to motivate why even savings of a few percent are worthwhile: According to the U.S. Environmental Protection Agency [11], servers in mid-sized data centers in the US alone consumed a total of 3.7 billion (1012) kWh. Reducing this energy consumption by a modest 5% already saves thousands of millions of dollars 5 let alone carbon emissions. Of course, not all of the data centers serve as infrastructure “clouds”. However, even for a single data center, the potential dollar savings are on the order of $100,000 per year.
5At $0.08/kWh 5% of 3.7 ∗ 1012 ∗ 0.08 is $14,800,000,000

ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers for their comments, as well as Björn Döbel and Christoph Seidl for feedback on early drafts of this paper. This research was funded as part of the ParaDIME project supported by the European Commission under the Seventh Framework Program (FP7) with grant agreement number 318693.

References
[1]L. Barroso,U. HölzleThe Case for Energyproportional ComputingComputer, vol. 40, no. 12, pp. 33–37, 2007.2007
[2]A. Gandhi,M. Harchol-Balter,M.A. KozuchThe case for sleep states in serversHotPower Workshop, 2011.2011
[3]J. HamiltonCooperative Expendable Micro-Slice Servers (CEMS): Low Cost, Low Power Servers for Internet-Scale ServicesConference on Innovative Data Systems Research. Citeseer, 2009.2009
[4]F. Hermenier,X. Lorca,J. Menaud,G. Muller,J. LawallEntropy: a Consolidation Manager for ClustersInternational Conference on Virtual Execution Environments. ACM, 2009, pp. 41–50.2009
[5]T. Knauth,C. FetzerSpot-on for timed instances: striking a balance between spot and on-demand instancesInternational Conference on Cloud and Green Computing. IEEE, 2012. [Online]. Available: http://se.inf.tu-dresden.de/pubs/ papers/knauth2012spot.pdf2012
[6]D. Meisner,B. Gold,T. WenischPowernap: eliminating server idle powerACM SIGPLAN Notices, vol. 44, no. 3, pp. 205–216, 2009.2009
[7]D. Meisner,C. Sadler,L. Barroso,W. Weber,T. WenischPower management of online data-intensive servicesInternational Symposium on Computer Architecture, 2011.2011
[8]J. Park,D. Lee,B. Kim,J. Huh,S. MaengLocality-aware Dynamic VM Reconfiguration on MapReduce CloudsInternational Symposium on High-Performance Parallel and Distributed Computing. ACM, 2012, pp. 27–36.2012
[9]C. Reiss,A. Tumanov,G.R. Ganger,R.H. Katz,M.A. KozuchTowards understanding heterogeneous clouds at scale: Google trace analysisIntel Science and Technology Center for Cloud Computing, Tech. Rep., 2012. [Online]. Available: http://www.pdl.cmu.edu/ PDL-FTP/CloudComputing/ISTC-CC-TR-12-101.pdf2012
[10]M. Salehi,P. Krishna,K. Deepak,R. BuyyaPreemption-Aware Energy Management in Virtualized Data CentersInternational Conference on Cloud Computing. IEEE, 2012, pp. 844–851.2012
