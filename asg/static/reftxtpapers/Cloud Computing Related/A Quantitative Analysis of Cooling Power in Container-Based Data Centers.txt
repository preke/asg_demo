A Quantitative Analysis of Cooling Power in Container-Based Data Centers
Amer Qouneh,Chao Li,Tao Li
chaol}@ufl.edu,,taoli@ece.ufl.edu

In this paper, we provide comparative and quantitative analysis of cooling power in both container-based and raised-floor data centers. Our results show that a container achieves 80% and 42% savings in cooling and facility powers respectively compared to a raised-floor data center and that savings of 41% in cooling power are possible when workloads are consolidated onto the least number of containers. We also show that cooling optimizations are not very effective at high utilizations; and that a raised-floor data center can approach the efficiency of a container at low utilizations when employing a simple cooling optimization.

1. Introduction
The explosive growth of large-scale Internet services coupled with the rising popularity of cloud computing [1] have affirmed the role of data centers as the imperative IT infrastructure for supporting those services. It is expected that data centers in the US will consume 100 billion kWh by the year 2011 at a cost of $7.4 billion per year [2]. Of that, cooling costs represent 30-50% of the total energy overhead [1]. It is estimated that 0.5-1 W of cooling power is required to remove 1 W of heat. Due to the high costs of cooling, designers have focused their attention on optimizing cooling power [3, 4, 5, 6].
Due to their low cost, modular design, and relatively short time between construction and deployment, container-based data centers [7, 8, 9, 10] are emerging as an attractive solution to many data center problems. They pose fewer requirements to the hosting facility and are weatherized to withstand various climates. Moreover, upfront capital expenditure is significantly reduced since
additional modules can be added as need arises. Once the containers are plugged in to the power and cooling grids, they are ready for production. Some containers do come with their own infrastructure for power and cooling in a separate container. In 2005, Google deployed its first container-based data center [11] comprising 45 containers. Driven by its cloud computing initiative, Microsoft commissioned its own 225 container-based data center in Chicago [12]. Various hardware vendors have also introduced their own versions of containers like HP’s Performance-Optimized Data Center (POD) [8], Sun Microsystems’ Blackbox [9], SGI’s ICE Cube [10], and Dell’s Modular Data Center [13]. Although containers can help alleviate the cooling cost of data centers, a detailed quantitative analysis of this cost and of the relationship of cooling power to workload variability is still lacking
In this paper, we analyze the major issues that affect cooling in data centers in order to best utilize available cooling power, and to help IT staff make informed decisions about acquiring or using containers in data centers. In particular, we compare containerized data centers to raised-floor data centers, and we make the following contributions:
• Analysis of temperature profiles for containers. We show that containers can safely use higher inlet temperatures and thus expend less cooling power.
• Characterization of power efficiency. Our results show that higher efficiencies can be attained when containers are run at higher utilizations and higher temperatures. Moreover, we demonstrate that containers that minimize idle power and employ aggressive adaptive cooling schemes can save 38% and approach the upper bound of cooling savings limited only by the characteristics of the cooling system.
• Impact of virtualization and consolidation on cooling power. Although virtualization and consolidation have been amply studied, their effect on cooling power has not been quantified. We answer questions like how much cooling power is saved when consolidating workloads.
At low server loadings, we show that containers employing adaptive cooling schemes result in 80% and 42% savings in cooling and facility powers respectively compared to raised-floor data centers. Our results show
978-1-4577-2064-2/11/$26.00 ©2011 IEEE 61
that maximum efficiency is achieved when target design power matches actual expended power. We also highlight one limitation of the efficiency metric when it declares consolidation is degradation.
• Multiple-container data centers. By viewing the container as a building block, we demonstrate that for servers with low utilization, concentrating workloads and high-density racks in the least number of containers is 81% and 44% more efficient than raised-floor data centers in terms of cooling and facility powers. While for servers with high utilization, multiple containers outperform raised-floor data centers by 46% and 13% respectively. We recommend consolidating workloads onto the least number of containers and summon a new container to service only if full capacity is reached.
• Response time. We demonstrate that response time is maintained when consolidation is implemented and that savings up to 60% in cooling power are possible in containers compared to raised-floor data centers.
The rest of this paper is organized as follows: Section 2 discusses cooling mechanisms in data centers. Section 3 presents our experimental setup. We discuss rack inlet temperature, power efficiency, and impact of virtualization on data centers in Section 4. Section 5 introduces multiple-container-based data centers. Section 6 discusses response time. We present related work in Section 7 and our conclusions in Section 8.

2. Background
The efficiency of the cooling architecture greatly affects the recurring costs of data centers. In this section we describe the operation and characteristics of both raised-floor and container-based data centers. We also provide background on calculating the cooling power.

2.1. Data center layout: An overview
Data centers are limited by power and cooling capacity. However, server power density has been increasing at a potentially unsustainable rate demanding matched power from the existing cooling system. Inadequate cooling power and poor airflow result in hot spots forming in front of the servers causing their inlet temperatures to rise and exceed the specifications. Hot spots may form despite adequate cooling power and
airflow due to tile flow rate and pressure imbalances above the floor [4, 14]. Air-bypass is another inefficiency where cold air flowing out of air conditioning units is not ingested by the servers and is returned back to the units without doing any useful work. These inefficiencies translate into significant loss of valuable energy and money. 2.1.1. Raised-floor data centers. In data centers, server racks are organized as rows within the hosting facility. The floor is raised 2-4 ft above the hard floor forming a plenum to distribute cold air to the servers. Racks are arranged in hot-aisle/cold-aisle arrangement to avoid mixing of hot and cold air in the room, as shown in Figure 1(a). Computer Room Air Conditioning (CRAC) units on the perimeter of the data center room blow cold air into the plenum which flows out into a cold-aisle via perforated tiles in the raised-floor in front of the racks. Cold air is ingested by the servers and expelled as hot air to be cooled again by the CRAC units.
Data center control techniques range from a simple sensor placed at the intake of CRAC units to sensor networks for colleting temperatures at server inlets. CRAC fans are usually operated at full speed unless equipped with variable frequency drives (i.e. variablespeed fans) [4] for more efficient operation. A condition known as re-circulation occurs when hot exhaust air is pulled back into the inlet of a server forming a hot spot. To prevent re-circulation and air-bypass, aisle containment is sometimes used. Although aisle containment comes at an additional cost, it does reduce mixing and thus alleviates a significant source of inefficiency.
2.1.2. Container-based data centers. To minimize the drawbacks of raised-floor data centers, container-based data centers rely on containment, close-coupled cooling (bringing CRACs close to racks), and modularity. Barriers or racks are used to prevent intra-container air recirculation. A shipping container is weather resistant and either 20 or 40 ft long. Commercially available containers [8, 9, 10] possess different architectures, however, they all share the same principles. We describe three popular container architectures.
Overhead Cooling Container: A single row of racks divides the container into two parts, one forms a cold aisle and the other forms a hot aisle, as shown in Figure 1(b). CRAC units installed on top of the racks supply the cold air. This type of cooling architecture has been adopted by HP’s POD [8].
In-row Cooling Container: Racks are stacked on both sides along the length of the container creating a “cold” middle aisle and two “hot” narrow aisles behind the rows, as shown in Figure 1(c). CRAC units interspersed among the racks supply the cold air. ICE Cube modular data centers by SGI [10] are examples of in-row cooling containers.
Circular In-row Cooling Container: As shown in Figure 1(d), racks are stacked on both sides along the length of the container. However, the racks are stacked front to back rather than side-to-side with a CRAC unit sandwiched in between. Air flows in a circular fashion and hence we called it circular in-row cooling architecture. This cooling architecture has been adopted by Sun’s modular data centers [9].

2.2. Data center cooling mechanism
The typical infrastructure of a data center consists of IT equipment, power distribution and backup systems, and a cooling system. Chillers and CRAC units respectively consume 33% and 9% of the total energy overhead in a data center, while servers consume another 30% [1]. The total power consumed by a data center can be expressed as: total power = server power + cooling power + power distribution losses. The cooling power is given by: cooling power = fan power + cooling load/COP, where COP is the Coefficient of Performance. COP is an efficiency metric associated with CRAC units and defined as the ratio of heat removed to the amount of work required to remove the heat. The cooling load is the sum of powers dissipated in the data center room. A widely used metric to describe the efficiency of a data center is the Power Usage Effectiveness (PUE), which is defined as: total facility power / server power [15].
Sufficient airflow to server inlets is crucial to ensure a safe operating temperature, as recommended by the manufacturer. Inadequate airflow leads to redlining, giving rise to reliability issues and increased server failure rate. On the other hand, over-provisioning wastes a significant amount of energy. For maximum efficiency, the airflow supply should equal the server demand.

3. Experimental setup
In this study, we use a commercial data center airflow and cooling power simulator, CoolSim [16], to characterize the three containers described in Section 2. The CoolSim toolset is built on top of the ANSYS/Fluent computational fluid dynamics (CFD) airflow modeling technology that is well proven in the industry for modeling heat and air flows. The GUI environment of the framework provides the capability of model building and specification down to the server level. We use CoolSim’s Rack Builder Tool (RBT) to configure the utilization levels for servers, volumetric airflow, and the appropriate temperature drop across the servers. We modeled the containers based on commercially published specifications such that they all have equal server count and power consumption to establish a common baseline for comparison.
In our study, each container consists of 300 servers of size 2U arranged in racks similar to those used by commercial containers. Our servers are modeled after the 2U Dell 2850 [17] servers, with 200 W idle power and 524 W at 100% utilization. We use a linear power model to determine server power at a given utilization level. For example, at 20% utilization, server power is 200 + 0.2×(524-200) = 264.8 W. It has been shown that this linear model is within 1% of the actual dynamic power usage and that more elaborate models are not much better [18]. Since we are interested in steady state cooling characteristics, we simulate varying load situations by sweeping the servers from 10% to 100% utilization. We also use a real trace from an Internet-based data center to evaluate the setup. Fans are assumed to consume low power (~2 W) and run at full speed at all utilization levels. CRAC units were modeled to supply cold air at specified temperatures throughout the experiments. A model that describes the COP of a water-chilled CRAC unit at the HP Utility Data Center is given by: COP = 0.0068T2 + 0.0008T + 0.458, where T is the temperature of the supplied cold air [6]. Because manufacturers do not publish their cooling models, we use the above COP model to represent the cooling systems for both raisedfloor and container-based data centers.
We define a cooling optimization as one that responds to dynamic changes in utilization by adjusting the temperature and/or airflow rate; we call it adaptive cooling scheme. For non-adaptive cooling, the airflow supply was set to support the nameplate power dissipation
of servers. Generally, airflow rate is determined by the heat rate and temperature rise across it. In our experiments, we used two progressively aggressive adaptive cooling schemes: adaptive supply temperature and adaptive supply temperature combined with adaptive airflow rate. For adaptive supply temperature cooling, airflow was supplied to support 100% utilization while the supply temperature was varied. For adaptive airflow rate cooling, the airflow demand for each server was determined based on the actual server utilization. For maximum energy efficiency, the CRAC supply temperature was set at 90 F as suggested by container manufacturers and in accordance with the ASHRAE recommendations [19]. Table 1 shows the data center configurations and Table 2 summarizes the power characteristics of the modeled data centers.

4. Characterization of container-based data centers
Although extensive studies have been performed on conventional data centers, a detailed characterization of temperature profile, cooling power, and efficiency of container-based data centers is not available to the computer architecture research community. In this study, we investigate these issues and provide a detailed, comparative analysis of power profiles and Power Usage Effectiveness (PUE) between conventional raised-floor and container-based data centers. We also evaluate the impact of virtualization, adaptive cooling, and idle power on the operation of both types of data centers.

4.1. Rack inlet temperature
Operating the servers at temperatures outside their specified range leads to failure or to unnecessary increase in cooling costs. To address this issue, the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) recommends maximum “allowable” and “recommended” server inlet temperatures of 90 F and 80.6 F [19] respectively in order to achieve maximum efficiency. Although manufacturers’ specifications indicate that servers can operate at the maximum allowable temperature of 90 F, conventional raised-floor data centers have been operating at well below 80 F due to re-circulation and air bypass resulting in higher cooling cost and lower overall efficiency. It should be noted that blindly raising the operating temperature may actually increase the power requirement of fans and thus offset any gains [5]. For maximum efficiency, we simulated our servers to run at the maximum allowable inlet temperature of 90 F. The mean and standard deviation for inlet temperatures are shown in Figure 2. Server inlet temperatures for containers are uniform and follow the supply temperature very closely. In contrast, the inlet temperatures for the raised-floor data center are not uniform and vary depending on the location of the servers. Servers located at the edges of a row and at the top of racks are the most vulnerable to re-circulation
and hot spot formation. Due to re-circulation, it is difficult to operate a raised-floor data center at 90 F. At 60% utilization, only 70 servers out of 300 have safe inlet temperatures. The rest of the servers exceed the maximum allowable temperature of 90 F and run the risk of degradation and ultimately failure.
To eliminate hot spots, we conservatively assumed that all servers are fully utilized and then reduced the operating temperature (78 F in this case) until all servers had safe inlet temperatures. However, this is inefficient since the CRAC units are exerting more effort than necessary in order to mitigate the effects of re-circulation. In addition, the cooling is over-provisioned for utilization levels less than 100%. A more efficient method is to find an operating temperature such that the cooling power matches the heat dissipation.
Figure 2 shows the mean and standard deviation for inlet temperatures after reducing the raised-floor supply temperature to 78 F. Now, all servers have inlet temperatures below 90 F. Re-circulation still has an effect, but because the air is colder inlet temperatures remain at safe levels. Figure 3(a) shows how inlet temperatures follow the supply temperature for containerbased and raised-floor data centers across different utilization levels. Each of the curves represents the average of twenty of the hottest server inlet temperatures for each configuration.
For the raised-floor data center, the temperatures plotted are those of the servers near the top of the rack at row-ends that experience maximum variation. At these locations, re-circulation is more likely to occur, causing inlet temperatures to rise considerably. Since there is no recirculation in containers, rack and server positions are irrelevant; all servers have the same inlet temperature regardless of the cooling architectures, and thus all three containers have the same curve at all utilization levels. As indicated in the figure, inlet temperatures for the raisedfloor data center increase in response to the CRAC supply temperature although at a slightly higher value due to recirculation. The container curve expresses a closer relationship to the CRAC supply temperature due to containment and close coupling. The results suggest that containers are more efficient than raised-floor data centers because they allow higher inlet temperatures and thus less cooling power.
Figure 3(b) shows a psychrometric chart indicating the allowable and recommended envelopes for servers
operating in data centers as recommended by ASHRAE. A psychrometric chart is a graphic representation of the conditions of air and includes temperature, humidity, and dew point. The envelopes indicate the regions in which the servers can safely operate. Raised-floor data centers were limited by temperature and could operate safely up to 78 F. On the other hand, container-based data centers have a wider operating range than raised-floor data centers and can possibly span the whole range of the allowable envelope.

4.2. Power usage effectiveness
PUE is a measure of the efficiency of a data center that can be used as a guide to determine the overall health of the data center and to improve its operational efficiency. Figure 4(a) shows a set of PUE curves for a containerbased data center at various operating temperatures. It is evident that higher efficiencies can be achieved if a data center is run at higher temperatures and higher loading levels. The figure also illustrates PUE’s non-linear dependence on temperature. To compare the PUEs of container-based to raised-floor data centers, we simulated each data center using the highest inlet temperature possible. The operating temperature for the containerbased data centers was 90 F, while it was 78 F for the raised-floor data center as indicated in Section 4.1. To improve efficiency, we employed intelligent cooling as described in Section 3.
Figure 4(b) shows the PUE curves for the three containers and the raised-floor data center. The three container curves are overlapping. Because the manufacturers do not publish their cooling data, we assumed that all containers have a similar cooling model, and henceforth, the three containers are represented by
one curve. However, in practice, cooling architectures may have different COP models resulting in varying efficiencies. The gap between the raised-floor curves that lie above the containers’ curves is solely determined by the additional cooling power required to compensate for the effects of re-circulation in order to maintain inlet temperatures below 90 F. The adaptive airflow rate plus adaptive supply temperature curve of the raised-floor data center represents a theoretical lower bound on cooling power because it is very difficult to achieve in real data centers. The difficulty is due to the method of air delivery through floor tiles. It is evident that this scheme performs the best at low utilizations. However, at higher utilization levels, more heat is dissipated in the data center, recirculation effects become more pronounced requiring more cooling power to offset the additional heat, and thus the curves converge to the constant temperature case.
Not surprisingly, containers perform better than raisedfloor data centers. However, it is rather interesting that the curve for raised-floor with adaptive supply temperature approaches the container curve at low utilizations, where Internet-based data centers normally operate. This suggests that raised-floor data centers that operate at low utilizations and employ a simple form of intelligent cooling can deliver a performance comparable to that of containers that do not.
However, at high utilizations, containers outperform raised-floor data centers no matter how aggressive an intelligent cooling they employ. The most efficient performer, the container operating at 90 F and employing adaptive airflow rate scheme, achieves an almost constant PUE of 1.35 across all utilizations. This suggests that the most efficient scheme is to deliver the appropriate cooling power to where it is needed without over-provisioning.
The figure indicates another interesting observation; the effectiveness of intelligent cooling diminishes as utilization increases. If a data center is highly loaded, there is very little benefit in employing any sort of intelligent cooling. Intelligent cooling shines at low loadings. Figure 5(a) shows how savings in cooling power diminish for all schemes as the loading increases, and that containers are 25% more efficient at maximum loading. The operating temperatures for the three cases are shown in Figure 5(b).
Server idle power does not do any useful work but generates heat that has to be removed from the data center. At high utilizations, compute power dominates server power, while at low utilizations idle power dominates, and most of the cooling power is used to remove the heat due to idle power. In order to improve the efficiency of servers, [20] proposed energy-proportional machines that dissipate zero idle power but consume energy proportional to the amount of work they perform. In Figure 6(a), we configured our design with energyproportional servers and computed the cooling powers required for container and raised-floor data centers.
Because there is no idle power, the slopes of the cooling power curves for energy-proportional systems are greater than those for real systems. At high utilizations, the cooling powers for both data centers are the same as before. However, at low utilizations, the cooling powers for containers and raised-floor data centers have decreased by 30% and 36% respectively. Internet data centers normally operate at 10-20% utilization and hence a significant portion of the cooling power can be saved with energy-proportional systems. In all cases, containers outperform raised-floor data centers at all utilization levels and this suggests that energy-proportional computing systems, if they existed, are ideal for container-based data centers.
Eliminating idle power improves total facility power suggesting that the efficiency has also improved, as shown in Figure 6(b). However, plotting the container PUE curves for real and energy-proportional computing systems indicates that PUE for energy-proportional systems has become worse especially at low utilizations, as shown in Figure 6(c). This underlines one limitation of PUE as a metric since it fails to account for the inefficiencies in servers [21].

4.3. The impact of virtualization and consolidation
It has been established in the literature that server utilization in data centers is around 10-20% [18, 22] and that servers remain idle most of the time. To reduce idle power, energy-proportional computing systems can be approximated by consolidating workloads onto fewer servers while powering off the rest or putting them in standby mode. Consolidation through virtualization can increase the utilization of servers to around 80% by allowing one physical machine to host several virtual machines. Reducing the number of servers translates into procurement and recurring costs savings. In this section, we study the impact of virtualization on container-based data centers under varying loads and different cooling optimizations, and compare to that on raised-floor data centers.
For each optimization scheme, servers were consolidated to 80%, 90%, and 100% of their capacity. Often, the consolidated servers are not utilized to their maximum in case additional capacity is suddenly required. Initially, we assume that each physical server hosts one virtual machine (VM) at a particular utilization. After consolidation, each physical server will host several VMs such that the total utilization of each physical server is some percentage of its total capacity.
Figure 7(a) shows that for the raised-floor data center at 10% utilization, consolidation to 100% capacity saves 37% and 32% of cooling power for the constant supply temperature and adaptive supply temperature respectively. When combined with adaptive airflow rate and adaptive supply temperature, consolidation saves the most at 69%. On the other hand, consolidation saves 30% of cooling power for containers, and 85% when combined with adaptive airflow rate.
This suggests that consolidating servers having low utilization levels results in greater power savings. Note that the raised-floor curve where virtualization is combined with adaptive airflow rate and adaptive supply temperature represents a theoretical lower bound on cooling power. Although virtualization benefits both raised-floor and container-based data centers, containers employing adaptive airflow rate achieve 80% power savings at low utilizations compared to raised-floor with adaptive supply temperature scheme. As expected, when more active servers are employed, idle power increases requiring a corresponding increase in cooling power.
Figure 8 shows the cooling powers for servers consolidated to 80% of their capacity. Containers consume about 24 kW at 30% utilization, while raised-floor data centers consume about 26 kW of cooling power. Since virtualization minimizes idle power, it also affects facility power, as shown in Figure 7(b). For servers with initial utilization of 10%, virtualization saves 65% power for both data centers: containers (at 90 F) and raised-floor with adaptive supply temperature. However, containers employing adaptive airflow rate scheme realize the most savings at low utilizations, reaching 79% over the base case, and 42% over raised-floor data centers with adaptive supply temperature.
The same savings could theoretically be achieved by a raised-floor data center that can deliver adaptive airflow rate to the servers, but again it is a lower bound and very difficult to achieve. We infer that containers employing the most aggressive cooling scheme achieve rather significant cooling and facility power savings (80% and 42%) over raised-floor data centers employing adaptive supply temperature when low utilization servers are virtualized and consolidated. For servers with initially higher utilizations, the benefits of virtualization and consolidation diminish where server power is dominated by computation power.
Although consolidation improves total facility power, the PUE metric does not reflect this improvement. Figure 9 shows PUE curves for both data centers with and without consolidation. Without consolidation, PUEs for both data centers are blow 1.5 but reach 2.2 after consolidation.
Even though 67 kW of power were saved at 10% utilization, PUE values have worsened as was mentioned in Section 4.2. However, for the most aggressive cooling schemes, PUE is rather constant for both containers and raised-floor data centers. This suggests that consolidation alone does not improve the efficiency of a data center. To improve efficiency, the cooling and power delivery systems should match the load. Variable speed fans and variable cooling capacity can contribute significantly to the improvement of data center efficiency [4]. Figure 9 indicates that containers that employ aggressive cooling schemes and scale the design load to match the actual load realize the highest efficiency and are better than the theoretical lower bound of raised-floor data centers.
5. Multiple-container based data centers In this section, we further examine the container as a module and as a building block for scaled data centers. We model a raised-floor data center consisting of 900 2U servers arranged in thirty-six racks. The container-based data center consists of 3 containers similar to those in Section 4, each housing 300 servers of size 2U. Figure 10 shows the layouts of the larger raised-floor and threecontainer-based data centers.

5.1. PUE and cooling power
As in Section 4.2, the operating temperature of the raised-floor data center with constant supply temperature was gradually reduced to 68 F (all server inlet temperatures below redlining point). The larger data center employing constant supply temperature dissipates more heat, requires more cooling power, and experiences more airflow inefficiencies as indicated by its greater PUE as shown in Figure 11(a).
Again, intelligent cooling matters at low utilizations, but its effectiveness diminishes at higher utilizations. The PUE for the 3-container data center is the same as that of a single container as shown in Figure 4. The differences in efficiency between containers and raised-floor data centers are more pronounced at larger scales and the benefits of modularity are more evident. The more aggressive scheme in containers employing adaptive airflow rate achieves the highest efficiency with a PUE of 1.35.
The cooling power for containers is well below 118 kW, while it is below 220 kW for the raised-floor data centers. At low utilizations, a simple form of intelligent cooling can deliver a performance comparable to containers. But for high utilization hosting facilities, containers are superior and can deliver around 46% savings in cooling power indicating that containers are a good match for consolidation of workloads.
The operating temperatures for all schemes are shown in Figure 12(a). Due to more pronounced inefficiencies, larger raised-floor data centers may have to operate at a lower temperature than a smaller one as indicated by the operating envelope in Figure 12(b). On the other hand, the envelope for the multiple-container-based data center occupies the whole range of the allowable envelope no matter what the aggregate size is 5.2. The impact of virtualization on modular data centers using containers
In this section, we investigate how containers behave when combined with other containers to form a large-scale data center. Specifically, how do cooling and total power savings at the single container level affect the overall facility power for a large-scale data center? How do they compare to a large raised-floor data center of the same server count? When consolidating workloads in containers, is it better to distribute the consolidated workload among all available containers or is it better to concentrate workloads on the least number of containers and turn off the rest? We simulated our data centers with and without virtualization, and studied how virtualization affects the cooling power in both data centers under different cooling schemes. The same supply temperatures in Figure 12(a) are used in this section.
Figure 13(a) illustrates the cooling powers as the schemes become progressively more aggressive moving down the chart. It is interesting to see that, at low utilizations, a large raised-floor data center performing consolidation but having no intelligent cooling can deliver a performance comparable to a multiple-container-based data center that does not perform consolidation; and with a simple form of intelligent cooling, it can surpass the container and even approach a containerized data center that does perform consolidation. Interestingly, there are two methods to perform consolidation: (i) workloads are consolidated locally (or distributed evenly) in each container, (ii) all workloads are consolidated onto the least number of containers and turning off the rest (if any exist).
The second method is the most efficient as shown in Figure 13. A new container is summoned into action when there is no more capacity to handle demand. The step jump is due to the cooling and fan powers of the new container. As new containers are brought into service, cooling power takes on a stair-like shape until maximum loading is achieved. To obtain further savings, workloads are consolidated onto the least number of containers and adaptive airflow rate is employed. Savings up to 85% were possible at low utilizations compared to the container base case.
Employing the most aggressive scheme, containerbased data centers achieve about 81% savings over the feasible adaptive supply temperature scheme in raisedfloor data centers at low utilizations. As the loading increases, cooling power always converges to the maximum value no matter what cooling scheme was used and whether virtualization was performed or not. At maximum loading, container-based data centers outperform raised-floor data centers by a significant 46%. As before, the effectiveness of intelligent cooling diminishes as utilization increases. There is very little benefit in employing any sort of intelligent cooling when a data center is close to its maximum loading.
Idle power savings due to virtualization and consolidation combined with cooling power savings due to efficient cooling schemes aggregate at the data center level and are reflected in the total facility power as shown
in Figure 14. Applying consolidation alone, raised-floor data centers employing the inefficient cooling scheme can reduce total facility power from 359 kW to 125 kW at low utilizations, a 65% savings. Container-based data centers reduce total facility power from 310 kW to 108 kW when consolidating workloads locally, and to 75 kW when consolidating workloads onto minimum number of containers, that is 65% and 76% savings in total facility power. Employing the most aggressive cooling scheme, container-based data centers consume 64 kW of power resulting in 80% savings in total facility power.
Container-based data centers with adaptive airflow rate outperform raised-floor data centers with adaptive supply temperature by 44% at low utilizations and by 13.7% at maximum loading. As the loading increases, the number of active servers increases, and the savings due to virtualization diminish because of the growing idle power, leaving the cooling power alone to offset the difference between containers and raised-floor data centers. As in Section 4, although total facility power is improved by consolidation, PUE metric does not reflect this improvement. Figure 13(b) shows PUE curves for both data centers before and after consolidation. Even though 234 kW have been saved for raised-floor data centers, PUE has worsened.
Interestingly, PUE values for the container-based data center with consolidation onto the least number of containers wiggles above and below the PUE curve of the base case without consolidation. Every time a container is brought into service, a peak is introduced in the curve. The effective PUE for a multiple-container-based data center consisting of N containers can be represented as:
N
eff i i i i = 1 i i
1 1PUE (R + P + F)(1 + ), (Eq-1) R COP =
where Ri is the total rack power, Pi is the total PDU and UPS power, Fi is total fan power, and COPi is the COP value for each container. We have shown that the savings of individual containers aggregate at the data center level exposing the power of modularity and that virtualization achieves the most savings when workloads are concentrated into the least number of containers.

6. Response time
Under service level agreements (SLA), data centers guarantee the specified response time under maximum loading by providing the necessary number of servers. However, optimizing for response time requires keeping more servers on-line and incurs higher cooling power due to the additional idle power. On the other hand, optimizing for cooling power by concentrating the data center workloads on fewer servers and turning off the rest or putting them in standby mode may degrade response time. Here, we concentrate workloads on the smallest number of servers to save cooling power while maintaining response time within 10% degradation.
We modeled the data center service as a GI/G/m queue using the Allen-Cunnen approximation [23, 24]. The average response time for a data center consisting of m servers is given by:
2 2 / W= (Eq-2) 1 2 m A BP c c m μ ρ + ⋅ −
where Pm is the waiting probability, μ is the service rate of a single server, cA and cB are the coefficients of variation of the inter-arrival time and the service time, respectively, and is the traffic intensity (i.e. server utilization). We obtained the values of these parameters from the raw data provided by the Internet Traffic Archive [25]. The traces we used represent a one-week server load variation including idle period, peak hours, and daily surge. We scaled up the values of request rate and request size to match our hosting facility.
Without virtualization, server utilization is around 10% since Internet-based data centers typically operate between 10-20% utilization. After virtualization and consolidation, server utilization can reach 80-85%. To make the comparisons fair, we configured the multiplecontainer-based data centers to operate at the highest supply temperature of 90 F and we did not employ the most aggressive scheme of adaptive airflow rate since this scheme is infeasible to achieve in raised-floor data centers. We configured the raised-floor data centers to employ the adaptive supply temperature scheme since this is more realistic to achieve.
Table 3 shows the data center configurations we evaluated in this study and the achieved savings in cooling power. The results indicate that multiple-
container-based data centers with virtualization achieve substantial savings in cooling power compared to the large raised-floor data centers while maintaining the response time. In particular, the containers running NASA, ClarkNet, and UC Berkeley traces, achieve at least 60% savings over raised-floor data centers while both incurring a response time degradation of less than 6%. The savings are considerable because the total size of the data centers is large and the cooling inefficiencies of the raised-floor data center are more pronounced.
For the traces Calgary and Saskatchewan, the container savings over a raised-floor are negligible because the size of the data center is small (the size of a single container). At a small scale, the cooling inefficiencies of a raised-floor data center employing virtualization are not as significant when compared to a container. Although containers do not show any savings over raised-floor data centers when running Calgary and Saskatchewan traces, there are considerable savings due to virtualization for each type as shown in Section 4.3. For Calgary trace, containers save about 17% cooling power due to virtualization while a raised-floor data center with equivalent server count saves about 20% and both incur a response time degradation of 12%. For Saskatchewan trace, containers achieve 21% savings while raised-floor achieves 24% savings in cooling power and both incur a response time degradation of 3.7%.
To summarize, our results show that with consolidation and virtualization, containers outperform raised-floor data centers in cooling power savings while maintaining response time within an acceptable range.

7. Related work
Prior work on data centers and energy efficiency revolved around traditional data centers. Most work addresses the issue of reducing idle power and power consumption in general. Others call for the efficient use of the available power while reducing idle power. Fan et al. [18] considered available capacity and power provisioning. Meisner et al. [22] proposed to minimize idle power and transition time. Ahmad et al. [3] proposed a joint optimization of idle power and cooling power while maintaining response time. Moore et al. [6] took a different approach to minimizing cooling power and proposed temperature-aware workload placement algorithms that help reduce cooling costs. Sharma et al. [26] also addressed the efficiency of the cooling system and proposed techniques and metrics for efficient thermal management. Boucher et al. [4] proposed dynamic
cooling control in a data center to improve thermal management and energy performance.
Little published work on modular data centers exists due to their novelty. Google Inc. filed for a patent in 2005 [11]. Hamilton [7] proposed using a fully populated shipping container as a basic growth and management unit. However, there is little work on the analysis of cooling power and its relation to workload variability. [27] proposed a model for performance, reliability and cost for service-less container-based data centers.

8. Conclusions
Although cooling is a significant energy overhead, it is not obvious how to evaluate this overhead from the perspective of computer architects and IT staff. Specifically, we demonstrate that it is not a taxed cost, but rather a cost that varies with workload variability, cooling optimizations, data center layout, and efficiency of the cooling system. In this paper, we quantify and evaluate cooling power under various cooling optimizations for both container and raised-floor data centers. We have shown that a container achieves 80% savings in cooling power compared to a raised-floor data center when the design power is matched to the actual load by scaling the data center’s cooling capacity. As a basic building block, multiple-container-based data centers achieve 46% savings in cooling power compared to raised-floor data centers when highly loaded, suggesting that containers are very well suited for supporting consolidation of workloads. At the facility level, container-based data centers achieve 44% power savings at low loading, and 13.7% at maximum loading compared to raised-floor data centers. Based on our results, we report some of the interesting findings:
• Raised-floor data centers that operate at low utilizations and employ a simple form of intelligent cooling can approach the performance of a container that does not employ any form of intelligent cooling.
• At high utilizations, containers without intelligent cooling outperform raised-floor data centers no matter how aggressive an intelligent cooling they employ.
• The effectiveness of intelligent cooling diminishes as utilization increases. It shines at low utilizations.
• Internet-based data centers that normally operate at low utilization levels benefit the most from consolidation in terms of cooling power.
Greater savings can be achieved by using an efficient self-contained module as the basic building block instead of treating the data center as one large monolithic unit where inefficiencies scale with size. Our results indicate that savings due to consolidation and efficient cooling at the single container level percolate and aggregate at the large-scale level while minimizing the effects of inefficiencies. This translates to substantial cost savings over the lifetime of the data center.

9. References
[1] L. Barroso et al. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines. Synthesis lectures on computer architecture # 6. 
[2] U.S. EPA. Report to Congress on Server and Data Center Energy Efficiency.Environmental Protection Agency, Tech. Rep., 2007. 
[3] F. Ahmad et al. Joint Optimization of Idle and Cooling Power in Data Centers While Maintaining Response Time, in ASPLOS, 2010. 
[4] T. Boucher et al. Viability of Dynamic Cooling Control in a Data Center Environment. ASME Journal of Electronic Packaging, 2006. 
[5] M. Patterson. The Effect of Data Center Temperature on Energy Efficiency, 14th Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems, 2008. 
[6] J. Moore et al. Making Scheduling "Cool": Temperature-aware Workload Placement in Data Centers. USENIX ATC, 2005. 
[7] J. Hamilton. An Architecture for Modular Data Centers. 3rd Biennial Conference on Innovative Data Systems Research, 2007. 
[8] HP Performance-Optimized Data Center. http://h20338. www2.hp.com/enterprise/cache/595887-0-0-0-121.html. 
[9] Sun modular data center. http://www.sun.com/service/ sunmd/. 
[10] SGI modular data center. http://www.sgi.com/products /data_center/ice_cube/. 
[11] Google Inc. Efficient Data Center Summit, 2009. http://www.google.com/corporate/green/datacenters/summit.html. 
[12] “Microsoft’s Top 10 Business Practices for Environmentally Sustainable Data Center.” Available at: http://www.microsoft.com/ environment/our_commitment/articles/datacenter_bp.aspx. 
[13] http://bits.blogs.nytimes.com/2008/12/08/dell-sees-double-withdata-center-in-a-container/. 
[14] R. Sharma et al. Experimental Investigation of Design and Performance of Data Centers. 9th Intersociety Conf. on Thermal and Thermomechanical Phenomena in Electronic Systems, 2003. 
[15] C. Belady et al. Green Grid Data Center Power Efficiency Metrics: PUE and DCIE. Metrics & Measurements, 2007. http://www.thegreendrid.org. 
[16] CoolSim. http://www.coolsimsoftware.com/ 
[17] Standard Performance Evaluation Corporation. http:// www.spec.org/power_ssj2008/results/power_ssj2008.html. 
[18] X. Fan et al. Power Provisioning for a Warehouse-sized Computer. International Symp. on Computer Architecture, 2007. 
[19] 2008 ASHRAE Environmental Guidelines for Datacom Equipment - Expanding the Recommended Environmental Envelope. ASHRAE. 
[20] L. Barroso et al. The Case for Energy-Proportional Computing. IEEE Computer, Vol. 40, No. 12, 2007. 
[21] S. Pelley et al. Understanding and Abstracting Total Data Center Power. The 2009 Workshop on Energy Efficient Design (WEED). 
[22] D. Meisner et al. PowerNap: Eliminating Server Idle Power. International Conference on Architecture Support for Programming Languages and Operating Systems, 2009. 
[23] O. Allen. Probability, Statistics and Queuing Theory with Computer Science Applications, 1990. 
[24] G. Bolch et al. Queuing Networks and Markov Chains: Modeling and Performance Evaluation with Computer Science Applications, 1998. 
[25] Internet Traffic Archive. http://ita.ee.lbl.gov/ html /traces.html. 
[26] R. Sharma et al. Balance of Power: Dynamic Thermal Management for Internet Data Centers. Internet Computing, IEEE, Volume 9, 2005. 
[27] K. Vishwanath et al. Modular Data Centers: How to Design Them? Workshop on Large-Scale System and Application Performance, 2009.
