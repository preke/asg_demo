Understanding the Impact of Multi-Core Architecture in Cluster Computing: A Case Study with Intel Dual-Core System
Lei Chai,Qi Gao,Dhabaleswar K. Panda
panda}@cse.ohio-state.edu

∗This research is supported in part by DOE’s Grants#DE-FC0206ER25749 and #DE-FC02-06ER25755; NSF’s Grants #CNS-0403342 and #CNS-0509452; grants from Intel, Mellanox, Cisco systems, Linux Networx and Sun Microsystems; and equipment donations from Intel, Mellanox, AMD, Apple, Appro, Dell, Microway, PathScale, IBM, SilverStorm and Sun Microsystems.

1. Introduction
The pace people pursuing computing power has never slowed down. Moore’s Law has been proven to be true over the passage of time - the performance of microchips has been increasing at an exponential rate, doubling every two years. “In 1978, a commercial flight between New York and Paris cost around $900 and took seven hours. If the principles of Moore’s Law had been applied to the airline industry the way they have to the semiconductor industry since 1978, that flight would now cost about a penny and take less than one second” (a statement from Intel). However, it becomes more difficult to speedup processors nowadays by increasing frequency. One major barrier is the overheat problem, and the other issue is power consumption. These concerns make it less cost-to-performance effective to increase processor clock rate. Therefore, computer architects have designed multi-core processor, which means to place two or more processing cores on the same chip [9]. Multicore processors speedup application performance by dividing the workload to different cores. It is also referred to as Chip Multiprocessor (CMP).
On the other hand, clusters have been one of the most popular models in parallel computing for decades. The emergence of multi-core architecture will bring clusters into a multi-core era. As a matter of fact, multi-core processors have already been widely deployed in clusters for parallel computing. In the new Top500 supercomputer list published in November 2006, more than 20% processors are multi-core processors from Intel and AMD [6]. In order to get optimal performance, it is crucial to have an in-depth understanding on application behaviors and trends on multicore clusters. It is also very important to identify potential bottlenecks in multi-core clusters through evaluation, and explore possible solutions. However, since multi-core is a relatively new technology, few research has been done in the literature.
In this paper, we take on these challenges and design a
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007
set of experiments to study the impact of multi-core architecture on cluster computing. The purpose is to give both application and communication middleware developers insights on how to improve overall performance on multi-core clusters. We aim to answer the following questions:
• What are the application communication characteristics in multi-core clusters?
• What are the potential bottlenecks in multi-core clusters and how to possibly avoid them?
• Can multi-core clusters scale well?
We choose to use one of the most advanced servers, Intel Bensley system [3] with dual-core Woodcrest processor, as a case study platform. The benchmarks used include HPL, NAMD, and NAS parallel benchmarks. From our message distribution experiments, we find that on an average about 50% of messages are transferred through intranode communication, which is much higher than intuition. This trend indicates that optimizing intra-node communication is as important as optimizing inter-node communication in a multi-core cluster. An interesting observation from our bottleneck identification experiments is that cache and memory contention may be a potential bottleneck in multicore cluster, and communication middleware and applications should be written in a multi-core aware manner to alleviate this problem. We demonstrate that data tiling, a data locality optimization technique, improves benchmark execution time by up to 70%. We also compare the scalability of a multi-core cluster with that of a single-core cluster and find that the scalability of multi-core clusters is promising.
The rest of the paper is organized as follows: In Section 2 we introduce the background knowledge of multi-core architecture. In Section 3 we describe the methodology of our evaluation. Setup of the evaluation system is described in Section 4 and the evaluation results and analysis are presented in Section 5. Related work is discussed in Section 6. And finally we conclude and point out future work directions in Section 7.

2. Multi-core Clusters
Multi-core means to integrate two or more complete computational cores within a single chip [9]. The motivation is the fact that scaling up processor speed results in dramatic rise in power consumption and heat generation. In addition, it becomes more difficult to increase processor speed nowadays that even a little increase in performance will be costly. Realizing these factors, computer architects have proposed multi-core processors that speed up application performance by dividing the workload among multiple processing cores instead of using one “super fast” single processor. Multi-core processor is also referred to as Chip Multiprocessor (CMP). Since a processing core can
be viewed as an independent processor, in this paper we use processor and core interchangeably.
Most processor vendors have multi-core products, e.g. Intel Quad- and Dual-Core Xeon, AMD Quad- and DualCore Opteron, Sun Microsystems UltraSPARC T1 (8 cores), IBM Cell, etc. There are various alternatives in designing cache hierarchy organization and memory access model. Figure 1 illustrates two typical multi-core system designs. The left box shows a NUMA [1] based dual-core system in which each core has its own L2 cache. Two cores on the same chip share the memory controller and local memory. Processors can also access remote memory, although local memory access is much faster. The right box shows a bus based dual-core system, in which two cores on the same chip share the same L2 cache and memory controller, and all the cores access the main memory through a shared bus.
Multi-core processors have been deployed in cluster computing. In a multi-core cluster, there are three levels of communication as shown in Figure 1. The communication between two processors on the same chip is referred to as intra-CMP communication in this paper. The communication across chips but within a node is referred to as inter-CMP communication. And the communication between two processors on different nodes is referred to as inter-node communication. Multi-core clusters impose new challenges in software design, both at the middleware level and application level.

3. Design of Experiments for Evaluating Multicore Clusters
In this section we describe the evaluation methodology and explain the design and rational of each experiment.
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007

3.1. Programming Model and Benchmarks
We choose to use MPI [4] as the programming model because it is the de facto standard used in cluster computing. The MPI library used is MVAPICH2 [5], which is a high performance MPI-2 implementation over InfiniBand [2] and other RDMA-enabled interconnects. In MVAPICH2, intra-node communication, including both intraCMP and inter-CMP, is achieved by user level memory copy. The hybrid programming of OpenMP and MPI [16] is also a popular paradigm in cluster computing. However, it is out of the scope of this paper and is not considered.
We evaluate both microbenchmarks and application level benchmarks to get a comprehensive understanding on the system. Microbenchmarks include latency and bandwidth tests. And application level benchmarks include HPL from HPCC benchmark suite [17], NAMD [22] apoa1 data set, and NAS parallel benchmarks [12].

3.2. Design of Experiments
We have designed to carry out four sets of experiments for our study: latency and bandwidth, message distribution, potential bottleneck identification, and scalability tests. We describe them in detail below.
• Latency and Bandwidth: These are the standard pingpong latency and bandwidth tests to characterize the three levels of communication in a multi-core cluster: intra-CMP, inter-CMP, and inter-node communication.
• Message Distribution: We define message distribution as a two dimensional metric. One dimension is with respect to the communication channel, i.e. the percentage of traffic going through intra-CMP, inter-CMP, and inter-node respectively. The other dimension is in terms of message size. This experiment is very important because understanding message distribution facilitates communication middleware developers, e.g. MPI implementors, to optimize critical communication channels and message size range for applications. The message distribution is measured in terms of both number of messages and data volume.
• Potential Bottleneck Identification: In this experiment, we run application level benchmarks on different configurations, e.g. four processes on the same node, four processes on two different nodes, and four processes on four different nodes. We want to discover the potential bottlenecks in multi-core cluster if any, and explore approaches to alleviate or eliminate the bottlenecks. This will give insights to application writers how to optimize algorithms and/or data distribution for multicore clusters. We also design an example to demonstrate the effect of multi-core aware algorithm.
• Scalability Tests: This set of experiments is carried out to study the scalability of multi-core clusters.

3.3. Processor Affinity
In all our experiments, we use sched affinity system call to ensure the binding of process with processor. The effect of processor affinity is two-fold. First, it eases our analysis, because we know exactly the mapping of processes with processors. Second, it makes application performance more stable, because process migration requires cache invalidation and may degrade performance.

4. Evaluation Platforms
Our evaluation system consists of four Intel Bensley systems connected by InfiniBand. Each node is equipped with two sets of dual-core 2.6GHz Woodcrest processor, i.e. four processors per node. Two processors on the same chip share a 4MB L2 cache. The overall architecture is similar to that shown in the right box in Figure 1. However, Bensley system has more dedicated memory bandwidth per processor by doubling up on memory buses. The InfiniBand HCA is Mellanox MT25208 DDR and the operating system is Linux 2.6.
To compare scalability, we also used a single-core Intel cluster connected by InfiniBand. Each node is equipped with dual Intel Xeon 3.6GHz processor and each processor has a 2MB L2 cache.

5. Evaluation Results
In this section we present the experimental results and analyze them in depth. We use the format pxq to represent a configuration. Here p is the number of nodes, and q is the number of processors per node.

5.1. Latency and Bandwidth
Figure 2 shows the MPI latency and bandwidth of the three levels of communication in a multi-core cluster. The small message latency is 0.42us, 0.89us, and 2.83us for intra-CMP, inter-CMP, and inter-node communication, respectively. The corresponding peak bandwidths are 6684MB/s, 1258MB/s, and 1532MB/s.
From Figure 2 we can see that intra-CMP performance is far better than inter-CMP and inter-node performance, especially for small and medium messages. This is because in Intel Bensley system two cores on the same chip share the same L2 cache. Therefore, the communication just involves two cache operations if the communication buffers are in the cache. From the figure we can also see that for large messages, inter-CMP performance is not as good as inter-node performance, although memory performance is supposed to be better than network performance. This is because the intra-node communication is achieved through
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007
a shared buffer, where two memory copies are involved. On the other hand, the inter-node communication uses the Remote Direct Memory Access (RDMA) operation provided by InfiniBand and rendezvous protocol [21], which forms a zero-copy and high performance scheme. This also explains why for large messages (when the buffers are out of cache) intra-CMP and inter-node perform comparably.
This set of results indicate that to optimize MPI intranode communication performance, one way is to have better L2 cache utilization to keep communication buffers in the L2 cache as much as possible, and the other way is to reduce the number of memory copies. We have proposed a preliminary enhanced MPI intra-node communication design in our previous work [10].

5.2. Message Distribution
As mentioned in Section 3.2, this set of experiments is designed to get more insights with respect to the usage pattern of the communication channels, as well as the message size distribution. Figures 3 and 4 show the profiling results for NAMD and HPL respectively. The results for NAS benchmarks are listed in Table 1. The experiments are carried out on a 4x4 configuration and the numbers are the average of all the processes.
The bars and lines in Figures 3 and 4 are interpreted as follows. Suppose there are n messages transferred during the application run, in which m messages are in the range (a,b]. Also suppose in these m messages, m1 are transferred through intra-CMP, m2 through inter-CMP, and m3 through inter-node. Then:
• Bar Intra-CMP(a, b] = m1/m • Bar Inter-CMP(a, b] = m2/m • Bar Inter-node(a, b] = m3/m • Point Overall(a, b] = m/n
From Figure 3 we can observe that most of the messages in NAMD are of size 4KB to 64KB. Messages in this range take more than 90% of the total number of messages and byte volume. Optimizing medium message communication is important to NAMD performance. In the 4KB to 64KB message range, about 10% messages are transferred through intra-CMP, 30% are transferred through inter-CMP, and 60% are transferred through inter-node. This is interesting and kind of surprising. Intuitively, in a cluster environment intra-node communication is much less than internode communication, because a process has much more inter-node peers than intra-node peers. E.g. in our testbed, a process has 1 intra-CMP peer, 2 inter-CMP peers, and 15 inter-node peers. If a process has the same chance to communicate with every other process, then theoretically:
• Intra-CMP = 6.7% • Inter-CMP = 13.3%
• Inter-node = 80%
If we call this distribution as even distribution, then we see that intra-node communication in NAMD is well above that in even distribution, for almost all the message sizes. Optimizing intra-node communication is as important as optimizing inter-node communication to NAMD.
From Figure 4 we can observe that most messages are small messages in HPL, from 256 bytes to 4KB. However, with respect to data volume messages larger than 256KB take more percentage. We also find that almost all the messages are transferred through intra-node in our experiment. However, this is a special case. In HPL, a process only talks to processes on the same row or column with itself. In our 4x4 configuration, a process and its row or column peers are always mapped to the same node, therefore, almost all the communication take place within a node. We have also conducted the same experiment on a 16x4 configuration for HPL. The results show that 15% messages are transferred through intra-CMP, 42% through inter-CMP, and 43% through inter-node. Although the trend is not as extreme as in the 4x4 case, we can still see that intra-node communication in HPL is well above that in the even distribution.
Table 1 presents the total message distribution in NAS benchmarks, in terms of communication channels. Again, we see that the amount of intra-node (intra-CMP and interCMP) communication is much larger than that in even distribution for most benchmarks. On an average, about 50% messages going through intra-node communication. This trend is not random. It is because most applications have certain communication patterns, e.g. row or column based communication, ring based communication, etc. which increase the intra-node communication chance. Therefore, even in a large multi-core cluster, optimizing intra-node communication is critical to the overall application performance.

5.3. Potential Cache and Memory Contention
In this experiment, we run all the benchmarks on 1x4, 2x2, and 4x1 configurations, respectively, to examine the potential bottleneck in the system. As mentioned in the beginning of Section 5, we use the format pxq to represent a configuration, in which p is the number of nodes, and q is the number of processors per node. It is to be noted that in the 2x2 configuration one core per socket is used. The results are shown in Figure 5. The execution time is normalized to that on the 4x1 configuration.
One of the observations from Figure 5 is that 1x4 configuration does not perform as well as 2x2 and 4x1 configurations for many applications, e.g. IS, FT, CG, SP, and HPL. This is because in 1x4 configuration all the cores are activated for execution. As mentioned in Section 4, two cores
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007
Figure 7. Effect of Data Tiling
on the same chip share the L2 cache and memory controller, thus cache and memory contention is a potential bottleneck. Memory contention is not a problem for processors on different chips, because Intel Bensley system has dedicated bus for each chip for higher memory bandwidth. This is why 2x2 and 4x1 configurations perform comparably.
The same trend can be observed from Figure 6. In this experiment, we run 2 processes on 2 processors from the same chip, 2 processors across chips, and 2 processors across nodes respectively. We see that inter-CMP and internode performance are comparable and higher than intraCMP. The only special case is IS, whose inter-CMP performance is noticeably lower than inter-node. This is because IS uses many large messages and inter-node performs better than inter-CMP for large messages as shown in Figure 2.
This set of experiments indicates that to fully take advantage of multi-core architecture, both communication middleware and applications should be multi-core aware to reduce cache and memory contention. Communication middleware should avoid cache pollution as much as possible, e.g. increase communication buffer reuse [10], use cache bypass memory copy [8], or eliminate intermediate buffer [18]. Applications should be optimized to increase data locality. Data tiling [19] is a common technique to reduce unnecessary memory traffic. If a large data buffer is to be processed multiple times, then instead of going through the whole buffer multiple times, we can divide the buffer into smaller chunks and process the buffer in a chunk granularity so that the data chunks stay in the cache for multiple operations. We show a small example in the next section to demonstrate how data tiling can potentially improve application performance on multi-core system.

5.4. Benefits of Data Tiling
To study the benefits of data tiling on multi-core clusters, we design a microbenchmark, which does computation and communication in a ring-based manner. Each process has a
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007
piece of data (64MB) to be processed for a number of iterations. During execution, each process does vector multiplication on its own data, sends them to its right neighbor and receives data from its left neighbor, and then starts another iteration of computation. In the original scheme, the data processed in the original chunk size (64MB) while in the data tiling scheme, the data are divided in to smaller chunks in the size of 256KB, which can easily fit in L2 cache.
Figure 7 shows the benefits of data-tiling, from which we observe that the execution time reduces significantly. This is because in the tiling case, since the intra-node communication is using CPU-based memory copy, the data are actually preloaded into L2 cache during the communication. In addition, we observe that in the cases where 2 processes running on 2 cores on the same chip, since most communication happens in L2 cache in data tiling case, the improvement is most significant, around 70% percent. The improvement in the case where 4 processes running on 4 cores on the same node, 8 processes running on 2 nodes, and 16 processes running on 4 nodes is 60%, 50%, and 50%, respectively. The improvements are not as large as that in the 2 process case because the communication of inter-CMP and inter-node is not as efficient as the intra-CMP for 256KB message size.

5.5. Scalability
Scalability is always an important angle to look at when evaluating clusters. Although our testbed only contains four nodes, we want to do an initial study on multi-core cluster scalability. We also compare the scalability of a multi-core cluster with that of a single-core cluster. The results are shown in Figure 8. It is to be noted that the performance is normalized to that on 2 processes, so 8 is the ideal speedup for the 16 process case.
It can be seen from Figure 8(a) that some applications (LU and MG) show almost ideal speedup on the multi-core cluster. Compared with the single-core cluster scalability, we find that for applications that show cache or memory contention in Figure 5, such as IS, FT, and CG, the scalability on single-core cluster is better than that on the multicore cluster. For other applications such as MG, LU and NAMD, the multi-core cluster shows the same scalability as the single-core cluster. Thus, as an initial study, we find that the scalability of the multi-core cluster is promising.

6. Related Work
There have been studies on multi-core systems. Koop, et al in [20] have evaluated the memory subsystem of Bensley platform using microbenchmarks. In this work, we not only evaluate microbenchmark performance, but also focus on application level benchmark profiling, evaluation, and analysis. Alam, et al have done a scientific workloads characterization on AMD Opteron based multi-core systems [14].
Our work distinguishes from theirs in the sense that our evaluation has been done in a cluster environment while they focus on a single multi-core node. Besides, the evaluation methodology is also different. Realizing the importance and popularity of multi-core architecture, researchers start to propose techniques for application optimization on multi-core systems. Some of the techniques are discussed in [11], [15], and [23]. Discussions of OpenMP on multicore processors can be found in [13].
Different approaches have been proposed to optimize MPI intra-node communication. A kernel assisted memory map approach has been designed in [18]. Optimizations on user space memory copy scheme have been discussed in [8] and [10]. Buntinas, et al have evaluated and compared different intra-node communication approaches in [7].

7. Conclusions and Future Work
In this paper we have done a comprehensive performance evaluation, profiling, and analysis on multi-core cluster, using both microbenchmarks and application level benchmarks. We have several interesting observations from the experimental results that give insights to both application and communication middleware developers. From microbenchmark results, we see that there are three levels of communication in a multi-core cluster with different performances: intra-CMP, inter-CMP, and inter-node communication. Intra-CMP has the best performance because data can be shared through L2 cache. Large message performance of inter-CMP is not as good as inter-node because of memory copy cost. With respect to applications, the first observation is that, much more intra-node communication takes place in applications than that in even distribution, which indicates that optimizing intra-node communication is as important as optimizing inter-node communication in a multi-core cluster. Another observation is that when all the cores are activated for execution, cache and memory contention may prevent the multi-core system from achieving best performance, because two cores on the same chip share the same L2 cache and memory controller. This indicates that communication middleware and applications should be written in a multi-core aware manner to get optimal performance. We have demonstrated an example on application optimization technique which improves benchmark performance by up to 70%. Compared with a single-core cluster, the multicore cluster does not scale well for applications that show cache/memory contention. However, for other applications the multi-core cluster has the same scalability as the singlecore cluster.
In the future we would like to continue our study on Intel quad-core systems and other multi-core architectures, such as quad- and dual-core opteron and Sun UltraSPARC T1 (Niagara) systems. We will do extensive evaluation on large multi-core clusters with large scale applications to study the scalability of multi-core clusters in depth. We will
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007
also study the impact of MPI collective algorithms on message distribution and explore novel approaches to further optimize MPI intra-node communication by reducing cache pollution.
References
[1] http://lse.sourceforge.net/numa/faq/. [2] InfiniBand Trade Association. http://www.infinibandta.com. [3] Intel Unleashes New Server Processors That De-
liver World-Class Performance And Power Efficiency. http://www.intel.com/pressroom/archive/releases/20060626 comp.htm?cid=rss-83642-c1-132087. [4] MPI: A Message-Passing Interface Standard. http://www.mpi-forum.org/docs/mpi-11-html/mpireport.html. [5] MPI over InfiniBand Project. http://nowlab.cse.ohiostate.edu/projects/mpi-iba/. [6] Top 500 SuperComputer Sites. http://www.top500.org/. [7] Darius Buntinas, Guillaume Mercier, and William Gropp.
Data Transfers Between Processes in an SMP System: Performance Study and Application to MPI. In International Conference on Parallel Processing, 2006. [8] Darius Buntinas, Guillaume Mercier, and William Gropp. The Design and Evaluation of Nemesis, a Scalable LowLatency Message-Passing Communication Subsystem. In International Symposium on Cluster Computing and the Grid, 2006. [9] Thomas W. Burger. Intel Multi-Core Processors: Quick Reference Guide. http://cachewww.intel.com/cd/00/00/23/19/231912 231912.pdf. [10] L. Chai, A. Hartono, and D. K. Panda. Designing High Performance and Scalable MPI Intra-node Communication Support for Clusters. In The IEEE International Conference on Cluster Computing, 2006. [11] Max Domeika and Lerie Kane. Optimization Techniques for Intel Multi-Core Processors. http://www3.intel.com/cd/ids/developer/asmona/eng/261221.htm?page=1.
[12] D. H. Bailey et al. The NAS parallel benchmarks. volume 5, pages 63–73, Fall 1991. [13] Matthew Curtis-Maury et al. An Evaluation of OpenMP on Current and Emerging Multithreaded/Multicore Processors. In IWOMP, 2005. [14] Sadaf R. Alam et al. Characterization of Scientific Workloads on Systems with Multi-Core Processors. In International Symposium on Workload Characterization, 2006. [15] Kittur Ganesh. Optimization Techniques for Optimizing Application Performance on Multi-Core Processors. http://tree.celinuxforum.org/CelfPubWiki/ELC2006Present ations?action=AttachFile&do=get&target=GaneshCELF.pdf. [16] Yun He and Chris Ding. Hybrid OpenMP and MPI Programming and Tuning. www.nersc.gov/nusers/services/training/classes/NUG/Jun04 /NUG2004 yhe hybrid.ppt. [17] Innovative Computing Laboratory (ICL). HPC Challenge Benchmark. http://icl.cs.utk.edu/hpcc/. [18] H. W. Jin, S. Sur, L. Chai, and D. K. Panda. LiMIC: Support for High-Performance MPI Intra-Node Communication on Linux Cluster. In International Conference on Parallel Processing, 2005. [19] I. Kadayif and M. Kandemir. Data Space-oriented Tiling for Enhancing Locality. ACM Transactions on Embedded Computing Systems, 4(2):388–414, 2005. [20] M. Koop, W. Huang, A. Vishnu, and D. K. Panda. Memory Scalability Evaluation of the Next-Generation Intel Bensley Platform with InfiniBand. In Hot Interconnect, 2006. [21] J. Liu, J. Wu, and D. K. Panda. High performance RDMAbased MPI implementation over InfiniBand. Int’l Journal of Parallel Programming, In Press, 2005. [22] J. C. Phillips, G. Zheng, S. Kumar, and L. V. Kale. NAMD: Biomolecular Simulation on Thousands of Processors. In SuperComputing, 2002. [23] Tian Tian and Chiu-Pi Shih. Software Techniques for Shared-Cache Multi-Core Systems. http://www.intel.com/cd/ids/developer/asmona/eng/recent/286311.htm?page=1.
Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07) 0-7695-2833-3/07 $20.00 © 2007

References
[1]Darius Buntinas,Guillaume Mercier,William GroppData Transfers Between Processes in an SMP System: Performance Study and Application to MPIIn International Conference on Parallel Processing,2006
[2]Darius Buntinas,Guillaume Mercier,William GroppThe Design and Evaluation of Nemesis, a Scalable Low- Latency Message-Passing Communication SubsystemIn International Symposium on Cluster Computing and the Grid,2006
[3]Thomas W. BurgerIntel Multi-Core Processors: Quick Reference Guide. http://cachewww.intel.com/cd/00/00/23/19/231912 231912.pdf1912
[4]L. Chai,A. Hartono,D.K. PandaDesigning High Performance and Scalable MPI Intra-node Communication Support for ClustersIn The IEEE International Conference on Cluster Computing,2006
[5]D.H. BaileyThe NAS parallel benchmarks1991
[6]Matthew Curtis-MauryAn Evaluation of OpenMP on Current and Emerging Multithreaded/Multicore ProcessorsIn IWOMP,2005
[7]Sadaf R. AlamCharacterization of Scientific Workloads on Systems with Multi-Core ProcessorsIn International Symposium on Workload Characterization,2006
[8]Kittur GaneshOptimization Techniques for Optimizing Application Performance on Multi-Core Processors. http://tree.celinuxforum.org/CelfPubWiki/ELC2006Present ations?action=AttachFile&do=get&target=Ganesh- CELF.pdf2006
[9]Yun He,Chris DingHybrid OpenMP and MPI Programming and Tuning. www.nersc.gov/nusers/services/training/classes/NUG/Jun04 /NUG2004 yhe hybrid.ppt2004
[10]H.W. Jin,S. Sur,L. Chai,D.K. PandaLiMIC: Support for High-Performance MPI Intra-Node Communication on Linux ClusterIn International Conference on Parallel Processing,2005
[11]I. Kadayif,M. KandemirData Space-oriented Tiling for Enhancing LocalityACM Transactions on Embedded Computing Systems,2005
[12]M. Koop,W. Huang,A. Vishnu,D.K. PandaMemory Scalability Evaluation of the Next-Generation Intel Bensley Platform with InfiniBandIn Hot Interconnect,2006
[13]J. Liu,J. Wu,D.K. PandaHigh performance RDMAbased MPI implementation over InfiniBandInt’l Journal of Parallel Programming,2005
[14]J.C. Phillips,G. Zheng,S. Kumar,L.V. KaleNAMD: Biomolecular Simulation on Thousands of ProcessorsIn SuperComputing,2002
[15]Tian Tian,Chiu-Pi ShihSoftware Techniques for Shared-Cache Multi-Core Systems. http://www.intel.com/cd/ids/developer/asmona/eng/recent/286311.htm?page=1Seventh IEEE International Symposium on Cluster Computing and the Grid(CCGrid'07)2007
