Combating the Small Sample Class Imbalance Problem Using Feature Selection
Mike Wasikowski


suboptimal performance. Researchers have rigorously studied the resampling, algorithms, and feature selection approaches to this problem. No systematic studies have been conducted to understand how well these methods combat the class imbalance problem and which of these methods best manage the different challenges posed by imbalanced data sets. In particular, feature selection has rarely been studied outside of text classification problems. Additionally, no studies have looked at the additional problem of learning from small samples. This paper presents a first systematic comparison of the three types of methods developed for imbalanced data classification problems and of seven feature selection metrics evaluated on small sample data sets from different applications. We evaluated the performance of these metrics using area under the receiver operating characteristic (AUC) and area under the precisionrecall curve (PRC). We compared each metric on the average performance across all problems and on the likelihood of a metric yielding the best performance on a specific problem. We examined the performance of these metrics inside each problem domain. Finally, we evaluated the efficacy of these metrics to see which perform best across algorithms. Our results showed that signal-tonoise correlation coefficient (S2N) and Feature Assessment by Sliding Thresholds (FAST) are great candidates for feature selection in most applications, especially when selecting very small numbers of features.
Index Terms—Class imbalance problem, feature evaluation and selection, machine learning, pattern recognition, bioinformatics, text mining.
Ç

1 INTRODUCTION
THE class imbalance problem is a challenge to machinelearning and data mining, and it has attracted significant research in the last 10 years. A classifier affected by the class imbalance problem for a specific data set would see strong accuracy overall but very poor performance on the minority class. This problem can appear in two different types of data sets: binary problems, where one of the two classes comprises considerably more samples than the other, and multiclass problems, where each class only contains a tiny fraction of the samples and we use one-versus-rest classifiers. Data sets meeting one of these two criteria have different misclassification costs, either implicitly or explicitly stated, for the different classes [1], and these types of data sets are pervasive in real-world applications. Examples of these kinds of applications include biological data analysis [2], text classification [3], [4], [5], and image classification [6] among many others. The skew of an imbalanced data set can be severe. Some imbalanced data sets will only have one minority sample for every 100 majority samples.
Why is the class imbalance problem so prevalent and difficult to overcome? First, modern classifiers assume that unseen data points on which the classifier will be asked to
make a prediction are drawn from the same distribution as the training data [7]. If testing and validation data samples were drawn from a different distribution, the trained classifier may give poor results because of the flawed model [8].
Second, most modern classifiers try to optimize a specific loss function on the training data. For example, logistic regression attempts to minimize the least-squares error, and the support vector machine (SVM) tries to minimize regularized hinge loss. The one constant between most of these loss functions is that they generalize very well to overall predictive accuracy on training data. Thus, while it’s not necessarily the stated goal for using a given classifier, it’s implied that a classifier tries to maximize the accuracy of its predictions [7].
Based on these assumptions, a classifier will almost always produce poor accuracy on an imbalanced data set. Induction algorithms have trouble beating the trivial majority classifier on a skewed data set [4]. A classifier that attempts to classify minority samples correctly will very likely see a significant reduction in accuracy [7], which tells us that the accuracy of the classifier is underrepresenting the value of classification on the minority class [4]. In most cases, we would prefer a classifier that performs well on the minority class, even at the expense of performance on the majority class. Researchers use statistics like the F-measure [4], [5] and area under the receiver operating characteristic, or ROC (AUC), [7] to better evaluate minority class performance.
Researchers have crafted many techniques to combat the class imbalance problem, including resampling, new algorithms, and feature selection. To date, no researchers have compared these different approaches in one study. In this paper, we compare the performance of example resampling, algorithms, and feature selection methods to show
. M. Wasikowski is with the Methods and Research Office, US Army Training and Doctrine Command Analysis Center, 255 Sedgwick Avenue, Fort Leavenworth, KS 66027. E-mail: mike.wasikowski@us.army.mil. . X.-w. Chen is with the Electrical Engineering and Computer Science Department, 2001 Eaton Hall, University of Kansas, 1520 West 15th Street, Lawrence, KS 66045. E-mail: xwchen@ku.edu.
Manuscript received 4 Mar. 2009; revised 12 May 2009; accepted 27 Aug. 2009; published online 25 Sept. 2009. Recommended for acceptance by E. Keogh. For information on obtaining reprints of this article, please send e-mail to: tkde@computer.org, and reference IEEECS Log Number TKDE-2009-03-0107. Digital Object Identifier no. 10.1109/TKDE.2009.187.
1041-4347/10/$26.00 2010 IEEE Published by the IEEE Computer Society
which of these approaches best manages the challenges posed by imbalanced data sets. The data sets, we will examine, have an average ratio around 1:10. With this knowledge, researchers will have a number of good approaches to try with their own data sets.
The newest of the techniques to resolving the class imbalance problem is feature selection. Most research on feature selection metrics has focused on text classification [3], [4], [5]. There are many other applications which would be advantageous to investigate using feature selection. We will look at the performance of different feature selection metrics on microarray, mass spectrometry, text mining, and character recognition applications. We aim to inform data mining practitioners which feature selection metrics would be worthwhile to try and which they should not consider using.
Another problem that can arise in data sets is a small sample size. Induction algorithms need a sufficient amount of data to make generalizations about the distribution of samples. Without a large training set, a classifier may not generalize characteristics of the data; the classifier could also overfit the training data and be misled on test points [9]. Some of the different methods used to combat class imbalance could make the problems with learning on a small data set even worse. In fact, Forman and Cohen [8] showed that when we use very skewed small samples, the best performance is typically achieved by the naı̈ve Bayes and multinomial naı̈ve Bayes algorithms; if the data sets are only marginally skewed, the linear SVM performs the best.
The rest of this paper is organized as follows: Section 2 provides a summary of the various approaches taken to combat the class imbalance problem. Section 3 explains the various learning methods we used with Section 4 rigorously defining the questions we aim to answer in this paper. Section 5 follows with the results of these experiments. Finally, Section 6 ties everything together with our concluding remarks.

2 CURRENT APPROACHES
The two Learning from Imbalanced Data Sets workshops thoroughly explored the approaches to combating the class imbalance problem: sampling, new algorithms, and feature selection. The first was held at the AAAI conference in 2000 [7], and the second was held at the ICML conference in 2003 [10]. Also, Weiss reviewed these approaches in SIGKDD Explorations [11].

2.1 Sampling
Sampling techniques aim to correct problems with the distribution of a data set. Weiss and Provost noted that the original distribution of samples is sometimes not the optimal distribution to use for a given classifier [12], and different sampling techniques can modify the distribution to one that is closer to the optimal distribution.
One simple sampling technique is to obtain more samples from the minority class for inclusion in the data set. However, this is not always possible in real-world applications. In some fields, the imbalance is an inherent part of the data [13]. There are many more people living without cancer at any time than those with cancer. In other fields, the cost of the data gathering procedure limits the
number of samples we can collect for use in a data set and results in an artificial imbalance [13]. Sequencing a person’s genome requires expensive equipment, so it may not be feasible to include a large number of samples on a microarray. Many machine learning researchers simply find that you are limited to the data you have [7].
Other sampling techniques involve artificially resampling the data set. This can be accomplished by undersampling the majority class [14], [15], oversampling the minority class [16], [17], or by combining over- and undersampling techniques in a systematic manner [18]. The end result is a data set that has a balanced distribution. While these methods can result in greatly improved results over the original data set, there are significant issues surrounding their use. Undersampling methods have the potential of eliminating valuable samples from consideration of the classifier entirely. Oversampling methods, whether they duplicate existing samples or synthetically create new samples, can cause a classifier to overfit the data [13]. While many studies have shown some benefits to artificial rebalancing schemes, many classifiers are relatively insensitive to a distribution’s skew [19], so the question of whether simply modifying the class ratio of a data set will result in significant improvement is considered open by some researchers [7].

2.2 Algorithms
A wide variety of new learning methods have been created specifically to combat the class imbalance problem. While these methods attack the problem in different ways, the goal of each is still to optimize the performance of the learning machine on unseen data.
One-class learning methods aim to combat the overfitting problem that occurs with most classifiers learning from imbalanced data by approaching it from an unsupervised learning angle. A one-class learner is built to recognize samples from a given class and reject samples from other classes. These methods accomplish this goal by learning using only positive data points and no other background information. These algorithms often give a confidence level of the resemblance between unseen data points and the learned class; a classification can be made from these values by requiring a minimum threshold of similarity between a novel sample and the class [20]. The two most prominent types of one-class learners are oneclass SVMs [21] and neural networks or autoencoders [20]. Raskutti and Kowalczyk [21] showed that one-class SVMs performed better than two-class SVMs on genomic data and argued that this method generalizes well to imbalanced, high-dimensional data sets. However, Manevitz and Yousef [22] believe that the results from a classifier trained using only positive data will not be as good as the results using positive and negative data. Elkan and Noto [23] agreed and also showed that when a user has negative samples, using them instead as unlabeled samples hindered a classifier’s performance. Thus, unless one has only training samples known to be from one class and no other information, oneclass learners are likely not the best approach.
Boosting schemes, similar to the AdaBoost algorithm, create a series of classifiers all applied to the same data set. The AdaBoost algorithm samples with replacement from a data set with initially equal probabilities for each sample. After each iteration, the probabilities are updated. A
sample that is correctly classified receives a lower probability to be drawn in the next iteration, and a misclassified sample receives a higher probability [24]. The result is that classifiers, further in the series, draw a training set consisting of hard-to-classify samples. Chawla et al. [25] applied boosting to the SMOTE algorithm at each step to change the weight of each sample and adjust for skew in the distribution. Sun et al. [26] used a genetic algorithm to identify the best cost matrix for an imbalanced problem and then applied a standard boosting algorithm to classify the data set.
Cost-sensitive learning methods try to maximize a loss function associated with a data set. These learning methods are motivated by the finding that most real-world applications do not have uniform costs for misclassifications [27]. The actual costs associated with each kind of error are unknown typically, so these methods need to determine the cost matrix based on the data and apply that to the learning stage. Many algorithms do this explicitly, especially with decision trees and rule induction algorithms [27], [1], [28]. Newer cost-sensitive methods have been expanded for use with more than two classes using boosting techniques [29]. The asymmetric boosting algorithm uses the principles of AdaBoost to find the best cost matrix using gradient descent [30].
A closely related idea to cost-sensitive learners is shifting the bias of a machine to favor the minority class [31], [32]. If we have a cost matrix, we can solve the risk function associated with this matrix to obtain the optimal decision boundaries based on the posterior probability of a class given the sample. We can then use this value to set the bias of a classifier accordingly [24].
A new type of SVM developed by Brefeld and Scheffer [33] maximizes the AUC rather than accuracy. They empirically showed that the AUC SVM improved the AUC score established by the standard SVM. However, the standard algorithm was noted to be a Oðn4Þ algorithm and much too slow for anything larger than a trivially small data set. The k-means AUC SVM is an approximation of the exact algorithm that improves the computation time and still gives improved results over the standard SVM. Joachims [34] developed an extension of his program SVMlight, called SVMperf , that allows a user to train linear SVMs in linear time while optimizing on a variety of loss functions, including AUC, precision-recall break-even rate, F1 measure, and the standard accuracy metric.

2.3 Feature Selection
The goal of feature selection, in general, is to select a subset of j features that allows a classifier to reach optimal performance, where j is a user-specified parameter. For high-dimensional data sets, we use filters that score each feature independently based on a rule. Feature selection is a key step for many machine learning algorithms, especially when the data is high-dimensional. Microarray-based classification data sets often have tens of thousands of features [35], and text classification data sets using just a bag of words feature set have orders of magnitude more features than documents [4]. The curse of dimensionality tells us that if many of the features are noisy, the cost of using a classifier can be very high, and the performance may be severely hindered [13].
Because the class imbalance problem is commonly accompanied by the issue of high dimensionality of the data set [13], applying feature selection techniques is a necessary course of action. Ingenious sampling techniques and algorithmic methods may not be enough to combat highdimensional class imbalance problems. Van der Putten and van Someren [36] analyzed the data sets from the CoIL Challenge 2000 and found that feature selection was more significant to strong performance than the choice of classification algorithm and helped combat the overfitting problem the most. Forman [4] noted a similar observation on highly imbalanced text classification problems and stated “no degree of clever induction can make up for a lack of predictive signal in the input space.” Research shows that in high-dimensional data sets, feature selection alone can combat the class imbalance problem [3], [5], [4], [37].
However, another analysis of the CoIL Challenge 2000 by Elkan [38] found that feature selection metrics were not good enough for this task; instead, the interaction between different features also needed to be considered in the selection phase. The biggest flaw that he found with most of the applied feature selection methods is that they did not consider selecting highly correlated features because they were thought to be redundant. Guyon and Elisseeff [39] gave a strong theoretical analysis as to the limits of feature selection metrics. They showed that irrelevant features on their own can be useful in conjunction with other features, and the combination of two highly correlated features can be better than either feature independently.
Wrappers and embedded methods are feature subset selection methods that consider feature interaction in the selection process. They suffer from two primary problems in high-dimensional data sets. First, they cannot find the best feature subset because an algorithm that finds the optimal feature set has a Oð2nÞ runtime and is intractable for high-dimensional data. Second, a subset selection method may find a feature subset that severely overfits the training data and causes even worse performance than the baseline [40]. Using feature selection metrics on highdimensional data sets avoids both of these problems. Metrics have a linear runtime in the size of the feature set. Also, metrics are robust against overfitting because the selected feature set usually has slightly more bias but decreases the variance considerably [39].
Feature selection as a general part of machine learning and data mining algorithms has been thoroughly researched [2], [39], [41], [42], [43], [44], [45], but its importance to resolving the class imbalance problem is a recent development with most research appearing in the previous seven years. In this time period, a number of researchers have conducted research on using feature selection to combat the class imbalance problem [4], [5].
Mladeni c and Grobelnik [3] studied the performance of feature selection metrics in classifying text data drawn from the Yahoo Web hierarchy. They applied nine different metrics to the data set and measured the power of the best features using the naı̈ve Bayes classifier. Their results showed that the odds ratio was nearly universally the best performing metric on the F1, F2, precision, and recall measures; in general, they concluded that the best metrics choose common features and consider the domain and learning method’s
characteristics. This explains why odds ratio would be very good for the naı̈ve Bayes classifier: it tries to maximize the posterior probability of a sample being in a class.
Forman [4] examined the classification power of feature selection metrics on a number of text sets drawn from the TREC competitions, MEDLINE, OHSUMED, and Reuters databases. He used the features selected by different metrics to train linear SVMs and evaluated their performance using accuracy, precision, recall, and the F1-measure. While a small number of metrics saw mild improvements over the performance of an SVM using all of the available features, the best overall performance came from features selected by binormal separation (BNS). Forman’s general finding is that the metrics that performed best selected features that separated the minority class from the majority class well. He also concluded that the evenly balanced form of BNS that selected features with equal weight to true positive rate and false positive rate gave the best performance.
Zheng et al. [5] analyzed how the types of selected features affected classification performance. Positive features are those that indicate membership in a class, and negative features are those that indicate lack of membership in a class. They used the chi-square (CHI), information gain (IG), and odds ratio (OR) algorithms as a starting point to create one-sided and two-sided metrics; one-sided metrics only select positive features on their score, and two-sided metrics select both positive and negative features based on the absolute value of their score. They then compared the performance of the one- and two-sided metrics with different ratios of the best positive and negative features. The ratio selection method resulted in improved performance on the break-even point between precision and recall compared to general one- and two-sided metrics. Thus, both positive and negative features are important to obtain the best possible power from a classifier.
One common problem with many of the standard feature selection metrics used in the previous studies [3], [4], [5] is that they operate on Boolean data. Some metrics, such as information gain and chi-square, generalize well to nominal data, but they break down when handling continuous data. Thus, when one uses a discrete or Boolean feature selection metric on continuous data, its performance is entirely dependent on the choice of the preset threshold used to binarize the data. This threshold determines the confusion matrix’s true positive (TP), false negative (FN), false positive (FP), and true negative (TN) counts.
Consider a classification problem using two different feature sets such that a classifier using the first feature set yields a higher TP count and lower TN count than the second set. However, if we bias the classifier to change its decision threshold, then that classifier may, instead, have a higher TP count and lower TN count for the second feature set. Thus, it is impossible to tell which feature set is better because one threshold will not give us adequate information about the classifier’s performance. This happens because the confusion matrix is an evaluation statistic of a classifier’s predictive power based solely on this one threshold [46].
It would be advantageous to use a feature selection metric that is nonparametric and could use all possible confusion matrices when using ordinal or ratio data. Thus,
it would be possible to find the threshold that results in the highest performance possible. One possibility is the receiver operating characteristic, or ROC curve. This is a nonparametric measure of a system’s power that compares the true positive rate with the false positive rate. We can then quantify the ROC curve using the AUC and apply this score as our feature ranking. The Feature Assessment by Sliding Thresholds (FAST) metric [37] evaluates features using an approximation to the AUC. Both linear SVMs and nearest neighbor classifiers using FAST-selected features saw improved AUC and balanced error rates over the baseline score using all features and the scores achieved using RELIEF and correlation-coefficient-selected features.
Another nonparametric statistic for evaluating the performance of a classifier is the precision-recall (P-R) curve. Davis and Goadrich [47] noted that the ROC curve can sometimes give an overly optimistic idea of the performance of a learning method when the data set is extremely imbalanced. A classifier that has a very strong ROC curve and may appear to be close to perfect can have a rather weak P-R curve that shows much room for improvement. For highly imbalanced data, it may be advantageous to modify the FAST metric to use the area under the P-R curve (PRC) as the evaluation statistic rather than the area under the ROC curve.

3 METHODS
The bulk of this paper examines the performance of various feature selection metrics and how they combat the class imbalance problem. Because, there are two other general approaches to handling this problem, we must also examine the performance of sampling techniques and algorithms; we selected one representative example from each of these approaches to compare to the feature selection methods.
Feature selection metrics are either one-sided or twosided. They differ based on whether they select just positive features or a combination of positive and negative features. Positive features are those that, when present, indicate membership in a class, while negative features indicate nonmembership in a class. A one-sided metric uses the signed value of a feature’s score, and will thus only select positive features. A two-sided metric ignores the sign of a feature’s score and only considers the absolute value; it can select both positive and negative features [5]. The formulas for the feature selection metrics are detailed in Table 1.

3.1 Binary Feature Selection Metrics
The feature selection metrics depicted below can handle either binary or nominal data. Because the data sets we studied consist of continuous data, we must preprocess the data before applying these tests. We used the method described by Guyon and Elisseeff. After finding the mean feature value for the two classes, we set a threshold at the midpoint between the two mean values. The features are then binarized according to this threshold [39].
CHI is a statistical test measuring the independence of a feature from the class labels. It is a two-sided metric. Forman noted that this test can behave erratically when there are small expected counts of features; this is fairly common with imbalanced data sets [4]. While the chi-square test generalizes well for nominal data, it breaks down when testing on continuous data; see Section 2.3 for discussion.
IG measures the difference between the entropy of the class labels and the conditional entropy of the class labels given a feature. This measure is two-sided. Like the chisquare test, it generalizes for nominal data but cannot handle continuous data well for similar reasons; see Section 2.3 for discussion.
OR looks at the odds of a feature occurring in the positive class normalized by the odds of the feature occurring in the negative class. The standard odds ratio is a one-sided metric. If we have a zero count in the denominator, we replace the denominator with 1. This is consistent with how Forman computed his two-sided odds ratio [4]. A pilot study found the one-sided algorithm performed better on our data, but other researchers such as Forman [4] have used the two-sided algorithm. This metric is designed to operate solely on binary data sets.

3.2 Continuous Feature Selection Metrics
The following feature selection metrics are designed to operate on continuous data and do not require any preprocessing of the data to work:
Pearson Correlation Coefficient (PCC) measures the strength and degree of the linear relationship between a feature and the class labels. It is a one-sided metric; a twosided version can easily be created by taking the square value of the scores.
Feature Assessment by Sliding Thresholds (FAST) is the FAST algorithm designed by Chen and Wasikowski to find the ROC curve representing a feature’s predictions for the class labels [37]. It selects those features with the greatest area under the ROC. This is a two-sided metric.
Feature Assessment by Sliding Thresholds (FAIR): P-R uses a modification of the FAST algorithm that instead finds the P-R curve associated with a feature’s predictions for the class labels. Those features with the greatest area under the P-R curve are selected. This is a two-sided metric.
Signal-to-Noise Correlation Coefficient (S2N) measures the ratio of some desired signal (i.e., the class labels) to the background noise in a feature. While this ratio is originally an electrical engineering concept, in the machine learning community, it has been applied to leukemia classification with strong results [48]. It is a one-sided metric.

3.3 Other Approaches
Section 2.1 detailed a wide variety of different sampling techniques that aim to balance the data and make it more
likely that a classifier will make good predictions; likewise, Section 2.2 covered many of the new algorithms constructed specifically to combat the class imbalance problem. We use the resampling method studied by Chawla that combines SMOTE and full random undersampling [17] as our comparison resampling method. Random undersampling has been shown to be one of the best resampling algorithms [49], and SMOTE alleviates the small sample problem by doubling the minority class size. We also use the AUCmaximizing SVM as our comparison new algorithm because it is designed to maximize AUC, one of our evaluation statistics.

4 EXPERIMENTAL PROCEDURE
The experimental procedures are provided in this section. We discuss our choices of induction methods, evaluation statistics, data sets, and questions we aim to answer in this paper.

4.1 Induction Methods
The overall goal of this paper is to show the effectiveness of feature selection in combating the class imbalance problem. Feature selection affects different classifiers differently. While the linear SVM is fairly resistant to feature selection, it can see improved results. Conversely, feature selection has a stronger influence on the 1-nearest neighbor and naı̈ve Bayes classifiers. Because the best feature selection metrics should be able to help an SVM perform better despite the inherent resistance, we evaluated the goodness of feature sets using the linear SVM.

4.2 Evaluation Statistics
On extremely imbalanced data sets, algorithms will be hard pressed to classify points as in the minority because the discriminant scores are often weighted toward the majority class. However, there is usually still some separation in the probabilities between classes. If we were to compare across all possible thresholds, we could quantify the strength of a classifier with a nonparametric measure. The ROC and P-R curves will allow us to find the strength of a classifier at each possible threshold. To quantify these curves with a single statistic, we evaluated the induced classifiers using AUC and PRC.

4.3 Data Sets
Imbalanced data sets show up in many different domains. Many papers have explicitly looked at imbalanced data in single domains, but we have not seen research that experiments on imbalanced data sets from vastly different domains in the same framework. Thus, we used data sets from multiple types of sources to show the effectiveness of feature selection metrics on all kinds of imbalanced data. These sources include microarray and mass spectrometry (CNS, LYMPH, LEUK, OVARY, and PROST), text mining (NIPS), and spoken character recognition (SCR) data sets. The data sets we examined are summarized in Table 2. While there is only one data set for the text mining and character recognition domains, they are each divided into nine one-versus-rest problems for this study. Thus, there are 10 problems in the biological domain, nine in the text domain, and nine in the character recognition domain. In the
NIPS data set, we discarded those features that were present
in less than five samples; that left us with the 7,809 features
stated. No other data sets discarded features. All of the data
sets are publicly available on the author’s Web site.

4.4 Problem Frameworks
We aim to answer six different questions over the course of
this paper:
1. Which feature selection metrics perform best on average? The performance of a feature selection metric depends on the nature of the problem. Some problems may have equal misclassification costs; some problems may want to maximize the ratio of true positives to those samples marked positive; others may simply look for the highest proportion of true positive samples. To that end, we compared each of the seven feature selection metrics using AUC and PRC. These feature selection metrics were tested by training the SVM using 10, 20, 50, 100, 200, 500, and 1,000 features. Because many of the data sets are too small to be split into training and testing partitions, we evaluated each metric using fourfold stratified cross-validation repeated with five different sets of folds for each data set. We also compared the feature selection metrics to the baseline performance of a linear SVM using all features to show the gains in performance from using feature selection. 2. Which feature selection metrics are most likely to perform best for a single data set? Most data mining researchers would not care if a learning method gave great results on average if it did not give good results on their problem of interest. Thus, we need a new framework to better answer this question. Forman [4] proposed an analysis technique to compare various methods on a large number of data sets. First, for each data set, we take the best score achieved by all of the methods for an evaluation statistic. Then, for each method, we find the ratio of data sets for which this method gave scores within a certain tolerance of the best score for each data set. For example, if we allow a tolerance of one percent for a best score of 0.90, 0.895 would be a success, and 0.885 would be a failure. Those feature selection metrics with the highest ratios are considered the closest to optimal. We performed this test using the results from the above framework
with 10 features selected and with the optimal number of features selected. 3. Which feature selection metrics perform best for different domains? Imbalanced data sets come from vastly different applications, and the goals guiding a machine learning approach to these tasks often differ based on the inherent characteristics of their data sets. We divided these problems into the different areas (biological data analysis, text mining, and character recognition) and analyzed each feature selection metric on these subsets of problems like in the first problem framework. 4. How does feature selection compare with sampling techniques and algorithms on the class imbalance problem? As we stated in Section 1, no research has compared the feature selection, resampling, and algorithm approaches to the class imbalance problem. We would like to see whether one of these approaches is the best or if they all perform equally well. We compared the performance of the best feature selection metric from the first framework with the SMOTE/random undersampling hybrid and AUC-maximizing SVM approaches using each of the evaluation statistics. 5. Can the different approaches be used together to get even more improved results? Numerous studies have shown that the sampling, algorithms, and feature selection methods can improve the performance of a classifier on an imbalanced data set. Very little research has been conducted on whether these methods can be combined. The only study we are aware of is by Al-Shahib et al. [50], where they combined wrapper-based feature selection and full random undersampling. They showed that feature selection and undersampling were both important, but the most important factor was the undersampling. Because each of these approaches to the class imbalance problem manages the challenges in a different way, they could result in a greater performance than any individual part could on its own. We used the best feature selection metric from the first framework, the SMOTE/random undersampling hybrid, and the AUC-maximizing SVM together and compared it with the results of each component individually. 6. Which feature selection metrics perform best regardless of the classifier used? Some feature selection metrics work very well with specific learning methods. The odds ratio metric helps the naı̈ve Bayes classifier achieve the best result possible [3]. RELIEF was designed based on a nearest neighbor philosophy [51], [52] and gives the 1-NN more improvement than simple correlation coefficients [37]. C4.5 and other decision tree algorithms intrinsically use information gain as their node-splitting statistic. Finding a feature selection metric that performs well across different induction philosophies would make a data mining researcher’s work much easier. Thus, using the framework from the first problem, we evaluated the feature selection metrics on the nearest neighbor and naı̈ve Bayes classifiers as well. We then
took the mean of the performance of each classifier on each evaluation statistic to compare between feature selection metrics. Those metrics with the highest performance across different classifiers are judged to select the best features.

5 RESULTS
The experimental results are provided in this section. All
experiments were conducted using the MATLAB SPIDER
package and CLOP extensions with the following excep-
tions. Each of the feature selection metrics except for PCC
and S2N were custom implementations by the authors. The
nearest neighbor algorithm was modified to allow for an
ROC analysis based on the difference between the nearest
neighbor and the nearest neighbor from a different class than the closest point. The SVMperf algorithm was used
instead of the regular SVM using the MEX interface and
SPIDER objects developed by Oscar Luaces [53], [54]. All
statistical tests used in comparing different methods were
the paired T-tests.

5.1 Best Average Performance
Figs. 1 and 2 show the average ROC and PRC for the SVM
across the different feature selection metrics as the number
of features varies. Across all possible feature counts, the top
performing metrics, on average, are IG, S2N, and FAST.
These metrics are consistently close to the best average
performance across each of the evaluation statistics with
FAST being the best with less features selected and S2N or
IG being the best with more features selected. Any of these
three metrics would be a good choice for use on an arbitrary
imbalanced data set. There are three main trends we see in these graphs. First, regardless of the evaluation statistic used, FAST performs best by a decent margin for only 10 features selected. This margin is statistically significant at the ¼ 0:05 significance level. This is important, because while we would like to see the best predictive power using feature selection, there are many domains where selecting a large number of features would make later analysis of the selected features impossible. For example, in biological data analysis, we would like to
have a list of genes that influence whether a person has cancer or not. A biologist would likely prefer to see this list be as small as possible while still getting good results. This goal allows us to attain data savings as each of our tested data sets will be shrunk to a fraction of their original size.
We also see a trend in these data sets that 50 features appears to be the point where the best performing feature selection metrics peak across each evaluation statistic. With more than 50 features selected, these metrics see a significant decline in performance. The goal of data mining is to make the best predictions possible; on high-dimensional imbalanced data sets, it appears that we only need to select between 50 and 100 features to attain this peak performance. At the same time, we still see a data savings with feature selection where our tested data sets would be shrunk to hundredths of their previous size.
Finally, except for odds ratio, every single feature selection metric beat the baseline performance for 50 and more features selected; when we discount FAIR as well, the remaining feature selection metrics equalled or bettered the baseline performance with just 10 features selected. When using the linear SVM, not only we will see data savings from extensive feature selection, but we can see significant improvements in performance as well. Feature selection is a good approach to manage the class imbalance problem on high-dimensional data sets when using the SVM.

5.2 Probability of Best Performance
Figs. 3 and 4 show the ratio of data sets for which each metric is within a small percentage of the best score achieved by any feature selection metric for ROC and PRC with 50 features selected. We chose 50 features, as this is the level of best performance from the previous framework. We repeated this test with 10 features to evaluate the first features selected by these methods, but FAST was the clear winner at 1-3 percent tolerance, so the results are not shown here.
We have some trouble deciding if there is a best feature
selection choice. At the highest amount of tolerance
allowed, there is very little discernibility between CHI,
PCC, S2N, IG, and FAST for each evaluation statistic. The
marginally best odds most frequently come from the IG and
S2N feature selection metrics for tolerance levels at
two percent and above. When we allow only one percent
tolerance, FAST is virtually indistinguishable in odds
compared to S2N and IG. Most data mining researchers
would prefer the smallest tolerance possible, so the one
percent level is the most important. Thus, we believe that
for the odds of the best performance possible, any of the IG,
S2N, and FAST metrics would be strong choices.

5.3 Domain Analysis
Figs. 5 and 6 show the average ROC and PRC graphs for the biological data sets. Similarly, Figs. 7 and 8 show the results from the text mining problems and Figs. 9 and 10 show the results from the character recognition problems.
For the biological data sets, there are two conflicting goals: best performance possible, and performance with fewest features for biological analysis. At 1,000 features, IG and CHI have the best performance (though not statistically the best), and OR and FAST tie for the best with ROC. With
only 10 features, FAST is the clear winner as it performs statistically better than every other feature selection metric for both ROC and PRC scores at a significance level of ¼ 0:02, except for the IG-ROC combination. This jump in performance can be attributed to FAST selecting features that better spread out the means of the two classes compared to the first features selected by other metrics.
For the text mining data sets, the big result is that the feature selection metrics did not improve performance over the baseline. This appears to be counter to prior research [3], [5], [4], but those results only found that a few metrics improved performance. The primary difference between the NIPS data set we used and the many data sets experimented on in the previous work on text data is the feature representation. The data sets tested by the previous authors had Boolean features; in contrast, the NIPS data set contains the raw word counts. The features in a Boolean bag-ofwords data set are typically only weakly indicative of the class [55]; we believe that using the raw word count makes these features even more weakly indicative of the class. The feature representation in sparse data sets is extremely important. The discrete nature of the features explains why IG and CHI perform the best on both AUC and PRC: the difference between a word appearing infrequently versus a word showing up frequently is much greater than a word showing up exactly four or exactly five times.
With the spoken character recognition problems, we prefer a small number of features, but for a different reason than the biological analysis sets: the vast majority of features available contribute nothing but noise to the learning algorithm. Thus, we are not constrained to selecting just 10 features. With that in mind, the best feature selection metrics are S2N, FAST, and PCC. These metrics do not discretize the features; converting noisy continuous features into binary features appears to amplify the noise and makes IG and CHI perform worse with a small number of features. In contrast, the continuous metrics were not affected by the noise as much and separated the classes slightly better.

5.4 Analysis of Different Approaches
Fig. 11 compares the S2N algorithm results using 50 features with those achieved using the SMOTE/random undersampling hybrid (SMUS) and the AUC-maximizing SVM (AUCSVM) on AUC and PRC. This figure also includes information about three combination approaches: using S2N with 50 features on the AUCSVM (F/A), using the sampling technique and AUCSVM (S/A), and using all three approaches at once (ALL). The best results were seen using S2N alone. The next best performers are the AUCSVM and the F/A hybrid technique. All three methods using the sampling technique lagged far behind with ALL being the worst. In fact, the performance of the resampling methods is below the baseline with all features.
The results for each of the approaches other than S2N alone reflect that the dimensionality of the experimental data sets is a significant problem to generalization. This is likely due to overfitting; many algorithms overfit on highdimensional data sets because individual samples can be very dissimilar to each other. Even if the imbalance between the classes is addressed, an algorithm may not see the best performance. These results, along with those by Mladeni c and Grobelnik [3], Forman [4], and Zheng et al. [5] all showed that with a standard algorithm, feature selection alone combats the class imbalance problem in highdimensional data sets.

5.5 Feature Selection Metric Effectiveness
Figs. 12 and 13 show the mean of the average performance for the SVM, nearest neighbor, and naı̈ve Bayes classifiers
over each evaluation statistic as we vary the number of features. As well, Figs. 14 and 15 show the average performance for the nearest neighbor algorithm using ROC and PRC, respectively, and Figs. 16 and 17 show the average performance for the naı̈ve Bayes algorithm. Recall that we are interested in showing which feature selection metric performs the best regardless of the classifier a researcher may use.
The first clear trend is that the average performance of each classifier with feature selection is better than the average
baseline performance of each classifier. This is not a surprising result. Feature selection helped the SVM improve, and most researchers believe that the SVM is relatively insensitive to feature selection. In contrast, the nearest neighbor and naı̈ve Bayes classifiers are very sensitive to feature selection, and this is why the best gains over the baseline are larger than those seen in Figs. 1 and 2 with just the SVM classifier compared to Figs. 14, 15, 16, and 17.
The second trend is that there are two feature selection metrics that are most effective, but in different circumstances.
When selecting between 10 and 50 features, FAST is the best performer, and S2N is the second best. When selecting 100 or more features, S2N is the most effective; FAST is the second most effective until we select 1,000 features. FAST being the most effective with a small number of features selected is not very surprising given that it resulted in the biggest boosts at 10 features of any of the tested metrics. Likewise, it’s not surprising that S2N is effective at higher feature counts because its performance on the SVM was near the best. Thus, depending on the number of features desired, FAST or S2N would be a good feature selection metric to use regardless of the classifier choice.
Third, there is a significant drop in performance occurring for each feature selection metric across all evaluation statistics. This drop is large enough that PCC and OR are actually regressing beyond the baseline performance. Additionally, every single feature selection metric, except for FAIR, performed worse when selecting 1,000 features than when selecting just 10 features; some of these drops in performance occur as early as 100 features are selected. It is vital to choose the number of selected features carefully. With too few features, an algorithm’s performance may not be as good as possible. With too many features, you are more likely to see overfitting in your classifier. We recommend starting with 50 features as a baseline and empirically comparing test results from different feature counts to find the best feature count.
Finally, compare Fig. 1 with Fig. 16. When using feature selection, the performance of the linear SVM is actually substandard to the performance of the naı̈ve Bayes algorithm. The best results at 10 and 20 features selected are found using the naı̈ve Bayes classifier with FAST features. This result follows the findings of Forman and Cohen [8] on how different classifiers respond to different distributions of data. He found that the SVM gave the best results when the classification task was mostly balanced, but when the data set had a very small number of samples in the minority class, like the data sets we studied here, or a rather extreme skew between the two classes, the naı̈ve Bayes and multinomial naı̈ve Bayes algorithms performed the best. If the goal is to make the best generalizations possible, the simplest algorithms are often the best choice.

6 CONCLUSION
Machine learning and data mining techniques have problems handling imbalanced data sets that frequently arise from real-world applications. Researchers have developed many new methods for combating the class imbalance problem. Feature selection has been used frequently to reduce the dimensionality of a data set, but its merit in imbalanced problems has only recently been studied.
The biggest finding from this paper is that feature selection is beneficial to handling most high-dimensional imbalanced data sets. This holds across the biological and character recognition domains, as well as for each of the seven feature selection metrics we tested. For the text mining domain, other researches [3], [4], [5] have found improved results on other data sets with feature selection. We believe that this happens because the original data sets
provide samples that are too disparate to make generalizations about groups of the samples. When we reduce the number of features in the data set to approximately the same order of magnitude as the samples, we see a significant jump in performance regardless of the classifier used. At this point, each of the induction algorithms tested could make generalizations about different clusters of points. Thus, feature selection is paramount to achieving the best possible results in learning from high-dimensional imbalanced data.
There is still a lot of room for improvement. The best average PRC score achieved in this experiment was under 0.6. For a problem, like determining whether somebody has cancer based on their gene profile, this level of precision is extremely small and useless as a test. If we aim to learn from small samples, we need to find ways to mitigate this problem even further. One potential path of future research would be to use information from related problems to guide the construction of a kernel for an SVM after feature selection has been used. The KernelBoost algorithm is an implementation of this idea that can work on semisupervised and unsupervised data sets; thus, we would allow the use of unlabeled data in building a better classifier [56].
Improvement may also be found by using more optimal feature selection methods. Koller and Sahami [57] showed that the optimal feature ranking can be calculated using Markov blankets in a backward substitution algorithm; Fleuret [58] and Peng et al. [59] derived two approximations to this optimal ranking. However, they are forwardselecting methods that use the previously selected features to guide their search for the next feature to add to the feature set. Additionally, these methods were not designed to handle imbalanced data. All of the metrics, investigated in this study, have been used to varying degrees of success on imbalanced data. Future research should compare the performance of more optimal feature ranking methods with standard feature selection metrics.
One interesting result from this study is that resampling did not improve performance at all. Clearly, this should not be taken as evidence that the resampling approaches do not work. The survey of resampling techniques by Van Hulse et al. [49] showed that almost every resampling technique they investigated resulted in some improvement over no resampling. However, the data sets he examined varied in size from just over 200 samples to the tens of thousands. It appears that resampling approaches work better on larger data sets; future research should examine this issue specifically.
Additionally, we found that the AUC-maximizing SVM did not improve performance at all. According to Forman’s analysis of learning from small samples [8], this may be an issue with the use of stratified cross-validation rather than any condemnation of the resampling and algorithm approaches. Most of the learning algorithms that researchers use today were originally designed and tested on large, fully balanced data sets. Algorithms trained with real-world data sets must be able to handle potentially large mismatches between the training and test distributions; the development of algorithms that are less distribution-specific is an open research problem.
One final issue with this work is that many imbalanced data sets are not small samples, many others are not highdimensional, and still others have very large imbalances between the classes. In these cases, the results found in this experiment do not hold. The primary strength of feature selection metrics, that they are linear with respect to the feature dimensionality, is negated when dealing with small feature sets. Different data sets that are larger, have fewer features, or have higher skew, likely play more to the strengths of algorithms and resampling methods. Varying levels of noise may also affect the performance of each class of approaches. Future research should explicitly compare and contrast the size, skew, noise, and dimensionality of imbalanced data sets to determine the influence each of these characteristics has on the performance of resampling methods, algorithms, and feature selection methods.

ACKNOWLEDGMENTS
This work is supported by the US National Science Foundation Award IIS-0644366. The authors would like to thank Oscar Luaces for his assistance in compiling the MEX interfaces for SVMperf .

References
[1]C. ElkanThe Foundations of Cost-Sensitive Learning,Proc. 17th Int’l Joint Conf. Artificial Intelligence,2001
[2]I. Guyon,J. Weston,S. Barnhill,V. VapnikGene Selection for Cancer Classification Using Support Vector Machines,Machine Learning,2002
[3]D. Mladeni c,M. GrobelnikFeature Selection for Unbalanced Class Distribution and Naive Bayes,Proc. 16th Int’l Conf. Machine Learning,1999
[4]G. FormanAn Extensive Empirical Study of Feature Selection Metrics for Text Classification,J. Machine Learning Research,2003
[5]Z. Zheng,X. Wu,R. SrihariFeature Selection for Text Categorization on Imbalanced Data,ACM SIGKDD Explorations Newsletter,2004
[6]D. Casasent,X. ChenFeature Reduction and Morphological Processing for Hyperspectral Image Data,Applied Optics,2004
[7]G. Forman,I. CohenLearning from Little: Comparison of Classifiers Given Little Training,Proc. Eighth European Conf. Principles and Practice of Knowledge Discovery in Databases,2004
[8]Y.j. Cui,S. Davis,C.k. Cheng,X. BaiA Study of Sample Size with Neural Network,Proc. Third Int’l Conf. Machine Learning and Cybernetics,2004
[9]G. WeissMining with Rarity: A Unifying Framework,ACM SIGKDD Explorations Newsletter,2004
[10]G. Weiss,F. ProvostLearning when Training Data Are Costly: The Effect of Class Distribution on Tree Induction,J. Artificial Intelligence Research,2003
[11]M. Kubat,S. MatwinAddressing the Curse of Imbalanced Data Sets: One Sided Sampling,Proc. 14th Int’l Conf. Machine Learning,1997
[12]X. Chen,B. Gerlach,D. CasasentPruning Support Vectors for Imbalanced Data Classification,Proc. Int’l Joint Conf. Neural Networks,2005
[13]M. Kubat,S. MatwinLearning When Negative Examples Abound,Proc. Ninth European Conf. Machine Learning (ECML1997
[14]N. Chawla,K. Bowyer,L. Hall,P. KegelmeyerSMOTE: Synthetic Minority Over-Sampling Technique,J. Artificial Intelligence Research,2002
[15]A. Estabrooks,T. Jo,N. JapkowiczA Multiple Resampling Method for Learning from Imbalanced Data Sets,Computational Intelligence, vol. 20,2004
[16]C. Drummond,R.C. HolteExploiting the Cost (In)sensitivity of Decision Tree Splitting Criteria,Proc. 17th Int’l Conf. Machine Learning,2000
[17]A. Raskutti,A. KowalczykExtreme Rebalancing for SVMs: A SVM Study,ACM SIGKDD Explorations Newsletter,2004
[18]L.M. Manevitz,M. YousefOne-Class SVMs for Document Classification,J. Machine Learning Research,2001
[19]C. Elkan,K. NotoLearning Classifiers from Only Positive and Unlabeled Data,Proc. ACM SIGKDD2008
[20]N. Chawla,A. Lazarevic,L. Hall,K. BowyerSMOTEBoost: Improving Prediction of the Minority Class in Boosting,Proc. Seventh European Conf. Principles and Practice of Knowledge Discovery in Databases (PKDD),2003
[21]Y. Sun,M. Kamel,Y. WangBoosting for Learning Multiple Classes with Imbalanced Class Distribution,Proc. Sixth Int’l Conf. Data Mining,2006
[22]P. DomingosMetaCost: A General Method for Making Classifiers Cost-Sensitive,Proc. ACM SIGKDD1999
[23]T. Fawcett,F. ProvostAdaptive Fraud Detection,Data Mining and Knowledge Discovery,1997
[24]N. Abe,B. Zadrozny,J. LangfordAn Iterative Method for Multi-Class Cost-Sensitive Learning,Proc. ACM SIGKDD2004
[25]H. Masnadi-Shirazi,N. VasconcelosAsymmetric Boosting,Proc. 24th Int’l Conf. Machine Learning,2007
[26]K. TingThe Problem of Small Disjuncts: Its Remedy on Decision Trees,Proc. 10th Canadian Conf. Artificial Intelligence,1994
[27]U. Brefeld,T. SchefferAUC Maximizing Support Vector Learning,Proc. Int’l Conf. Machine Learning (ICML) Workshop ROC Analysis in Machine Learning,2005
[28]T. JoachimsTraining Linear SVMs in Linear Time,Proc. ACM SIGKDD2006
[29]H. Xiong,X. ChenKernel-Based Distance Metric Learning for Microarray Data Classification,BMC Bioinformatics,2006
[30]P.V. der Putten,M. van SomerenA Bias-Variance Analysis of a Real World Learning Problem: The CoIL Challenge 2000,Machine Learning,2004
[31]M.X. ChenWasikowski, “FAST: A ROC-Based Feature Selection Metric for Small Samples and Imbalanced Data Classification Problems,Proc. ACM SIGKDD2008
[32]C. ElkanMagical Thinking in Data Mining: Lessons from CoIL Challenge 2000,Proc. ACM SIGKDD2001
[33]I. Guyon,A. ElisseeffAn Introduction to Variable and Feature Selection,J. Machine Learning Research,2003
[34]J. Loughrey,P. CunninghamOverfitting in Wrapper-Based Feature Subset Selection: The Harder You Try the Worse It Gets,Proc. 24th SGAI Int’l Conf. Innovative Techniques and Applications of Artificial Intelligence,2004
[35]J. Weston,S. Mukherjee,O. Chapelle,M. Pontil,T. Poggio,V. VapnikFeature Selection for Support Vector Machines,Advances in Neural Information Processing Systems,2000
[36]X. ChenAn Improved Branch and Bound Algorithm for Feature Selection,Pattern Recognition Letters,2003
[37]X. Chen,J.C. JeongMinimum Reference Set Based Feature Selection for Small Sample Classifications,Proc. 24th Int’l Conf. Machine Learning,2006
[38]L. Yu,H. LiuEfficient Feature Selection via Analysis of Relevance and Redundancy,J. Machine Learning Research,2004
[39]P. Pudil,J. Novovicova,J. KittlerFloating Search Methods in Feature Selection,Pattern Recognition Letters,1994
[40]J. Davis,M. GoadrichThe Relationship between Precision- Recall and ROC Curves,Proc. 23rd Int’l Conf. Machine Learning,2006
[41]J.V. Hulse,T.M. Khoshgoftaar,A. NapolitanoExperimental Perspectives on Learning from Imbalanced Data,Proc. 24th Int’l Conf. Machine Learning,2007
[42]A. Al Shahib,R. Breitling,D. GilbertFeature Selection and the Class Imbalance Problem in Predicting Protein Function from Sequence,Applied Bioinformatics,2005
[43]K. Kira,L. RendellThe Feature Selection Problem: Traditional Methods and New Algorithm,Proc. Ninth Int’l Conf. Machine Learning,1992
[44]I. KononenkoEstimating Attributes: Analysis and Extensions of RELIEF,Proc. Seventh European Conf. Machine Learning,1994
[45]O. LuacesMEX Interface for SVMperf,http://web.me.com/ oluaces/,2008
[46]O. LuacesSVMperf MATLAB Spider Object,http:// web.me.com/oluaces/,2008
[47]M. Dredze,K. Crammer,F. PereiraConfidence-Weighted Linear Classification,Proc. 25th Int’l Conf. Machine Learning,2008
[48]T. Hertz,A.B. Hillel,D. WeinshallLearning a Kernel Function for Classification with Small Training Samples,Proc. 23rd Int’l Conf. Machine Learning,2006
[49]D. Koller,M. SahamiToward Optimal Feature Selection,Proc. 13th Int’l Conf. Machine Learning,1996
[50]F. FleuretFast Binary Feature Selection with Conditional Mutual Information,J. Machine Learning Research,2004
[51]H. Peng,F. Long,C. DingFeature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy,IEEE Trans. Pattern Analysis and Machine Intelligence,2005
