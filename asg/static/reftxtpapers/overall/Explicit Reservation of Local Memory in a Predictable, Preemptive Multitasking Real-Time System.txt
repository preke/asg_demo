Explicit Reservation of Local Memory in a Predictable, Preemptive Multitasking Real-Time System
Jack Whitham,Neil C. Audsley
neil@cs.york.ac.uk

Carousel is experimentally evaluated using a simulator. We demonstrate that preemption has no effect on task execution times, and that the Carousel technique compares well to the conventional approach to handling interference, where worst-case interference costs are simply added to the worst-case execution times (WCETs) of lower-priority tasks.
I. INTRODUCTION
A key issue for predictable preemptive multitasking system implementations is bounding the cost of context switches. A context switch occurs when one task stops executing and another begins [1]. Simple real-time scheduling models assume that context switching only imposes a small, fixed cost. In a sufficiently simple system this is true, but when task execution times are affected by the state of CPU components such as caches, it is not [2]. Preemptive multitasking allows any task to be interrupted by a higher-priority task during execution, and when the original task resumes, stateful components including cache will be in a different state. In this paper, we refer to this phenomenon as inter-task interference. It may change the execution times of preempted tasks. Figure 1 illustrates a preemption resulting in some inter-task interference.
For the remainder of the paper, we concentrate exclusively on local RAM as the stateful component. Local RAM can be implemented as either cache or scratchpad memory (SPM) and is high-speed, low-latency memory, physically located close to the CPU (Figure 2). Other components such as branch prediction units can cause similar inter-task interference effects [3], [4], but these are outside the scope of this paper.
Three possible solutions can be used to deal with inter-task interference. Firstly, we can statically partition the stateful components so that each task gets a small, reserved portion of the local RAM (Figure 3). Cache partitioning is an example [5] which eliminates inter-task interference by locking cache lines used by other tasks. Secondly, we can explicitly save and
restore the component state when tasks begin and end. This is the approach used for the CPU register file. Finally, we can ignore the component state at runtime and instead statically bound the worst-case effects of inter-task interference, which is possible for caches under some circumstances [6]–[8].
These solutions can be compared by considering both their limitations and their effect on the schedulability of the realtime system as a whole. A real-time system is schedulable if every task is guaranteed to meet its deadline [1]. Schedulability analysis is used to check this property. It relies on task properties such as their priorities and worst-case execution times (WCETs): the maximum amount of CPU time required to complete execution, determined by WCET analysis [9].
The static partitioning approach leads to larger WCETs and hence schedulability is reduced. This is because each task has an optimal memory requirement [10]. If this is not available within its partition, the WCET is increased. Partitioning is therefore an impractical approach for large numbers of tasks, because the local RAM space is not shared efficiently.
The bounding approach is necessarily pessimistic as it is not generally possible to know exactly which cache blocks will be evicted or needed again. The supersets of useful and/or evicted cache blocks can be computed for tasks [6] as a safe but inexact upper bound. Again, this leads to larger WCETs, or at least, larger WCET estimates: because while the WCET may not be increased in reality, the safe upper bound determined by analysis certainly is. Hence, schedulability is reduced. Bounding is also only suitable for timing-compositional systems in which interference can be accounted by simple addition [4], and it is only suitable for cache, not SPM.
The explicit save/restore approach can also reduce schedulability, because additional time is required for saving and restoring the local RAM state. If this activity is performed within the context switch (i.e. along with saving and restoring the CPU register values) then the schedulability analysis will consider it a global, static cost applying to every context switch. This would be undesirable because of the size of local RAM, with a typical minimum of several kilobytes. Schedulability would be dramatically reduced by incorporating such a large cost.
However, if the save/restore process is explicit, it can be carried out within a task. Figure 4 illustrates this: by the time
1080-1812/12 $26.00 © 2012 IEEE DOI 10.1109/RTAS.2012.19
3
Task 1 resumes, Task 2 has restored the local RAM state. Carousel, proposed in this paper, is based on the principle that each task should pay the reservation cost of any local RAM that it wishes to use. This is the cost of saving the local RAM state (the first step carried out by any task) and the cost of restoring it (the last step). Carousel is a practical implementation of the explicit save/restore approach.
With Carousel, the inter-task interference is zero, much like a partitioning approach. But Carousel allows each task to use as much (or as little) local RAM space as it wants. The reservation cost is internalized as part of the WCET of the task that actually uses the memory, rather than being treated as an externality affecting the WCET of other tasks, as in bounding approaches.
Carousel is useful for any priority-based real-time scheduling paradigm in which task execution is strictly nested, i.e. once a task starts to execute, no task of lower priority can run until the higher priority task completes its execution. In particular, it may be used with Baker’s stack resource protocol (SRP) [11], and for the stack-based function call protocol typically used by programming languages [12]. This is because the SRP ensures that a task is never blocked after it commences execution (unlike the priority ceiling protocol [13]). This property means that all tasks can share a common stack. The SRP can be used with fixed-priority and earliest deadline first (EDF) schedulers. Furthermore, Carousel blocks can be used as cache or SPM, which may be useful in mixed-criticality systems mixing hard real-time and non realtime tasks.
After presenting related work in section II, this paper gives a problem analysis (section III) leading to the Carousel hardware and software (section IV). A suitable schedulability analysis technique is given (section V) followed by some experiments (section VI) which demonstrate that task execution times are completely unaffected by preemption. Section VII describes some possible improvements, and section VIII concludes.

II. RELATED WORK
Inter-task interference may occur whenever tasks share a stateful resource such as a cache [14] (e.g. Figure 1).
The problem of intra-task interference due to cache state is now quite well-known and has been studied thoroughly. Cache WCET analyses model the state of a cache at each point within a task in order to estimate the worst-case miss count, and hence the maximum execution time [4], [9], [15]. Each cache state is dependent on earlier cache states. Earlier task activity may result in subsequent activity producing either a hit or a miss. This dependence is a form of interference, but intra-task interference, since it occurs between one part of a task and another.
Inter-task interference has also been examined [5], [14]. This occurs between two or more tasks in a multitasking system. When two or more tasks share a cache, activity in one task can disturb data used by the other, producing hits or misses at unpredictable times. Inter-task analysis is pessimistic because the exact set of evicted and/or useful cache blocks of tasks cannot usually be computed offline.
Earlier work has prevented inter-task interference entirely by static partitioning: reserving local RAM space for each task [5], [16]. Each task is only permitted to update its own partition (Figure 3). The remainder is locked [14]. While the size of the partitions can vary between tasks, and non realtime tasks can share a single partition, the assignment is static. Tasks cannot use more than their fixed share of local RAM, not even temporarily. This is a problem, because tasks may have a suboptimal local RAM allocation [10]. This situation becomes a near-certainty as the number of tasks increases.
Consequently, some researchers have suggested allowing inter-task interference, but bounding its impact. A number of approaches are evaluated in [6]. They are suitable for timing-compositional systems where interference does not
cause any timing anomalies [4]. They incorporate the cost of reloading evicted cache blocks into the worst-case response time equation [7].
A third approach involves explicitly saving and restoring the state of local RAM. This idea has been successfully applied within tasks for SPM [17], [18]. It avoids intra-task interference by ensuring that the state of local RAM is known at each point within the task. But it is not trivial to expand the idea to multitasking. The large size of local RAM (several kilobytes or more) prevents saving and restoring the entire state on each context switch.

III. PROBLEM ANALYSIS
Dynamic interference acts in the “wrong direction”, in that the preempted task pays the additional cost due to interference (Figure 1). In order to bound interference, we must assume the worst-case number of preemptions, and also assume the worst results from each preemption. Pessimism is essentially certain.
The other direction, where the preempting task pays the additional cost, would be preferable (Figure 4). In this model, the preempting task must save and restore the state of any local RAM it wishes to use. The cost is incurred once per preemption, the preempted task’s execution time is unaffected, and pessimism is avoidable. In this way, inter-task interference can be eliminated entirely while still allowing each task to use any amount of local RAM.
This idea is simple, but its implementation is tricky. Firstly, we are obliged to manage scheduling with a stack policy, so that tasks start and complete in last-in first-out (LIFO) order. If we use any other policy, we will need a way to resume any task after any preemption. In that arrangement, when a task τi completes, we cannot just restore the state of the local RAM as it was before τi started, because this assumes that the previous task and the next task are both τj . With a nonLIFO task ordering, the next task may be τk = τj , requiring a wholly different state to be restored, potentially of any size. We absolutely require the swapping cost to be constant for each task, so this will not do. Fortunately, scheduling policies that assure the required LIFO ordering are already well-known and in common use, having been described by Baker as the stack resource protocol (SRP) as early as 1991 [11].
A second problem is apparent if tasks need to communicate via shared memory, which is very likely in any practical system. If one task may access memory updated by another, then we have to ensure that both tasks have a coherent view of that memory. We must ensure this even if one task is “swapped out”, and if one or both tasks copy the relevant data into local RAM. The issues are tricky, but they have already been investigated during our earlier work on the scratchpad memory management unit (SMMU) [19]. The SMMU gave a logical address space to both local RAM and external RAM, so that blocks could be swapped between the two without changing their logical address. This proved an effective solution, dealing with many edge cases related to pointer aliasing.

IV. CAROUSEL
We propose an extension to local RAM named Carousel. With Carousel, inter-task interference is eliminated. But unlike a partitioning approach, each task can use any amount of local RAM.
In memory, Carousel acts as a stack (Figure 5). The stack elements are blocks of fixed size 2x bytes. The top n blocks are stored in local RAM. The remaining blocks are stored in external RAM.
Starting a new task τi involves allocating pi blocks on Carousel. These blocks are pushed onto the top of the stack, reserving them for τi, while pi blocks are swapped out to external RAM (Figure 6). As always, the top n blocks remain in local RAM, and the remainder are in external RAM.
When a task ends, the process is reserved. pi blocks are popped from the top of the stack, and pi blocks are swapped in from external RAM (Figure 7). Again, the top n blocks remain in local RAM.
pi is task-specific. Each task τi will have an optimal pi that minimizes either the average execution time or the worstcase execution time (WCET), noting that the time required to reserve pi blocks (i.e. swap in and out) is incorporated into the execution time.

A. Task Invocation Procedure
When a task τi is invoked on a Carousel-architecture machine, the following steps are taken by the real-time operating system (RTOS) in order to execute τi (Figure 8):
1) Save registers used by previous task τj (context switch) 2) Swap out Carousel blocks T through (T + pi) mod n,
where T is the top of Carousel’s stack (Figure 5) and n is the total number of blocks
3) Set T = (T + pi) mod n 4) Call task τi 5) Set T = (T − pi) mod n 6) Swap in Carousel blocks T to (T + pi) mod n 7) Restore registers used by τj (context switch)
The process is re-entrant: a further task τk can be started during step 4. This may happen any number of times; τk may also be preempted, or initiate subtasks (i.e. call methods).

B. Overhead of Carousel
As Carousel shifts the cost of reserving local RAM for task τi onto task τi itself, the execution time of any task preempted by τi is unaffected by that preemption. There is no inter-task interference: the execution time of the lower-priority task is the same regardless of how many times τi preempts it1.
The cost of reserving the local RAM is proportional to the amount of space required. Steps 2 and 6 (Figure 8) operate in O(pi) time, while steps 1, 3, 5, and 7 operate in O(1) time. Therefore, the overhead of Carousel can be expressed precisely as a linear expression of the form ap+ b.
1Here, “execution time” is defined conventionally for real-time systems theory as the CPU time used by one task [1]. By definition, τi’s execution time explicitly excludes any time when any other task is running. The minimum possible execution time for a task is its best-case execution time (BCET), while the maximum possible execution time is its worst-case execution time (WCET).
By incorporating this cost into the execution of τi, Carousel’s model differs from earlier approaches that considered interference as separate from the execution times of tasks [6] or as part of preempted tasks [8]. It is also unlike the approach of considering the whole local RAM as context, which increases the cost of every context switch. Though a Carousel task could use the entire local RAM, there is no requirement to do so, and tasks can choose their memory usage to minimize execution time. Lastly, it is unlike a partitioning approach, which would avoid both interference and swapin/swap-out costs but prevent any task using more than a small, statically reserved area of local RAM space [5], [16].
The reserved space for τi, 2xpi bytes, is typically larger than the space that is actually required because of the need to round up to the nearest multiple of blocks. Some SPM space is therefore unused; some data is transferred unnecessarily. There is a tradeoff between block size and system performance which is not examined within this paper.

C. Hardware Design
In its most basic form, Carousel is implemented using three components (Figure 9), which we describe for a generic 32-bit CPU2. They are (1) an SPM of size 2xn (the local memory), (2) n memory-mapped registers of width 32− x named r0 to rn−1, and (3) a translation unit. The SPM is logically divided into n blocks, each of size 2x bytes.
The translation unit receives addresses, A, from the CPU. These are addresses for accessing code or data, produced in the course of instruction fetches and load or store operations. The addresses are split at bit x, so we have:
Ahi = A 2x Alo = A mod 2x (1)
Each Ahi is compared against all n registers in parallel. If some ri = Ahi, then the memory access is a hit, and is redirected to SPM. The new address is:
ASPM = Alo + 2xi+ S (2)
(Where S is the base address of the SPM.) If ∀i.ri = Ahi, then the memory access is a miss, and it passes to external RAM. This is slow, and uses more energy
2Carousel is not a CPU-specific technology, but certain CPU designs may themselves be a source of inter-task interference because of stateful components such as branch predictors. [3] gives an overview of problematic CPU designs.
than an SPM access, so tasks achieve better performance by mapping code and data to SPM.
The purpose of the translation unit is to ensure that code and data retains the same logical address A whether it is in SPM or external memory. This simplifies software design [19].
Carousel allows software to access the SPM and r0 to rn−1 directly, as well as via the translation unit. This is used to swap blocks in and out of local memory at the beginning and end of each task. Tasks can also swap blocks during execution if necessary.

D. Software Design
Carousel’s supporting software needs to be able to copy data between SPM and external RAM. We define a dma copy(Ad, As, s) method which copies s bytes from [As : As + s] to [Ad : Ad + s].
The open(A, i) method assigns the data at address A to block i. Two steps are required:
1) Set ri = A 2) dma copy(S + 2xi, A, 2x)
The close(i) method writes back the data in block i to its associated external memory address ri.
1) dma copy(ri, S + 2xi, 2x) 2) Set ri = UNUSED
The swap out(p) method swaps out Carousel blocks T through T + p− 1 so that they can be used by a new task (Figure 6). It is necessary to store the values of rT through rT+p−1 so that they can be restored later, so stack operations push and pop are introduced:
1) Set j = 0 2) push(rj+T mod n) 3) close(j + T mod n) 4) Set j = j + 1 5) If j < p goto step 2
The swap in(p) method restores Carousel blocks to their former positions, undoing an earlier swap out (Figure 7):
1) Set j = p 2) Set j = j − 1 3) open(pop(), j + T mod n) 4) If j > 0 goto step 2
These system methods are enough to implement the Carousel functionality, but a further method-invoking method is extremely useful. This executes an application method after
opening its code and stack space. call method takes three arguments: Ai (the address of the target method for τi), yi (the code size of τi), and zi (the stack size of τi). The method invoking process is shown in Figure 10.
Two simple improvements to this basic design are used within our experiments. Firstly, since code is read-only, the close operation for code blocks does not need to write them back to external RAM. They can simply be discarded. Secondly, since stack data is only valid for addresses above the current stack pointer, the open and close operations for stack blocks do not need to access external RAM at all. However, swap in and swap out must always copy blocks, regardless of their contents.

E. Prototype System Architecture
We constructed a prototype of the Carousel system within a simulator. Based on our existing FPGA experience, we know that the simulated system can be translated to FPGA hardware, but a simulator can be built more quickly, which is valuable for experimentation. Furthermore a simulator provides an easy way to tweak design parameters such as n (the number of Carousel blocks) and 2x (the size of each Carousel block) during experiments. The simulator includes a Microblaze CPU [20], an interrupt timer, an external RAM, a small SPM for the OS, a DMA controller, and Carousel.
The OS is Carousel OS, a prototype RTOS implementing the functions listed in section IV-D and specifically designed for the Carousel architecture. This RTOS cannot easily be stored in Carousel because it must remain in local RAM at all times. It is placed in a separate SPM.
The simulator includes some assumptions about timing, based on real FPGA hardware. The simulated Microblaze CPU uses the same execution times as the real CPU as documented
in [20]. Bus operation times are chosen to be typical of embedded systems (Table I). The bus used in our experiments imposes an overhead of 49 clock cycles for every transaction, plus a further overhead of 1 clock cycle for every 4 bytes in the transaction (rounding up). The maximum transaction size is 64 bytes: the same limit as that imposed by PLB, the Microblaze system bus [20].
These settings mean that it takes 50 clock cycles to load or store a single word in external RAM, and a single clock cycle to load or store a word in local RAM. Transferring 32 bytes with dma copy takes 324 + 49 = 57 clock cycles; transferring 64 bytes takes 644 + 49 = 65.

V. SCHEDULABILITY ANALYSIS
Schedulability analysis is straightforward with Carousel. We consider steps 2 through 6 within Figure 8 as part of the task τi. This means that the cost of swap in and swap out are incorporated into Ci, along with the whole of call method if used (Figure 10).
If there were no static context-switch cost at all, i.e. the RTOS scheduler and interrupt handler operate in zero time, then the basic task scheduling equation for the worst-case response time R applies [1]:
Ri = Ci +Bi + ∑
j∈hp(i) Ri Tj Cj (3)
In that equation, Ri represents the worst-case response time for τi, hp(i) is the set of tasks with higher-priority than τi, and Ti represents task period. Bi is the blocking time: the longest time for which a lower priority task locks a resource (including Carousel) that is shared with τi or any task j ∈ hp(i). Each task τi also has a deadline Di where Di ≤ Ti.
As described in [1], the equation is easily refined to incorporate context switch costs to a task (CS1) and from a task (CS2), provided that these are constant3. For Carousel, they are indeed constant, because all variable task-switching costs are incorporated into the cost of running the higher-priority task. The final equation is:
Ri = CS 1+CS2+Ci+Bi+
∑
j∈hp(i) Ri Tj (CS1+CS2+Cj)
(4) The system is considered schedulable if ∀i.Ri ≤ Di,
3This paper does not consider clock handler overheads as discussed in [1].
The blocking time Bi in equation 4 includes Carousel as it is a resource shared by all tasks. The Carousel operations (open, swap in, etc.) are critical sections: they cannot be interrupted, not even by the RTOS. The worst-case blocking time for task preemption is incurred when the scheduling event occurs right at the beginning of either the longest resource access or the longest critical section created by Carousel operations, i.e. the point marked on Figure 11. That critical section incorporates a complete context switch (from and to, CS2 and CS1), plus close, swap in, swap out, and open. For all τi except the lowest-priority task τN , Bi is greater than or equal to the execution time of this critical section.

VI. EXPERIMENTS
Our experiments use the prototype system architecture described in section IV-E. Source code for experiments may be downloaded from http://www.cs.york.ac.uk/rts/rtslab/.

A. Benchmark Tasks
We took some sample applications from the Mälardalen Real-time Technology Center (MRTC) benchmarks [21]. These have the useful property of being single-path, so any execution always produces the same execution time provided that there is no interference from other tasks.
We excluded benchmarks that use floating-point, and benchmarks requiring more than a combined 4kbytes of code and data memory for efficient (in-SPM) execution (e.g. adpcm, edn). Furthermore, we excluded benchmarks that can be optimized to nothing (e.g. loop3, fac) as they take no input and have no effect on RAM. The list of remaining benchmarks is shown in Table II. In general, larger benchmarks could be accommodated by dynamic SPM allocation, e.g. [17], [18]. Note that a cache/SPM partitioning strategy would require more than 16kb of local RAM to accommodate all the tasks, even in this small-scale example!
Each benchmark was manually adapted for use with SPM, and hence Carousel. The SPM usage scheme is very simple: all of the code is stored in SPM along with the call stack, and frequently-used global data is stored in SPM where possible. Most of the tasks consist of a single method once optimized for size, inlining methods that are only called from one place.
More complex techniques were only needed in three cases. These reallocate the Carousel blocks dynamically within the task, changing the code and data during execution. An exact
description is outside the scope of this paper, where experiments require only that tasks make use of Carousel. However, a short description follows.
The crc and matmult benchmarks used library subroutines more than once. These could not be inlined without increasing the code size, and were therefore invoked as subtasks using the method-invoking method (section IV-D). The matrices multiplied within the matmult benchmark also proved too large to be loaded into Carousel, but it was possible to load a few rows at a time into SPM through a loop tiling optimization of the sort described in [22]. Though necessary changes were applied by hand in this case, a dynamic SPM allocator (e.g. [17], [18]) could have done some of the required work.

B. Assumptions
Using the simple strategy of mapping all code and stack data to SPM, at least 2kbytes of space are required for some tasks (e.g. compress, Table II). We set the total size of the Carousel blocks (n2x) accordingly.
Experience with evaluating the SMMU tells us that we cannot set n, the number of blocks, too high because all comparisons must be performed in parallel and this can affect the maximum frequency of the CPU. n = 16 was found to be practical [19]. This leaves x = 7 for 128-byte blocks. The resulting allocation of Carousel blocks is as shown in Table III. Note that there is no way to fit all tasks in Carousel simultaneously. Furthermore, some tasks (crc, matmult) are themselves split across more than one method.

C. Carousel Overheads
On Microblaze, Carousel OS requires 3.2 kbytes of SPM space for its code and data tables. This includes all device drivers and the scheduler. The (observed) worst-case value for CS1 is 401 clock cycles, and the (observed) worst-case value for CS2 is 387 clock cycles.
To measure the swapping overhead for different numbers of Carousel blocks, we introduced a trivial task that immediately returns - but nevertheless requires y blocks for code and z blocks for stack. Table IV shows how the execution time of this task varies with different values of y and z.

D. Task Set Generation
Carousel is intended to eliminate inter-task interference, and this property is easily tested by looking for the main effect of interference, specifically a change in execution time due to preemption.
We generated one thousand task sets by assigning random priorities, periods and offsets to each task listed in Table II. The priority of each τi was assigned so that no two tasks have equal priority. The offset of each τi was assigned a value between 0 and Ti using a uniform pseudorandom number generator. The period Ti was also assigned using the same generator, with a range from 2Ci to max(4Ci, SZ4 ), where SZ is the execution time for the entire task set, not including bootup time. SZ = 15×106 clock cycles for our experiments.
Each deadline Di = Ti. We checked the schedulability of each task set using equation 4, and rejected and regenerated each task set found to be unschedulable.

E. Results
We observed that every execution of a specific task always results in the same execution time (Table V). This is expected given the single-path nature of the benchmark programs, provided that preemption has no effect on the execution time. Table V also shows how many times each task was preempted across all task sets, demonstrating that task execution times are indeed unaffected by preemption.
Figure 12 illustrates the relationship between the observed WCETs of each task and the Carousel system overheads. It is plain that the very short tasks (janne, expint, binarysearch) are dominated by the overhead. However, the overhead is only a very small part of the execution time of longer tasks.

F. Comparison with Cache
Carousel should also be competitive with techniques used in previous work, and in order to examine this aspect, we
compared Carousel with an equally-sized cache (Table VI). The cache provides 1kbyte of code space and 1kbyte of data space, organized as 16 byte blocks. Like Microblaze’s cache, this is a direct-mapped Harvard-architecture cache with a write-through policy and no allocate on write.
There is now some variation in execution time, clearly visible in Figure 13. Although the tasks are single-path, their execution times are now dependent on the state of the cache, which is affected by the execution of other tasks. This is the effect of inter-task interference, as shown in Figure 1. Sometimes, the variation is quite small (e.g. 1.2% for matmult). This is because the benchmark task runs for a relatively long time, and the effects of any interference are amortized across this time. (The true WCET, considering all possible interference, may be much greater than the observed value given in Table VI.)
Four tasks see an increase in observed WCET for Carousel, i.e. the task is slower on a Carousel architecture. In three cases (binarysearch, expint, janne) this is because the tasks are short and complete very quickly. Thus, the overhead of using Carousel is not made up by any improvement in execution time. For instance, expint requires only one code block (Table III), so an overhead of 864 clock cycles is imposed by Carousel (Table IV). This accounts for almost half of the task’s execution time (1838). The cache has a lower overhead and completes in 1046 clock cycles. For compress, the difference is marginal, and improvements could be obtained by better use of SPM.
Nine tasks see an improvement in observed WCET for Carousel versus a cache. Sometimes the improvement is very large. For instance, the bsort100 benchmark requires over 590,000 clock cycles with a cache, but under 95,000 clock cycles with Carousel! Carousel has improved performance.
Carousel has also improved schedulability versus a cache. Of the one thousand task sets used to generate the data in Table V, only 366 were schedulable with cache for Table VI.
The remaining 75% missed at least one deadline.

G. Causes of Improvement
The improved schedulability is not due to the avoidance of inter-task interference. There are two causes, both pleasant consequences of Carousel’s design.
Firstly, Carousel permits a task-specific split between code and data, but the cache split is fixed, with exactly 1kb for code and 1kb for data. This is a problem for matmult, where the working set is larger than 1kb. If we double the data cache size, the matmult execution time drops to around 697,000. A cache does not allow any dynamic flexibility of this sort; Carousel does.
Secondly, the write-through policy of the Microblaze cache is causing a significant slowdown. bsort100 performs over 10,000 stores, and the overwhelming majority of these use addresses that are already in cache. However, they must be sent to the external RAM as well, and subsequent operations must
wait for them to complete. If we were to substitute a writeback policy [23], the execution time would drop to 94,000 clock cycles.
SPM technology (and thus, Carousel) already operates in write-back mode. This is a natural consequence of requiring tasks to explicitly copy data between external RAM and SPM. It requires no extra hardware. The same is not true for writeback mode in a cache, which adds complexity to both hardware and WCET analysis [15] due to the need to need to track which data in the cache has changed, and write that data back upon eviction. The timing-related state space of the cache analysis is greatly expanded by the possibility that data may not only be in cache (or not) but also modified (or not).
That said, if a write-back cache were introduced, the results would appear as shown in Table VII. This brings the cache and Carousel results closer together. Carousel remains a great option for some benchmarks (e.g. bsort100, crc), but the gap is closed. Smaller benchmarks are now much better with cache, because the overhead is much lower.

H. Worst-case Blocking
The task sets executed on our prototype system do not share any resources other than Carousel itself. This means that blocking is limited to that incurred by Carousel (section V). Measurements from our prototype system indicate that Bi = 4321 for all τi except the lowest-priority task τN , as BN = 0.
For a task set where resources other than Carousel are shared, ∀i = N.Bi is the maximum of 4321 and whatever worst-case blocking time is imposed by the other resources used by τi.
VII. IMPROVEMENTS TO CAROUSEL The basic design of Carousel can be improved in a number of ways. Two simple improvements, already used within our experiments, are discarding read-only data rather than writing it back, and not reading uninitialized data (section IV-D).
Another obvious improvement is to place code and data into two separate Carousels. This fits well with the Harvard architecture of many embedded systems CPUs [23]. Furthermore,
the code-side Carousel can be explicitly read-only, meaning that swap out never needs to write back its contents. Finally, dividing Carousel in this way allows more blocks to be used, as the number of parallel comparisons is halved.
Carousel blocks are used with an SPM for our experiments, but they can also be used with a cache, by substituting cache for SPM within Figure 9. Rather than dma copying during open, the open method just invalidates all cache blocks within the Carousel block, and code or data is loaded on demand. Instruction and data cache analyses will be needed to determine the WCET, but with the advantage that there is still no inter-task interference. However, swap out will need to save the state of the cache tag store in addition to the block contents. This will increase the cost of swap in and swap out.
This technique is likely to be most advantageous in mixedcriticality systems with both hard real-time and non real-time tasks. But caches are usable in hard real-time systems given appropriate hardware [4], so even hard real-time software development may benefit from the technique.

VIII. CONCLUSION
This paper has presented Carousel, a mechanism to manage local RAM space. Carousel eliminates inter-task interference by requiring each task to save and restore any local RAM that it wishes to use. Thus, the costs of preemption are handled explicitly by the preempting task, instead of being imposed on the preempted tasks.
Our experiments show that Carousel does indeed eliminate inter-task interference. It provides substantially better performance than a cache-architecture machine with a write-through policy, and a comparison with a write-back policy also gives good results. And unlike earlier approaches for eliminating interference, such as cache partitioning, Carousel allows each task to use as much of the local RAM as required. One consequence of this improvement is that task sets are more likely to be schedulable; for instance, of one thousand task sets schedulable with Carousel, only 366 were schedulable with an cache-architecture machine (section VI).
However, Carousel does have unavoidable limitations. Because it swaps memory to and from a stack, it cannot support any scheduling paradigm where task execution is not strictly nested. It requires something similar to Baker’s stack resource protocol (SRP). There is also a somewhat higher overhead for invoking tasks. Nevertheless, the elimination of inter-task interference brings plenty of advantages.

IX. ACKNOWLEDGMENTS
This work was supported by EPSRC project TEMPO, no. EP/G055548/1. Thanks go to Robert Davis for his greatly helpful advice on scheduling theory and response time equations, to Martin Schoeberl and Ian Gray for their comments on drafts, and to the reviewers for their helpful suggestions.
References [1] A. Burns and A. J. Wellings, Real-Time Systems and Programming
Languages, 4th Edition. Addison Wesley, 2009. [2] J. Stärner and L. Asplund, “Measuring the cache interference cost in
preemptive real-time systems,” in Proc. LCTES, 2004, pp. 146–154. [3] R. Heckmann, M. Langenbach, S. Thesing, and R. Wilhelm, “The
influence of processor architecture on the design and the results of WCET tools.” Proc. IEEE, vol. 91, no. 7, pp. 1038–1054, 2003. [4] R. Wilhelm, D. Grund, J. Reineke, M. Schlickling, M. Pister, and C. Ferdinand, “Memory hierarchies, pipelines, and buses for future architectures in time-critical embedded systems,” IEEE Trans. on CAD of Integrated Circuits and Systems, vol. 28, no. 7, pp. 966–978, 2009. [5] R. Reddy and P. Petrov, “Eliminating inter-process cache interference through cache reconfigurability for real-time and low-power embedded multi-tasking systems,” in Proc. CASES, 2007, pp. 198–207. [6] S. Altmeyer, R. Davis, and C. Maiza, “Cache related pre-emption delay aware response time analysis for fixed priority pre-emptive systems,” in Proc. RTSS, 2011, pp. 261–271. [7] J. V. Busquets-Mataix, J. J. Serrano, R. Ors, P. Gil, and A. Wellings, “Adding instruction cache effect to schedulability analysis of preemptive real-time systems,” in Proc. RTAS, 1996, pp. 204–. [8] H. Ramaprasad and F. Mueller, “Bounding preemption delay within data cache reference patterns for real-time tasks,” in Proc. RTAS, 2006, pp. 71–80. [9] R. Wilhelm, J. Engblom, A. Ermedahl, N. Holsti, S. Thesing, D. Whalley, G. Bernat, C. Ferdinand, R. Heckmann, T. Mitra, F. Mueller, I. Puaut, P. Puschner, J. Staschulat, and P. Stenström, “The worst-case execution-time problem—overview of methods and survey of tools,” Trans. on Embedded Computing Sys., vol. 7, no. 3, pp. 1–53, 2008. [10] O. Temam, “An algorithm for optimally exploiting spatial and temporal locality in upper memory levels,” IEEE Trans. Computers, vol. 48, no. 2, pp. 150–158, 1999. [11] T. P. Baker, “Stack-based scheduling of real-time processes,” Real-Time Syst., vol. 3, no. 1, pp. 67–100, 1991. [12] A. V. Aho, R. Sethi, and J. D. Ullman, Compilers: principles, techniques, and tools, 1986. [13] L. Sha, R. Rajkumar, and J. P. Lehoczky, “Priority inheritance protocols: An approach to real-time synchronization,” IEEE Trans. Comput., vol. 39, no. 9, pp. 1175–1185, 1990. [14] I. Puaut, “Cache analysis vs static cache locking for schedulability analysis in multitasking real-time systems,” in Proc. WCET, Vienna, Austria, June 2002. [15] C. Cullmann, C. Ferdinand, G. Gebhard, D. Grund, C. Maiza, J. Reineke, B. Triquet, and R. Wilhelm, “Predictability considerations in the design of multi-core embedded systems,” in Proceedings of Embedded Real Time Software and Systems, 2010. [16] F. Mueller, “Compiler support for software-based cache partitioning,” in Proc. LCTES. New York, NY, USA: ACM Press, 1995, pp. 125–133. [17] S. Udayakumaran, A. Dominguez, and R. Barua, “Dynamic allocation for scratch-pad memory using compile-time decisions,” Trans. on Embedded Computing Sys., vol. 5, no. 2, pp. 472–511. [18] J.-F. Deverge and I. Puaut, “WCET-Directed Dynamic Scratchpad Memory Allocation of Data,” in Proc. ECRTS, 2007, pp. 179–190. [19] J. Whitham and N. Audsley, “Implementing Time-Predictable Load and Store Operations,” in Proc. EMSOFT, 2009, pp. 265–274. [20] Xilinx, “Microblaze processor reference guide,” http://www.xilinx.com/ bvdocs/userguides/ug081.pdf, Manual UG081, 2005. [21] MRTC, “WCET Benchmarks,” http://www.mrtc.mdh.se/projects/wcet/ benchmarks.html. [22] M. Kandemir, J. Ramanujam, J. Irwin, N. Vijaykrishnan, I. Kadayif, and A. Parikh, “Dynamic management of scratch-pad memory space,” in Proc. DAC, 2001, pp. 690–695. [23] J. L. Hennessy and D. A. Patterson, Computer Architecture, Fourth Edition: A Quantitative Approach. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2006.

References
[1]J. Stärner,L. AsplundMeasuring the cache interference cost in preemptive real-time systemsProc. LCTES, 2004, pp. 146–154.2004
[2]R. Heckmann,M. Langenbach,S. Thesing,R. WilhelmThe influence of processor architecture on the design and the results of WCET tools.Proc. IEEE,2003
[3]R. Wilhelm,D. Grund,J. Reineke,M. Schlickling,M. Pister,C. FerdinandMemory hierarchies, pipelines, and buses for future architectures in time-critical embedded systemsIEEE Trans. on CAD of Integrated Circuits and Systems, vol. 28, no. 7, pp. 966–978, 2009.2009
[4]R. Reddy,P. PetrovEliminating inter-process cache interference through cache reconfigurability for real-time and low-power embedded multi-tasking systemsProc. CASES, 2007, pp. 198–207.2007
[5]S. Altmeyer,R. Davis,C. MaizaCache related pre-emption delay aware response time analysis for fixed priority pre-emptive systemsProc. RTSS, 2011, pp. 261–271.2011
[6]J.V. Busquets-Mataix,J.J. Serrano,R. Ors,P. Gil,A. WellingsAdding instruction cache effect to schedulability analysis of preemptive real-time systemsProc. RTAS, 1996, pp. 204–.1996
[7]H. Ramaprasad,F. MuellerBounding preemption delay within data cache reference patterns for real-time tasksProc. RTAS, 2006, pp. 71–80.2006
[8]R. Wilhelm,J. Engblom,A. Ermedahl,N. Holsti,S. Thesing,D. Whalley,G. Bernat,C. Ferdinand,R. Heckmann,T. Mitra,F. Mueller,I. Puaut,P. Puschner,J. Staschulat,P. StenströmThe worst-case execution-time problem—overview of methods and survey of toolsTrans. on Embedded Computing Sys., vol. 7, no. 3, pp. 1–53, 2008.2008
[9]O. TemamAn algorithm for optimally exploiting spatial and temporal locality in upper memory levelsIEEE Trans. Computers, vol. 48, no. 2, pp. 150–158, 1999.1999
[10]T.P. BakerStack-based scheduling of real-time processesReal-Time Syst., vol. 3, no. 1, pp. 67–100, 1991.1991
[11]L. Sha,R. Rajkumar,J.P. LehoczkyPriority inheritance protocols: An approach to real-time synchronizationIEEE Trans. Comput., vol. 39, no. 9, pp. 1175–1185, 1990.1990
[12]I. PuautCache analysis vs static cache locking for schedulability analysis in multitasking real-time systemsProc. WCET, Vienna, Austria, June 2002.2002
[13]C. Cullmann,C. Ferdinand,G. Gebhard,D. Grund,C. Maiza,J. Reineke,B. Triquet,R. WilhelmPredictability considerations in the design of multi-core embedded systemsProceedings of Embedded Real Time Software and Systems, 2010.2010
[14]F. MuellerCompiler support for software-based cache partitioningProc. LCTES. New York, NY, USA: ACM Press, 1995, pp. 125–133.1995
[15]S. Udayakumaran,A. Dominguez,R. BaruaDynamic allocation for scratch-pad memory using compile-time decisionsTrans. on Embedded Computing Sys., vol. 5, no. 2, pp. 472–511.0
[16]J.-F. Deverge,I. PuautWCET-Directed Dynamic Scratchpad Memory Allocation of DataProc. ECRTS, 2007, pp. 179–190.2007
[17]J. Whitham,N. AudsleyImplementing Time-Predictable Load and Store OperationsProc. EMSOFT, 2009, pp. 265–274.2009
[18]XilinxMicroblaze processor reference guidehttp://www.xilinx.com/ bvdocs/userguides/ug081.pdf, Manual UG081, 2005.2005
[19]MRTCWCET Benchmarkshttp://www.mrtc.mdh.se/projects/wcet/ benchmarks.html.0
[20]M. Kandemir,J. Ramanujam,J. Irwin,N. Vijaykrishnan,I. Kadayif,A. ParikhDynamic management of scratch-pad memory spaceProc. DAC, 2001, pp. 690–695.2001
[21]J.L. Hennessy,D.A. PattersonComputer Architecture, Fourth Edition: A Quantitative Approach2006
