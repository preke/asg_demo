Statistical Pattern Recognition: A Review
Anil K. Jain


Statistical Pattern Recognition: A Review Anil K. Jain, Fellow, IEEE, Robert P.W. Duin, and Jianchang Mao, Senior Member, IEEE
AbstractÐThe primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.
Index TermsÐStatistical pattern recognition, classification, clustering, feature extraction, feature selection, error estimation, classifier combination, neural networks.
æ

1 INTRODUCTION
BY the time they are five years old, most children canrecognize digits and letters. Small characters, large characters, handwritten, machine printed, or rotatedÐall are easily recognized by the young. The characters may be written on a cluttered background, on crumpled paper or may even be partially occluded. We take this ability for granted until we face the task of teaching a machine how to do the same. Pattern recognition is the study of how machines can observe the environment, learn to distinguish patterns of interest from their background, and make sound and reasonable decisions about the categories of the patterns. In spite of almost 50 years of research, design of a general purpose machine pattern recognizer remains an elusive goal.
The best pattern recognizers in most instances are
humans, yet we do not understand how humans recognize
patterns. Ross [140] emphasizes the work of Nobel Laureate
Herbert Simon whose central finding was that pattern
recognition is critical in most human decision making tasks:
ªThe more relevant patterns at your disposal, the better
your decisions will be. This is hopeful news to proponents
of artificial intelligence, since computers can surely be
taught to recognize patterns. Indeed, successful computer
programs that help banks score credit applicants, help
doctors diagnose disease and help pilots land airplanes
depend in some way on pattern recognition... We need to pay much more explicit attention to teaching pattern recognition.º Our goal here is to introduce pattern recognition as the best possible way of utilizing available sensors, processors, and domain knowledge to make decisions automatically.

1.1 What is Pattern Recognition?
Automatic (machine) recognition, description, classification, and grouping of patterns are important problems in a variety of engineering and scientific disciplines such as biology, psychology, medicine, marketing, computer vision, artificial intelligence, and remote sensing. But what is a pattern? Watanabe [163] defines a pattern ªas opposite of a chaos; it is an entity, vaguely defined, that could be given a name.º For example, a pattern could be a fingerprint image, a handwritten cursive word, a human face, or a speech signal. Given a pattern, its recognition/classification may consist of one of the following two tasks [163]: 1) supervised classification (e.g., discriminant analysis) in which the input pattern is identified as a member of a predefined class, 2) unsupervised classification (e.g., clustering) in which the pattern is assigned to a hitherto unknown class. Note that the recognition problem here is being posed as a classification or categorization task, where the classes are either defined by the system designer (in supervised classification) or are learned based on the similarity of patterns (in unsupervised classification).
Interest in the area of pattern recognition has been renewed recently due to emerging applications which are not only challenging but also computationally more demanding (see Table 1). These applications include data mining (identifying a ªpattern,º e.g., correlation, or an outlier in millions of multidimensional patterns), document classification (efficiently searching text documents), financial forecasting, organization and retrieval of multimedia databases, and biometrics (personal identification based on
. A.K. Jain is with the Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824. E-mail: jain@cse.msu.edu. . R.P.W. Duin is with the Department of Applied Physics, Delft University of Technology, 2600 GA Delft, the Netherlands. E-mail: duin@ph.tn.tudelft.nl. . J. Mao is with the IBM Almaden Research Center, 650 Harry Road, San Jose, CA 95120. E-mail: mao@almaden.ibm.com.
Manuscript received 23 July 1999; accepted 12 Oct. 1999. Recommended for acceptance by K. Bowyer. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number 110296.
0162-8828/00/$10.00 ß 2000 IEEE
various physical attributes such as face and fingerprints). Picard [125] has identified a novel application of pattern recognition, called affective computing which will give a computer the ability to recognize and express emotions, to respond intelligently to human emotion, and to employ mechanisms of emotion that contribute to rational decision making. A common characteristic of a number of these applications is that the available features (typically, in the thousands) are not usually suggested by domain experts, but must be extracted and optimized by data-driven procedures.
The rapidly growing and available computing power, while enabling faster processing of huge data sets, has also facilitated the use of elaborate and diverse methods for data analysis and classification. At the same time, demands on automatic pattern recognition systems are rising enormously due to the availability of large databases and stringent performance requirements (speed, accuracy, and cost). In many of the emerging applications, it is clear that no single approach for classification is ªoptimalº and that multiple methods and approaches have to be used. Consequently, combining several sensing modalities and classifiers is now a commonly used practice in pattern recognition.
The design of a pattern recognition system essentially involves the following three aspects: 1) data acquisition and preprocessing, 2) data representation, and 3) decision making. The problem domain dictates the choice of sensor(s), preprocessing technique, representation scheme, and the decision making model. It is generally agreed that a
well-defined and sufficiently constrained recognition problem (small intraclass variations and large interclass variations) will lead to a compact pattern representation and a simple decision making strategy. Learning from a set of examples (training set) is an important and desired attribute of most pattern recognition systems. The four best known approaches for pattern recognition are: 1) template matching, 2) statistical classification, 3) syntactic or structural matching, and 4) neural networks. These models are not necessarily independent and sometimes the same pattern recognition method exists with different interpretations. Attempts have been made to design hybrid systems involving multiple models [57]. A brief description and comparison of these approaches is given below and summarized in Table 2.

1.2 Template Matching
One of the simplest and earliest approaches to pattern recognition is based on template matching. Matching is a generic operation in pattern recognition which is used to determine the similarity between two entities (points, curves, or shapes) of the same type. In template matching, a template (typically, a 2D shape) or a prototype of the pattern to be recognized is available. The pattern to be recognized is matched against the stored template while taking into account all allowable pose (translation and rotation) and scale changes. The similarity measure, often a correlation, may be optimized based on the available training set. Often, the template itself is learned from the training set. Template matching is computationally demanding, but the availability of faster processors has now
made this approach more feasible. The rigid template
matching mentioned above, while effective in some
application domains, has a number of disadvantages. For
instance, it would fail if the patterns are distorted due to the
imaging process, viewpoint change, or large intraclass
variations among the patterns. Deformable template models
[69] or rubber sheet deformations [9] can be used to match
patterns when the deformation cannot be easily explained
or modeled directly.

1.3 Statistical Approach
In the statistical approach, each pattern is represented in
terms of d features or measurements and is viewed as a
point in a d-dimensional space. The goal is to choose those
features that allow pattern vectors belonging to different
categories to occupy compact and disjoint regions in a
d-dimensional feature space. The effectiveness of the
representation space (feature set) is determined by how
well patterns from different classes can be separated. Given
a set of training patterns from each class, the objective is to
establish decision boundaries in the feature space which
separate patterns belonging to different classes. In the
statistical decision theoretic approach, the decision bound-
aries are determined by the probability distributions of the
patterns belonging to each class, which must either be
specified or learned [41], [44]. One can also take a discriminant analysis-based ap-
proach to classification: First a parametric form of the
decision boundary (e.g., linear or quadratic) is specified;
then the ªbestº decision boundary of the specified form is
found based on the classification of training patterns. Such
boundaries can be constructed using, for example, a mean
squared error criterion. The direct boundary construction
approaches are supported by Vapnik's philosophy [162]: ªIf
you possess a restricted amount of information for solving
some problem, try to solve the problem directly and never
solve a more general problem as an intermediate step. It is
possible that the available information is sufficient for a
direct solution but is insufficient for solving a more general
intermediate problem.º

1.4 Syntactic Approach
In many recognition problems involving complex patterns, it is more appropriate to adopt a hierarchical perspective where a pattern is viewed as being composed of simple subpatterns which are themselves built from yet simpler subpatterns [56], [121]. The simplest/elementary subpatterns to be recognized are called primitives and the given complex pattern is represented in terms of the interrelationships between these primitives. In syntactic pattern recognition, a formal analogy is drawn between the structure of patterns and the syntax of a language. The patterns are viewed as sentences belonging to a language, primitives are viewed as the alphabet of the language, and the sentences are generated according to a grammar. Thus, a large collection of complex patterns can be described by a small number of primitives and grammatical rules. The grammar for each pattern class must be inferred from the available training samples.
Structural pattern recognition is intuitively appealing because, in addition to classification, this approach also provides a description of how the given pattern is constructed from the primitives. This paradigm has been used in situations where the patterns have a definite structure which can be captured in terms of a set of rules, such as EKG waveforms, textured images, and shape analysis of contours [56]. The implementation of a syntactic approach, however, leads to many difficulties which primarily have to do with the segmentation of noisy patterns (to detect the primitives) and the inference of the grammar from training data. Fu [56] introduced the notion of attributed grammars which unifies syntactic and statistical pattern recognition. The syntactic approach may yield a combinatorial explosion of possibilities to be investigated, demanding large training sets and very large computational efforts [122].

1.5 Neural Networks
Neural networks can be viewed as massively parallel computing systems consisting of an extremely large number of simple processors with many interconnections. Neural network models attempt to use some organizational principles (such as learning, generalization, adaptivity, fault tolerance and distributed representation, and
computation) in a network of weighted directed graphs in which the nodes are artificial neurons and directed edges (with weights) are connections between neuron outputs and neuron inputs. The main characteristics of neural networks are that they have the ability to learn complex nonlinear input-output relationships, use sequential training procedures, and adapt themselves to the data.
The most commonly used family of neural networks for pattern classification tasks [83] is the feed-forward network, which includes multilayer perceptron and Radial-Basis Function (RBF) networks. These networks are organized into layers and have unidirectional connections between the layers. Another popular network is the Self-Organizing Map (SOM), or Kohonen-Network [92], which is mainly used for data clustering and feature mapping. The learning process involves updating network architecture and connection weights so that a network can efficiently perform a specific classification/clustering task. The increasing popularity of neural network models to solve pattern recognition problems has been primarily due to their seemingly low dependence on domain-specific knowledge (relative to model-based and rule-based approaches) and due to the availability of efficient learning algorithms for practitioners to use.
Neural networks provide a new suite of nonlinear algorithms for feature extraction (using hidden layers) and classification (e.g., multilayer perceptrons). In addition, existing feature extraction and classification algorithms can also be mapped on neural network architectures for efficient (hardware) implementation. In spite of the seemingly different underlying principles, most of the wellknown neural network models are implicitly equivalent or similar to classical statistical pattern recognition methods (see Table 3). Ripley [136] and Anderson et al. [5] also discuss this relationship between neural networks and statistical pattern recognition. Anderson et al. point out that ªneural networks are statistics for amateurs... Most NNs conceal the statistics from the user.º Despite these similarities, neural networks do offer several advantages such as, unified approaches for feature extraction and classification and flexible procedures for finding good, moderately nonlinear solutions.

1.6 Scope and Organization
In the remainder of this paper we will primarily review statistical methods for pattern representation and classification, emphasizing recent developments. Whenever appropriate, we will also discuss closely related algorithms from the neural networks literature. We omit the whole body of literature on fuzzy classification and fuzzy clustering which are in our opinion beyond the scope of this paper. Interested readers can refer to the well-written books on fuzzy pattern recognition by Bezdek [15] and [16]. In most of the sections, the various approaches and methods are summarized in tables as an easy and quick reference for the reader. Due to space constraints, we are not able to provide many details and we have to omit some of the approaches and the associated references. Our goal is to emphasize those approaches which have been extensively evaluated
and demonstrated to be useful in practical applications, along with the new trends and ideas.
The literature on pattern recognition is vast and scattered in numerous journals in several disciplines (e.g., applied statistics, machine learning, neural networks, and signal and image processing). A quick scan of the table of contents of all the issues of the IEEE Transactions on Pattern Analysis and Machine Intelligence, since its first publication in January 1979, reveals that approximately 350 papers deal with pattern recognition. Approximately 300 of these papers covered the statistical approach and can be broadly categorized into the following subtopics: curse of dimensionality (15), dimensionality reduction (50), classifier design (175), classifier combination (10), error estimation (25) and unsupervised classification (50). In addition to the excellent textbooks by Duda and Hart [44],1 Fukunaga [58], Devijver and Kittler [39], Devroye et al. [41], Bishop [18], Ripley [137], Schurmann [147], and McLachlan [105], we should also point out two excellent survey papers written by Nagy [111] in 1968 and by Kanal [89] in 1974. Nagy described the early roots of pattern recognition, which at that time was shared with researchers in artificial intelligence and perception. A large part of Nagy's paper introduced a number of potential applications of pattern recognition and the interplay between feature definition and the application domain knowledge. He also emphasized the linear classification methods; nonlinear techniques were based on polynomial discriminant functions as well as on potential functions (similar to what are now called the kernel functions). By the time Kanal wrote his survey paper, more than 500 papers and about half a dozen books on pattern recognition were already published. Kanal placed less emphasis on applications, but more on modeling and design of pattern recognition systems. The discussion on automatic feature extraction in [89] was based on various distance measures between classconditional probability density functions and the resulting error bounds. Kanal's review also contained a large section on structural methods and pattern grammars.
In comparison to the state of the pattern recognition field as described by Nagy and Kanal in the 1960s and 1970s, today a number of commercial pattern recognition systems are available which even individuals can buy for personal use (e.g., machine printed character recognition and isolated spoken word recognition). This has been made possible by various technological developments resulting in the availability of inexpensive sensors and powerful desktop computers. The field of pattern recognition has become so large that in this review we had to skip detailed descriptions of various applications, as well as almost all the procedures which model domain-specific knowledge (e.g., structural pattern recognition, and rule-based systems). The starting point of our review (Section 2) is the basic elements of statistical methods for pattern recognition. It should be apparent that a feature vector is a representation of real world objects; the choice of the representation strongly influences the classification results.
1. Its second edition by Duda, Hart, and Stork [45] is in press.
The topic of probabilistic distance measures is currently not as important as 20 years ago, since it is very difficult to estimate density functions in high dimensional feature spaces. Instead, the complexity of classification procedures and the resulting accuracy have gained a large interest. The curse of dimensionality (Section 3) as well as the danger of overtraining are some of the consequences of a complex classifier. It is now understood that these problems can, to some extent, be circumvented using regularization, or can even be completely resolved by a proper design of classification procedures. The study of support vector machines (SVMs), discussed in Section 5, has largely contributed to this understanding. In many real world problems, patterns are scattered in high-dimensional (often) nonlinear subspaces. As a consequence, nonlinear procedures and subspace approaches have become popular, both for dimensionality reduction (Section 4) and for building classifiers (Section 5). Neural networks offer powerful tools for these purposes. It is now widely accepted that no single procedure will completely solve a complex classification problem. There are many admissible approaches, each capable of discriminating patterns in certain portions of the feature space. The combination of classifiers has, therefore, become a heavily studied topic (Section 6). Various approaches to estimating the error rate of a classifier are presented in Section 7. The topic of unsupervised classification or clustering is covered in Section 8. Finally, Section 9 identifies the frontiers of pattern recognition.
It is our goal that most parts of the paper can be appreciated by a newcomer to the field of pattern
recognition. To this purpose, we have included a number of examples to illustrate the performance of various algorithms. Nevertheless, we realize that, due to space limitations, we have not been able to introduce all the concepts completely. At these places, we have to rely on the background knowledge which may be available only to the more experienced readers.

2 STATISTICAL PATTERN RECOGNITION
Statistical pattern recognition has been used successfully to design a number of commercial recognition systems. In statistical pattern recognition, a pattern is represented by a set of d features, or attributes, viewed as a d-dimensional feature vector. Well-known concepts from statistical decision theory are utilized to establish decision boundaries between pattern classes. The recognition system is operated in two modes: training (learning) and classification (testing) (see Fig. 1). The role of the preprocessing module is to segment the pattern of interest from the background, remove noise, normalize the pattern, and any other operation which will contribute in defining a compact representation of the pattern. In the training mode, the feature extraction/selection module finds the appropriate features for representing the input patterns and the classifier is trained to partition the feature space. The feedback path allows a designer to optimize the preprocessing and feature extraction/selection strategies. In the classification mode, the trained classifier assigns the input pattern to one of the pattern classes under consideration based on the measured features.
The decision making process in statistical pattern recognition can be summarized as follows: A given pattern is to be assigned to one of c categories !1; !2; ; !c based on a vector of d feature values x x1; x2; ; xd . The features are assumed to have a probability density or mass (depending on whether the features are continuous or discrete) function conditioned on the pattern class. Thus, a pattern vector x belonging to class !i is viewed as an observation drawn randomly from the class-conditional probability function p xj!i . A number of well-known decision rules, including the Bayes decision rule, the maximum likelihood rule (which can be viewed as a particular case of the Bayes rule), and the Neyman-Pearson rule are available to define the decision boundary. The ªoptimalº Bayes decision rule for minimizing the risk (expected value of the loss function) can be stated as follows: Assign input pattern x to class !i for which the conditional risk
R !ijx Xc j 1 L !i; !j P !jjx 1
is minimum, where L !i; !j is the loss incurred in deciding !i when the true class is !j and P !jjx is the posterior probability [44]. In the case of the 0/1 loss function, as defined in (2), the conditional risk becomes the conditional probability of misclassification.
L !i; !j 0; i j1; i 6 j :
2
For this choice of loss function, the Bayes decision rule can be simplified as follows (also called the maximum a posteriori (MAP) rule): Assign input pattern x to class !i if
P !ijx > P !jjx for all j 6 i: 3 Various strategies are utilized to design a classifier in statistical pattern recognition, depending on the kind of information available about the class-conditional densities. If all of the class-conditional densities are completely specified, then the optimal Bayes decision rule can be used to design a classifier. However, the class-conditional densities are usually not known in practice and must be learned from the available training patterns. If the form of the class-conditional densities is known (e.g., multivariate Gaussian), but some of the parameters of the densities (e.g., mean vectors and covariance matrices) are unknown, then we have a parametric decision problem. A common strategy for this kind of problem is to replace the unknown parameters in the density functions by their estimated values, resulting in the so-called Bayes plug-in classifier. The optimal Bayesian strategy in this situation requires additional information in the form of a prior distribution on the unknown parameters. If the form of the class-conditional densities is not known, then we operate in a nonparametric mode. In this case, we must either estimate the density function (e.g., Parzen window approach) or directly construct the decision boundary based on the training data (e.g., k-nearest neighbor rule). In fact, the multilayer perceptron can also be viewed as a
supervised nonparametric method which constructs a decision boundary.
Another dichotomy in statistical pattern recognition is that of supervised learning (labeled training samples) versus unsupervised learning (unlabeled training samples). The label of a training pattern represents the category to which that pattern belongs. In an unsupervised learning problem, sometimes the number of classes must be learned along with the structure of each class. The various dichotomies that appear in statistical pattern recognition are shown in the tree structure of Fig. 2. As we traverse the tree from top to bottom and left to right, less information is available to the system designer and as a result, the difficulty of classification problems increases. In some sense, most of the approaches in statistical pattern recognition (leaf nodes in the tree of Fig. 2) are attempting to implement the Bayes decision rule. The field of cluster analysis essentially deals with decision making problems in the nonparametric and unsupervised learning mode [81]. Further, in cluster analysis the number of categories or clusters may not even be specified; the task is to discover a reasonable categorization of the data (if one exists). Cluster analysis algorithms along with various techniques for visualizing and projecting multidimensional data are also referred to as exploratory data analysis methods.
Yet another dichotomy in statistical pattern recognition can be based on whether the decision boundaries are obtained directly (geometric approach) or indirectly (probabilistic density-based approach) as shown in Fig. 2. The probabilistic approach requires to estimate density functions first, and then construct the discriminant functions which specify the decision boundaries. On the other hand, the geometric approach often constructs the decision boundaries directly from optimizing certain cost functions. We should point out that under certain assumptions on the density functions, the two approaches are equivalent. We will see examples of each category in Section 5.
No matter which classification or decision rule is used, it must be trained using the available training samples. As a result, the performance of a classifier depends on both the number of available training samples as well as the specific values of the samples. At the same time, the goal of designing a recognition system is to classify future test samples which are likely to be different from the training samples. Therefore, optimizing a classifier to maximize its performance on the training set may not always result in the desired performance on a test set. The generalization ability of a classifier refers to its performance in classifying test patterns which were not used during the training stage. A poor generalization ability of a classifier can be attributed to any one of the following factors: 1) the number of features is too large relative to the number of training samples (curse of dimensionality [80]), 2) the number of unknown parameters associated with the classifier is large (e.g., polynomial classifiers or a large neural network), and 3) a classifier is too intensively optimized on the training set (overtrained); this is analogous to the
phenomenon of overfitting in regression when there are too many free parameters.
Overtraining has been investigated theoretically for classifiers that minimize the apparent error rate (the error on the training set). The classical studies by Cover [33] and Vapnik [162] on classifier capacity and complexity provide a good understanding of the mechanisms behind overtraining. Complex classifiers (e.g., those having many independent parameters) may have a large capacity, i.e., they are able to represent many dichotomies for a given dataset. A frequently used measure for the capacity is the Vapnik-Chervonenkis (VC) dimensionality [162]. These results can also be used to prove some interesting properties, for example, the consistency of certain classifiers (see, Devroye et al. [40], [41]). The practical use of the results on classifier complexity was initially limited because the proposed bounds on the required number of (training) samples were too conservative. In the recent development of support vector machines [162], however, these results have proved to be quite useful. The pitfalls of overadaptation of estimators to the given training set are observed in several stages of a pattern recognition system, such as dimensionality reduction, density estimation, and classifier design. A sound solution is to always use an independent dataset (test set) for evaluation. In order to avoid the necessity of having several independent test sets, estimators are often based on rotated subsets of the data, preserving different parts of the data for optimization and evaluation [166]. Examples are the optimization of the covariance estimates for the Parzen kernel [76] and
discriminant analysis [61], and the use of bootstrapping
for designing classifiers [48], and for error estimation [82]. Throughout the paper, some of the classification meth-
ods will be illustrated by simple experiments on the
following three data sets: Dataset 1: An artificial dataset consisting of two classes
with bivariate Gaussian density with the following para-
meters:
m1 1; 1 ;m2 2; 0 ; 1 1 00 0:25 and 2 0:8 00 1 :
The intrinsic overlap between these two densities is
12.5 percent. Dataset 2: Iris dataset consists of 150 four-dimensional
patterns in three classes (50 patterns each): Iris Setosa, Iris
Versicolor, and Iris Virginica. Dataset 3: The digit dataset consists of handwritten
numerals (ª0º-ª9º) extracted from a collection of Dutch
utility maps. Two hundred patterns per class (for a total of
2,000 patterns) are available in the form of 30 48 binary images. These characters are represented in terms of the
following six feature sets:
1. 76 Fourier coefficients of the character shapes; 2. 216 profile correlations; 3. 64 Karhunen-LoeÁve coefficients; 4. 240 pixel averages in 2 3 windows; 5. 47 Zernike moments; 6. 6 morphological features.
Details of this dataset are available in [160]. In our experiments we always used the same subset of 1,000 patterns for testing and various subsets of the remaining 1,000 patterns for training.2 Throughout this paper, when we refer to ªthe digit dataset,º just the Karhunen-Loeve features (in item 3) are meant, unless stated otherwise.

3 THE CURSE OF DIMENSIONALITY AND PEAKING
PHENOMENA
The performance of a classifier depends on the interrelationship between sample sizes, number of features, and classifier complexity. A naive table-lookup technique (partitioning the feature space into cells and associating a class label with each cell) requires the number of training data points to be an exponential function of the feature dimension [18]. This phenomenon is termed as ªcurse of dimensionality,º which leads to the ªpeaking phenomenonº (see discussion below) in classifier design. It is well-known that the probability of misclassification of a decision rule does not increase as the number of features increases, as long as the class-conditional densities are completely known (or, equivalently, the number of training samples is arbitrarily large and representative of the underlying densities). However, it has been often observed in practice that the added features may actually degrade the performance of a classifier if the number of training samples that are used to design the classifier is small relative to the number of features. This paradoxical behavior is referred to as the peaking phenomenon3 [80], [131], [132]. A simple explanation for this phenomenon is as follows: The most commonly used parametric classifiers estimate the unknown parameters and plug them in for the true parameters in the class-conditional densities. For a fixed sample size, as the number of features is increased (with a corresponding increase in the number of unknown parameters), the reliability of the parameter estimates decreases. Consequently, the performance of the resulting plug-in classifiers, for a fixed sample size, may degrade with an increase in the number of features.
Trunk [157] provided a simple example to illustrate the curse of dimensionality which we reproduce below. Consider the two-class classification problem with equal prior probabilities, and a d-dimensional multivariate Gaussian distribution with the identity covariance matrix for each class. The mean vectors for the two classes have the following components
m1 1; 1 2 p ; 1 3 p ; ; 1 d p and m2 ÿ1;ÿ 1 2 p ;ÿ 1 3 p ; ;ÿ 1 d p :
Note that the features are statistically independent and the discriminating power of the successive features decreases monotonically with the first feature providing the max-
imum discrimination between the two classes. The only parameter in the densities is the mean vector, m m1 ÿm2 .
Trunk considered the following two cases:
1. The mean vector m is known. In this situation, we can use the optimal Bayes decision rule (with a 0/1 loss function) to construct the decision boundary. The probability of error as a function of d can be expressed as:
Pe d Z 1 Pd
i 1 1i q 1 2 p eÿ12z2dz: 4
It is easy to verify that limd!1 Pe d 0. In other words, we can perfectly discriminate the two classes by arbitrarily increasing the number of features, d. 2. The mean vector m is unknown and n labeled training samples are available. Trunk found the maximum likelihood estimate m̂ of m and used the plug-in decision rule (substitute m̂ for m in the optimal Bayes decision rule). Now the probability of error which is a function of both n and d can be written as:
Pe n; d Z 1 d 1 2 p eÿ12z2dz;where 5
d Pd
i 1 1i 1 1n Pd i 1 1i dn q : 6 Trunk showed that limd!1 Pe n; d 12 , which implies that the probability of error approaches the maximum possible value of 0.5 for this two-class problem. This demonstrates that, unlike case 1) we cannot arbitrarily increase the number of features when the parameters of class-conditional densities are estimated from a finite number of training samples. The practical implication of the curse of dimensionality is that a system designer should try to select only a small number of salient features when confronted with a limited training set.
All of the commonly used classifiers, including multilayer feed-forward networks, can suffer from the curse of dimensionality. While an exact relationship between the probability of misclassification, the number of training samples, the number of features and the true parameters of the class-conditional densities is very difficult to establish, some guidelines have been suggested regarding the ratio of the sample size to dimensionality. It is generally accepted that using at least ten times as many training samples per class as the number of features (n=d > 10) is a good practice to follow in classifier design [80]. The more complex the classifier, the larger should the ratio of sample size to dimensionality be to avoid the curse of dimensionality.

4 DIMENSIONALITY REDUCTION
There are two main reasons to keep the dimensionality of the pattern representation (i.e., the number of features) as small as possible: measurement cost and classification
2. The dataset is available through the University of California, Irvine Machine Learning Repository (www.ics.uci.edu/~mlearn/MLRepository.html)
3. In the rest of this paper, we do not make distinction between the curse of dimensionality and the peaking phenomenon.
accuracy. A limited yet salient feature set simplifies both the pattern representation and the classifiers that are built on the selected representation. Consequently, the resulting classifier will be faster and will use less memory. Moreover, as stated earlier, a small number of features can alleviate the curse of dimensionality when the number of training samples is limited. On the other hand, a reduction in the number of features may lead to a loss in the discrimination power and thereby lower the accuracy of the resulting recognition system. Watanabe's ugly duckling theorem [163] also supports the need for a careful choice of the features, since it is possible to make two arbitrary patterns similar by encoding them with a sufficiently large number of redundant features.
It is important to make a distinction between feature selection and feature extraction. The term feature selection refers to algorithms that select the (hopefully) best subset of the input feature set. Methods that create new features based on transformations or combinations of the original feature set are called feature extraction algorithms. However, the terms feature selection and feature extraction are used interchangeably in the literature. Note that often feature extraction precedes feature selection; first, features are extracted from the sensed data (e.g., using principal component or discriminant analysis) and then some of the extracted features with low discrimination ability are discarded. The choice between feature selection and feature extraction depends on the application domain and the specific training data which is available. Feature selection leads to savings in measurement cost (since some of the features are discarded) and the selected features retain their original physical interpretation. In addition, the retained features may be important for understanding the physical process that generates the patterns. On the other hand, transformed features generated by feature extraction may provide a better discriminative ability than the best subset of given features, but these new features (a linear or a nonlinear combination of given features) may not have a clear physical meaning.
In many situations, it is useful to obtain a two- or threedimensional projection of the given multivariate data (n d pattern matrix) to permit a visual examination of the data. Several graphical techniques also exist for visually observing multivariate data, in which the objective is to exactly depict each pattern as a picture with d degrees of freedom, where d is the given number of features. For example, Chernoff [29] represents each pattern as a cartoon face whose facial characteristics, such as nose length, mouth curvature, and eye size, are made to correspond to individual features. Fig. 3 shows three faces corresponding to the mean vectors of Iris Setosa, Iris Versicolor, and Iris Virginica classes in the Iris data (150 four-dimensional patterns; 50 patterns per class). Note that the face associated with Iris Setosa looks quite different from the other two faces which implies that the Setosa category can be well separated from the remaining two categories in the fourdimensional feature space (This is also evident in the twodimensional plots of this data in Fig. 5).
The main issue in dimensionality reduction is the choice of a criterion function. A commonly used criterion is the
classification error of a feature subset. But the classification error itself cannot be reliably estimated when the ratio of sample size to the number of features is small. In addition to the choice of a criterion function, we also need to determine the appropriate dimensionality of the reduced feature space. The answer to this question is embedded in the notion of the intrinsic dimensionality of data. Intrinsic dimensionality essentially determines whether the given d-dimensional patterns can be described adequately in a subspace of dimensionality less than d. For example, d-dimensional patterns along a reasonably smooth curve have an intrinsic dimensionality of one, irrespective of the value of d. Note that the intrinsic dimensionality is not the same as the linear dimensionality which is a global property of the data involving the number of significant eigenvalues of the covariance matrix of the data. While several algorithms are available to estimate the intrinsic dimensionality [81], they do not indicate how a subspace of the identified dimensionality can be easily identified.
We now briefly discuss some of the commonly used methods for feature extraction and feature selection.

4.1 Feature Extraction
Feature extraction methods determine an appropriate subspace of dimensionality m (either in a linear or a nonlinear way) in the original feature space of dimensionality d (m d). Linear transforms, such as principal component analysis, factor analysis, linear discriminant analysis, and projection pursuit have been widely used in pattern recognition for feature extraction and dimensionality reduction. The best known linear feature extractor is the principal component analysis (PCA) or Karhunen-LoeÁve expansion, that computes the m largest eigenvectors of the d d covariance matrix of the n d-dimensional patterns. The linear transformation is defined as
Y XH; 7 where X is the given n d pattern matrix, Y is the derived n m pattern matrix, and H is the d m matrix of linear transformation whose columns are the eigenvectors. Since PCA uses the most expressive features (eigenvectors with the largest eigenvalues), it effectively approximates the data by a linear subspace using the mean squared error criterion. Other methods, like projection pursuit [53] and independent component analysis (ICA) [31], [11], [24], [96] are more appropriate for non-Gaussian distributions since they do not rely on the second-order property of the data. ICA has been successfully used for blind-source separation [78]; extracting linear feature combinations that define independent sources. This demixing is possible if at most one of the sources has a Gaussian distribution.
Whereas PCA is an unsupervised linear feature extraction method, discriminant analysis uses the category information associated with each pattern for (linearly) extracting the most discriminatory features. In discriminant analysis, interclass separation is emphasized by replacing the total covariance matrix in PCA by a general separability measure like the Fisher criterion, which results in finding the eigenvectors of Sÿ1w Sb (the product of the inverse of the within-class scatter matrix, Sw, and the between-class
scatter matrix, Sb) [58]. Another supervised criterion for non-Gaussian class-conditional densities is based on the Patrick-Fisher distance using Parzen density estimates [41].
There are several ways to define nonlinear feature extraction techniques. One such method which is directly related to PCA is called the Kernel PCA [73], [145]. The basic idea of kernel PCA is to first map input data into some new feature space F typically via a nonlinear function (e.g., polynomial of degree p) and then perform a linear PCA in the mapped space. However, the F space often has a very high dimension. To avoid computing the mapping explicitly, kernel PCA employs only Mercer kernels which can be decomposed into a dot product,
K x; y x y : As a result, the kernel space has a well-defined metric. Examples of Mercer kernels include pth-order polynomial xÿ y p and Gaussian kernel
eÿ kxÿyk2 c :
Let X be the normalized n d pattern matrix with zero mean, and X be the pattern matrix in the F space. The linear PCA in the F space solves the eigenvectors of the correlation matrix X X T , which is also called the kernel matrix K X;X . In kernel PCA, the first m eigenvectors of K X;X are obtained to define a transformation matrix, E. (E has size n m, where m represents the desired number of features, m d). New patterns x are mapped by K x;X E, which are now represented relative to the training set and not by their measured feature values. Note that for a complete representation, up to m eigenvectors in E may be needed (depending on the kernel function) by kernel PCA, while in linear PCA a set of d eigenvectors represents the original feature space. How the kernel function should be chosen for a given application is still an open issue.
Multidimensional scaling (MDS) is another nonlinear feature extraction technique. It aims to represent a multidimensional dataset in two or three dimensions such that the distance matrix in the original d-dimensional feature space is preserved as faithfully as possible in the projected space. Various stress functions are used for measuring the performance of this mapping [20]; the most popular
criterion is the stress function introduced by Sammon [141] and Niemann [114]. A problem with MDS is that it does not give an explicit mapping function, so it is not possible to place a new pattern in a map which has been computed for a given training set without repeating the mapping. Several techniques have been investigated to address this deficiency which range from linear interpolation to training a neural network [38]. It is also possible to redefine the MDS algorithm so that it directly produces a map that may be used for new test patterns [165].
A feed-forward neural network offers an integrated procedure for feature extraction and classification; the output of each hidden layer may be interpreted as a set of new, often nonlinear, features presented to the output layer for classification. In this sense, multilayer networks serve as feature extractors [100]. For example, the networks used by Fukushima [62] et al. and Le Cun et al. [95] have the so called shared weight layers that are in fact filters for extracting features in two-dimensional images. During training, the filters are tuned to the data, so as to maximize the classification performance.
Neural networks can also be used directly for feature extraction in an unsupervised mode. Fig. 4a shows the architecture of a network which is able to find the PCA subspace [117]. Instead of sigmoids, the neurons have linear transfer functions. This network has d inputs and d outputs, where d is the given number of features. The inputs are also used as targets, forcing the output layer to reconstruct the input space using only one hidden layer. The three nodes in the hidden layer capture the first three principal components [18]. If two nonlinear layers with sigmoidal hidden units are also included (see Fig. 4b), then a nonlinear subspace is found in the middle layer (also called the bottleneck layer). The nonlinearity is limited by the size of these additional layers. These so-called autoassociative, or nonlinear PCA networks offer a powerful tool to train and describe nonlinear subspaces [98]. Oja [118] shows how autoassociative networks can be used for ICA.
The Self-Organizing Map (SOM), or Kohonen Map [92], can also be used for nonlinear feature extraction. In SOM, neurons are arranged in an m-dimensional grid, where m is usually 1, 2, or 3. Each neuron is connected to all the d input units. The weights on the connections for each neuron form a d-dimensional weight vector. During training, patterns are
presented to the network in a random order. At each presentation, the winner whose weight vector is the closest to the input vector is first identified. Then, all the neurons in the neighborhood (defined on the grid) of the winner are updated such that their weight vectors move towards the input vector. Consequently, after training is done, the weight vectors of neighboring neurons in the grid are likely to represent input patterns which are close in the original feature space. Thus, a ªtopology-preservingº map is formed. When the grid is plotted in the original space, the grid connections are more or less stressed according to the density of the training data. Thus, SOM offers an m-dimensional map with a spatial connectivity, which can be interpreted as feature extraction. SOM is different from learning vector quantization (LVQ) because no neighborhood is defined in LVQ.
Table 4 summarizes the feature extraction and projection methods discussed above. Note that the adjective nonlinear may be used both for the mapping (being a nonlinear function of the original features) as well as for the criterion function (for non-Gaussian data). Fig. 5 shows an example of four different two-dimensional projections of the fourdimensional Iris dataset. Fig. 5a and Fig. 5b show two linear mappings, while Fig. 5c and Fig. 5d depict two nonlinear mappings. Only the Fisher mapping (Fig. 5b) makes use of the category information, this being the main reason why this mapping exhibits the best separation between the three categories.

4.2 Feature Selection
The problem of feature selection is defined as follows: given a set of d features, select a subset of size m that leads to the smallest classification error. There has been a resurgence of interest in applying feature selection methods due to the large number of features encountered in the following situations: 1) multisensor fusion: features, computed from
different sensor modalities, are concatenated to form a feature vector with a large number of components; 2) integration of multiple data models: sensor data can be modeled using different approaches, where the model parameters serve as features, and the parameters from different models can be pooled to yield a high-dimensional feature vector.
Let Y be the given set of features, with cardinality d and let m represent the desired number of features in the selected subset X;X Y . Let the feature selection criterion function for the set X be represented by J X . Let us assume that a higher value of J indicates a better feature subset; a natural choice for the criterion function is J 1ÿ Pe , where Pe denotes the classification error. The use of Pe in the criterion function makes feature selection procedures dependent on the specific classifier that is used and the sizes of the training and test sets. The most straightforward approach to the feature selection problem would require 1) examining all dm ÿ possible subsets of size m, and 2) selecting the subset with the largest value of J . However, the number of possible subsets grows combinatorially, making this exhaustive search impractical for even moderate values of m and d. Cover and Van Campenhout [35] showed that no nonexhaustive sequential feature selection procedure can be guaranteed to produce the optimal subset. They further showed that any ordering of the classification errors of each of the 2d feature subsets is possible. Therefore, in order to guarantee the optimality of, say, a 12-dimensional feature subset out of 24 available features, approximately 2.7 million possible subsets must be evaluated. The only ªoptimalº (in terms of a class of monotonic criterion functions) feature selection method which avoids the exhaustive search is based on the branch and bound algorithm. This procedure avoids an exhaustive search by using intermediate results for obtaining bounds on the final criterion value. The key to this algorithm is the
monotonicity property of the criterion function J ; given two features subsets X1 and X2, if X1 X2, then J X1 < J X2 . In other words, the performance of a feature subset should improve whenever a feature is added to it. Most commonly used criterion functions do not satisfy this monotonicity property.
It has been argued that since feature selection is typically done in an off-line manner, the execution time of a particular algorithm is not as critical as the optimality of the feature subset it generates. While this is true for feature sets of moderate size, several recent applications, particularly those in data mining and document classification, involve thousands of features. In such cases, the computational requirement of a feature selection algorithm is extremely important. As the number of feature subset evaluations may easily become prohibitive for large feature sizes, a number of suboptimal selection techniques have been proposed which essentially tradeoff the optimality of the selected subset for computational efficiency.
Table 5 lists most of the well-known feature selection methods which have been proposed in the literature [85]. Only the first two methods in this table guarantee an optimal subset. All other strategies are suboptimal due to the fact that the best pair of features need not contain the best single feature [34]. In general: good, larger feature sets do not necessarily include the good, small sets. As a result, the simple method of selecting just the best individual features may fail dramatically. It might still be useful, however, as a first step to select some individually good features in decreasing very large feature sets (e.g., hundreds of features). Further selection has to be done by more advanced methods that take feature dependencies into account. These operate either by evaluating growing feature sets (forward selection) or by evaluating shrinking feature sets (backward selection). A simple sequential method like SFS (SBS) adds (deletes) one feature at a time. More sophisticated techniques are the ªPlus l - take away rº strategy and the Sequential Floating Search methods, SFFS and SBFS [126]. These methods backtrack as long as they find improvements compared to previous feature sets of the same size. In almost any large feature selection problem, these methods perform better than the straight sequential searches, SFS and SBS. SFFS and SBFS methods find ªnestedº sets of features that remain hidden otherwise, but the number of feature set evaluations, however, may easily increase by a factor of 2 to 10.
In addition to the search strategy, the user needs to select
an appropriate evaluation criterion, J and specify the value of m. Most feature selection methods use the
classification error of a feature subset to evaluate its
effectiveness. This could be done, for example, by a k-NN
classifier using the leave-one-out method of error estima-
tion. However, use of a different classifier and a different
method for estimating the error rate could lead to a
different feature subset being selected. Ferri et al. [50] and
Jain and Zongker [85] have compared several of the feature
selection algorithms in terms of classification error and run time. The general conclusion is that the sequential forward floating search (SFFS) method performs almost as well as the branch-and-bound algorithm and demands lower computational resources. Somol et al. [154] have proposed an adaptive version of the SFFS algorithm which has been shown to have superior performance.
The feature selection methods in Table 5 can be used with any of the well-known classifiers. But, if a multilayer feed forward network is used for pattern classification, then the node-pruning method simultaneously determines both the optimal feature subset and the optimal network classifier [26], [103]. First train a network and then remove the least salient node (in input or hidden layers). The reduced network is trained again, followed by a removal of yet another least salient node. This procedure is repeated until the desired trade-off between classification error and size of the network is achieved. The pruning of an input node is equivalent to removing the corresponding feature.
How reliable are the feature selection results when the ratio of the available number of training samples to the number of features is small? Suppose the Mahalanobis distance [58] is used as the feature selection criterion. It depends on the inverse of the average class covariance matrix. The imprecision in its estimate in small sample size situations can result in an optimal feature subset which is quite different from the optimal subset that would be obtained when the covariance matrix is known. Jain and Zongker [85] illustrate this phenomenon for a two-class classification problem involving 20-dimensional Gaussian class-conditional densities (the same data was also used by Trunk [157] to demonstrate the curse of dimensionality phenomenon). As expected, the quality of the selected feature subset for small training sets is poor, but improves as the training set size increases. For example, with 20 patterns in the training set, the branch-and-bound algorithm selected a subset of 10 features which included only five features in common with the ideal subset of 10 features (when densities were known). With 2,500 patterns in the training set, the branch-and-bound procedure selected a 10- feature subset with only one wrong feature.
Fig. 6 shows an example of the feature selection procedure using the floating search technique on the PCA features in the digit dataset for two different training set sizes. The test set size is fixed at 1,000 patterns. In each of the selected feature spaces with dimensionalities ranging from 1 to 64, the Bayes plug-in classifier is designed assuming Gaussian densities with equal covariance matrices and evaluated on the test set. The feature selection criterion is the minimum pairwise Mahalanobis distance. In the small sample size case (total of 100 training patterns), the curse of dimensionality phenomenon can be clearly observed. In this case, the optimal number of features is about 20 which equals n=5 n 100 , where n is the number of training patterns. The rule-of-thumb of having less than n=10 features is on the safe side in general.

5 CLASSIFIERS
Once a feature selection or classification procedure finds a proper representation, a classifier can be designed using a
number of possible approaches. In practice, the choice of a classifier is a difficult problem and it is often based on which classifier(s) happen to be available, or best known, to the user.
We identify three different approaches to designing a classifier. The simplest and the most intuitive approach to classifier design is based on the concept of similarity: patterns that are similar should be assigned to the same class. So, once a good metric has been established to define similarity, patterns can be classified by template matching or the minimum distance classifier using a few prototypes per class. The choice of the metric and the prototypes is crucial to the success of this approach. In the nearest mean classifier, selecting prototypes is very simple and robust; each pattern class is represented by a single prototype which is the mean vector of all the training patterns in that class. More advanced techniques for computing prototypes are vector quantization [115], [171] and learning vector quantization [92], and the data reduction methods associated with the one-nearest neighbor decision rule (1-NN), such as editing and condensing [39]. The most straightforward 1-NN rule can be conveniently used as a benchmark for all the other classifiers since it appears to always provide a reasonable classification performance in most applications. Further, as the 1-NN classifier does not require any user-specified parameters (except perhaps the distance metric used to find the nearest neighbor, but Euclidean distance is commonly used), its classification results are implementation independent.
In many classification problems, the classifier is expected to have some desired invariant properties. An example is the shift invariance of characters in character recognition; a change in a character's location should not affect its classification. If the preprocessing or the representation scheme does not normalize the input pattern for this invariance, then the same character may be represented at multiple positions in the feature space. These positions define a one-dimensional subspace. As more invariants are considered, the dimensionality of this subspace correspondingly increases. Template matching
or the nearest mean classifier can be viewed as finding the nearest subspace [116].
The second main concept used for designing pattern classifiers is based on the probabilistic approach. The optimal Bayes decision rule (with the 0/1 loss function) assigns a pattern to the class with the maximum posterior probability. This rule can be modified to take into account costs associated with different types of misclassifications. For known class conditional densities, the Bayes decision rule gives the optimum classifier, in the sense that, for given prior probabilities, loss function and class-conditional densities, no other decision rule will have a lower risk (i.e., expected value of the loss function, for example, probability of error). If the prior class probabilities are equal and a 0/1 loss function is adopted, the Bayes decision rule and the maximum likelihood decision rule exactly coincide. In practice, the empirical Bayes decision rule, or ªplug-inº rule, is used: the estimates of the densities are used in place of the true densities. These density estimates are either parametric or nonparametric. Commonly used parametric models are multivariate Gaussian distributions [58] for continuous features, binomial distributions for binary features, and multinormal distributions for integer-valued (and categorical) features. A critical issue for Gaussian distributions is the assumption made about the covariance matrices. If the covariance matrices for different classes are assumed to be identical, then the Bayes plug-in rule, called Bayesnormal-linear, provides a linear decision boundary. On the other hand, if the covariance matrices are assumed to be different, the resulting Bayes plug-in rule, which we call Bayes-normal-quadratic, provides a quadratic decision boundary. In addition to the commonly used maximum likelihood estimator of the covariance matrix, various regularization techniques [54] are available to obtain a robust estimate in small sample size situations and the leave-one-out estimator is available for minimizing the bias [76].
A logistic classifier [4], which is based on the maximum likelihood approach, is well suited for mixed data types. For a two-class problem, the classifier maximizes:
max Y xi 1 2 !1 q1 xi 1 ; Y xi 2 2 !2 q2 xi 2 ; 8<: 9=;; 8
where qj x; is the posterior probability of class !j, given x, denotes the set of unknown parameters, and xi j denotes the ith training sample from class !j, j 1; 2. Given any discriminant function D x; , where is the parameter vector, the posterior probabilities can be derived as
q1 x; 1 exp ÿD x; ÿ1; q2 x; 1 exp D x; ÿ1;
9
which are called logistic functions. For linear discriminants, D x; , (8) can be easily optimized. Equations (8) and (9) may also be used for estimating the class conditional posterior probabilities by optimizing D x; over the training set. The relationship between the discriminant function D x; and the posterior probabilities can be
derived as follows: We know that the log-discriminant function for the Bayes decision rule, given the posterior probabilities q1 x; and q2 x; , is log q1 x; =q2 x; . Assume that D x; can be optimized to approximate the Bayes decision boundary, i.e.,
D x; log q1 x; =q2 x; : 10 We also have
q1 x; q2 x; 1: 11 Solving (10) and (11) for q1 x; and q2 x; results in (9).
The two well-known nonparametric decision rules, the k-nearest neighbor (k-NN) rule and the Parzen classifier (the class-conditional densities are replaced by their estimates using the Parzen window approach), while similar in nature, give different results in practice. They both have essentially one free parameter each, the number of neighbors k, or the smoothing parameter of the Parzen kernel, both of which can be optimized by a leave-one-out estimate of the error rate. Further, both these classifiers require the computation of the distances between a test pattern and all the patterns in the training set. The most convenient way to avoid these large numbers of computations is by a systematic reduction of the training set, e.g., by vector quantization techniques possibly combined with an optimized metric or kernel [60], [61]. Other possibilities like table-look-up and branch-and-bound methods [42] are less efficient for large dimensionalities.
The third category of classifiers is to construct decision boundaries (geometric approach in Fig. 2) directly by optimizing certain error criterion. While this approach depends on the chosen metric, sometimes classifiers of this type may approximate the Bayes classifier asymptotically. The driving force of the training procedure is, however, the minimization of a criterion such as the apparent classification error or the mean squared error (MSE) between the classifier output and some preset target value. A classical example of this type of classifier is Fisher's linear discriminant that minimizes the MSE between the classifier output and the desired labels. Another example is the single-layer perceptron, where the separating hyperplane is iteratively updated as a function of the distances of the misclassified patterns from the hyperplane. If the sigmoid function is used in combination with the MSE criterion, as in feed-forward neural nets (also called multilayer perceptrons), the perceptron may show a behavior which is similar to other linear classifiers [133]. It is important to note that neural networks themselves can lead to many different classifiers depending on how they are trained. While the hidden layers in multilayer perceptrons allow nonlinear decision boundaries, they also increase the danger of overtraining the classifier since the number of network parameters increases as more layers and more neurons per layer are added. Therefore, the regularization of neural networks may be necessary. Many regularization mechanisms are already built in, such as slow training in combination with early stopping. Other regularization methods include the addition of noise and weight decay [18], [28], [137], and also Bayesian learning [113].
One of the interesting characteristics of multilayer perceptrons is that in addition to classifying an input pattern, they also provide a confidence in the classification, which is an approximation of the posterior probabilities. These confidence values may be used for rejecting a test pattern in case of doubt. The radial basis function (about a Gaussian kernel) is better suited than the sigmoid transfer function for handling outliers. A radial basis network, however, is usually trained differently than a multilayer perceptron. Instead of a gradient search on the weights, hidden neurons are added until some preset performance is reached. The classification result is comparable to situations where each class conditional density is represented by a weighted sum of Gaussians (a so-called Gaussian mixture; see Section 8.2).
A special type of classifier is the decision tree [22], [30], [129], which is trained by an iterative selection of individual features that are most salient at each node of the tree. The criteria for feature selection and tree generation include the information content, the node purity, or Fisher's criterion. During classification, just those features are under consideration that are needed for the test pattern under consideration, so feature selection is implicitly built-in. The most commonly used decision tree classifiers are binary in nature and use a single feature at each node, resulting in decision boundaries that are parallel to the feature axes [149]. Consequently, such decision trees are intrinsically suboptimal for most applications. However, the main advantage of the tree classifier, besides its speed, is the possibility to interpret the decision rule in terms of individual features. This makes decision trees attractive for interactive use by experts. Like neural networks, decision trees can be easily overtrained, which can be avoided by using a pruning stage [63], [106], [128]. Decision tree classification systems such as CART [22] and C4.5 [129] are available in the public domain4 and therefore, often used as a benchmark.
One of the most interesting recent developments in classifier design is the introduction of the support vector classifier by Vapnik [162] which has also been studied by other authors [23], [144], [146]. It is primarily a two-class classifier. The optimization criterion here is the width of the margin between the classes, i.e., the empty area around the decision boundary defined by the distance to the nearest training patterns. These patterns, called support vectors, finally define the classification function. Their number is minimized by maximizing the margin.
The decision function for a two-class problem derived by the support vector classifier can be written as follows using a kernel function K xi; x) of a new pattern x (to be classified) and a training pattern xi.
D x X 8xi2S i iK xi; x 0; 12
where S is the support vector set (a subset of the training set), and i 1 the label of object xi. The parameters i 0 are optimized during training by
min T K C X j "j 13
constrained by jD xj 1ÿ "j; 8xj in the training set. is a diagonal matrix containing the labels j and the matrix K stores the values of the kernel function K xi; x for all pairs of training patterns. The set of slack variables "j allow for class overlap, controlled by the penalty weight C > 0. For C 1, no overlap is allowed. Equation (13) is the dual form of maximizing the margin (plus the penalty term). During optimization, the values of all i become 0, except for the support vectors. So the support vectors are the only ones that are finally needed. The ad hoc character of the penalty term (error penalty) and the computational complexity of the training procedure (a quadratic minimization problem) are the drawbacks of this method. Various training algorithms have been proposed in the literature [23], including chunking [161], Osuna's decomposition method [119], and sequential minimal optimization [124]. An appropriate kernel function K (as in kernel PCA, Section 4.1) needs to be selected. In its most simple form, it is just a dot product between the input pattern x and a member of the support set: K xi; x xi x, resulting in a linear classifier. Nonlinear kernels, such as

K xi; x xi x 1 p;
result in a pth-order polynomial classifier. Gaussian radial basis functions can also be used. The important advantage of the support vector classifier is that it offers a possibility to train generalizable, nonlinear classifiers in high-dimensional spaces using a small training set. Moreover, for large training sets, it typically selects a small support set which is necessary for designing the classifier, thereby minimizing the computational requirements during testing.
The support vector classifier can also be understood in terms of the traditional template matching techniques. The support vectors replace the prototypes with the main difference being that they characterize the classes by a decision boundary. Moreover, this decision boundary is not just defined by the minimum distance function, but by a more general, possibly nonlinear, combination of these distances.
We summarize the most commonly used classifiers in Table 6. Many of them represent, in fact, an entire family of classifiers and allow the user to modify several associated parameters and criterion functions. All (or almost all) of these classifiers are admissible, in the sense that there exist some classification problems for which they are the best choice. An extensive comparison of a large set of classifiers over many different problems is the StatLog project [109] which showed a large variability over their relative performances, illustrating that there is no such thing as an overall optimal classification rule.
The differences between the decision boundaries obtained by different classifiers are illustrated in Fig. 7 using dataset 1 (2-dimensional, two-class problem with Gaussian densities). Note the two small isolated areas for R1 in Fig. 7c for the 1-NN rule. The neural network classifier in Fig. 7d even shows a ªghostº region that seemingly has nothing to do with the data. Such regions are less probable for a small number of hidden layers at the cost of poorer class separation.
JAIN ET AL.: STATISTICAL PATTERN RECOGNITION: A REVIEW 19
4. http://www.gmd.de/ml-archive/
A larger hidden layer may result in overtraining. This is
illustrated in Fig. 8 for a network with 10 neurons in the
hidden layer. During training, the test set error and the
training set error are initially almost equal, but after a certain point (three epochs5) the test set error starts to
increase while the training error keeps on decreasing. The
final classifier after 50 epochs has clearly adapted to the
noise in the dataset: it tries to separate isolated patterns in a
way that does not contribute to its generalization ability.

6 CLASSIFIER COMBINATION
There are several reasons for combining multiple classifiers to solve a given classification problem. Some of them are listed below:
1. A designer may have access to a number of different classifiers, each developed in a different context and
for an entirely different representation/description of the same problem. An example is the identification of persons by their voice, face, as well as handwriting.
2. Sometimes more than a single training set is available, each collected at a different time or in a different environment. These training sets may even use different features.
3. Different classifiers trained on the same data may not only differ in their global performances, but they also may show strong local differences. Each classifier may have its own region in the feature space where it performs the best.
4. Some classifiers such as neural networks show different results with different initializations due to the randomness inherent in the training procedure. Instead of selecting the best network and discarding the others, one can combine various networks,
TABLE 6 Classification Methods
5. One epoch means going through the entire training data once.
thereby taking advantage of all the attempts to learn from the data.
In summary, we may have different feature sets, different training sets, different classification methods or
different training sessions, all resulting in a set of classifiers
whose outputs may be combined, with the hope of improving the overall classification accuracy. If this set of
classifiers is fixed, the problem focuses on the combination function. It is also possible to use a fixed combiner and
optimize the set of input classifiers, see Section 6.1. A large number of combination schemes have been
proposed in the literature [172]. A typical combination
scheme consists of a set of individual classifiers and a
combiner which combines the results of the individual
classifiers to make the final decision. When the individual
classifiers should be invoked or how they should interact
with each other is determined by the architecture of the
combination scheme. Thus, various combination schemes
may differ from each other in their architectures, the
characteristics of the combiner, and selection of the
individual classifiers.
Various schemes for combining multiple classifiers can be grouped into three main categories according to their architecture: 1) parallel, 2) cascading (or serial combination), and 3) hierarchical (tree-like). In the parallel architecture, all the individual classifiers are invoked independently, and their results are then combined by a combiner. Most combination schemes in the literature belong to this category. In the gated parallel variant, the outputs of individual classifiers are selected or weighted by a gating device before they are combined. In the cascading architecture, individual classifiers are invoked in a linear sequence. The number of possible classes for a given pattern is gradually reduced as more classifiers in the sequence have been invoked. For the sake of efficiency, inaccurate but cheap classifiers (low computational and measurement demands) are considered first, followed by more accurate and expensive classifiers. In the hierarchical architecture, individual classifiers are combined into a structure, which is similar to that of a decision tree classifier. The tree nodes, however, may now be associated with complex classifiers demanding a large number of features. The advantage of this architecture is the high efficiency and flexibility in
exploiting the discriminant power of different types of features. Using these three basic architectures, we can build even more complicated classifier combination systems.

6.1 Selection and Training of Individual Classifiers
A classifier combination is especially useful if the individual classifiers are largely independent. If this is not already guaranteed by the use of different training sets, various resampling techniques like rotation and bootstrapping may be used to artificially create such differences. Examples are stacking [168], bagging [21], and boosting (or ARCing) [142]. In stacking, the outputs of the individual classifiers are used to train the ªstackedº classifier. The final decision is made based on the outputs of the stacked classifier in conjunction with the outputs of individual classifiers.
In bagging, different datasets are created by bootstrapped versions of the original dataset and combined using a fixed rule like averaging. Boosting [52] is another resampling technique for generating a sequence of training data sets. The distribution of a particular training set in the sequence is overrepresented by patterns which were misclassified by the earlier classifiers in the sequence. In boosting, the individual classifiers are trained hierarchically to learn to discriminate more complex regions in the feature space. The original algorithm was proposed by Schapire [142], who showed that, in principle, it is possible for a combination of weak classifiers (whose performances are only slightly better than random guessing) to achieve an error rate which is arbitrarily small on the training data.
Sometimes cluster analysis may be used to separate the individual classes in the training set into subclasses. Consequently, simpler classifiers (e.g., linear) may be used and combined later to generate, for instance, a piecewise linear result [120].
Instead of building different classifiers on different sets of training patterns, different feature sets may be used. This even more explicitly forces the individual classifiers to contain independent information. An example is the random subspace method [75].

6.2 Combiner
After individual classifiers have been selected, they need to be combined together by a module, called the combiner. Various combiners can be distinguished from each other in their trainability, adaptivity, and requirement on the output of individual classifiers. Combiners, such as voting, averaging (or sum), and Borda count [74] are static, with no training required, while others are trainable. The trainable combiners may lead to a better improvement than static combiners at the cost of additional training as well as the requirement of additional training data.
Some combination schemes are adaptive in the sense that the combiner evaluates (or weighs) the decisions of individual classifiers depending on the input pattern. In contrast, nonadaptive combiners treat all the input patterns the same. Adaptive combination schemes can further exploit the detailed error characteristics and expertise of individual classifiers. Examples of adaptive combiners include adaptive weighting [156], associative switch, mixture of local experts (MLE) [79], and hierarchical MLE [87].
Different combiners expect different types of output from individual classifiers. Xu et al. [172] grouped these expectations into three levels: 1) measurement (or confidence), 2) rank, and 3) abstract. At the confidence level, a classifier outputs a numerical value for each class indicating the belief or probability that the given input pattern belongs to that class. At the rank level, a classifier assigns a rank to each class with the highest rank being the first choice. Rank value cannot be used in isolation because the highest rank does not necessarily mean a high confidence in the classification. At the abstract level, a classifier only outputs a unique class label or several class labels (in which case, the classes are equally good). The confidence level conveys the richest information, while the abstract level contains the least amount of information about the decision being made.
Table 7 lists a number of representative combination schemes and their characteristics. This is by no means an exhaustive list.

6.3 Theoretical Analysis of Combination Schemes
A large number of experimental studies have shown that classifier combination can improve the recognition accuracy. However, there exist only a few theoretical explanations for these experimental results. Moreover, most explanations apply to only the simplest combination schemes under rather restrictive assumptions. One of the most rigorous theories on classifier combination is presented by Kleinberg [91].
A popular analysis of combination schemes is based on the well-known bias-variance dilemma [64], [93]. Regression or classification error can be decomposed into a bias term and a variance term. Unstable classifiers or classifiers with a high complexity (or capacity), such as decision trees, nearest neighbor classifiers, and large-size neural networks, can have universally low bias, but a large variance. On the other hand, stable classifiers or classifiers with a low capacity can have a low variance but a large bias.
Tumer and Ghosh [158] provided a quantitative analysis of the improvements in classification accuracy by combining multiple neural networks. They showed that combining
networks using a linear combiner or order statistics combiner reduces the variance of the actual decision boundaries around the optimum boundary. In the absence of network bias, the reduction in the added error (to Bayes error) is directly proportional to the reduction in the variance. A linear combination of N unbiased neural networks with independent and identically distributed (i.i.d.) error distributions can reduce the variance by a factor of N . At a first glance, this result sounds remarkable for as N approaches infinity, the variance is reduced to zero. Unfortunately, this is not realistic because the i.i.d. assumption breaks down for large N . Similarly, Perrone and Cooper [123] showed that under the zero-mean and independence assumption on the misfit (difference between the desired output and the actual output), averaging the outputs of N neural networks can reduce the mean square error (MSE) by a factor of N compared to the averaged MSE of the N neural networks. For a large N , the MSE of the ensemble can, in principle, be made arbitrarily small. Unfortunately, as mentioned above, the independence assumption breaks down as N increases. Perrone and Cooper [123] also proposed a generalized ensemble, an optimal linear combiner in the least square error sense. In the generalized ensemble, weights are derived from the error correlation matrix of the N neural networks. It was shown that the MSE of the generalized ensemble is smaller than the MSE of the best neural network in the ensemble. This result is based on the assumptions that the rows and columns of the error correlation matrix are linearly independent and the error correlation matrix can be reliably estimated. Again, these assumptions break down as N increases. Kittler et al. [90] developed a common theoretical framework for a class of combination schemes where individual classifiers use distinct features to estimate the posterior probabilities given the input pattern. They introduced a sensitivity analysis to explain why the sum (or average) rule outperforms the other rules for the same class. They showed that the sum rule is less sensitive than others (such as the ªproductº rule) to the error of individual classifiers in estimating posterior probabilities. The sum rule is most appropriate for combining different estimates of the same posterior probabilities, e.g., resulting from different classifier initializations (case (4) in the introduction of this section). The product rule is most appropriate for combining preferably error-free independent probabilities, e.g. resulting from well estimated densities of different, independent feature sets (case (2) in the introduction of this section). Schapire et al. [143] proposed a different explanation for the effectiveness of voting (weighted average, in fact) methods. The explanation is based on the notion of ªmarginº which is the difference between the combined score of the correct class and the highest combined score among all the incorrect classes. They established that the generalization error is bounded by the tail probability of the margin distribution on training data plus a term which is a function of the complexity of a single classifier rather than
the combined classifier. They demonstrated that the boosting algorithm can effectively improve the margin distribution. This finding is similar to the property of the support vector classifier, which shows the importance of training patterns near the margin, where the margin is defined as the area of overlap between the class conditional densities.

6.4 An Example
We will illustrate the characteristics of a number of different classifiers and combination rules on a digit classification problem (Dataset 3, see Section 2). The classifiers used in the experiment were designed using Matlab and were not optimized for the data set. All the six different feature sets for the digit dataset discussed in Section 2 will be used, enabling us to illustrate the performance of various classifier combining rules over different classifiers as well as over different feature sets. Confidence values in the outputs of all the classifiers are computed, either directly based on the posterior probabilities or on the logistic output function as discussed in Section 5. These outputs are also used to obtain multiclass versions for intrinsically two-class discriminants such as the Fisher Linear Discriminant and the Support Vector Classifier (SVC). For these two classifiers, a total of 10 discriminants are computed between each of the 10 classes and the combined set of the remaining classes. A test pattern is classified by selecting the class for which the discriminant has the highest confidence.
The following 12 classifiers are used (also see Table 8): the Bayes-plug-in rule assuming normal distributions with different (Bayes-normal-quadratic) or equal covariance matrices (Bayes-normal-linear), the Nearest Mean (NM) rule, 1-NN, k-NN, Parzen, Fisher, a binary decision tree using the maximum purity criterion [21] and early pruning, two feed-forward neural networks (based on the Matlab Neural Network Toolbox) with a hidden layer consisting of 20 (ANN-20) and 50 (ANN-50) neurons and the linear (SVC-linear) and quadratic (SVC-quadratic) Support Vector classifiers. The number of neighbors in the k-NN rule and the smoothing parameter in the Parzen classifier are both optimized over the classification result using the leave-oneout error estimate on the training set. For combining classifiers, the median, product, and voting rules are used, as well as two trained classifiers (NM and 1-NN). The training set used for the individual classifiers is also used in classifier combination.
The 12 classifiers listed in Table 8 were trained on the same 500 (10 50) training patterns from each of the six feature sets and tested on the same 1,000 (10 100) test patterns. The resulting classification errors (in percentage) are reported; for each feature set, the best result over the classifiers is printed in bold. Next, the 12 individual classifiers for a single feature set were combined using the five combining rules (median, product, voting, nearest mean, and 1-NN). For example, the voting rule (row) over the classifiers using feature set Number 3 (column) yields an error of 3.2 percent. It is underlined to indicate that this combination result is better than the performance of individual classifiers for this feature set. Finally, the outputs of each classifier and each classifier combination scheme over all the six feature sets are combined using the five
combination rules (last five columns). For example, the voting rule (column) over the six decision tree classifiers (row) yields an error of 21.8 percent. Again, it is underlined to indicate that this combination result is better than each of the six individual results of the decision tree. The 5 5 block in the bottom right part of Table 8 presents the combination results, over the six feature sets, for the classifier combination schemes for each of the separate feature sets.
Some of the classifiers, for example, the decision tree, do not perform well on this data. Also, the neural network classifiers provide rather poor optimal solutions, probably due to nonconverging training sessions. Some of the simple classifiers such as the 1-NN, Bayes plug-in, and Parzen give good results; the performances of different classifiers vary substantially over different feature sets. Due to the relatively small training set for some of the large feature sets, the Bayes-normal-quadratic classifier is outperformed by the linear one, but the SVC-quadratic generally performs better than the SVC-linear. This shows that the SVC classifier can find nonlinear solutions without increasing the overtraining risk.
Considering the classifier combination results, it appears that the trained classifier combination rules are not always better than the use of fixed rules. Still, the best overall result (1.5 percent error) is obtained by a trained combination rule, the nearest mean method. The combination of different classifiers for the same feature set (columns in the table) only slightly improves the best individual classification results. The best combination rule for this dataset is voting. The product rule behaves poorly, as can be expected, because different classifiers on the same feature set do not provide independent confidence values. The combination of results obtained by the same classifier over different feature sets (rows in the table) frequently outperforms the best individual classifier result. Sometimes, the improvements are substantial as is the case for the decision tree. Here, the product rule does much better, but occasionally it performs surprisingly bad, similar to the combination of neural network classifiers. This combination rule (like the minimum and maximum rules, not used in this experiment) is sensitive to poorly trained individual classifiers. Finally, it is worthwhile to observe that in combining the neural network results, the trained combination rules do very well (classification errors between 2.1 percent and 5.6 percent) in comparison with the fixed rules (classification errors between 16.3 percent to 90 percent).

7 ERROR ESTIMATION
The classification error or simply the error rate, Pe, is the ultimate measure of the performance of a classifier. Competing classifiers can also be evaluated based on their error probabilities. Other performance measures include the cost of measuring features and the computational requirements of the decision rule. While it is easy to define the probability of error in terms of the classconditional densities, it is very difficult to obtain a closedform expression for Pe. Even in the relatively simple case of multivariate Gaussian densities with unequal covariance matrices, it is not possible to write a simple
analytical expression for the error rate. If an analytical expression for the error rate was available, it could be used, for a given decision rule, to study the behavior of Pe as a function of the number of features, true parameter values of the densities, number of training samples, and prior class probabilities. For consistent training rules the value of Pe approaches the Bayes error for increasing sample sizes. For some families of distributions tight bounds for the Bayes error may be obtained [7]. For finite sample sizes and unknown distributions, however, such bounds are impossible [6], [41].
In practice, the error rate of a recognition system must be estimated from all the available samples which are split into training and test sets [70]. The classifier is first designed using training samples, and then it is evaluated based on its classification performance on the test samples. The percentage of misclassified test samples is taken as an estimate of the error rate. In order for this error estimate to be reliable in predicting future classification performance, not only should the training set and the test set be sufficiently large, but the training samples and the test samples must be independent. This requirement of independent training and test samples is still often overlooked in practice.
An important point to keep in mind is that the error estimate of a classifier, being a function of the specific training and test sets used, is a random variable. Given a classifier, suppose is the number of test samples (out of a total of n) that are misclassified. It can be shown that the probability density function of has a binomial distribution. The maximum-likelihood estimate, P̂e, of Pe is given by P̂e =n, with E P̂e Pe and V ar P̂e Pe 1ÿ Pe =n. Thus, P̂e is an unbiased and consistent estimator. Because P̂e is a random variable, a confidence interval is associated with it. Suppose n 250 and 50 then P̂e 0:2 and a 95 percent confidence interval of P̂e is 0:15; 0:25 . The confidence interval, which shrinks as the number n of test
samples increases, plays an important role in comparing two competing classifiers, C1 and C2. Suppose a total of 100 test samples are available and C1 and C2 misclassify 10 and 13, respectively, of these samples. Is classifier C1 better than C2? The 95 percent confidence intervals for the true error probabilities of these classifiers are 0:04; 0:16 and 0:06; 0:20 , respectively. Since these confidence intervals overlap, we cannot say that the performance of C1 will always be superior to that of C2. This analysis is somewhat pessimistic due to positively correlated error estimates based on the same test set [137].
How should the available samples be split to form training and test sets? If the training set is small, then the resulting classifier will not be very robust and will have a low generalization ability. On the other hand, if the test set is small, then the confidence in the estimated error rate will be low. Various methods that are commonly used to estimate the error rate are summarized in Table 9. These methods differ in how they utilize the available samples as training and test sets. If the number of available samples is extremely large (say, 1 million), then all these methods are likely to lead to the same estimate of the error rate. For example, while it is well known that the resubstitution method provides an optimistically biased estimate of the error rate, the bias becomes smaller and smaller as the ratio of the number of training samples per class to the dimensionality of the feature vector gets larger and larger. There are no good guidelines available on how to divide the available samples into training and test sets; Fukunaga [58] provides arguments in favor of using more samples for testing the classifier than for designing the classifier. No matter how the data is split into training and test sets, it should be clear that different random splits (with the specified size of training and test sets) will result in different error estimates.
Fig. 9 shows the classification error of the Bayes plug-in linear classifier on the digit dataset as a function of the number of training patterns. The test set error gradually approaches the training set error (resubstitution error) as the number of training samples increases. The relatively large difference between these two error rates for 100 training patterns per class indicates that the bias in these two error estimates can be further reduced by enlarging the training set. Both the curves in this figure represent the average of 50 experiments in which training sets of the given size are randomly drawn; the test set of 1,000 patterns is fixed.
The holdout, leave-one-out, and rotation methods are versions of the cross-validation approach. One of the main disadvantages of cross-validation methods, especially for small sample size situations, is that not all the available n samples are used for training the classifier. Further, the two extreme cases of cross validation, hold out method and leave-one-out method, suffer from either large bias or large variance, respectively. To overcome this limitation, the bootstrap method [48] has been proposed to estimate the error rate. The bootstrap method resamples the available patterns with replacement to generate a number of ªfakeº data sets (typically, several hundred) of the same size as the given training set. These new training sets can be used not only to estimate the bias of the resubstitution estimate, but also to define other, so called bootstrap estimates of the error rate. Experimental results have shown that the bootstrap estimates can outperform the cross validation estimates and the resubstitution estimates of the error rate [82].
In many pattern recognition applications, it is not adequate to characterize the performance of a classifier by a single number, P̂e, which measures the overall error rate of a system. Consider the problem of evaluating a fingerprint matching system, where two different yet related error rates are of interest. The False Acceptance Rate (FAR) is the ratio of the number of pairs of different fingerprints that are incorrectly matched by a given system to the total number of match attempts. The False Reject Rate (FRR) is the ratio of the number of pairs of the same fingerprint that are not matched by a given system to the total number of match attempts. A fingerprint matching system can be tuned (by setting an appropriate threshold on the matching score) to operate at a desired value of FAR. However, if we try to decrease the FAR of the system, then it would increase the FRR and vice versa. The Receiver Operating Characteristic (ROC) Curve [107] is a plot of FAR versus FRR which permits the system designer to assess the performance of the recognition system at various operating points (thresholds in the decision rule). In this sense, ROC provides a more comprehensive performance measure than, say, the equal error rate of the system (where FRR = FAR). Fig. 10 shows the ROC curve for the digit dataset where the Bayes plug-in linear classifier is trained on 100 patterns per class. Examples of the use of ROC analysis are combining classifiers [170] and feature selection [99].
In addition to the error rate, another useful perfor-
mance measure of a classifier is its reject rate. Suppose a
test pattern falls near the decision boundary between the
two classes. While the decision rule may be able to
correctly classify such a pattern, this classification will be
made with a low confidence. A better alternative would
be to reject these doubtful patterns instead of assigning
them to one of the categories under consideration. How
do we decide when to reject a test pattern? For the Bayes
decision rule, a well-known reject option is to reject a
pattern if its maximum a posteriori probability is below a
threshold; the larger the threshold, the higher the reject
rate. Invoking the reject option reduces the error rate; the
larger the reject rate, the smaller the error rate. This
relationship is represented as an error-reject trade-off
curve which can be used to set the desired operating
point of the classifier. Fig. 11 shows the error-reject curve
for the digit dataset when a Bayes plug-in linear classifier
is used. This curve is monotonically non-increasing, since
rejecting more patterns either reduces the error rate or
keeps it the same. A good choice for the reject rate is
based on the costs associated with reject and incorrect
decisions (See [66] for an applied example of the use of
error-reject curves).

8 UNSUPERVISED CLASSIFICATION
In many applications of pattern recognition, it is extremely difficult or expensive, or even impossible, to reliably label a training sample with its true category. Consider, for example, the application of land-use classification in remote sensing. In order to obtain the ªground truthº information (category for each pixel) in the image, either the specific site associated with the pixel should be visited or its category should be extracted from a Geographical Information System, if one is available. Unsupervised classification refers to situations where the objective is to construct decision boundaries based on unlabeled training data. Unsupervised classification is also known as data clustering which is a generic label for a variety of procedures designed to find natural groupings, or clusters, in multidimensional data, based on measured or perceived similarities among the patterns [81]. Category labels and other information
about the source of the data influence the interpretation of the clustering, not the formation of the clusters.
Unsupervised classification or clustering is a very difficult problem because data can reveal clusters with different shapes and sizes (see Fig. 12). To compound the problem further, the number of clusters in the data often depends on the resolution (fine vs. coarse) with which we view the data. One example of clustering is the detection and delineation of a region containing a high density of patterns compared to the background. A number of functional definitions of a cluster have been proposed which include: 1) patterns within a cluster are more similar to each other than are patterns belonging to different clusters and 2) a cluster consists of a relatively high density of points separated from other clusters by a relatively low density of points. Even with these functional definitions of a cluster, it is not easy to come up with an operational definition of clusters. One of the challenges is to select an appropriate measure of similarity to define clusters which, in general, is both data (cluster shape) and context dependent.
Cluster analysis is a very important and useful technique. The speed, reliability, and consistency with which a clustering algorithm can organize large amounts of data constitute overwhelming reasons to use it in applications such as data mining [88], information retrieval [17], [25], image segmentation [55], signal compression and coding [1], and machine learning [25]. As a consequence, hundreds of clustering algorithms have been proposed in the literature and new clustering algorithms continue to appear. However, most of these algorithms are based on the following two popular clustering techniques: iterative square-error partitional clustering and agglomerative hierarchical clustering. Hierarchical techniques organize data in a nested sequence of groups which can be displayed in the form of a dendrogram or a tree. Square-error partitional algorithms attempt to obtain that partition which minimizes the within-cluster scatter or maximizes the between-cluster scatter. To guarantee that an optimum solution has been obtained, one has to examine all possible partitions of the n
d-dimensional patterns into K clusters (for a given K), which is not computationally feasible. So, various heuristics are used to reduce the search, but then there is no guarantee of optimality.
Partitional clustering techniques are used more frequently than hierarchical techniques in pattern recognition applications, so we will restrict our coverage to partitional methods. Recent studies in cluster analysis suggest that a user of a clustering algorithm should keep the following issues in mind: 1) every clustering algorithm will find clusters in a given dataset whether they exist or not; the data should, therefore, be subjected to tests for clustering tendency before applying a clustering algorithm, followed by a validation of the clusters generated by the algorithm; 2) there is no ªbestº clustering algorithm. Therefore, a user is advised to try several clustering algorithms on a given dataset. Further, issues of data collection, data representation, normalization, and cluster validity are as important as the choice of clustering strategy.
The problem of partitional clustering can be formally stated as follows: Given n patterns in a d-dimensional metric space, determine a partition of the patterns into K clusters, such that the patterns in a cluster are more similar to each other than to patterns in different clusters [81]. The value of K may or may not be specified. A clustering criterion, either global or local, must be adopted. A global criterion, such as square-error, represents each cluster by a prototype and assigns the patterns to clusters according to the most similar prototypes. A local criterion forms clusters by utilizing local structure in the data. For example, clusters can be formed by identifying high-density regions in the pattern space or by assigning a pattern and its k nearest neighbors to the same cluster.
Most of the partitional clustering techniques implicitly assume continuous-valued feature vectors so that the patterns can be viewed as being embedded in a metric space. If the features are on a nominal or ordinal scale, Euclidean distances and cluster centers are not very meaningful, so hierarchical clustering methods are normally applied. Wong and Wang [169] proposed a clustering
algorithm for discrete-valued data. The technique of conceptual clustering or learning from examples [108] can be used with patterns represented by nonnumeric or symbolic descriptors. The objective here is to group patterns into conceptually simple classes. Concepts are defined in terms of attributes and patterns are arranged into a hierarchy of classes described by concepts.
In the following subsections, we briefly summarize the two most popular approaches to partitional clustering: square-error clustering and mixture decomposition. A square-error clustering method can be viewed as a particular case of mixture decomposition. We should also point out the difference between a clustering criterion and a clustering algorithm. A clustering algorithm is a particular implementation of a clustering criterion. In this sense, there are a large number of square-error clustering algorithms, each minimizing the square-error criterion and differing from the others in the choice of the algorithmic parameters. Some of the well-known clustering algorithms are listed in Table 10 [81].

8.1 Square-Error Clustering
The most commonly used partitional clustering strategy is based on the square-error criterion. The general objective is to obtain that partition which, for a fixed number of clusters, minimizes the square-error. Suppose that the given set of n patterns in d dimensions has somehow been partitioned into K clusters fC1; C2; ; Ckg such that cluster Ck has nk patterns and each pattern is in exactly one cluster, so that PK k 1 nk n.
The mean vector, or center, of cluster Ck is defined as the centroid of the cluster, or
m k 1 nk Xnk i 1 x k i ; 14
where x k i is the ith pattern belonging to cluster Ck. The square-error for cluster Ck is the sum of the squared Euclidean distances between each pattern in Ck and its cluster center m k . This square-error is also called the within-cluster variation
e2k Xnk i 1 x k i ÿm k T x k i ÿm k : 15
The square-error for the entire clustering containing K clusters is the sum of the within-cluster variations:
E2K XK k 1 e2k: 16
The objective of a square-error clustering method is to find a partition containing K clusters that minimizes E2K for a fixed K. The resulting partition has also been referred to as
the minimum variance partition. A general algorithm for
the iterative partitional clustering method is given below.
Agorithm for iterative partitional clustering:
Step 1. Select an initial partition with K clusters. Repeat steps 2 through 5 until the cluster membership stabilizes. Step 2. Generate a new partition by assigning each pattern to its closest cluster center. Step 3. Compute new cluster centers as the centroids of the clusters. Step 4. Repeat steps 2 and 3 until an optimum value of the criterion function is found. Step 5. Adjust the number of clusters by merging and splitting existing clusters or by removing small, or outlier, clusters.
The above algorithm, without step 5, is also known as the K-means algorithm. The details of the steps in this algorithm must either be supplied by the user as parameters or be implicitly hidden in the computer program. However, these details are crucial to the success of the program. A big frustration in using clustering programs is the lack of guidelinesavailable forchoosingK, initialpartition,updating the partition, adjusting the number of clusters, and the stopping criterion [8].
The simple K-means partitional clustering algorithm described above is computationally efficient and gives surprisingly good results if the clusters are compact, hyperspherical in shape and well-separated in the feature space. If the Mahalanobis distance is used in defining the squared error in (16), then the algorithm is even able to detect hyperellipsoidal shaped clusters. Numerous attempts have been made to improve the performance of the basic K-means algorithm by 1) incorporating a fuzzy criterion function [15], resulting in a fuzzy K-means (or c-means) algorithm, 2) using genetic algorithms, simulated annealing, deterministic annealing, and tabu search to
TABLE 10 Clustering Algorithms
optimize the resulting partition [110], [139], and 3) mapping it onto a neural network [103] for possibly efficient implementation. However, many of these so-called enhancements to the K-means algorithm are computationally demanding and require additional user-specified parameters for which no general guidelines are available. Judd et al. [88] show that a combination of algorithmic enhancements to a square-error clustering algorithm and distribution of the computations over a network of workstations can be used to cluster hundreds of thousands of multidimensional patterns in just a few minutes.
It is interesting to note how seemingly different concepts for partitional clustering can lead to essentially the same algorithm. It is easy to verify that the generalized Lloyd vector quantization algorithm used in the communication and compression domain is equivalent to the K-means algorithm. A vector quantizer (VQ) is described as a combination of an encoder and a decoder. A d-dimensional VQ consists of two mappings: an encoder which maps the input alphabet A to the channel symbol set M , and a decoder which maps the channel symbol set M to the output alphabet Â , i.e., y : A!M and v : M! Â. A distortion measure D y; ŷ specifies the cost associated with quantization, where ŷ y . Usually, an optimal quantizer minimizes the average distortion under a size constraint on M. Thus, the problem of vector quantization can be posed as a clustering problem, where the number of clusters K is now the size of the output alphabet, Â : fŷi ; i 1; . . . ; Kg, and the goal is to find a quantization (referred to as a partition in the K-means algorithm) of the d-dimensional feature space which minimizes the average distortion (mean square error) of the input patterns. Vector quantization has been widely used in a number of compression and coding applications, such as speech waveform coding, image coding, etc., where only the symbols for the output alphabet or the cluster centers are transmitted instead of the entire signal [67], [32]. Vector quantization also provides an efficient tool for density estimation [68]. A kernel-based approach (e.g., a mixture of Gaussian kernels, where each kernel is placed at a cluster center) can be used to estimate the probability density of the training samples. A major issue in VQ is the selection of the output alphabet size. A number of techniques, such as the minimum description length (MDL) principle [138], can be used to select this parameter (see Section 8.2). The supervised version of VQ is called learning vector quantization (LVQ) [92].

8.2 Mixture Decomposition
Finite mixtures are a flexible and powerful probabilistic modeling tool. In statistical pattern recognition, the main use of mixtures is in defining formal (i.e., model-based) approaches to unsupervised classification [81]. The reason behind this is that mixtures adequately model situations where each pattern has been produced by one of a set of alternative (probabilistically modeled) sources [155]. Nevertheless, it should be kept in mind that strict adherence to this interpretation is not required: mixtures can also be seen as a class of models that are able to represent arbitrarily complex probability density functions. This makes mixtures also well suited for representing complex class-conditional
densities in supervised learning scenarios (see [137] and references therein). Finite mixtures can also be used as a feature selection tool [127].

8.2.1 Basic Definitions
Consider the following scheme for generating random samples. There are K random sources, each characterized by a probability (mass or density) function pm yj m , parameterized by m, for m 1; :::; K. Each time a sample is to be generated, we randomly choose one of these sources, with probabilities f 1; :::; Kg, and then sample from the chosen source. The random variable defined by this (two-stage) compound generating mechanism is characterized by a finite mixture distribution; formally, its probability function is
p yj K XK m 1 mpm yj m ; 17
where each pm yj m is called a component, and K f 1; :::; K; 1; :::; Kÿ1g. It is usually assumed that all the components have the same functional form; for example, they are all multivariate Gaussian. Fitting a mixture model to a set of observations y fy 1 ; :::;y n g consists of estimating the set of mixture parameters that best describe this data. Although mixtures can be built from many different types of components, the majority of the literature focuses on Gaussian mixtures [155].
The two fundamental issues arising in mixture fitting are: 1) how to estimate the parameters defining the mixture model and 2) how to estimate the number of components [159]. For the first question, the standard answer is the expectation-maximization (EM) algorithm (which, under mild conditions, converges to the maximum likelihood (ML) estimate of the mixture parameters); several authors have also advocated the (computationally demanding) Markov chain Monte-Carlo (MCMC) method [135]. The second question is more difficult; several techniques have been proposed which are summarized in Section 8.2.3. Note that the output of the mixture decomposition is as good as the validity of the assumed component distributions.

8.2.2 EM Algorithm
The expectation-maximization algorithm interprets the given observations y as incomplete data, with the missing part being a set of labels associated with y;
z fz 1 ; :::; z K g: Missing variable z i z i 1 ; :::; z i K T indicates which of the K components generated y i ; if it was the mth component, then z i m 1 and z i p 0, for p 6 m [155]. In the presence of both y and z, the (complete) log-likelihood can be written as
Lc K ;y; z ÿ Xn
j 1 XK m 1 z j m log mpm y j j m h i ; XK i 1 i 1:
18 The EM algorithm proceeds by alternatively applying the following two steps:
. E-step: Compute the conditional expectation of the complete log-likelihood (given y and the current parameter estimate, b t K ). Since (18) is linear in the missing variables, the E-step for mixtures reduces to the computation of the conditional expectation of the missing variables: w i;t m E z i m j b t K ;y . . M-step: Update the parameter estimates:
b t 1 K arg max K Q K ; b t K For the mixing probabilities, this becomes
b t 1 m 1nXn i 1 w i;t m ;m 1; 2; ; K ÿ 1: 19
In the Gaussian case, each m consists of a mean vector and a covariance matrix which are updated using weighted versions (with weights b t 1 m ) of the standard ML estimates [155].
The main difficulties in using EM for mixture model fitting, which are current research topics, are: its local nature, which makes it critically dependent on initialization; the possibility of convergence to a point on the boundary of the parameter space with unbounded likelihood (i.e., one of the m approaches zero with the corresponding covariance becoming arbitrarily close to singular).

8.2.3 Estimating the Number of Components
The ML criterion can not be used to estimate the number of mixture components because the maximized likelihood is a nondecreasing function of K, thereby making it useless as a model selection criterion (selecting a value for K in this case). This is a particular instance of the identifiability problem where the classical ( 2-based) hypothesis testing cannot be used because the necessary regularity conditions are not met [155]. Several alternative approaches that have been proposed are summarized below.
EM-based approaches use the (fixed K) EM algorithm to obtain a sequence of parameter estimates for a range of values of K, fb K ; K Kmin; :::; Kmaxg; the estimate of K is then defined as the minimizer of some cost function,
bK arg minK C b K ; K ; K Kmin; :::;Kmaxn o: 20 Most often, this cost function includes the maximized loglikelihood function plus an additional term whose role is to penalize large values of K. An obvious choice in this class is to use the minimum description length (MDL) criterion [10] [138], but several other model selection criteria have been proposed: Schwarz's Bayesian inference criterion (BIC), the minimum message length (MML) criterion, and Akaike's information criterion (AIC) [2], [148], [167].
Resampling-based schemes and cross-validation-type approaches have also been suggested; these techniques are (computationally) much closer to stochastic algorithms than to the methods in the previous paragraph. Stochastic approaches generally involve Markov chain Monte Carlo (MCMC) [135] sampling and are far more computationally intensive than EM. MCMC has been used in two different ways: to implement model selection criteria to actually estimate K; and, with a more ªfully Bayesian flavorº, to
sample from the full a posteriori distribution where K is included as an unknown. Despite their formal appeal, we think that MCMC-based techniques are still far too computationally demanding to be useful in pattern recognition applications.
Fig. 13 shows an example of mixture decomposition, where K is selected using a modified MDL criterion [51]. The data consists of 800 two-dimensional patterns distributed over three Gaussian components; two of the components have the same mean vector but different covariance matrices and that is why one dense cloud of points is inside another cloud of rather sparse points. The level curve contours (of constant Mahalanobis distance) for the true underlying mixture and the estimated mixture are superimposed on the data. For details, see [51]. Note that a clustering algorithm such as K-means will not be able to identify these three components, due to the substantial overlap of two of these components.

9 DISCUSSION
In its early stage of development, statistical pattern recognition focused mainly on the core of the discipline: The Bayesian decision rule and its various derivatives (such as linear and quadratic discriminant functions), density estimation, the curse of dimensionality problem, and error estimation. Due to the limited computing power available in the 1960s and 1970s, statistical pattern recognition employed relatively simple techniques which were applied to small-scale problems.
Since the early 1980s, statistical pattern recognition has experienced a rapid growth. Its frontiers have been expanding in many directions simultaneously. This rapid expansion is largely driven by the following forces.
1. Increasing interaction and collaboration among different disciplines, including neural networks, machine learning, statistics, mathematics, computer science, and biology. These multidisciplinary efforts have fostered new ideas, methodologies, and techniques which enrich the traditional statistical pattern recognition paradigm. 2. The prevalence of fast processors, the Internet, large and inexpensive memory and storage. The advanced computer technology has made it possible to implement complex learning, searching and optimization algorithms which was not feasible a few decades ago. It also allows us to tackle large-scale real world pattern recognition problems which may involve millions of samples in high dimensional spaces (thousands of features). 3. Emerging applications, such as data mining and document taxonomy creation and maintenance. These emerging applications have brought new challenges that foster a renewed interest in statistical pattern recognition research. 4. Last, but not the least, the need for a principled, rather than ad hoc approach for successfully solving pattern recognition problems in a predictable way. For example, many concepts in neural networks, which were inspired by biological neural networks,
can be directly treated in a principled way in statistical pattern recognition.

9.1 Frontiers of Pattern Recognition
Table 11 summarizes several topics which, in our opinion, are at the frontiers of pattern recognition. As we can see from Table 11, many fundamental research problems in statistical pattern recognition remain at the forefront even as the field continues to grow. One such example, model selection (which is an important issue in avoiding the curse of dimensionality), has been a topic of continued research interest. A common practice in model selection relies on cross-validation (rotation method), where the best model is selected based on the performance on the validation set. Since the validation set is not used in training, this method does not fully utilize the precious data for training which is especially undesirable when the training data set is small. To avoid this problem, a number of model selection schemes [71] have been proposed, including Bayesian methods [14], minimum description length (MDL) [138], Akaike information criterion (AIC) [2] and marginalized likelihood [101], [159]. Various other regularization schemes which incorporate prior knowledge about model structure and parameters have also been proposed. Structural risk minimization based on the notion of VC dimension has also been used for model selection where the best model is the one with the best worst-case performance (upper bound on the generalization error) [162]. However, these methods do not reduce the complexity of the search for the best model. Typically, the complexity measure has to be evaluated for every possible model or in a set of prespecified models. Certain assumptions (e.g., parameter independence) are often made in order to simplify the complexity evaluation. Model selection based on stochastic complexity has been applied to feature selection in both supervised learning and unsupervised learning [159] and pruning in decision
Fig.13. Mixture Decomposition Example.
trees [106]. In the latter case, the best number of clusters is also automatically determined.
Another example is mixture modeling using EM algorithm (see Section 8.2), which was proposed in 1977 [36], and which is now a very popular approach for density estimation and clustering [159], due to the computing power available today.
Over the recent years, a number of new concepts and techniques have also been introduced. For example, the maximum margin objective was introduced in the context of support vector machines [23] based on structural risk minimization theory [162]. A classifier with a large margin separating two classes has a small VC dimension, which yields a good generalization performance. Many successful applications of SVMs have demonstrated the superiority of this objective function over others [72]. It is found that the boosting algorithm [143] also improves the margin distribution. The maximum margin objective can be considered as a special regularized cost function, where the regularizer is the inverse of the margin between the two classes. Other regularized cost functions, such as weight decay and weight elimination, have also been used in the context of neural networks.
Due to the introduction of SVMs, linear and quadratic programming optimization techniques are once again being extensively studied for pattern classification. Quadratic programming is credited for leading to the nice property that the decision boundary is fully specified by boundary patterns, while linear programming with the L1 norm or the inverse of the margin yields a small set of features when the optimal solution is obtained.
The topic of local decision boundary learning has also received a lot of attention. Its primary emphasis is on using patterns near the boundary of different classes to construct or modify the decision boundary. One such an example is the boosting algorithm and its variation (AdaBoost) where misclassified patterns, mostly near the decision boundary, are subsampled with higher probabilities than correctly classified patterns to form a new training set for training subsequent classifiers. Combination of local experts is also related to this concept, since local experts can learn local decision boundaries more accurately than global methods. In general, classifier combination could refine decision boundary such that its variance with respect to Bayes decision boundary is reduced, leading to improved recognition accuracy [158].
Sequential data arise in many real world problems, such as speech and on-line handwriting. Sequential pattern recognition has, therefore, become a very important topic in pattern recognition. Hidden Markov Models (HMM), have been a popular statistical tool for modeling and recognizing sequential data, in particular, speech data [130], [86]. A large number of variations and enhancements of HMMs have been proposed in the literature [12], including hybrids of HMMs and neural networks, inputoutput HMMs, weighted transducers, variable-duration HMMs, Markov switching models, and switching statespace models.
The growth in sensor technology and computing power has enriched the availability of data in several ways. Real world objects can now be represented by
many more measurements and sampled at high rates. As physical objects have a finite complexity, these measurements are generally highly correlated. This explains why models using spatial and spectral correlation in images, or the Markov structure in speech, or subspace approaches in general, have become so important; they compress the data to what is physically meaningful, thereby improving the classification accuracy simultaneously.
Supervised learning requires that every training sample be labeled with its true category. Collecting a large amount of labeled data can sometimes be very expensive. In practice, we often have a small amount of labeled data and a large amount of unlabeled data. How to make use of unlabeled data for training a classifier is an important problem. SVM has been extended to perform semisupervised learning [13].
Invariant pattern recognition is desirable in many applications, such as character and face recognition. Early research in statistical pattern recognition did emphasize extraction of invariant features which turns out to be a very difficult task. Recently, there has been some activity in designing invariant recognition methods which do not require invariant features. Examples are the nearest neighbor classifier using tangent distance [152] and deformable template matching [84]. These approaches only achieve invariance to small amounts of linear transformations and nonlinear deformations. Besides, they are computationally very intensive. Simard et al. [153] proposed an algorithm named Tangent-Prop to minimize the derivative of the classifier outputs with respect to distortion parameters, i.e., to improve the invariance property of the classifier to the selected distortion. This makes the trained classifier computationally very efficient.
It is well-known that the human recognition process relies heavily on context, knowledge, and experience. The effectiveness of using contextual information in resolving ambiguity and recognizing difficult patterns in the major differentiator between recognition abilities of human beings and machines. Contextual information has been successfully used in speech recognition, OCR, and remote sensing [173]. It is commonly used as a postprocessing step to correct mistakes in the initial recognition, but there is a recent trend to bring contextual information in the earlier stages (e.g., word segmentation) of a recognition system [174]. Context information is often incorporated through the use of compound decision theory derived from Bayes theory or Markovian models [175]. One recent, successful application is the hypertext classifier [176] where the recognition of a hypertext document (e.g., a web page) can be dramatically improved by iteratively incorporating the category information of other documents that point to or are pointed by this document.

9.2 Concluding Remarks
Watanabe [164] wrote in the preface of the 1972 book he edited, entitled Frontiers of Pattern Recognition, that ªPattern recognition is a fast-moving and proliferating discipline. It is not easy to form a well-balanced and well-informed summary view of the newest developments in this field. It is still harder to have a vision of its future progress.º

ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers,
and Drs. Roz Picard, Tin Ho, and Mario Figueiredo for their
critical and constructive comments and suggestions. The
authors are indebted to Dr. Kevin Bowyer for his careful
review and to his students for their valuable feedback. The
authors are also indebted to following colleagues for their
valuable assistance in preparing this manuscript: Elzbieta
Pekalska, Dick de Ridder, David Tax, Scott Connell, Aditya
Vailaya, Vincent Hsu, Dr. Pavel Pudil, and Dr. Patrick Flynn.

References
[1]H.M. Abbas,M.M. FahmyaNeural Networks for Maximum Likelihood Clustering,oSignal Processing,1994
[2]H. AkaikeaA New Look at Statistical Model Identification,oIEEE Trans. Automatic Control,1974
[3]S. Amari,T.P. Chen,A. CichockiaStability Analysis of Learning Algorithms for Blind Source Separation,oNeural Networks,1997
[4]A. Antos,L. Devroye,L. GyorfiaLower Bounds for Bayes Error Estimation,oIEEE Trans. Pattern Analysis and Machine Intelligence,1999
[5]H. Avi-Itzhak,T. DiepaArbitrarily Tight Upper and Lower Bounds on the Bayesian Probability of Error,oIEEE Trans. Pattern Analysis and Machine Intelligence,1996
[6]E. BackerComputer-Assisted Reasoning in Cluster Analysis1995
[7]R. Bajcsy,S. KovacicaMultiresolution Elastic Matching,oComputer Vision Graphics Image Processing,1989
[8]A. Barron,J. Rissanen,B. Yu,aTheMinimum Description Length Principle in Coding and Modeling,oIEEE Trans. Information Theory, vol. 44,1998
[9]A. Bell,T. SejnowskiaAn Information-Maximization Approach to Blind Separation,oNeural Computation,1995
[10]Y. BengioaMarkovian Models for Sequential Data,oNeural Computing Surveys,1999
[11]K.P. BennettaSemi-Supervised Support Vector Machines,oProc. Neural Information Processing Systems, Denver,1998
[12]J.C. BezdekPattern Recognition with Fuzzy Objective Function Algorithms1981
[13]J.S.S.K. BhatiaDeogun, aConceptual Clustering in Information Retrieval,oIEEE Trans. Systems, Man, and Cybernetics,1998
[14]C.M. BishopNeural Networks for Pattern Recognition1995
[15]A.L. Blum,P. LangleyaSelection of Relevant Features and Examples in Machine Learning,oArtificial Intelligence,1997
[16]C.J.C. BurgesaA Tutorial on Support Vector Machines for Pattern Recognition,oData Mining and Knowledge Discovery,1998
[17]J. CardosoaBlind Signal Separation: Statistical Principles,oProc. IEEE,1998
[18]C. Carpineto,G. Romano,aALattice Conceptual Clustering System and Its Application to Browsing Retrieval,oMachine Learning,1996
[19]G. Castellano,A.M. Fanelli,M. Pelillo,aAnIterative Pruning Algorithm for Feedforward Neural Networks,oIEEE Trans. Neural Networks,1997
[20]C. Chatterjee,V.P. RoychowdhuryaOn Self-Organizing Algorithms and Networks for Class-Separability Features,oIEEE Trans. Neural Networks,1997
[21]B. Cheng,D.M. TitteringtonaNeural Networks: A Review from Statistical Perspective,oStatistical Science,1994
[22]H. ChernoffaThe Use of Faces to Represent Points in k-Dimensional Space Graphically,oJ. Am. Statistical Assoc.,1973
[23]P.A. ChouaOptimal Partitioning for Classification and Regression Trees,oIEEE Trans. Pattern Analysis and Machine Intelligence,1991
[24]P. ComonaIndependent Component Analysis, a New Concept?,oSignal Processing,1994
[25]T.M. CoveraGeometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition,oIEEE Trans. Electronic Computers,1965
[26]T.M. CoveraThe Best Two Independent Measurements are not the Two Best,oIEEE Trans. Systems, Man, and Cybernetics,1974
[27]T.M. Cover,J.M. Van CampenhoutaOn the Possible Orderings in the Measurement Selection Problem,oIEEE Trans. Systems, Man, and Cybernetics,1977
[28]A. Dempster,N. Laird,D. RubinaMaximum Likelihood from Incomplete Data via the (EM) Algorithm,oJ. Royal Statistical Soc.,1977
[29]H. Demuth,H.M. BealeNeural Network Toolbox for Use with Matlab. version 3, Mathworks1998
[30]D. De Ridder,R.P.W. DuinaSammon's Mapping Using Neural Networks: Comparison,oPattern Recognition Letters,1997
[31]P.A. Devijver,J. KittlerPattern Recognition: A Statistical Approach1982
[32]L. DevroyeaAutomatic Pattern Recognition: A Study of the Probability of Error,oIEEE Trans. Pattern Analysis and Machine Intelligence,1988
[33]E.A. DjouadiBouktache, aA Fast Algorithm for the Nearest- Neighbor Classifier,oIEEE Trans. Pattern Analysis and Machine Intelligence,1997
[34]H. Drucker,C. Cortes,L.D. Jackel,Y. Lecun,V. VapnikaBoosting and Other Ensemble Methods,oNeural Computation,1994
[35]R.O. Duda,P.E. HartPattern Classification and Scene Analysis, New York1973
[36]R.O. Duda,P.E. Hart,D.G. StorkPattern Classification and Scene Analysis. second ed2000
[37]R.P.W. Duin,D. De Ridder,D.M.J. TaxaExperiments with a Featureless Approach to Pattern Recognition,oPattern Recognition Letters,1997
[38]B. EfronThe Jackknife, the Bootstrap and Other Resampling PlansPhiladelphia: SIAM,1982
[39]U. Fayyad,G. Piatetsky-Shapiro,P. SmythaKnowledge Discovery and Data Mining: Towards a Unifying Framework,oProc. Second Int'l Conf. Knowledge Discovery and Data Mining,1999
[40]F. Ferri,P. Pudil,M. Hatef,J. KittleraComparative Study of Techniques for Large Scale Feature Selection,oPattern Recognition in Practice IV,1994
[41]M. Figueiredo,J. Leitao,A.K. JainaOn Fitting Mixture Models,o Energy Minimization Methods in Computer Vision and Pattern RecognitionIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,1999
[42]Y. Freund,R. SchapireaExperiments with a New Boosting Algorithm,oProc. 13th Int'l Conf. Machine Learning,1996
[43]J.H. FriedmanaExploratory Projection Pursuit,oJ. Am. Statistical Assoc.,1987
[44]J.H. FriedmanaRegularized Discriminant Analysis,oJ. Am. Statistical Assoc.,1989
[45]H. Frigui,R. Krishnapuram,aARobust Competitive Clustering Algorithm with Applications in Computer Vision,oIEEE Trans. Pattern Analysis and Machine Intelligence,1999
[46]K.S. FuSyntactic Pattern Recognition and ApplicationsEnglewood Cliffs, N.J.: Prentice-Hall,1982
[47]K.S. FuaA Step Towards Unification of Syntactic and Statistical Pattern Recognition,oIEEE Trans. Pattern Analysis and Machine Intelligence,1983
[48]K. FukunagaIntroduction to Statistical Pattern Recognition1990
[49]K. Fukunaga,R.R. HayesaEffects of Sample Size in Classifier Design,oIEEE Trans. Pattern Analysis and Machine Intelligence,1989
[50]K. Fukunaga,R.R. Hayes,aTheReduced Parzen Classifier,oIEEE Trans. Pattern Analysis and Machine Intelligence,1989
[51]D.M.K. FukunagaHummels, aLeave-One-Out Procedures for Nonparametric Error Estimates,oIEEE Trans. Pattern Analysis and Machine Intelligence,1989
[52]K. Fukushima,S. Miyake,T. ItoaNeocognitron: A Neural Network Model for a Mechanism of Visual Pattern Recognition,oIEEE Trans. Systems, Man, and Cybernetics,1983
[53]S.B. Gelfand,C.S. Ravishankar,E.J. Delp,aAnIterative Growing and Pruning Algorithm for Classification Tree Design,oIEEE Trans. Pattern Analysis and Machine Intelligence,1991
[54]S. Geman,E. Bienenstock,R. DoursataNeural Networks and the Bias/Variance Dilemma,oNeural Computation,1992
[55]C. Glymour,D. Madigan,D. Pregibon,P. SmythaStatistical Themes and Lessons for Data Mining,oData Mining and Knowledge Discovery,1997
[56]M. Golfarelli,D. Maio,D. MaltoniaOn the Error-Reject Trade-Off in Biometric Verification System,oIEEE Trans. Pattern Analysis and Machine Intelligence,1997
[57]R.M. Gray,R.A. OlshenaVector Quantization and Density Estimation,oProc. Int'l Conf. Compression and Complexity of Sequences,1997
[58]D.J. Hand,aRecentAdvances in Error Rate Estimation,oPattern Recognition Letters,1986
[59]M.H. Hansen,B. YuaModel Selection and the Principle of Minimum Description Length,o1998
[60]M.A. HearstaSupport Vector Machines,oIEEE Intelligent Systems,1998
[61]S. HaykinNeural Networks, A Comprehensive Foundation1999
[62]T.K. Ho,J.J. Hull,S.N. SrihariaDecision Combination in Multiple Classifier Systems,oIEEE Trans. Pattern Analysis and Machine Intelligence,1994
[63]T.K. HoaThe Random Subspace Method for Constructing Decision Forests,oIEEE Trans. Pattern Analysis and Machine Intelligence,1998
[64]J.P. Hoffbeck,D.A. LandgrebeaCovariance Matrix Estimation and Classification with Limited Training Data,oIEEE Trans. Pattern Analysis and Machine Intelligence,1996
[65]A. HyvarinenaSurvey on Independent Component Analysis,oNeural Computing Surveys,1999
[66]A. Hyvarinen,E. OjaaA Fast Fixed-Point Algorithm for Independent Component Analysis,oNeural Computation,1997
[67]R.A. Jacobs,M.I. Jordan,S.J. Nowlan,G.E. HintonaAdaptive Mixtures of Local Experts,oNeural Computation,1991
[68]A.K. Jain,B. ChandrasekaranaDimensionality and Sample Size Considerations in Pattern Recognition Practice,oHandbook of Statistics. P.R. Krishnaiah and L.N. Kanal, eds.,1982
[69]R.C.A.K. JainDubes, Algorithms for Clustering DataEnglewood Cliffs, N.J.: Prentice Hall,1988
[70]A.K. Jain,R.C. Dubes,C.-C. ChenaBootstrap Techniques for Error Estimation,oIEEE Trans. Pattern Analysis and Machine Intelligence,1987
[71]A. Jain,Y. Zhong,S. LakshmananaObject Matching Using Deformable Templates,oIEEE Trans. Pattern Analysis and Machine Intelligence,1996
[72]A.K. Jain,D. ZongkeraFeature Selection: Evaluation, Application, and Small Sample Performance,oIEEE Trans. Pattern Analysis and Machine Intelligence,1997
[73]F. JelinekStatistical Methods for Speech Recognition1998
[74]M.I. Jordan,R.A. JacobsaHierarchical Mixtures of Experts and the EM Algorithm,oNeural Computation,1994
[75]D. Judd,P. Mckinley,A.K. JainaLarge-Scale Parallel Data Clustering,oIEEE Trans. Pattern Analysis and Machine Intelligence,1998
[76]L.N. KanalaPatterns in Pattern Recognition: 1968-1974,oIEEE Trans. Information Theory, vol. 20,1974
[77]J. Kittler,M. Hatef,R.P.W. Duin,J. MatasaOn Combining Classifiers,oIEEE Trans. Pattern Analysis and Machine Intelligence,1998
[78]R.M. KleinbergaStochastic Discrimination,oAnnals of Math. and Artificial Intelligence,1990
[79]T. KohonenSelf-Organizing MapsSpringer Series in Information Sciences,1995
[80]J.A. KroghVedelsby, aNeural Network Ensembles, Cross Validation, and Active Learning,oAdvances in Neural Information Processing Systems,1995
[81]L. Lam,C.Y. SuenaOptimal Combinations of Pattern Classifiers,oPattern Recognition Letters,1995
[82]T.W. LeeIndependent Component AnalysisDordrech: Kluwer Academic Publishers,1998
[83]C. Lee,D.A. LandgrebeaFeature Extraction Based on Decision Boundaries,oIEEE Trans. Pattern Analysis and Machine Intelligence,1993
[84]D.R. Lovell,C.R. Dance,M. Niranjan,R.W. Prager,K.J. Dalton,R. DeromaFeature Selection Using Expected Attainable Discrimination,oPattern Recognition Letters,1998
[85]D. Lowe,A.R. WebbaOptimized Feature Extraction and the Bayes Decision in Feed-Forward Classifier Networks,oIEEE Trans. Pattern Analysis and Machine Intelligence,1991
[86]D.J.C. MacKay,aTheEvidence Framework Applied to Classification Networks,oNeural Computation,1992
[87]J. Mao,K. Mohiuddin,A.K. JainaParsimonious Network Design and Feature Selection through Node Pruning,oProc. 12th Int'l Conf. Pattern on Recognition,1994
[88]K.M.J.C. MaoMohiuddin, aImproving OCR Performance Using Character Degradation Models and Boosting Algorithm,oPattern Recognition Letters,1997
[89]G. McLachlanDiscriminant Analysis and Statistical Pattern Recognition1992
[90]M. Mehta,J. Rissanen,R. AgrawalaMDL-Based Decision Tree Pruning,oProc. First Int'l Conf. Knowledge Discovery in Databases and Data1995
[91]C.E. MetzaBasic Principles of ROC Analysis,oSeminars in Nuclear Medicine, vol. VIII,1978
[92]R.S. Michalski,R.E. SteppaAutomated Construction of Classifications: Conceptual Clustering versus Numerical Taxonomy,oIEEE Trans. Pattern Analysis and Machine Intelligence,1983
[93]D. Michie,D.J. Spiegelhalter,C.C. TaylorMachine Learning, Neural and Statistical ClassificationNew York: Ellis Horwood,1994
[94]S.K. Mishra,V.V. RaghavanaAn Empirical Study of the Performance of Heuristic Methods for Clustering,o Pattern Recognition in Practice1994
[95]G. NagyaState of the Art in Pattern Recognition,oProc. IEEE,1968
[96]G. NagyaCandide's Practical Principles of Experimental Pattern Recognition,oIEEE Trans. Pattern Analysis and Machine Intelligence,1983
[97]R. NealBayesian Learning for Neural NetworksNew York: Spring Verlag,1996
[98]H. NiemannaLinear and Nonlinear Mappings of Patterns,oPattern Recognition,1980
[99]K.L. Oehler,R.M. GrayaCombining Image Compression and Classification Using Vector Quantization,oIEEE Trans. Pattern Analysis and Machine Intelligence,1995
[100]E. OjaSubspace Methods of Pattern Recognition, LetchworthHertfordshire, England: Research Studies Press,1983
[101]E. OjaaPrincipal Components, Minor Components, and Linear Neural Networks,oNeural Networks,1992
[102]E. Oja,aTheNonlinear PCA Learning Rule in Independent Component Analysis,oNeurocomputing, vol. 17,1997
[103]E. Osuna,R. Freund,F. Girosi,aAnImproved Training Algorithm for Support Vector Machines,oProc. IEEE Workshop Neural Networks for Signal Processing1997
[104]T. PavlidisStructural Pattern RecognitionNew York: Springer- Verlag,1977
[105]L.I. PerlovskyaConundrum of Combinatorial Complexity,oIEEE Trans. Pattern Analysis and Machine Intelligence,1998
[106]M.P. Perrone,L.N. CooperaWhen Networks Disagree: Ensemble Methods for Hybrid Neural Networks,oNeural Networks for Speech and Image Processing. R.J. Mammone, ed., Chapman-Hall,1993
[107]J. PlattaFast Training of Support Vector Machines Using Sequential Minimal Optimization,oAdvances in Kernel MethodsÐ- Support Vector Learning. B. Scholkopf,1999
[108]R. PicardAffective Computing1997
[109]J.R. QuinlanaSimplifying Decision Trees,oInt'l J. Man-Machine Studies,1987
[110]J.R. QuinlanPrograms for Machine Learning1993
[111]L.R. RabineraA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,oProc. IEEE,1989
[112]V.S.J. RaudysPikelis, aOn Dimensionality, Sample Size, Classification Error, and Complexity of Classification Algorithms in Pattern Recognition,oIEEE Trans. Pattern Analysis and Machine Intelligence,1980
[113]S.J. Raudys,A.K. JainaSmall Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners,oIEEE Trans. Pattern Analysis and Machine Intelligence,1991
[114]S. RaudysaEvolution and Generalization of a Single Neuron; Single-Layer Perceptron as Seven Statistical Classifiers,oNeural Networks,1998
[115]S. Raudys,R.P.W. DuinaExpected Classification Error of the Fisher Linear Classifier with Pseudoinverse Covariance Matrix,oPattern Recognition Letters,1998
[116]S. Richardson,P. Green,aOnBayesian Analysis of Mixtures with Unknown Number of Components,oJ. Royal Statistical Soc. (B),1997
[117]B. RipleyaStatistical Aspects of Neural Networks,o Networks on Chaos: Statistical and Probabilistic Aspects1993
[118]B. RipleyPattern Recognition and Neural Networks1996
[119]J. RissanenStochastic Complexity in Statistical Inquiry, SingaporeWorld Scientific,1989
[120]K. RoseaDeterministic Annealing for Clustering, Compression, Classification, Regression and Related Optimization Problems,oProc. IEEE,1998
[121]J.W. Sammon Jr.,aANonlinear Mapping for Data Structure Analysis,oIEEE Trans. Computer, vol1969
[122]R.E. SchapireaThe Strength of Weak Learnability,oMachine Learning,1990
[123]R.E. Schapire,Y. Freund,P. Bartlett,W.S. LeeaBoosting the Margin: A New Explanation for the Effectiveness of Voting Methods,oAnnals of Statistics,1999
[124]B. SchoÈlkopfaSupport Vector Learning,oPh.D. thesis, Technische UniversitaÈt, Berlin,1997
[125]B. SchoÈlkopf,A. Smola,K.R. MulleraNonlinear Component Analysis as a Kernel Eigenvalue Problem,oNeural Computation,1998
[126]J. SchurmannPattern Classification: A Unified View of Statistical and Neural Approaches1996
[127]S. ScloveaApplication of the Conditional Population Mixture Model to Image Segmentation,oIEEE Trans. Pattern Recognition and Machine Intelligence,1983
[128]G.P.R.I.K. SethiSarvarayudu, aHierarchical Classifier Design Using Mutual Information,oIEEE Trans. Pattern Recognition and Machine Intelligence,1979
[129]R. Setiono,H. LiuaNeural-Network Feature Selector,oIEEE Trans. Neural Networks,1997
[130]W. Siedlecki,J. SklanskyaA Note on Genetic Algorithms for Large-Scale Feature Selection,oPattern Recognition Letters,1989
[131]P. Simard,Y. LeCun,J. DenkeraEfficient Pattern Recognition Using a New Transformation Distance,oAdvances in Neural Information Processing1993
[132]P. Simard,B. Victorri,Y. LeCun,J. DenkeraTangent PropÐA Formalism for Specifying Selected Invariances in an Adaptive Network,oAdvances in Neural Information Processing1992
[133]V. Tresp,M. TaniguchiaCombining Estimators Using Non- Constant Weighting Functions,oAdvances in Neural Information Processing Systems,1995
[134]G.V. Trunk,aAProblem of Dimensionality: A Simple Example,oIEEE Trans. Pattern Analysis and Machine Intelligence,1979
[135]K. Tumer,J. GhoshaAnalysis of Decision Boundaries in Linearly Combined Neural Classifiers,oPattern Recognition,1996
[136]S. Vaithyanathan,B. DomaModel Selection in Unsupervised Learning with Applications to Document Clustering,oProc. Sixth Int'l Conf. Machine Learning,1999
[137]M. van Breukelen,R.P.W. Duin,D.M.J. Tax,J.E. den HartogaHandwritten Digit Recognition by Combined Classifiers,oKybernetika, vol. 34,1998
[138]V.N. VapnikEstimation of DependencesBased on Empirical Data, Berlin: Springer-Verlag,1982
[139]V.N. VapnikStatistical Learning Theory1998
[140]S. WatanabePattern Recognition: Human and MechanicalNew York: Wiley,1985
[141]A.R. WebbaMultidimensional Scaling by Iterative Majorization Using Radial Basis Functions,oPattern Recognition,1995
[142]M. Whindham,A. CutleraInformation Ratios for Validating Mixture Analysis,oJ. Am. Statistical Assoc.,1992
[143]D. WolpertaStacked Generalization,oNeural Networks,1992
[144]A.K.C. Wong,D.C.C. WangaDECA: A Discrete-Valued Data Clustering Algorithm,oIEEE Trans. Pattern Analysis and Machine Intelligence,1979
[145]K. Woods,W.P. Kegelmeyer Jr.,K. BowyeraCombination of Multiple Classifiers Using Local Accuracy Estimates,oIEEE Trans. Pattern Analysis and Machine Intelligence,1997
[146]L. Xu,A. Krzyzak,C.Y. SuenaMethods for Combining Multiple Classifiers and Their Applications in Handwritten Character Recognition,oIEEE Trans. Systems, Man, and Cybernetics,1992
[147]G.T. ToussaintaThe Use of Context in Pattern Recognition,oPattern Recognition,1978
[148]K. Mohiuddin,J. MaoaOptical Character Recognition,o Wiley Encyclopedia of Electrical and Electronic EngineeringJ.G. Webster, ed.,1999
[149]R.M. HaralickaDecision Making in Context,oIEEE Trans. Pattern Analysis and Machine Intelligence,1983
