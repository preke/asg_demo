FSVM-CIL: Fuzzy Support Vector Machines for Class Imbalance Learning
Rukshan Batuwita,Vasile Palade
manb@comlab.ox.ac.uk;,vasile.palade@comlab.ox.ac.uk).

Index Terms—Class imbalance learning (CIL), fuzzy support vector machines (FSVMs), outliers, support vector machines (SVMs).
I. INTRODUCTION
SUPPORT Vector machines (SVMs) [1]–[3] is a widely usedmachine learning technique, which has been successfully applied to many real-world classification problems in various domains. Due to its solid mathematical background, high generalization capability and ability to find global classification solutions, SVMs are usually preferred by many researchers over other available classification paradigms.
Although the SVM algorithm works effectively with balanced datasets, when it comes to imbalanced datasets, it could often produce suboptimal results [4]–[14], i.e., an SVM classifier, which is trained with an imbalanced dataset, can produce a model that is biased toward the majority class and has a low performance on the minority class. In this paper, we consider the binary classification problem, and the positive class is treated as the minority class, while the negative class is treated as the
Manuscript received July 29, 2009; revised November 10, 2009; accepted January 19, 2010. Date of publication February 8, 2010; date of current version May 25, 2010.
The authors are with the Oxford University Computing Laboratory, Oxford University, Oxford, OX1 3QD, U.K. (e-mail: manb@comlab.ox.ac.uk; vasile.palade@comlab.ox.ac.uk).
Digital Object Identifier 10.1109/TFUZZ.2010.2042721
majority class, as in most of the real-world imbalanced datasets. Generally, a classifier should perform well with respect to both positive and negative classes for it to be useful in real-world application. There exist techniques to develop better performing classifiers with imbalanced datasets, which are generally called class imbalance learning (CIL) methods. These methods can be broadly divided into two categories, namely, external methods and internal methods. External methods involve preprocessing of training datasets in order to make them balanced, while internal methods deal with modifications of the learning algorithms in order to reduce their sensitiveness to class imbalance. The available methods for training SVMs with imbalanced datasets are discussed later in the paper.
On the other hand, since the SVM learning algorithm considers all the training examples uniformly, it is sensitive to outliers and noise present in datasets [15], [16]. Most of the real-world datasets usually contain some outliers and noisy examples. Therefore, although the existing imbalance-learning methods applied for normal SVMs can solve the problem of class imbalance, they can still suffer from the problem of outliers and noise. Fuzzy support vector machines (FSVMs) is a variant of the SVM learning algorithm, which was originally proposed in [17] in order to handle the problem of outliers and noise. The FSVM technique first assigns different fuzzy-membership values (weights) for different training examples to reflect their importance and then incorporates these membership values in the SVM learning algorithm to reduce the effect of outliers/noise when finding the separating hyperplane. However, the FSVM technique, like the normal SVM method, can still be sensitive to the class imbalance problem. We will show that this, indeed, is the case with the empirical results obtained in this study.
In this paper, we propose a method to improve FSVMs for CIL, which can be used to address both the problems of class imbalance and outliers/noise. In the proposed method, we assign fuzzy-membership values for training examples in order to reduce the effect of both of these problems under the principle of cost-sensitive learning. We name this method FSVMs for class imbalance learning (FSVM-CIL). We systematically evaluated the FSVM-CIL method on ten real-world imbalanced datasets and compared its performance with five different existing internal- and external-imbalance-learning methods applied for normal SVMs. We found that the proposed FSVM-CIL method is a very effective method for CIL in the presence of outliers/noise in the datasets.
This paper is organized as follows. Section II briefly reviews the SVM learning theory, and Section III discusses the existing class imbalance solutions that are available for SVM training. In Section IV, we present the FSVM algorithm, and
1063-6706/$26.00 © 2010 IEEE
in Section V, we discuss the proposed method of using the FSVM technique for CIL. Section VI presents the imbalanced datasets used to validate the proposed method, while Section VII discusses the SVM/FSVM training methods used in this study, such as the parameter-selection and the performanceevaluation methods. In Section VIII, we present and discuss, in detail, the classification results obtained by the proposed method and compare them with the results obtained by different existing imbalance-learning methods. In Section IX, we apply a statistical test to compare the performance of the proposed method with the existing methods considered, and finally, in Section X, we conclude the paper.

II. SUPPORT VECTOR MACHINES LEARNING THEORY
In this section, we briefly review the learning algorithm of SVMs, which has been initially proposed in [1], [2]. Consider we have a binary classification problem, which is represented by a dataset {(x1 , y1), (x2 , y2), . . . , (xl,yl)}, where xi ∈ n represents an n-dimensional data point, and yi ∈ {−1, 1} represents the class of that data point, for i = 1, . . . , l. The goal of the SVM learning algorithm is to find a separating hyperplane that separates these data points into two classes. In order to find a better separation of classes, the data are first transformed into a higher dimensional feature space by a mapping function Φ. Then, a possible separating hyperplane, which resides in the higher dimensional feature space, can be represented by
w · Φ(x) + b = 0. (1) If the dataset is completely linearly separable, the separating hyperplane with the maximum margin can be found by solving the following maximal-margin optimization problem:
Min (
1 2 w · w
)
s.t. yi(w · Φ(xi) + b) ≥ 1 i = 1, . . . , l. (2)
However, in most real-world problems, the datasets are not completely linearly separable, although they are mapped into a higher dimensional feature space. Therefore, the constrains in the aforementioned optimization problem in (2) are relaxed by introducing a slack variable εi ≥ 0, and then, the soft-margin optimization problem is formulated as follows:
Min (
1 2 w · w + C l∑ i=1 εi
)
s.t. yi(w · Φ(xi) + b) ≥ 1 − εi εi ≥ 0, i = 1, . . . , l. (3)
The slack variables εi > 0 hold for misclassified examples, and therefore, ∑l i=1 εi can be thought of as a measure of the amount of misclassifications. This new objective function in (3) has two goals. One is to maximize the margin, and the other one is to minimize the number of misclassifications. The parameter C controls the tradeoff between these two goals, and it can also be treated as the misclassification cost of a training
example. This quadratic-optimization problem can be solved by constructing a Lagrangian representation and transforming it into the following dual problem:
Max W (α) = l∑
i=1
αi − 1 2 l∑ i=1 l∑ j=1 αiαjyiyjΦ(xi) · Φ(xj )
s.t. l∑
i=1
yiαi = 0, 0 ≤ αi ≤ C, i = 1, . . . , l (4)
where αis are Lagrange multipliers, which should satisfy the following Karush–Kuhn–Tucker (KKT) conditions:
αi(yi(w · Φ(xi) + b) − 1 + εi) = 0, i = 1, . . . , l (5) (C − αi)ξi = 0, i = 1, . . . , l. (6)
An important property of SVMs is that it is not necessary to know the mapping function Φ(x) explicitly. By applying a kernel function, such that K(xi, xj ) = Φ(xi) · Φ(xj ), we would be able to transform the dual-optimization problem in (4) into
MaxW (α) = l∑
i=1
αi − 1 2 l∑ i=1 l∑ j=1 αiαjyiyjK(xi, xj )
s.t. l∑
i=1
yiαi = 0, 0 ≤ αi ≤ C, i = 1, . . . , l. (7)
By solving (7) and finding the optimal values for αi , w can be recovered as follows:
w = l∑
i=1
αiyiΦ(xi) (8)
and b can be determined from the KKT conditions in (5). The data points having nonzero αi values are called support vectors. Finally, the SVM decision function is given by
f(x) = sign(w · Φ(x) + b) = sign ( l∑
i=1
αiyiK(xi, x) + b ) .
(9)

III. CLASS IMBALANCE LEARNING METHODS AVAILABLE FOR
SUPPORT VECTOR MACHINES
It has been well-studied that the SVM algorithm can be sensitive to class imbalance [4]–[14], i.e., the separating hyperplane of an SVM model, which was developed with an imbalanced dataset, can be skewed toward the minority (positive) class [5], [6]. This skewness generally causes the generation of a high number of false-negative predictions, which lower the model’s performance on the positive class compared with the performance on the negative (majority) class.
As mentioned earlier, there exist CIL methods that can be applied to reduce the effect of data imbalance, when developing SVM classifiers with imbalanced datasets. Generally, these methods can be divided into two categories: external methods and internal methods [14], [18], [19]. A comprehensive review of different CIL methods can be found in [14]. The following two
sections briefly discuss the external- and internal-imbalancelearning methods that can be applied for SVM training.

A. External Methods
The external methods are independent from the learning algorithm being used, and they involve preprocessing of the training datasets to balance them before training the classifiers. Different resampling methods, such as random and focused oversampling and undersampling, fall into to this category. In random undersampling, the majority-class examples are removed randomly, until a particular class ratio is met [18]. In random oversampling, the minority-class examples are randomly duplicated, until a particular class ratio is met [14]. Synthetic minority oversampling technique (SMOTE) [20] is an oversampling method, where new synthetic examples are generated in the neighborhood of the existing minority-class examples rather than directly duplicating them. In addition, several informed sampling methods have been introduced in [21]. A clustering-based sampling method has been proposed in [22], while a genetic algorithmbased sampling method has been proposed in [10].
Ensemble learning has also been applied to handle the problem of class imbalance [23]. SVM-based ensemble systems for imbalanced dataset learning have been developed in [11]–[13]. Moreover, different boosting methods have also been adopted for CIL in [24] and [25].
B. Internal Methods
Internal methods deal with modifications of a learning algorithm to make it less sensitive for the class imbalance. Algorithmic techniques have been developed for the different classification algorithms, such as neural networks [26], decision trees [27], fuzzy systems [28], [29] etc., for imbalanced dataset learning. For SVMs, several algorithmic solutions have been proposed in the literature. Veropoulos et al. [4] has proposed a method called different error costs (DEC), where the SVM objective function has been modified to assign two misclassification cost values C+ and C− as follows:
Min (
1 2 w · w + C+ l∑ [i|yi =+1] εi + C− l∑ [i|yi =−1] εi
)
s.t. yi(w · Φ(xi) + b) ≥ 1 − εi εi ≥ 0, i = 1, . . . , l (10)
where C+ is the misclassification cost for the positive-class examples, while C− is the misclassification cost for the negativeclass examples. By assigning a higher misclassification cost for the positive (minority) class examples than the negative (majority) class examples (i.e., C+ > C−), the effect of class imbalance could be reduced.
zSVM is another algorithmic modification, which is proposed for SVMs in [9] for learning-imbalance datasets. In this method, first, an SVM model is developed by using the available imbalanced training dataset. Then, the decision boundary of the resulted model is modified to remove the classifier’s bias towards the majority (negative) class. Consider the SVM decision
function given in (9), which can be rewritten as follows:
f(x) = sign ( l∑
i=1
αiyiK(xi, x) + b )
= sign { l1∑
i=1
α+i yiK(xi, x) + l2∑
j=1
α−j yjK(xj , x) + b }
(11)
where α+i are the coefficients of the positive-support vectors, α−i are the coefficients of the negative-support vectors, and l1 and l2 represent the number of positive- and negative-training examples, respectively. In the zSVM method, the magnitude of the α+i values of the positive-support vectors are increased by multiplying all of them by a particular small positive value z. Then, the modified SVM decision function can be represented as follows: f(x) = sign { z
l1∑ i=1 α+i yiK(xi, x) + l2∑ j=1 α−i yjK(xj , x) + b } .
(12) This modification of α+i would increase the weight of the positive-support vectors in the decision function, and therefore, it would decrease its bias toward the majority negative class. In [9], the value of z, which gives the best prediction for the training dataset, was selected as the optimal value of z to be used in (12).
In addition to these methods, several kernel-modification methods for training SVMs with imbalanced datasets have been proposed in [6] and [7].

IV. FUZZY SUPPORT VECTOR MACHINES
There has been some significant research carried out towards the combination of fuzzy logic and SVMs in different ways, and the term ‘‘Fuzzy SVMs’’ has been used to refer to most of these different approaches. Lin and Wang [17] applies a fuzzymembership value for each training example, which is based on the importance of that example of its class, and reformulates the SVM learning algorithm such that different input points can make different contributions when finding the separating hyperplane. Wang et al. [30] extended this approach by introducing two membership values for each training example, which define the memberships of both positive and negative classes. This approach has been further extended, which is based on the concept of “vague sets” in [31]. Another type of fuzzy SVM method, which uses a special type of kernel function constructed from fuzzy-basis functions, has been proposed in [32]. A method for fuzzy support vector regression has been proposed in [33].
In contrast, several other approaches have been developed to assign fuzzy-membership values for the outputs generated by SVMs together with the output class. Xie et al. [34] use the decision value, which is generated by the SVMs, to define a membership for the output class, while [35] uses the fuzzyoutput decision values for multiclass classification. Mill and Inoue [36] propose a method that uses the strengths (αi) of
positive- and negative-support vectors to generate the fuzzymembership values for the output classes. On the other hand, several methods have been proposed in [37]–[40] to extract fuzzy rules from the trained SVM models.
In this paper, we consider the Fuzzy-SVM method, which was initially proposed in [17], as a solution for the problem of outliers and noise. Although, fuzzy-membership functions are used, this method is different from general fuzzy-classification methods that usually involve fuzzification, defuzzification, and fuzzy reasoning. Generally, fuzzy-classification methods are based on fuzzy-rule-based systems [41], neurofuzzy systems [42], [43], and fuzzy measures [44]. Other fuzzy-classification methods handle fuzzy attribute values and/or fuzzy targets [45]. On the other hand, the FSVM method considered in this paper uses fuzzy-membership functions to define a different misclassification cost for each training pattern, as discussed below.
The normal SVM-learning algorithm considers all the training points uniformly, and therefore, it can be sensitive to outliers and noise [15], [16]. As mentioned earlier, the FSVM method proposed in [17] assigns different fuzzy-membership values mi (or weights) for different examples to reflect their importance for their own class, where more important examples are assigned higher membership values, while the less-important ones, such as outliers and noise, are assigned lower membership values. Then, the SVM soft-margin optimization problem is reformulated as follows:
Min (
1 2 w · w + C l∑ i=1 miεi
)
s.t. yi(w · Φ(xi) + b) ≥ 1 − εi εi ≥ 0, i = 1, . . . l. (13)
In this formulation, the membership mi of a data point xi is incorporated into the objective function in (13), and therefore, a smaller mi could reduce the effect of the parameter ξi in the objective function, where the corresponding xi is treated as less important. In another view, if we consider C as the cost assigned for a misclassification, then each example is assigned with a different misclassification cost value miC, such that more important examples are assigned higher costs, while less important ones are assigned lower costs. Therefore, the SVM algorithm can find a more robust hyperplane by maximizing the margin by letting some misclassification of less-important examples, such as outliers and noise.
In order to solve the FSVM optimization problem, (13) is transformed into the following dual Lagrangian [17]:
MaxW (α) = l∑
i=1
αi − 1 2 l∑ i=1 l∑ j=1 αiαjyiyjK(xi, xj )
s.t. l∑
i=1
yiαi = 0, 0 ≤ αi ≤ miC, i = 1, . . . , l. (14)
The only difference between the original SVM dualoptimization problem in (7) and the FSVM dual-optimization problem in (14) is the upper bound of the values of αi . By solving this dual problem in (14) for optimal αi , w and b can be recovered in the same way, as in the normal SVM learning
algorithm. The same SVM decision function in (9) applies for FSVMs as well.
This FSVM algorithm has been successfully applied to reduce the effect of outliers and noise in different domains with different ways of assigning fuzzy-membership values (see [46]–[50]).

V. FSVM-CIL: FUZZY SUPPORT VECTOR MACHINES FOR
CLASS IMBALANCE LEARNING
As stated earlier, the SVM algorithm is sensitive to both the problems of class imbalance and outliers/noise. Although the existing CIL methods available for SVMs, which were discussed in Section III, can overcome the problem of class imbalance, they can still be sensitive to the problem of outliers and noise. On the other hand, although the FSVM method considered in this paper can overcome the problem of outliers/noise, it can suffer from the problem of class imbalance, since there is no modification in the FSVM algorithm compared with the original SVM algorithm to make it less sensitive to class imbalance. We show that this is in fact the case with empirical evidence presented in Section VIII. In this section, we present an improvement for the FSVM learning method, which is called FSVM-CIL, to overcome the problem of class imbalance.
As we discussed in the previous section, in FSVMs, we can assign different membership values (or weights) for training examples to reflect their different importance. We also showed that this is similar to assign different misclassification costs Cmi for different training examples. Therefore, in order to reduce the effect of class imbalance, we can assign higher membership values, or higher misclassification costs, for the minority-class examples, while we assign lower membership values, or lower misclassification costs, for the majority-class examples, as done in the DEC method discussed in Section III, i.e., if we assign a single membership value, say m+ , for all the minority-class examples and another single membership value, say m−, for all the majority-class examples such that m+ > m−, this is exactly equivalent to the DEC method applied for normal SVMs, where C+ = m+C, and C− = m−C. However, the DEC method applied for normal SVMs can suffer the problem of outliers/noise, because all the examples in a separate class are given equal importance.
In the proposed FSVM-CIL method, we assign the membership values for training examples in such a way to satisfy the following two goals:
1) to suppress the effect of between class imbalance 2) to reflect the within-class importance of different training
examples in order to suppress the effect of outliers and noise. Let m+i represent the membership value of a positive-class example x+i , while m − i represents the membership of a negativeclass example x−i in their own classes. In the proposed FSVMCIL method, we define these membership functions as follows:
m+i = f(x + i )r + (15) m−i = f(x − i )r − (16)
wheref(xi) generates a value between 0 and 1, which reflects the importance of xi in its own class. Moreover, we assign the values for r+ and r− in order to reflect the class imbalance, such that r+ > r−. Therefore, a positive-class example can take a membership value in the [0, r+ ] interval, while a negative-class example can take a membership value in the [0, r−] interval. This way of assigning membership values serves both of the aforementioned goals. Therefore, the proposed FSVM-CIL method can be used to handle both the problems of class imbalance and outliers/noise together.

A. Assigning Fuzzy-Membership Values
When assigning fuzzy memberships for training examples in the FSVM-CIL method, in order to reflect the class imbalance, we assign r+ = 1, and r− = r, where r is the minorityto-majority class ratio. This was following the findings reported in [5], where the optimal results for the DEC method could be obtained when C−/C+ equals to the minority-to-majority class ratio. According to this assignment of values, a positiveclass example can take a membership value in the [0, 1] interval, while a negative-class example can take a membership value in the [0, r] interval, where r < 1.
In order to define the function f(xi) introduced in (15) and (16), which gives the within-class importance of a training example, we consider three different methods discussed in the following sections.
1) f(xi) is Based on the Distance from the Own Class Center: In this method, f(xi) is defined with respect to dceni , which is the distance between xi and its own class center, as previously proposed for FSVMs in [17]. The examples closer to the class center are treated as more informative and assigned higher f(xi) values, while the examples far away from the center are treated as outliers or noise and assigned lower f(xi) values. Here, we use two separate decaying functions of dceni to define f(xi), which are represented by f cenlin (xi) and f cen exp (xi) as follows:
f cenlin (xi) = 1 − dceni
max(dceni ) + ∆ (17)
is a linearly decaying function of dceni . ∆ is a small positive value, which is used to avoid the case, where f cenlin (xi) becomes zero.
f cenexp (xi) = 2
1 + exp(βdceni )
β ∈ [0, 1] (18)
is an exponentially decaying function of dceni , where β determines the steepness of the decay. dceni = ‖xi − x̄‖
1/2 is the Euclidean distance to xi from its own class center x̄.
2) f(xi) is Based on the Distance from the Estimated Hyperplane: In this method, f(xi) is defined based on d sph i , which is the distance to xi from the estimated separating hyperplane, as introduced in [46]. Here, dsphi is estimated by the distance to xi from the center of the “spherical region,” which can be defined as a hypersphere covering the overlapping region of the two classes, where the separation hyperplane is more likely to pass through. Both linear- and exponential-decaying functions
are used to define the function f(xi), which are represented by f sphlin (xi) and f sph exp (xi) as follows:
f sphlin (xi) = 1 − dsphi
max(dsphi ) + ∆ (19)
f sphexp (xi) = 2
1 + exp(βdsphi )
β ∈ [0, 1] (20)
where dsphi = ‖xi − x̄‖ 1/2 , and x̄ is the center of the spherical region, which is estimated by the center of the entire positive and negative dataset.
3) f(xi) is Based on the Distance from the Actual Hyperplane: In this method, we define f(xi) based on the distance from the actual separating hyperplane to xi , which is found by training a conventional SVM model on the imbalanced dataset. The examples closer to the actual separating hyperplane are treated as more informative and assigned higher membership values, while the examples far away from the separating hyperplane are treated as less informative and assigned lower membership values. We do the followings in this method.
1) Train a normal SVM model with the original imbalanced dataset. 2) Find the functional margin dhypi of each example xi [given in (21)] (this is equivalent to the absolute value of the SVM decision value) with respect to the separating hyperplane found. The functional margin is proportional to the geometric margin of a training example with respect to the hyperplane.
dhypi = yi(w · Φ(xi) + b). (21)
3) Consider both linear- and exponential-decaying functions to define f(xi) as follows:
fhyplin (xi) = 1 − dhypi
max(dhypi ) + ∆ (22)
fhypexp (xi) = 2
1 + exp(βdhypi )
β ∈ [0, 1]. (23)

B. Different FSVM-CIL Settings
Table I defines the different FSVM-CIL experimental settings, which are considered in this paper, according to the different ways of assigning membership values for positive- and negative-training examples, by following the methods earlier presented.

VI. DATASETS
We considered nine benchmark real-world imbalanced dataset from the UCI machine learning repository [51] to validate the proposed FSVM-CIL method. In addition to these UCI datasets, we considered an imbalanced bioinformatics dataset (“miRNA” dataset) from our previous research on human miRNA gene recognition [52], [53]. It is highly likely that
these real-world datasets contain some outliers and noisy examples in different amounts. Table II summarizes the details of these datasets in the ascending order of the positive-to-negative dataset ratio. This contains the number of positive-class examples (Pos.), the number of negative-class examples (Neg.), the total number of examples (Total), the positive-to-negative imbalance ratio (Imb. Ratio), total number of classes, and the class selected as the positive class for each dataset. For multiclass datasets, the examples belonging to all the other classes, except the class selected as the positive class, were selected as the negative dataset. These datasets represent a whole variety of domains, complexities, and imbalance ratios.

VII. SUPPORT VECTOR MACHINE/FUZZY SUPPORT VECTOR MACHINE TRAINING AND PERFORMANCE EVALUATION
Before training any classifier, each dataset was scaled into [−1,+1] interval. As the SVM training environment, we selected the widely used libsvm software package [54]. For the FSVM program, we used a variant of the libsvm program found in the libsvm tools (http://www.csie.ntu.edu.tw/ ∼cjlin/libsvmtools/#14), which can be used to assign different weights for different training examples. It has been well-studied that when the training dataset is imbalanced, the commonly used performance measure accuracy, which is the proportion of correctly classified instances, could lead to suboptimal models [5], [18]. Therefore, we used the geometric mean of sensitivity (SE = proportion of the positives correctly recognized) and specificity (SP = proportion of the negatives correctly rec-
ognized), given by Gm = √
SE × SP, for the classifier performance evaluation in this study, as commonly used in class imbalanced-learning research [5], [14], [55].
In order to evaluate the performance of all the SVM/FSVM/FSVM-CIL classifiers, which were developed on each dataset, we carried out an extensive five-fold crossvalidation method. In this method, a complete dataset was first randomly divided into five partitions. We used stratified random sampling such that each partition contained the same imbalanced ratio. We used four partitions as the training dataset, and the remaining one as the testing dataset. Using the training dataset partition, we develop an SVM/FSVM/FSVM-CIL classifier either by applying a CIL method or by not applying one, in the following way. We considered the radial-basis function (RBF) kernel (K(xi, xj ) = e−γ (‖xi −xj ‖
2 )) for SVM training and selected the following ranges for parameters: log2 C = {1, 2, . . . , 15}, and log2 γ = {−15,−14, . . . ,−1}. First, we conducted a coarse grid-parameter-search to find the optimal values for log2 C and log2 γ over the aforementioned ranges of values. We evaluated the performance of a model trained on each parameter pair (log2 C, log2 γ) by using five-fold cross-validation results on the training dataset. After finding the optimal values for the parameters, say (C̄,γ̄), again, a narrow grid-parametersearch was conducted over the ranges log2 C = {c̄ − 0.75, c̄ − 0.5, . . . , c̄ + 0.75}, and log2 γ = {γ̄ − 0.75, γ̄ − 0.5, . . . , γ̄ + 0.75}. Finally, after finding the optimal values for (log2 C, log2 γ), a new SVM model was trained by using the complete training dataset on these parameter values. Then, the performance of that model was tested on the remaining testing partition. This whole training and testing procedure was repeated five times with different training and testing partitions in an outer five-fold cross-validation loop, and finally, the results on the testing partitions were averaged and reported. Hereafter, we refer to this whole cross-validation procedure as the outer–inner–cv method.

VIII. EXPERIMENTAL RESULTS
In this section, we present and discuss, in detail, the results obtained by the experiments carried out in this research.

A. SVM and Normal FSVM Results
First, we developed a set of normal SVM classifiers on the original imbalanced datasets and evaluated their performance by using the outer–inner–cv method. These results are given in column 3 of Table III. From these results, we can clearly observe that for all the datasets, the resulted SVM classifiers favored the majority negative class over the minority positive class by giving suboptimal classification results (higher SP and lower SE). These results demonstrate the fact that SVMs are sensitive to imbalance in datasets.
Next, we applied normal FSVM method to develop classifiers for each dataset and evaluated their results using the outer–inner–cv method. When assigning membership values for the training examples in normal FSVMs, we considered the membership of any training example to be equal to f(xi) so that any positive- or negative-training example was assigned
a membership value in [0,1] interval, which was based on its within-class importance. In order to define the function f(xi), we considered the three methods described in Section V-A1– A3, by using both linear- and exponential-decaying functions. Thus, this way of assigning memberships also resulted in six normal FSVM settings, which are represented as FSVMcenlin , FSVMcenexp , FSVM sph lin , FSVM sph exp , FSVM hyp lin , and FSVM hyp exp . When an exponentially decaying function was used as f(xi), the optimal value of β was chosen from the range β = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}, based on the results of outer–inner–cv method for each dataset separately. When the linearly decaying function was used, we set ∆ = 10−6 .
The results obtained through these six FSVM settings for each dataset are given in columns 4–9 of Table III. From these results, we can see that the highest normal FSVM results, which
are depicted in bold type, are better than the results given by the normal SVM method for all datasets, due to the better handling of outliers and noise present in the datasets by the FSVM method. For Page-blocks, Yeast, Satimage, Transfusion, Haberman, Waveform, and Pima-Indians datasets, FSVMcenexp method resulted in the highest Gm value. For Escherichia coli (E. coli) dataset, FSVMhyplin method yielded the highest results, while for Abanole and miRNA datasets, FSVMhypexp yielded the highest results. Therefore, we can observe that the best FSVM setting is dataset-dependent.
More importantly, we can see that some FSVM settings performed worse than the normal SVM algorithm for some datasets. For example, FSVMcenlin method gave lower results for Abanole, Yeast, miRNA, Satimage, and Waveform datasets than the results given by the normal SVMs. Therefore, we can say that
the method of assigning fuzzy-membership values for training examples would play a major role when applying the FSVM method for datasets. In other words, the wrong way of assigning membership values can result in worse-performing models than the normal SVM model. Therefore, in order to find the best FSVM setting, ideally, all the available settings should be explored, which is very computationally demanding, especially with large datasets. However, interestingly, we can observe that for all the datasets considered, the results given by the FSVMhypexp setting were higher than the normal SVM results. Therefore, we can state that FSVMhypexp setting with the appropriate selection of β value would be an effective choice for normal FSVMs applied to any dataset.
Nevertheless, we can clearly observe that the results given by all the normal FSVM settings for all the datasets were biased toward the majority of negative class (i.e., higher SP and lower SE). Therefore, these results justify the fact that, like the normal SVMs, normal FSVM algorithm is also sensitive to the class imbalance problem.

B. FSVM-CIL Results
We next focused on the proposed FSVM-CIL method and considered the six FSVM-CIL settings, which are presented in Table I. As mentioned earlier, the value of r was set to the positive-to-negative dataset ratio of the corresponding dataset. When the exponentially decaying function was used as f(xi), the optimal value of β was chosen from the range β = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}, based on the results of the outer–inner–cv method for each dataset. For linearly decaying function, we set ∆ = 10−6 . The classification results, which are obtained by the six FSVM-CIL settings for each dataset via the outer–inner–cv method, are given in columns 4–9 of Table IV. The best normal FSVM results for each dataset are also given in columns 3 for comparative reasons. From these results, we can see that all FSVM-CIL settings performed well on all datasets, which gives both higher SE and SP and, hence, higher Gm compared with the original classification results, which were obtained by the normal SVM
and normal FSVM methods. Moreover, we can observe that the best classification results, which are depicted in bold type, for different datasets were given by different FSVM-CIL settings: FSVM − CILcenlin method gave the highest Gm for Yeast dataset, while the FSVM − CILcenexp method gave the highest Gm for Satimage and E. coli datasets; FSVM − CILsphlin yielded the highest results for Pageblocks, Transfusion, and Pima-Indians datasets, and FSVM − CILhyplin gave the highest results for the miRNA dataset; the FSVM − CILhypexp method gave the highest Gm for Abanole, Haberman, and Waveform datasets. This is due to the characteristic inherited from FSVMs that the optimal way of defining f(xi) is dependent on the dataset.

C. Existing Class Imbalance Learning Results
In order to compare the effectiveness of the proposed FSVMCIL method with the existing CIL methods available for SVMs, we applied random undersampling (Under), random oversampling (Over), SMOTE, DEC, and zSVM methods to develop normal SVM classifiers for each imbalanced dataset and evaluated their performance using the outer—inner–cv method. The resampling methods were applied until the training partitions were balanced while keeping the testing partitions in their original imbalanced distributions. Since randomness is associated with all of these resampling methods, each method was repeated five times for each cross-validation run in the outer–inner–cv method, and the results were averaged. In the SMOTE algorithm, the number of nearest neighbors (k) was set to 5. When applying the DEC method, the value of C−/C+ was set to the positive-to-negative class ratio of the dataset by following the findings in [5]. In the zSVM method, the range of z was fixed such that z ∈ {1 + 10−10 , 1 + 10−9 , . . . , 1 + 10−3}, and the value of z giving the highest classification Gm for the training dataset partition was selected as the optimal value of z to modify the coefficients of the positive-support vectors.
Tables V and VI present the results obtained by applying these existing CIL methods with normal SVMs for each dataset. The results on each dataset are sorted in the descending order of the Gm metric value. From the results presented in Tables V and VI, we can observe that all the existing imbalance-learning methods obtained better classification results than the results given by the normal SVM training for all the imbalanced datasets, as expected. Moreover, for different datasets, the highest Gm was yielded by different existing CIL methods. This evidence supported the fact reported in [18] that the CIL method giving the best results is dataset-dependent.

D. Comparison of Results
Next, we compare the results obtained by the proposed FSVM-CIL settings with the results obtained by the existing CIL methods, which were applied for normal SVMs by using the Gm metric values. Tables VII–X compare these results for each dataset separately. The normal SVM results and the best FSVM results are also presented for comparative reasons. For each dataset, the results are sorted in the descending order of the Gm metric value.
From results reported in Tables VII–X, we can observe that for nine out of ten datasets that were considered in this paper (all datasets except the Pima-Indians dataset), the results given by the best FSVM-CIL setting are better than the results given by all the existing imbalance-learning methods, which were applied for normal SVMs, with respect to the Gm metric. Although, for
the Pima-Indians dataset, the zSVM method yielded the best results, the results given by FSVM-CIL settings were still competitive with the results given by other imbalance-learning methods.

IX. STATISTICAL COMPARISON OF RESULTS
In this section, we present a statistical test used to validate the significance of the results obtained by the proposed FSVM-CIL methods over the results obtained by the existing CIL methods applied for normal SVMs. Demsar [56] recommends several nonparametric statistical tests that can be used to compare classifiers, which are developed on multiple datasets, when the conditions to apply the parametric tests are not satisfied. Among these tests, we selected the Friedman test [57], [58] to compare the proposed FSVM-CIL method with the existing CIL methods considered. This test first ranks the algorithms considered for each dataset separately, such that the best-performing algorithm would get rank 1, the second best one would get rank 2, etc. In case of ties, it assigns average ranks. Let rji be the rank of the jth algorithm out of k algorithms on the ith dataset out of
N datasets. The Friedman test compares the average ranks of algorithms Rj = 1N ∑ i r j i . Under the null-hypothesis (i.e., all the algorithms are equivalent so that any differences among their average ranks Rj are merely random), the Friedman statistic
χ2F = 12N
k(k + 1)
  k∑
j=1
R2j − k(k + 1)2
4
  (24)
is distributed according to the χ2F distribution with k − 1 degrees of freedom when N and k are large enough. However, Iman and Devenport [59] showed that the Friedman’s χ2F statistic is undesirably conservative and derived a better statistic
FF = (N − 1)χ2F
N(k − 1) − χ2F (25)
which is distributed according to the F-distribution with k − 1 and (k − 1)(N − 1) degrees of freedom.
In order to compare the results obtained by the best FSVMCIL method with the results of the existing CIL methods considered (Over, Under, SMOTE, DEC, and zSVM), we first determined the ranks of each algorithm for each dataset separately, which are given in Table XI.
Then, we calculated the following statistics when N = 10, and k = 6: χ2F = 12 × 10 6 × 7
× [ (1.12 + 2.82 + 4.22 + 4.62 + 4.12 + 4.22) − 6 × 7 2
4
]
= 25.14
FF = 9 × 25.14
10 × 5 − 25.14 = 9.10 ∼ F (5, 45).
The critical value of F (5, 45) at α = 0.05 is 2.42. Since FF > 2.42, we can reject the null-hypothesis, i.e., we can say that the compared algorithms are not equivalent at α = 0.05.
Since, we have rejected the null-hypothesis, we can now use the Benferroni–Dunn test [60] to compare the best FSVM-CIL method to the other imbalance-learning methods considered. Under this test, it is considered that the performance of the new algorithm (i.e., the control algorithm) is significantly different from another algorithm if the corresponding average ranks differ
by at least the critical difference (CD), which is defined as follows:
CD = qα
√ k(k + 1)
6N . (26)
The precalculated qα values are given in [56]. For our results, CD = 2.576 √ 6 × 7/6 × 10 = 2.15 at α = 0.05. Table XII gives the differences between the average ranks of the existing CIL methods and the average rank of the best FSVM-CIL method.
We can state that the best FSVM-CIL method is significantly different from (i.e., better than) DEC, Under, zSVM, and SMOTE methods at α = 0.05, since the differences in average ranks are greater than the CD = 2.15. However, based on the difference in average ranks, we cannot say that the best FSVMCIL method is significantly different from the Over method. Nevertheless, for all the datasets, the best FSVM-CIL method performed better than the Over method (see the actual results in Tables VII–X and the ranks in Table XI). We believe that this statistical test is less powerful in finding the difference between best FSVM-CIL method and the Over method due to the “granularity” property of the nonparametric tests, i.e., since the Over method performed better than the other existing CIL methods (DEC, Under, zSVM, and SMOTE) for most of the datasets, and therefore, it has obtained the best average rank among these existing CIL methods, we cannot find a significant difference between the average ranks of the best FSVMCIL method and the Over method, despite how much better the best FSVM-CIL method performed over the Over method. However, by considering all these results, we can state that the best FSVM-CIL method is significantly better than the existing imbalance-learning methods, which are considered in this paper. The reason of FSVM-CIL method to perform better than the existing CIL methods, which were applied for normal SVMs, would be the additional capability of the FSVM-CIL method to handle the problem of outliers and noise, to which the existing imbalance-learning methods available for normal SVMs are still sensitive.
Except for the small computational time required to calculate the membership values for the training examples, the proposed FSVM-CIL method does not increase the computational requirements of the underlying SVM training algorithm at all. However, as we observed, the best FSVM-CIL setting giving the highest results was dataset-dependent. Therefore, to explore all the FSVM-CIL settings in order to find the optimal one would be computationally very demanding. However, we can further
observe, from the results obtained, that for the nine datasets, for which the FSVM-CIL method gave the best results, the FSVM − CILhypexp setting gave higher results than the results given by the existing CIL methods. The results given by the FSVM − CILhypexp method are underlined in Tables VII–X. Based on these results, we can recommend that the FSVM − CILhypexp setting with the appropriate selection of β value would be a clever choice when applying FSVM-CIL method for any imbalanced dataset.

X. CONCLUSION
In this paper, we proposed an improved FSVM method, which is called FSVM-CIL, for learning from imbalanced datasets in the presence of outliers and noise. In this method, we assign fuzzy-membership values for training examples in order to handle both the problems of class imbalance and outliers/noise. We considered three ways to assign fuzzy-membership values and six FSVM-CIL settings. We used ten real-world imbalanced datasets to validate the proposed method. We compared the effectiveness of the FSVM-CIL method with five other popular existing imbalance-learning methods that are available for normal SVM training. We also applied a nonparametric statistical test to validate the significance of the performance of the proposed method over the existing CIL methods considered. From the overall results obtained, we can conclude that the proposed FSVM-CIL method could result in significantly better classification results than the existing imbalancelearning methods applied for normal SVMs, especially in the presence of outliers/noise in the datasets. Finally, as a rule of thumb, we can recommend that among the six FSVM-CIL methods considered, the FSVM − CILhypexp setting with the appropriate selection of β value would be an effective choice for learning from any imbalanced dataset. As future work, it would be interesting to investigate the effectiveness of using the FSVM method with other existing imbalance-learning methods, such as resampling techniques, for imbalanced dataset learning.

ACKNOWLEDGMENT
The authors would like to thank Oxford e-Research Center for providing access to their windows computer cluster to run parallel MATLAB programs.

References
[1]C. Cortes,V. VapnikSupport vector networksMach. Learning, vol. 20, pp. 273–297, 1995.1995
[2]N.J. Showe-TaylorChristianini, Support Vector Machines and Other Kernel-based Learning Methods2000
[3]K. Veropoulos,C. Campbell,N. CristianiniControlling the sensitivity of support vector machinesProc. Int. Joint Conf. Artif. Intell., Stockholm, Sweden, 1999, pp. 55–60.1999
[4]R. Akbani,S. Kwek,N. JapkowiczApplying support vector machines to imbalanced datasetsProc. 15th Eur. Conf. Mach. Learning, Pisa, Italy, 2004, pp. 39–50.2004
[5]G. Wu,E. ChangClass-boundary alignment for imbalanced data set learningpresented at the Int. Conf. Data Mining, Workshop Learning Imbalanced Data Sets II, Washington, DC, 2003.2003
[6]G. Wu,E. ChangKBA: Kernel boundary alignment considering imbalanced data distributionIEEE Trans. Knowl. Data Eng., vol. 17, no. 6, pp. 786–795, Jun. 2005.2005
[7]B. Raskutti,A. KowalczykExtreme re-balancing for SVMs: A case studyACM SIGKDD Explor. Newslett., vol. 6, no. 1, pp. 60–69, 2004.2004
[8]T. Imam,K. Ting,J. Kamruzzamanz-SVM: An SVM for improved classification of imbalanced dataProc. 19th Aust. Joint Conf. AI, Hobart, Australia, 2006, pp. 264–273.2006
[9]S. Zou,Y. Huang,Y. Wang,J. Wang,C. ZhouSVM learning from imbalanced data by GA sampling for protein domain predictionProc. 9th Int. Conf. Young Comput. Sci., Hunan, China, 2008, pp. 982– 987.2008
[10]Z. Lin,Z. Hao,X. Yang,X. LiumSeveral SVM ensemble methods integrated with under-sampling for imbalanced data learningAdvanced Data Mining and Applications. Berlin, Germany: Springer- Verlag, 2009, pp. 536–544.2009
[11]P. Kang,S. ChoEUS SVMs: Ensemble of under sampled SVMs for data imbalance problemsProc. 13th Int. Conf. Neural Inf. Process., Hong Kong, 2006, pp. 837–846.2006
[12]Y. Liu,A. An,X. HuangBoosting prediction accuracy on imbalanced data sets with SVM ensemblesProc. 10th Pac.-Asia Conf. Adv. Knowl. Discov. Data Mining, Singapore, 2006, pp. 107–118.2006
[13]H. Haibo,E. GarciaLearning from imbalanced dataIEEE Trans. Knowl. Data Eng., vol. 21, no. 9, pp. 1263–1284, Sep. 2009.2009
[14]B. Boser,I. Guyon,V. VapnikA training algorithm for optimal margin classifiersProc. 5th Annu. ACM Workshop Comput. Learning Theory, Pittsburgh, PA, 1992, pp. 144–152.1992
[15]X. ZhangUsing class centres vectors to build support vector machinesProc. IEEE Signal Process. Soc. Workshop, Madison, WI, 1999, pp. 3– 11.1999
[16]C.-F. Lin,S.-D. WangFuzzy support vector machinesIEEE Trans. Neural Netw., vol. 13, no. 2, pp. 464–471, Mar. 2002.2002
[17]G. WeissMining with rarity: A unifying frameworkSIGKDD Explor. Newslett., vol. 6, no. 1, pp. 7–19, 2004.2004
[18]N. Chawla,N. Japkowicz,A. KolczEditorial: Special issue on learning from imbalanced data setsSIGKDD Explor. Newslett., vol. 6, no. 1, pp. 1–6, 2004.2004
[19]N. Chawla,K. Bowyer,P. KegelmeyerSMOTE: Synthetic minority over-sampling techniqueJ. Artif. Intell. Res., vol. 16, pp. 321–357, 2002.2002
[20]J. Zhang,I. ManiKNN approach to unbalanced data distributions: A case study involving information extractionProc. Int. Conf. Mach. Learning, Workshop: Learning Imbalanced Data Sets, Washington, DC, 2003, pp. 42–48.2003
[21]T. Jo,N. JapkowiczClass imbalances versus small disjunctsACM SIGKDD Explor. Newslett., vol. 6, no. 1, pp. 40–49, 2004.2004
[22]X.Y. Liu,J. Wu,Z.H. ZhouExploratory under sampling for class imbalance learningProc. 6th IEEE Int. Conf. Data Mining, Hong Kong, 2006, pp. 965–969.2006
[23]N. Chawla,A. Lazarevic,L. Hall,K. BowyerSMOTEBoost: Improving prediction of the minority class in boostingProc. 7th Eur. Conf. Principles Pract. Knowl. Discov. Databases, Croatia, 2003, pp. 107–119.2003
[24]H. Guo,H. ViktorLearning from imbalanced data sets with boosting and data generation: The DataBoost IM approachACM SIGKDD Explor. Newslett., vol. 6, no. 1, pp. 30–39, 2004.2004
[25]Z.-H. Zhou,X.-Y. LiuTraining cost-sensitive neural networks with methods addressing the class imbalance problemIEEE Trans. Knowl. Data Eng., vol. 18, no. 1, pp. 63–77, Jan. 2006.2006
[26]D. Cieslak,N. ChawlaLearning decision trees for unbalanced dataMachine Learning and Knowledge Discovery in Databases. Berlin, Germany: Springer-Verlag, 2008, pp. 241–256.2008
[27]A. Fernandez,M. Jesus,F. HerreraHierarchical fuzzy rule based classification systems with genetic rule selection for imbalanced datasetsInt. J. Approx. Reason., vol. 50, no. 3, pp. 561–577, 2009.2009
[28]A. Fernández,M. Jesus,F. HerreraImproving the performance of fuzzy rule based classification systems for highly imbalanced datasets using an evolutionary adaptive inference systemBio-Inspired Systems: Computational and Ambient Intelligence. Berlin, Germany: Springer-Verlag, 2009, pp. 294–301.2009
[29]Y. Wang,S. Wang,K. LaiA new fuzzy support vector machine to evaluate credit riskIEEE Trans. Fuzzy Syst., vol. 13, no. 6, pp. 820–831, Dec. 2005. BATUWITA AND PALADE: FSVM-CIL: FUZZY SUPPORT VECTOR MACHINES FOR CLASS IMBALANCE LEARNING 5712005
[30]Y.-Y. Hao,Z.-X. Chi,D.-Q. YanFuzzy support vector machine based on vague sets for credit assessmentProc. 4th Int. Conf. Fuzzy Syst. Knowl. Discov., Changsha, China, 2007, vol. 1, pp. 603–607.2007
[31]E. Spyrou,G. Stamou,Y. Avrithis,S. KolliasFuzzy support vector machines for image classification fusing mpeg-7 visual descriptorsProc. 2nd Eur. Workshop Integr. Knowl., Semantics Dig. Media Technol., London, U.K., 2005, pp. 23–30.2005
[32]L. ChenFuzzy support vector machines for emg pattern recognition and mycroelectrical prosthesis controlProc. 4th Int. Symp. Neural Netw., Nanjing, China, 2007, pp. 1291–1298.2007
[33]Z. Xie,Q. Hu,D. YuFuzzy output support vector machine for classificationProc. Int. Conf. Adv. Natural Comput., Changsha, China, 2005, pp. 1190–1197.2005
[34]T. Inoue,S. AbeFuzzy support vector machine for pattern classificationProc. Int. Conf. Neural Netw., Washington, DC, 2001, pp. 1449– 1457.2001
[35]J. Mill,A. InoueAn application of fuzzy support vectorsProc. 22nd Int. Conf. N. Amer. Fuzzy Inf. Process. Soc., Chicago, IL, 2003, pp. 302–306.2003
[36]J.-H. Chiang,P.-Y. HaoSupport vector learning mechanism for fuzzy rule-based modeling: A new approachIEEE Trans. Fuzzy Syst., vol. 12, no. 1, pp. 1–12, Feb. 2004.2004
[37]Y. Chen,J. WangSupport vector learning for fuzzy rule-based classification systemsIEEE Trans. Fuzzy Syst., vol. 11, no. 6, pp. 716–728, Dec. 2003.2003
[38]A. Chaves,M. Vellasco,R. TanscheitFuzzy rule extraction from support vector machinespresented at the 5th Int. Conf. Hybrid Intell. Syst., Rio de Janeiro, Brazil, 2005.2005
[39]J. Castro,L. Flores-Hidalgo,C. Mantas,J. PucheExtraction of fuzzy rules from support vector machinesFuzzy Sets Syst., vol. 158, pp. 2057–2077, 2007.2007
[40]T. RossFuzzy Logic With Engineering Applications2004
[41]J.-S.R. JangANFIS: Adaptive-network-based fuzzy inference systemsIEEE Trans. Syst., Man Cybern., vol. 23, no. 3, p. 665—685, May/Jun. 1993.1993
[42]J.J. Buckley,Y. HayashiNeural networks for fuzzy systemsFuzzy Sets Syst., vol. 71, pp. 265–276, 1995.1995
[43]T. Lamata,S. MoralClassification of fuzzy measuresFuzzy Sets Syst., vol. 33, no. 2, pp. 243–253, 1989.1989
[44]S. Ling,F. Leung,P. TamDaily load forecasting with a fuzzy-inputneural network in an intelligent homeProc. 10th IEEE Int. Conf. Fuzzy Syst., Melbourne, Australia, 2001, pp. 449–452.2001
[45]C.-F. Lin,S.-D. WangTraining algorithms for fuzzy support vector machines with noisy dataProc. 13th IEEE Workshop Neural Netw. Signal Process., France, 2003, pp. 517–526.2003
[46]S. Xiong,H. Liu,X. NiuFuzzy support vector machines based on λ-cutProc. Int. Conf. Nat. Comput., Changsha, China, 2005, pp. 292– 600.2005
[47]H. Tang,L.-S. QuFuzzy support vector machines with a new fuzzy membership function for pattern classificationProc. 7th Int. Conf. Mach. Learning Cybern., Kunming, China, 2008, pp. 768–773.2008
[48]H.-B. Liu,S.-W. Xiong,X.-X. NiuFuzzy support vector machines based on spherical regionsProc. 3rd Int. Symp. Neural Netw., Chengdu, China, 2006, pp. 949–954.2006
[49]H. Xia,B. HuFeature selection using fuzzy support vector machinesFuzzy Optim. Decis. Making, vol. 5, no. 2, pp. 187–192, 2006.2006
[50]R. Batuwita,V. PalademicroPred: Effective classification of premiRNAs for human miRNA gene predictionBioinformatics, vol. 25, pp. 989–995, 2009.2009
[51]R. Batuwita,V. PaladeAn improved non-comparative classification method for human miRNA gene predictionProc. 8th IEEE Int. Conf. Bioinf. Bioneng., Athens, Greece, 2008, pp. 1–6.2008
[52]C.-C. Chang,C.-J. Lin. (2001LIBSVM: A library for support vector machines[Online]. Available: http://www.csie.ntu.edu.tw/ ∼cjlin/libsvm0
[53]M. Kubat,S. MatwinAddressing the curse of imbalanced training sets: One-sided selectionProc. 14th Int. Conf. Mach. Learning, Nashville, TN, 1997, pp. 179–186.1997
[54]J. DemsarStatistical comparisons of classifiers over multiple data setsJ. Mach. Learning Res., vol. 7, p. 1, 2006.2006
[55]M. FriedmanThe use of ranks to avoid the assumption of normality implicit in the analysis of varianceJ. Amer. Statist. Assoc., vol. 32, pp. 675–701, 1937.1937
[56]M. FriedmanA comparison of alternative tests of significance for the problem of m rankingsAnn. Math. Statist., vol. 11, pp. 86–92, 1940.1940
[57]R. Iman,J. DevenportApproximations of the critical region of the Friedman statisticsCommun. Statist., vol. 9, pp. 571–595, 1980.1980
[58]O. DunnMultiple comparison among meansJ. Amer. Statist. Assoc., vol. 56, pp. 52–64, 1961. Author’s photographs and biographies not available at the time of publication.1961
