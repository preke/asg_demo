Random KNN
Shengqiao Li,James Harner,Donald A. Adjeroh


I. INTRODUCTION
In this work, we consider high-dimensional datasets as those involving p ≥ 1000 variables. For microarray data, given the cost and other objective restrictions, the number of samples, n, is typically far less than the number of variables, p. This is known as the “small n, large p” problem. A model cannot be built directly since the model complexity is larger than that of the dataset. For example, technically, least-squares regression can fit a linear model up to n parameters. Such a model would provide a perfect fit, but has no prediction power. This problem essentially imposes an extra level of difficulty in the modeling. On the other hand, in complex pattern recognition problems, not all variables are equally important. For instance, in gene expression data analysis, most of the genes may not be related to (or useful in predicting) the target phenotype. The few important genes should be spotted and the other noisy genes should be excluded. Thus, efficiently building a good predictive model or determining the gene signature (for instance, the gene signature for a tumor) necessitates a serious look at dimensionality reduction or feature selection.
In this study, we develop a new method for data analysis and modeling, Random KNN, which is suitable for highdimensional datasets. The Random KNN approach can be applied to both qualitative and quantitative responses, i.e., classification and regression problems, and has applications in statistics, machine learning and pattern recognition.
Random KNN is a generalization of the k-nearest neighbor (KNN) algorithm; thus, Random KNN enjoys the many advantages of KNN. In particular, KNN is a nonparametric classification method. It does not assume any parametric form for the distribution of measured random variables for a population. Due to the flexibility of the nonparametric model, it is usually a good classifier for many situations in which the joint distribution is unknown, or hard to model parametrically. This is especially the case for high-dimensional datasets where it is common to have thousands of variables involved initially. KNN is generally more robust and more sensitive compared with other popular classifiers [1].
The KNN classification technique, however, has two major problems when applied to high-dimensional data. The first is the curse of dimensionality since KNN searching time grows exponentially with respect to the dimensionality parameter p [2]. Secondly, although the KNN algorithm can be shown to asymptotically converge to the optimal classification rate, in practice it has a comparatively low robustness and reliability on high-dimensional datasets. In this work, our Random KNN addresses these two key problems simultaneously.
In Random KNN, a collection of KNN models is fitted with randomly selected variables, and then the individual classification results are combined for the final decision. The idea of Random KNN is motivated by the technique of Random Forests [3], and is similar in spirit to the method of random subspace selection used for Decision Forests [4]. Both Random Forests and Decision Forests use decision trees as the base classifiers. Compared with these two, Random KNN uses a stable base of KNN classifiers, with no hierarchical structure involved. Thus, Random KNN can be stabilized with a relatively small number of base KNN’s and usually only a small number of important variables will be needed. This implies that the final model with Random KNN will be simpler than those with Random Forests or Decision Forests. We also show that Random KNN leads to significant performance improvements in terms of both computational complexity and classification accuracy.

II. RANDOM KNN
The proposed Random KNN has a connection with classifier fusion and ensemble algorithms, such as those studied by Kitler et al. [5], Freud and Schapire [6], [7], Ji and Ma [8], Breiman [9], or Sewell [10]. However, the method is most closely related to, and motivated by, the technique of Random Forests (RF) developed by Breiman [3].
One problem with the RF algorithm is that the prediction relies on all the input variables. Variable selection methods have been developed for RF [11], [12]. Various authors have studied Random Forests from different perspectives. For instance, Lin and Jeon studied the connections between Random Forests and adaptive nearest neighbor methods, which use adaptive local distance metrics [13]. Even with these extensions, which stabilize the hierarchical tree structures, RF may need to keep a relative large number of variables to generate many dissimilar trees.
Random KNN uses KNN as base classifiers that are simple to implement and are stable [14], compared with decision trees. Each KNN classifier classifies a test point by its majority, or weighted majority class, of its k neighbors. The final
978-1-4799-4274-9/14 $31.00 © 2014 IEEE DOI 10.1109/ICDMW.2014.112
629
classification in each case is determined by majority voting of r KNN classifications. This can be viewed as a sort of voting by Majority of a Majority.
More formally, let F = {f1, f2, . . . , fp} be the p input features, and X be the n original input data vectors of length p, i.e., an n × p matrix. For a given integer m < p, denote F
(m) = {fj1 , fj2 , . . . , fjm |fjl ∈ F, 1 ≤ l ≤ m} a random subset drawn from F with equiprobability. Similarly, let X(m) be the data vectors in the subspace defined by F(m), i.e., an n × m matrix. Then a KNN(m) classifier is constructed by applying the basic KNN algorithm to the random collection of features in X(m). A collection of r such base classifiers is then combined to build the final Random KNN classifier.

III. PROPERTIES OF RANDOM KNN
In this section, we study the properties of Random KNN, such as feature coverage, number of random KNN’s to be generated, and convergence.

A. Feature Coverage
The coverage of a classifier over the input features is an important parameter in evaluating classifiers. Here, the coverage essentially indicates whether all features are used in the collection of KNN(m)’s and for what number of Random KNN, r, are all features covered.
1. Feature Multiplicity
The variables that appear in the set of classifiers constitute a multiset. Define the random variable multiplicity, M , of an input feature as the number of times that feature appears in the multiset. Then M is a binomial random variable with probability m/p, i.e., M ∼ BIN(r, m/p). The average multiplicity of a feature f is then given by:
ν = E(M) = rm
p . (1)
2. Silent Features
A feature f may not be selected by any of the r KNN classifiers and thus it has no voice in classification decisions. We call such a feature a silent feature. The chance of a feature being silent will be small if m and r are sufficiently large. Specifically, define a binary indicator variable, If , to indicate if a feature f is silent (If = 1 when feature f is silent). The probability of f being silent is then given by:
P (If = 1) = P (M = 0) = [( p− 1 m )/( p m )]r
= ( 1− m
p
)r .
If we let S be the number of silent features, i.e., S =∑ f If , then 0 ≤ S ≤ p−m. Since the If ’s are dependent, S is not a binomial random variable. In fact, S is analagous to the number of coupons not yet collected in the coupon collector’s problem, while drawing a subset each time [15], [16]. The expectation of S and probability function are respectively given by the following [15]:
λ = E(S) = p ( 1− m
p
)r (2)
and
P (S = s) =
( p
s ) p−s∑ j=0 (−1)j ( p−s j )( p−s−j m )r ( p m
)r = ( p
s ) p−s∑ j=0 (−1)p−s−j ( p−s j )( j m )r ( p m
)r . 3. Coverage probability
The probability that all features are used at least once is defined as the coverage probability, η. That is,
η = P (S = 0)
= p∑ j=0 (−1)j ( p j )[( p− j m )/( p m )]r
= p∑ j=0 (−1)p−j ( p j )[( j m )/( p m )]r , (3)
where ( p j ) [( p−j m )/( p m )]r is the jth binomial moment of
S. The binomial coefficient ( p j ) in the above equations will overflow for large p and thus the probability cannot easily be computed. In fact, to accurately compute the maximum binomial coefficient ( p
p/2
) , the number of bits needed can be
estimated using Stirling approximation (n! ≈ √2πn (ne )n ) as follows:
log2
( p
p/2
) ≈ 1
2 log2 2πp+ p log2(p/e)− log2 πp
− p log2(p/2/e) = p+ (1− log2 πp)/2 ≈ p.
Thus, (
p p/2 ) ≈ 2p. It is difficult to represent such a large number by a standard data type on a computer for a large p (recall that p could be in tens of thousands). For example, the maximum 64-bit double precision floating number is 21024. Therefore arbitrary-precision arithmetic is required. Here, we give a simple approximation for the case when p is large and m/p is small. Since(
j m )( p m ) = (1− m p )( 1− m p− 1 ) . . . ( 1− m j + 1 )
< ( 1− m
p
)p−j , (4)
we may ignore the dependency among If ’s, and approximate S by a binomial random variable BIN(p, (1−m/p)r). Then we use ηb as an approximation of η; it is numerically justified that ηb is an upper bound of η, i.e.,
η = P (S = 0)
< p∑ j=0 (−1)p−j ( p j )[( 1− m p )p−j]r
= p∑ j=0 (−1)p−j ( p j )[( 1− m p )r]p−j
= [ 1− ( 1− m
p
)r]p = ηb. (5)
This bound is easy to compute. Another simple way to compute η is to approximate S by a Poisson random variable. Then
ηp = e −λ, (6)
where λ is defined by (2).
4. Size of Random KNN
The preceding discussion provides guidance for r, the number of KNNs to be generated. The value of r may be determined by inverting the binomial or Poisson approximation of η if η is pre-specified, respectively, as follows:
rb = ln(1− η1/p) ln(1−m/p) (7)
or
rp = ln(− ln η)− ln p ln(1−m/p) . (8)
If we pre-specify ν in (1), r can be derived by:
r = νp/m. (9)
On the other hand, we may consider a related variable R, the minimum number of KNNs required such that each feature is drawn at least once. The mean, rz , can be obtained [16] by:
rz = E(R) =
( p
m ) p∑ j=1 (−1)j+1 ( p j ) ( p m
)− (p−jm ) . (10) This can be a reference value for r. In (10), we must address the computation problem caused by large binomial coefficients. One simple way is to simulate the distribution of R and estimate its mean. As before, we will consider an approximate distribution of R. Let Rf be the minimum r until the specific feature f is drawn. Then Rf is a geometric random variable with parameter m/p, i.e.,
P (Rf ≤ r) = P (If = 0) = 1− ( 1− m
p
)r .
For p features f1, f2, . . ., fp, we have that Rf1 , Rf2 , . . ., Rfp are identically distributed, but dependent. R is the maximum of these dependent geometric random variables. For p m, the dependency is small. Ignoring the dependency and considering the maximum of the i.i.d. geometric random variables, Rg , we have:
P (Rg ≤ r) = [ 1− ( 1− m
p
)r]p .
Thus,
rz ≈ E(Rg) = ∞∑ r=0 P (Rg > r)
= ∞∑ r=0 { 1− [ 1− ( 1− m p )r]p} . (11)
The infinite series in (11) can be approximated by finite terms and can be easily computed.

B. Error Rate Analysis
Various authors have earlier analyzed the theoretical performance of traditional nearest neighbor classifiers. Examples can be found in [17], [18], [19], [20], [21], [22]. The theoretical analysis of the convergence of Random KNN turns out to be challenging. The study of convergence of Random KNN is very difficult due to the feature correlation.
In this section, we discuss the convergence of Random KNN for a simplified case where the variables are assumed to be independent of each other and each variable has equal importance. Under this condition, each KNN, with m p variables, is independent and has the same accuracy π and error rate 1−π. Without loss of generality, assuming the class label is binary, r ≥ 3 is an odd integer, and Y is the number of correct votes, then Y ∼ BIN(r, π). By majority voting, the error rate will be:
Pe = P (Y ≤ r/2 )
= r/2 ∑ y=0
( r
y
) πy(1− π)r−y. (12)
There are two facts related to Pe: (1) Pe < 1− π; (2) Pe is a decreasing function of r. Therefore, building multiple random KNN’s can reduce the error rate, and the more the better.
The above discussion is for the ideal case where the KNN error rate π is assumed to be a fixed value. More generally, we may consider the error rate to be a random variable Π. For example, a BETA(α, β) distributed random variable, where α and β are shape parameters, is conjugate to binomial distributions. The shape parameters give the BETA distribution the flexibility to model a wide range of different distributions. Hence we may assume Π ∼ BETA(α, β) and the integrated error rate will be the overall error rate for Random KNN. Since
Pe(π) = P (Y ≤ r/2|Π = π) = r/2∑ y=0
( r
y
) πy(1− π)r−y,
then
Pe = EΠ(Pe(π))
= ∫ 1 0 r/2∑ y=0 ( r y ) πy(1− π)r−y · π α−1(1− π)β−1 B(α, β) dπ
= 1
B(α, β) r/2∑ y=0
( r
y )∫ 1 0 πα+y−1(1− π)β+r−y−1 dπ
= 1
B(α, β) r/2∑ y=0
( r
y
) B(α+ y, β + r − y), (13)
where B is the beta function. Or,
Pe = 1
(r + 1)B(α, β) r/2∑ y=0 B(α+ y, β + r − y) B(1 + y, 1 + r − y) . (14)
The above equations are non-increasing functions of r.
For an example, let α = 2, β = 1, then
Pe = 2 r/2∑ y=0
r! y!(r − y)! (y + 1)!(r − y)! (r + 2)!
= 2 r/2∑ y=0
y + 1
(r + 1)(r + 2)
= 2
(r + 1)(r + 2) r/2∑ y=0 (y + 1)
= r + 4 4(r + 1) → 1 4 .
The error rate for one m-variable base KNN is 1 − E(Π) = 1− αα+β = 13 , which is greater than the above Random KNN error rate.
Figure 1 shows that error rates decrease with r for different α and β. Under the above conditions, the error rate of the Random KNN converges to minimum error rate fairly fast. Figure 2 shows a contour plot of the minimum error rates
for Random KNN (r = 200) for different α and β. We can see that, the larger α is compared with β, the smaller the error rate. For comparison, Figure 3 shows error rates for one KNN. The error rates change more evenly for Random KNN, when compared with a traditional (single) KNN. Figures 2 and 3 demonstrate that better performance can be obtained by combining weak KNN classifiers. But if the base KNN classifiers are weaker than simple classification “by-chance,” (i.e., having a classification rate less than 0.5), then the resulting Random KNN will be worse than the base KNNs.

C. Complexity of Random KNN
The time complexity of Random KNN classification is mainly dominated by the time needed by the underlying k-nearest neighbor searching algorithm. Various searching schemes have been proposed for the traditional KNN algorithm. Some are tree-based, such as the k-dimensional binary search tree, i.e., kd-tree [23], [24], metric-trees [25], [26], cover-trees [27], etc. Others use hashing functions, e.g., locality sensitive hashing [28]. These algorithms provide sublinear (logn) time searching through special data structures for a given data point and are very efficient for low-dimensional
data. In terms of dimensionality, most k-nearest neighbor searching algorithms have difficulty on high-dimensional data and exhibit exponential complexity [2]. Thus, they are not efficient for high-dimensional data. There have been efforts to improve performance in high-dimensional data, for instance, using BoostMap [29] or slicing [30]. These often use approximate nearest-neighbor approaches leading to some sacrifice in terms of accuracy.
Assuming a run-time bound of O(2pkn logn) for KNN searching with a reasonable implementation of any one of the aforementioned algorithms, then the time for Random KNN will be O(r2mkn logn). If m = √ p, the complexity becomes O(r2 √ pkn logn). Since the exponent is much smaller than that for the ordinary KNN method, Random KNN is expected to be much faster for high-dimensional data. Interestingly, if we use m = log p, we obtain a complexity in O(rpkn log n), which is linear in p, the number of variables.
We compared the running time of KNN with our Random KNN on a Dell Desktop OptiPlex 745 with 3GHz Pentium D CPU and 4GB RAM. The software was run in R 2.9.0 under the Windows XP (service pack 3). We took the average running time of 100 runs of KNN and Random KNN. For Random KNN, we take either m = √p , ν = 2, and r = 2 √p or m = log2 p , ν = 2, and r = 2(p/ log2 p). Table I shows the average running time on simulated data. One can see that the current implementation of the Random KNN algorithm is
quite fast.
In terms of computational complexity, our Random KNN approach partially addressed one aspect of the curse of dimensionality problem. The exponential growth of the running time with the number of features is eased. This makes the Random KNN model suitable for large scale pattern analysis applications with large p and/or large n. Since each base KNN can be run independently, the Random KNN method can be readily parallelized on a machine with multiple computational units such as cluster computers or multicore processor systems.

D. Parameter setting
Random KNN has three tuning parameters: k, the number of nearest neighbors; r, the number of random KNNs; and m, the number of features for each base KNN. Here we will discuss these parameters and how their values are determined.
1. Number of nearest neighbors, k
The choice of k for traditional KNN has been studied in the literature. But the problem is still not completely resolved. The general guideline proposed by Fix and Hodges [31] is the two-condition rule: limn→∞ k = ∞ and limn→∞ k/n = 0. Devroye (1982) [32] suggested : limn→∞ k/ log log n = ∞ and limn→∞ k/n = 0.; Devroye et al. (1994) [33] suggested: limn→∞ k/ logn =∞ and limn→∞ k/n = 0. In general, k ∝√ n is also widely used. These conditions imply that k should be large, but much smaller than n. These recommendations are for large samples (asymptotic behavior); it is still difficult to determine a functional form between k and n and the choice of k is also distribution-dependent.
In practice, since the sample sizes are often relatively small, k is also usually small. Cross-validation is therefore widely adopted. For Random KNN, the theoretical side of this problem is more difficult. Choosing k by experimentation is a realistic approach.
2. Number of features, m
The number of features for each random KNN, m, is another important parameter of the Random KNN procedure. To generate the maximum number of different feature subsets, we need to have m = p2 . Essentially, this means that r ≤ ( p m ) = ( p p/2 ) . On the other hand, to ensure diversity in these feature subsets (i.e., for less redundancy or overlap among these subsets), m needs to be small.
For any two subsets of m features, the number of common features between them, Z, is a hypergeometric random variable. Imagine the m features in set 1 are white and the remaining p −m features are black. Then randomly draw m features from these m white and p−m black features without replacement. The number of white features drawn is Z. This is the classical hypergeometric model and its probability mass function is:
P (Z = z) =
( m z )( p−m m−z ) ( p m
) . (15) Therefore, E(Z) = mmp = m2 p . If we let m ≤ √ p, then E(Z) ≤ 1. Smaller values of m are also important in considering the computational time required for the nearest neighbor searching. Other choices could be m = log p , or m = log2 p .
3. Number of random KNNs, r
The number of random KNN’s, r, may be chosen according to equations (7), (8), or (9).

IV. EXPERIMENT RESULTS
To evaluate the performance of the proposed Random KNN, we performed experiments using twelve datasets.

A. Data sets
Ten of the twelve datasets used relate to microarray gene expression on different human cancers. Eight of the microarray datasets are publicly available [34]. Details on these eight datasets are provided in Table II. In the table, SRBCT stands for Small Round Blue Cell Tumor.
The two other gene expression datasets are for the socalled NCI60 and non-small cell lung cancer (NSCLC) data. The latter is from a survival study [35]. The NCI60 dataset was downloaded from http://bioinf.ucd.ie/people/aedin/R/full datasets/. The NSCLC dataset is available from the NCI caArray database (https://array.nci.nih.gov/caarray/project/details. action?project.experiment.publicIdentifier=beer-00153).
For the NSCLC dataset, each chip contains 7,129 oligonucleotide probe sets (each gene serves as a variable). The complete dataset includes 86 lung cancer tissue samples. 5- year survival classification through signature gene expression was studied. Subjects who had operations less than 5 years previously and were still alive at the study time were excluded. Thus, 43 samples were left and further classified into low and high risk groups, i.e., whether the subject survived at least 5
years or not. Our method is compared with other proposed methods on this dataset.
The last two datasets are handwriting character recognition (HDR) and Arrhythmia (ARR), which were used in [36] for feature selection in high-dimensional data. The HDR and ARR datasets are available at the UCI repository (http://archive. ics.uci.edu/ml/index.html). HDR has 649 features and 2000 samples. ARR data has 278 features and 452 samples (420 samples are contained in the Peng et al. study [36]). We removed 17 all-zero features and the nominal feature–gender; the remaining numerical features are normalized to [0, 1]. Peng et al. also used the NCI60 and Lymphoma (LYM) microarray datasets. Our version of these two datasets obtained above are slightly different from those used in [36]. For NCI60, Peng et al. used 9703 genes, but there are a large number of missing values. It’s not clear how they impute these missing values. For the Lymphoma data, we worked with the widely used version, which only includes the 3 major classes.

B. Parameter setting for classification using Random KNN
As discussed above, feature multiplicity is an important parameter in the proposed Random KNN. For example, the distribution of the multiplicities using the Leukemia data is approximately symmetric with a mean of about 15 and the multiplicities ranged from 4 to 34.
We show empirical results based on the 8 microarray datasets in Table II using different tuning parameters: k, r and m. The results provide an initial measure of the classification performance of the proposed Random KNN. Figure 4 shows the variation of performance with k, for various values of r. The figure shows that Random KNN is rather robust for different k values. The best k values for these data sets range from 1 to 9. This suggests that an appropriate value of k can be easily determined based on initial tests for a given application.
As aforementioned, the number of features, m, for each KNN should not be larger than √ p. Smaller values of m are also important in considering the computational time required for the nearest neighbor searching. We tested Random KNN with different values of m. The results are shown in Figure 5. Sample sizes and numbers of features are given in the inset. From this test, most data sets perform well with m = √ p.
In addition, we tested different values of r on the 8 tumor microarrays in Table II. Figure 6 shows the results. For each data set and for a given r, Random KNN was repeated 30 times and the performance was averaged. This figure shows that the accuracy of Random KNN generally improves with increasing r. To guarantee convergence, a large r can be used. For these data sets, the accuracies converge after about r > 100 when m = √ p. When m = log2(p), r needs to be larger and if m = log22(p), r can be smaller. The number of KNN’s has a larger impact on prostate, SRBCT (small round blue cell tumors), brain and lymphoma datasets than others. For other datasets, the convergence is faster and needs a smaller r. The behavior using different r values is quite data-dependent. Fast convergence may also indicate that most of the features are similar and thus redundant. For microarray data, usually a small number of genes are the key genes; thus, feature selection may be required. Feature selection using Random KNN is discussed in the next section.
In summary, although we have three moving parts for Random KNN, they can be readily set. For microarray data applications, we recommend setting m = √p , and k between 1 and 15. r can be chosen using equation 7 or 8, based on p and the desired coverage probability, η.

C. Feature Selection using Random KNN
A two-stage feature selection procedure RKNN-FS based on Random KNN was developed. The procedure eliminates non-important features according to feature support that measures the classification capability of a feature. In the first stage, a fixed proportion of the feature set is removed at each step and only one feature is removed at each step in the second stage. The details are in [37]. The Random KNN variable selection procedure was tested on the microarray datasets in Table II. Results are given in Table III.
In the table, classification accuracy (or classification rate) for KNN using all genes is included as a reference. Random KNN feature selection was applied and 4 genes were selected for each data set. The classification accuracy using 4 selected genes is shown in the table. We can observe that (1) the classification accuracy using the 4 selected genes is quite high; (2) for Brain, Breast cancer, and Colon datasets (where the classification accuracy of traditional KNN is < 80% using all the genes), using only the 4 genes selected by Random KNN improved the performance significantly. For the Lymphoma and SRBCT data sets, the classification rate was not improved by using Random KNN feature selection since the full model already provided a high classification rate.

D. Comparative Results
We applied our Random KNN approach to the NSCLC dataset. We first removed 58 control gene probe sets with prefix “AFFX”. The two-stage Random KNN feature selection method is used. We get a set of four meaningful genes: NP, RFTN1, CHD4 and VDAC1.
Table IV shows the improved result obtained using the above 4 genes with the proposed Random KNN approach,
along with other approaches, which selected 6 different genes based on leave-one-out cross-validation. One can see that the proposed Random KNN approach is quite competitive when compared with other schemes. Using four selected genes, 38 cases are correctly classified, two high risk subjects are classified as low risk and three good prognoses classified into the opposite category. The accuracy is 88.4%, which is higher than all other methods.
To check the versatility of the proposed method, and to compare it with state-of-the-art methods for analysis of highdimensional datasets, we tested the scheme on four datasets, including those for handwritten character recognition (HDR), arrhythmia (ARR), and microarray datasets, namely, NCI cancer cell lines (NCI 60), and lymphoma (LYM). Details on these datasets are provided in [36]. Peng et al. [36] proposed a mRMR (minimal-redundancy-maximal-relevance) feature selection algorithm and a forward feature selection method based on mutual information, using different classification schemes such as support vector machines (SVM), Naı̈ve Bayes (NB) and linear discriminant analysis (LDA) as a wrapper. A comparison of the results using Random KNN and the three major algorithms reported in [36] is shown in Table V. We can observe that for any given mRMR method (SVM, NB, or LDA), the proposed Random KNN approach provided comparable or better overall performance.

V. CONCLUSIONS
In this paper, we introduce Random KNN, a new classification method for high-dimensional data. We performed a detailed study of its properties. We also performed an empirical analysis using the proposed Random KNN on twelve different datasets, and compared its performance with prior work. Using features selected by RKNN-FS resulted in an overall higher classification accuracy when compared with other proposed feature selection methods for high-dimensional data. In terms of computational complexity, Random KNN runs faster than traditional KNN for high-dimensional data. In summary, the Random KNN approach provides an efficient and effective solution to pattern analysis and modeling with high-dimensional data.
Through various tests, the Random KNN was shown to be an easy-to-use method. Although there are multiple moving parts, such as r, m and k, they can be readily set, e.g., taking m = √ p, searching for k by cross-validation, and choosing sufficiently large r(> 200), based on the required coverage probability. An R software package was developed, which can parallelize computations. The proposed methods have applications wherever high-dimensional datasets are involved. We believe the Random KNN approach is a significant contribution to statistics, machine learning and data mining.

References
[1]O. Troyanskaya,M. Cantor,G. Sherlock,P. Brown,T. Hastie,R. Tibshirani,D. Botstein,R.B. AltmanMissing value estimation methods for DNA microarraysBioinformatics, vol. 17, no. 6, pp. 520–525, 2001.2001
[2]A.M. KibriyaFast algorithms for nearest neighbour searchPh.D. dissertation, The University of Waikato, March 2007.2007
[3]L. BreimanRandom forestsMachine Learning, vol. 45, no. 1, pp. 5–32, 2001.2001
[4]T.K. HoThe random subspace method for constructing decision forestsIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, no. 8, pp. 832–844, 1998.1998
[5]J. Kittler,M. Hatef,R.P. Duin,J. MatasOn combining classifiersIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, no. 3, pp. 226–239, March 1998.1998
[6]Y. Freund,R.E. SchapireExperiments with a new boosting algorithmsProceedings of the Thirteenth International Conference on Machine Learning, 1996, pp. 148–156.1996
[7]R. Schapire,Y. Freund,P. Bartlett,W.S. LeeBoosting the margin: A new explanation for the effectiveness of voting methodsThe Annals of Statistics, vol. 26, no. 5, pp. 1651–1686, 1998.1998
[8]C. Ji,S. MaCombinations of weak classifiersIEEE Transactions on neural networks, vol. 8, no. 1, pp. 32–42, January 1997.1997
[9]L. BreimanArcing classifiersThe Annals of Statistics, vol. 26, no. 3, pp. 801–824, 1998.1998
[10]M. SewellEnsemble learningDepartment of Computer Science, University of College London, Tech. Rep., 2008.2008
[11]S. Li,A. Fedorowicz,H. Singh,S.C. SoderholmApplication of the random forest method in studies of local lymph node assay based skin sensitization dataJournal of Chemical Information and Modeling, vol. 45, no. 4, pp. 952–964, 2005.2005
[12]R. Dı́az-Uriarte,S.A. de Andrı́sGene selection and classification of microarray data using random forestBMC Bioinformatics, vol. 7, no. 3, pp. 1–13, 2006.2006
[13]Y. Lin,Y. JeonRandom forests and adaptive nearest neighborsJournal of the American Statistical Association, vol. 101, no. 474, pp. 578–590, 2006.2006
[14]T.G. DietterichMachine-learning research: Four current directionsThe AI Magazine, vol. 18, no. 4, pp. 97–136, 1998.1998
[15]W. StadjeThe collector’s problem with group drawingsAdvances in Applied Probability, vol. 22, no. 4, pp. 866–882, 1990.1990
[16]I. Alder,S.M. RossThe coupon subsets collection problemJournal of Applied Probability, vol. 38, no. 3, pp. 736–746, 2001.2001
[17]T. Cover,P. HartNearest neighbor pattern classificationIEEE Transactions on Information Theory, vol. IT-13, no. 1, pp. 21–27, 1967.1967
[18]T.J. WagnerConvergence of the nearest neighbor ruleIEEE Transactions on Information Theory, vol. IT-17, no. 5, pp. 566–571, 1971.1971
[19]J. FritzDistribution-free exponential error bound for nearest neighbor pattern classificationIEEE Transactions on Information Theory, vol. IT-21, no. 5, pp. 552–557, 1975.1975
[20]D. Psaltis,R.R. Snapp,S.S. VenkateshOn the finite sample performance of the nearest neighbor classifierIEEE Transactions on Information Theory, vol. 40, no. 3, pp. 820–837, 1994.1994
[21]S.R. Kulkarni,S.E. PosnerRates of convergence of nearest neighbor estimation under arbitrary samplingIEEE Transactions on Information Theory, vol. 41, no. 4, pp. 1028–1039, 1995.1995
[22]E. BaxValidation of nearest neighbor classifiersIEEE Transactions on Information Theory, vol. 46, no. 7, pp. 2746–2752, 2000.2000
[23]J. BentleyMultidimensional binary search trees in database applicationIEEE Transactions on Software Engineering, vol. 5, no. 4, pp. 333–334, 1979.1979
[24]S. Arya,D.M. Mount,N.S. Netanyahu,R. Silverman,A.Y. WuAn optimal algorithm for approximate nearest neighbor searching in fixed dimensionsJournal of the ACM, vol. 45, no. 6, pp. 891–923, 1998.1998
[25]J. UhlmannMetric treesApplied Mathematics Letters, vol. 4, no. 5, pp. 61–62, 1991.1991
[26]——Satisfying general proximity/similarity queries with metric treesInformation Processing Letters, vol. 40, no. 4, pp. 175–179, 1991.1991
[27]A. Beygelzimer,S. Kakade,J. LangfordCover trees for nearest neighborICML ’06: Proceedings of the 23rd international conference on Machine learning. New York, NY: ACM Press, 2006, pp. 97–104.2006
[28]A. Gionis,P. Indyk,R. MotwaniSimilarity search in high dimensions via hashingProceedings of the 25th Very Large Database (VLDB) Conference, 1999. [Online]. Available: http://people.csail.mit.edu/indyk/vldb99.ps1999
[29]V. Athitsos,J. Alon,S. Sclaroff,G. KolliosBoostMap: An embedding method for efficient nearest neighbor retrievalIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 1, pp. 89– 104, January 2008.2008
[30]S. Nene,S. NayarA simple algorithm for nearest neighbor search in high dimensionsPattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 19, no. 9, pp. 989–1003, 1997.1997
[31]E. Fix,J. HodgesDiscriminatory analysis-nonparametric discrimination: Consistency propertiesUS Air Force, School of Avaiation Medicine, Tech. Rep. 21-49-004, 4, 1951.1951
[32]L. DevroyeNecessary and sufficient conditions for the pointwise convergence of nearest neighbor regression function estimatesProbability Theory and Related Fields, vol. 61, no. 4, pp. 467–481, 1982.1982
[33]L. Devroye,L. Gyorfi,A. Krzyzak,G. LugosiOn the strong universal consistency of nearest neighbor regression function estimatesThe Annals of Statistics, vol. 22, no. 3, pp. 1371–1385, 1994.1994
[34]D.G. Beer,S.L. Kardia,C.-C. Huang,T.J. Giordano,A.M. Levin,D.E. Misek,L. Lin,G. Chen,T.G.G.D.G. Thomas,M.L. Lizyness,R. Kuick,S. Hayasaka,J.M. Taylor,M.D. Iannettoni,M.B. Orringer,S. HanashGene-expression profiles predict survival of patients with lung adenocarcinomaNature Medicine, vol. 8, no. 8, pp. 816– 824, August 2002.2002
[35]H. Peng,F. Long,C. DingFeature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancyIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 8, pp. 1226–1238, August 2005.2005
[36]S. Li,E.J. Harner,D.A. AdjerohRandom knn feature selction – a fast and stable alternative to random forestsBMC Bioinformatics, vol. 12, no. 450, pp. 1–11, 2011. 6362011
