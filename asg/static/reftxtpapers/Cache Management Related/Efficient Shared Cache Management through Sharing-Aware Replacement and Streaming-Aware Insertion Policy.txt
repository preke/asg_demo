Efficient Shared Cache Management through Sharing-Aware Replacement and Streaming-Aware Insertion Policy
Yu Chen,Wenlong Li,Changkyu Kim,Zhizhong Tang
chenyu00@gmail.com;,changkyu.kim}@intel.com;,tzz-dcs@tsinghua.edu.cn

Multi-core processors with shared caches are now commonplace. However, prior works on shared cache management primarily focused on multi-programmed workloads. These schemes consider how to partition the cache space given that simultaneously-running applications may have different cache behaviors. In this paper, we examine policies for managing shared caches for running single multi-threaded applications. First, we show that the shared-cache miss rate can be significantly reduced by reserving a certain amount of space for shared data. Therefore, we modify the replacement policy to dynamically partition each set between shared and private data. Second, we modify the insertion policy to prevent streaming data (data not reused before eviction) from promoting to the MRU position. Finally, we use a low-overhead sampling mechanism to dynamically select the optimal policy. Compared to LRU policy, our scheme reduces the miss rate on average by 8.7% on 8MB caches and 20.1% on 16MB caches respectively.

1. Introduction
Chip-Multiprocessors (CMPs) are now commonplace. While level-one caches in CMPs are private to each processor core, the major processor vendors have adopted a shared L2 cache design [14][22]. A shared cache is preferred as a L2 cache design because the shared L2 design maintains a single copy of shared data in the entire shared cache pool, resulting in a larger effective cache size. The shared L2 design can also
* Part of this work was done when he was an intern at Intel. † This work is supported partially by the National Natural Science Foundation of China under grants No. 60773149, 60573100, the National High Technology Research and Development Program of China under grant No.2008AA01Z108, and the National Basic Research Program of China under grant No. 2007CB310900.
tolerate imbalances across the working sets of different applications running in each processor core. There are many prior studies [24][25][13][9][6][20] that investigate the optimal shared cache partitioning for multiprogrammed workloads. However, little work has been done to examine the effect of data sharing on shared caches for multi-threaded workloads. We find multithreading causes cache behavior non-deterministic and thus makes finding the optimal cache management policy very challenging.
We first characterize the memory access behavior of single multi-threaded workloads and show that private data and shared data often have different locality and working-set sizes. However, traditional LRU policy is not aware of such characteristics, and it allocates cache resources based on the demand for each type of data. Besides, the emerging workloads often stream through large amount of data, and the cache thrashing will occur when the working set does not fit into the on-die last-level cache (LLC). We can manage the shared LLC more efficiently by having different allocation policies for private data and shared data and handling streaming and non-streaming data separately. Thus, we propose sharing-aware replacement and streaming-aware insertion policy for efficient shared cache management in the context of multi-threaded execution.
We also show that multi-threading can affect the cache behavior significantly due to the cache sharing and contention. To our knowledge, this is the first study to manage a shared cache efficiently by partitioning between private data and shared data in multi-threaded workloads. We find that changing replacement policy is better to keep the partition between shared data and private data close to the optimal partition while changing an insertion policy can be more effective in preventing streaming cache pollution.
To identify the shared data, we can use the underlying coherence mechanism in a shared cache. However, shared data classification is challenging because: (1)
when data gets evicted, we lose the sharing information and once the data is brought back, it takes time to reclassify the data as shared; (2) when the working set size of shared data exceed the cache space we may not have a chance to classify the data as shared since the data gets evicted before the data is touched by other core. Experiments and previous study [11] shows that the data sharing detection is highly dependent on the underlying cache size. Therefore we need a large cache to capture the sharing behavior. Similarly, when the streaming accesses thrash the cache, the low-level cache can be leveraged to store this thrashing pattern.
To capture the sharing and the streaming behavior
more precisely, we use the stacked DRAM-based cache. As more processor cores are integrated, the emerging stacked DRAM technology is explored to sustain highbandwidth requirement with low energy consumption. A stacked DRAM-based cache can be used to classify the shared data more precisely by capturing the larger working set and also keeping the history of sharing information detected by the upper-level cache. We show that a stacked DRAM-based cache can also be
used to classify the streaming data and thus preventing the streaming cache pollution. Unlike prior studies, our streaming prevention mechanism does not rely on any probability based policy that may require tuning for different applications.
To summarize, the main contributions of the paper
are:
• We characterize the memory access pattern and
find that the private data and the shared data have different working set sizes and different memory access behaviors.
• We show that multi-threading can affect the cache
behavior significantly and past sharing-oblivious cache management policies result in sub-optimal performance.
• We leverage the emerging stacked DRAM tech-
nology to classify the shared data and the streaming data more precisely.
• We propose the sharing-aware replacement and
the streaming-aware insertion policy for efficient shared cache management.

2. Motivation
In this section, we show that the shared cache blocks in multi-threaded applications can affect the cache behavior significantly, thus rendering the new opportunity to improve cache performance over prior shareoblivious cache management policies.

2.1 Characterization of Shared- and PrivateWorking Sets
Multi-threaded applications exhibit either or both of the following two access patterns: (1) private, only one thread accesses the particular cache block during its lifetime in cache, and (2) shared, more than one threads access the cache block. For the shared cache blocks, there are two possible access patterns. One pattern is that multiple threads contend the same block for a relatively short period of time. The other pattern is that only one thread touches the block for a given window (number of L2 accesses), but accesses from different threads are spread over time. When the shared block is contended, it shows high locality compared to the private block. Beckmann et al. show that the top 3% of shared blocks account for 70% of requests in commercial workloads [1].
Figure 1 illustrates the number of cache misses for four applications with each to represent the following four access patterns. To measure the working set of private blocks and shared blocks separately, we assign separate hit counters for private blocks and shared blocks and measure their utility [20] (i.e., the number of decreased misses when the number of ways is added). Blackscholes and ode have mostly either private block accesses or shared block accesses.
• blackscholes: most of cache accesses come from
the private blocks. When application developers leverage the data-level parallelism with multi-
threading, it is natural to partition the data structure and assign each thread to work on its partition. Partitioned data structures are generally private data since each thread primarily accesses elements from its partition.
• ode: most of cache accesses come from the shared
blocks. When the partitioned per-thread data structure is small enough to be filtered out by L1 cache, accesses to private block does not dominate the overall L2 cache accesses. Instead, to perform each data-parallel operation, a thread makes many accesses to a globally shared large data structure.
Most multi-threaded applications have mixture of both shared blocks and private blocks. For example, in matrix-vector multiplication, matrix elements are partitioned private to each thread while vector elements are shared by all threads. In some applications, a neighboring thread accesses elements just across the boundary of per-thread partitioned elements. This can lead to either limited read-sharing between two threads, or if the elements in a partition are frequently updated, to producer-consumer communication.
• pcg: mixture of the shared and private block. The
private blocks have the streaming access pattern. The shared blocks have the LRU-friendly access pattern. Since we target 8MB and 16MB for L2 capacity, the working set size of private blocks are too big to be fit in the cache. To maximize cache performance, we should allocate the cache space to the shared blocks until capturing the whole shared working set and use the rest of cache space to pin down the private blocks until the space is allowed.
• sph: mixture of the shared and private block. Both
private blocks and the shared blocks have the LRU-friendly access pattern. We should allocate the cache space to both private blocks and shared
blocks based on locality of each type of data.

2.2 The Effect of Multi-threading on SharingOblivious Cache Management Schemes
The performance of shared cache management schemes can be affected by sharing behaviors of multithreaded applications. We compare the performance of a recently proposed shared cache management policy - TADIP [10] (Thread-Aware Dynamic Insertion Policy) between single- and multi- threaded runs on our multithreaded workloads, as shown in Figure 2. Compared to single-threaded runs, TADIP gets limited benefit from multi-threaded runs. This is possibly because TADIP only takes the contention between threads into account, and ignores the constructive data sharing among threads.

3. Experimental Methodology
We evaluate cache management policies using CMP$im [11], a binary instrumentation tool built on Pin [18]. This tool captures the loads and stores from programs running on real machines and passes them to an on-line cache hierarchy simulator. We run our benchmarks on a four-socket Intel Core 2 Quad system (that is, 16 cores in total) so that we can study our proposal from 1 to 16 threads.
We simulate a three-level data cache hierarchy to model a chip with two levels of SRAM caches, and one level of stacked DRAM cache [4]. We primarily study the behavior of the last level of SRAM cache (the L2 cache), but leverage the DRAM cache (the L3) to improve the management of the L2.
Each core has a 32KB, 8-way set associative private L1 cache which uses an LRU replacement policy. All caches use 64B lines. The cores share a 256MB L3 cache, which is 16-way set associative. The cache hie-
rarchy is non-inclusive between all levels.
We model 8MB and 16MB shared L2 caches with 16-way set associativity. LRU is used as the baseline cache management policy for the L2 cache. For comparison, TADIP [10] is also evaluated in this work. In TADIP, each thread employs several sampling sets to determine whether or not it should use an optimized insertion policy for its memory accesses. As a result, some cache thrashing threads can yield some of their cache occupation to other cache demanding threads. For TADIP, we have 32 sets per-thread for sampling and use 10 bits for the counter.
We use PARSEC benchmarks [3] and RMS benchmarks [7][21] developed at Intel for our evaluation. They are listed in Table 1. These benchmarks are representative for emerging multi-threaded applications running on multi-core processors. Recently, there have been many other research works that use these benchmarks [2][8][15]. Here we only select benchmarks with high L2 miss rates, because these benchmarks are more sensitive to the efficiency of cache management. Table 1 also gives the average numbers of L2 misses per thousand instructions (MPKI) for our single-threaded benchmarks on 8MB with the LRU replacement policy. Note that in our experiment, we warm up the cache and discard the information before entering the region of interest.

4. Sharing-Aware and Streaming-Aware Cache Management
In Section 2, we showed that previous cache management policies are suboptimal for a multi-core processor with a shared cache. Those policies treat shared and private data the same, whereas we showed that for some applications, shared and private data behave quite differently. Therefore, by taking into account whether data is private or shared, we can significantly reduce
the cache miss rate.
Pollution from streaming data is also a well-known problem from a cache management point of view. Streaming data is data that has little to no temporal locality; in other words, it sees little to no reuse in the cache before it is evicted for capacity or conflict reasons. Recent cache management policies have attempted to limit the amount of cache space that streaming data occupies because caching streaming data is counterproductive since it occupies space that could be used to cache data with temporal locality [19][10]. In addition to treating shared and private data differently, a cache management scheme can derive benefits from treating streaming and non-streaming data differently.
We therefore propose a cache management scheme that (1) logically partitions the cache between shared and private data via a modified replacement policy, and (2) limits cache pollution from streaming data via a new insertion policy. We will next discuss each of these two policies, and then present our scheme to control these policies to adapt to both phase changes within an application and differing behavior across workloads.

4.1 Shared-Private Partitioning
Our novel replacement policy logically partitions each set into shared and private ways. Assume that the associativity of the shared cache is A, and all cache lines are classified into shared and private. Given a quota Q for shared cache lines, the cache controller is partitioning each cache set such that shared cache lines take Q ways in each set while private lines take A-Q. This partition strategy is done on victim selection time of the replacement policy described in Algorithm 1.
Algorithm 1. Sharing-Aware Partitioning. Input: associativity A, quota for shared cache lines Q (which is determined in dynamic set sampling phase discussed in Section 4.3), sharing attribute of the incoming line T (T=0: private, T=1: shared), total number of shared cache lines S in the cache set, total number of private cache lines P in the cache set.
Output: selected victim cache line.
1. Upon a cache miss, obtain the information A, Q, T,
S and P;
2. Compute shared requirement N T+S, and private
requirement M 1-T+P;
3. If N+M≤A (means there are invalid blocks in the
set), select the LRU block as the victim;
4. Otherwise, if N>Q, select the shared block that is
closet to the LRU position;
5. Otherwise (implies M>A-Q), select the private
block that is closet to the LRU position.

4.2 Handling Streaming Data
Assuming that all incoming cache lines have been classified into streaming and non-streaming, we now describe our streaming data management policy. Our scheme handles streaming data at cache line insertion time, right after the victim selection. If the incoming line is classified as streaming, and streaming-aware insertion is enabled (by dynamic set sampling logic), then the incoming line is remained at its position in the LRU stack, otherwise it is promoted to the MRU position (as a normal LRU policy does).

4.3 Dynamic Set Sampling
In order to get the optimal partition quota for sharing-aware replacement, and whether or not to enable the streaming-aware insertion policy, we employ an incache dynamic set sampling scheme that is similar to set-dueling [19], as illustrated in Figure 3. The cache now consists of two parts: the sampling sets and the follower sets. Sampling sets are divided into several groups, with each group using a specific policy. The best policy is selected among the sampling groups by a performance monitoring logic, and then the selected policy is applied to all the follower sets. In our scheme, each group of sampling sets is dedicated to a static shared quota, and assigned statically the streamingaware insertion policy or the default insertion policy. For example, the group 0 in Figure 3 is assigned with a shared quota of 2 (q=2), and disabled streaming insertion (s=0), while group 1 is assigned with a shared quota of 2 (q=2), and enabled streaming insertion (s=1). Upon a cache miss to a sampling group, the miss counter of that group is incremented. When one of the group
counters saturates, the policy of the group with the lowest miss number is selected to update the global cache management policy. In our cache management scheme, we use 32 sets for each sampling group, and 10 bits for each miss counter in a sampling group.
Our sampling scheme incorporates an oscillation prevention optimization in order to prevent the partition decision from swinging frequently. We keep a register in the cache that holds the search window size (SWS). When we make a partitioning decision, we are only allowed to look at partitions that fall within the search window. In other words, if the last decision we made was to give K ways to shared data, we will only consider giving the following # of ways to shared data: K-SWS, …, K-2, K-1, K, K+1, K+2, …, K+SWS. When we make a partitioning decision we also update the SWS. If the partition with the globally lowest miss rate is within the search window (i.e., if we pick the global optimum partitioning for the next time step), we set the SWS to 1. Otherwise, we increment the SWS.

4.4 Performance Evaluation
We evaluate three different sampling based management schemes in this paper. The first is sampling based shared-private partitioning, named Sharing-
Aware Replacement (SA), which has five sampling groups for shared quotas 2, 6, 10, 14, and LRU policy on our baseline 16-way set-associative shared cache. The second is sampling based streaming data handling, named Streaming-Aware Insertion (SI), which has two sampling groups for enabling and disabling streaming data handling respectively. The last scheme is a combination of the two, which is simply named SA+SI. Note that in SA+SI scheme, the number of sampling groups is a product of SA and SI schemes, i.e. 10 sampling groups in total.
Figure 4 compares the L2 miss rate between TADIP and our schemes. We give results of sharing-aware replacement (SA) and streaming-aware insertion (SI) separately to help us understand where the benefits come from.
The SA+SI scheme performs very well, and it reduces the miss rate by an average of 8.7% on 8MB caches and 20.1% on 16MB caches of LRU policy. Also it outperforms the TADIP policy significantly. On 16MB caches, our policy incurs 13.1% less misses on average and up to 22.5% less misses than the TADIP.
Our scheme manages the shared cache space much more efficiently by installing data with better data locality, and thus provides higher cache performance. Figure 5 shows the fraction of the cache resource occupied by shared data under three cache policies on
sparse_mvm and pcg applications. The figure shows that our policy gives more cache blocks to shared data, which has better locality than private data. In contrast, both LRU and TADIP policies dedicate more resources to the high demanded private data with streaming behavior, and thus suffer from high cache misses. The SA+SI scheme increases the shared cache capacity from 20% to 70% and 30% to 50%, which turns into 14% and 19% cache miss reductions on 16MB cache for the two applications.
The benefits of SA+SI come from both SharingAware Replacement and Streaming-Aware Insertion. For some applications (e.g. sph), SA scheme works better than SI, while on some other applications (e.g. ode), SI works better. The SA+SI scheme is able to pick up the best policy between SA and SI via dynamic sampling, and therefore adapts to the application behavior effectively. On some applications (e.g. face), the SA+SI scheme is slightly worse than the better one between SA and SI. This is because the hybrid scheme needs more sampling sets (10 groups) than each single strategy, and the management policy on half sampling sets is not optimal. However, the sampling overhead (3.9% and 2.0% of total cache sets on 8 and 16 MB caches) is marginal and endurable. For face application on 16MB cache, the miss rate for SA+SI scheme is 6.6% and 1.4% higher than SA and SI, respectively.
The dynamic sampling mechanism can adapt to the characteristics of different applications and the phasechanging behaviors. Figure 6 shows the management policy for sph and face applications. We can see (1) sph and face favor different insertion and replacement polices; (2) sph disables the streaming insertion policy due to the cache friendly behavior; (3) face enables the streaming prevention in the phases with streaming access pattern; (4) the shared quota is different for
these two workloads with distinct sharing pattern; (5) the management policy changes dynamically.

4.5 Scalability
Figure 7 shows L2 cache miss rate along the thread count. Our SA+SI scheme shows better cache performance with increasing thread number, while TADIP shows more cache misses. For example on a 16MB cache, it is able to reduce cache misses of the LRU policy on average by more than 20% with 1, 2, 4, 8, and 16 threads. We believe the SA+SI scheme is also scalable beyond 16 threads.

5. Detecting Memory Access Behavior
In Section 4, we described our cache management scheme, which relies on receiving classification of data on a cache miss. In particular, the scheme needs to know if an incoming cache line is shared or private, and if it is streaming or non-streaming. We now describe our classification mechanisms, which rely on another level of cache between the cache we are managing and main memory, such as a stacked DRAM cache.

5.1 Stacked DRAM Cache
Die stacking has recently emerged as a promising technology to effectively increase the die area of processors at a much lower cost than physically manufacturing a larger die [4]. The stacked dies can theoretically hold a mixture of additional processing cores, SRAM cache, and DRAM (used as cache or main memory). We believe that the memory bandwidth needs of future multi-core and many-core processors make it likely that at least some of the stacked die
space will be devoted to DRAM. We believe that, at least for the short term, stacked DRAM will be used as cache since effectively using it as part of main memory would require changes to the operating system.
We leverage the presence of a lower level cache to help our scheme classify data as shared or private, and as streaming or non-streaming. We also add two bits to each DRAM cache line to store these classifications: one bit indicates if data is shared or not, and the other bit indicates if data is streaming or not. When the lastlevel SRAM cache (the one we are concerned with in this paper) incurs a miss, the DRAM cache will return line, along with the two classification bits. In the next sections, we describe how we set these bits.

5.2 Detecting Streaming Data
Recall that streaming data is data that is not re-used within a cache before it is evicted for capacity or conflict reasons. Therefore, we directly detect streaming data by detecting reuse. We add a bit to each cache line (of the L2) to indicate if the line has been reused since it was brought into the cache. When a line is brought into the L2, this bit is cleared. When a line is accessed in the L2, if it is a hit, the reuse bit is set. When a line is evicted from the cache, we send the bit to the lower level cache, which stores its inverse—if the line was not reused in the L2, then the line is streaming with respect to the L2. If we could not store the streaming
information in the next level of cache, we would require a predictive mechanism, such as TADIP uses.

5.3 Detecting Shared Data
We detect shared data in both the cache we are managing (i.e., the L2) and the lower-level DRAM cache. To detect sharing in the L2, we add a bit to each L2 line to indicate if the line is shared. When a line is brought into the L2, its shared bit is copied from the lower level cache—if it was previously detected as streaming, it retains that classification. When a line is accessed in the L2, if the line was previously accessed by a different core (detected via the directory information), the sharing bit is set. When a line is evicted from the L2, we send the sharing bit to the lower level cache, which stores it.
To detect sharing in the lower level (i.e., L3) cache,
we add additional bits to each line to hold a core id (i.e., log2# of cores bits). On an L2 miss, the L2 sends the core ID that triggered the miss to the L3, which stores it. On future L2 misses, if the core id that triggered the miss is different from the stored core ID, we set the sharing bit in the L3.
We now evaluate the effectiveness of the different hardware components of our sharing detection mechanism. We compare three different options for detecting sharing: MD (Middle-Level Detection), DH (DRAM History), and DD (DRAM Detection). (1) In MD, we
store a sharing bit in the L2 only—this option treats all data as private on an L2 miss, and only detects shared data in the L2. (2) In DH, we store a sharing bit in both the L2 and L3—this option only detects data in the L2, but stores that bit in the L3, so on the next L2 miss, it remembers whether data was previously shared. (3) In DD, store a sharing bit in both the L2 and L3, and store a core id in the L3—this is the mechanism that detects data in both the L2 and L3.
Figure 8 shows the results of sharing-aware replacement under different sharing detection schemes. In this experiment, we use no streaming enhancement policy to isolate the effect of streaming-aware insertion from various sharing detection schemes. On average, the DD scheme improves the cache misses by 6% and 7% with respect to other two schemes on 16MB caches. The effects of different sharing detection options rely on how the shared data is reused in each program. Firstly, in some applications, shared data is reused frequently after brought into the cache due to constructing sharing pattern of the application. For example, in dense_mvm, though the size of the shared vector is larger than the cache size, most of its elements are reused by other threads very soon and frequently after filled into the cache. For applications of this kind, the MD scheme is good enough to capture most of the sharing behaviors. Secondly, however, in some other applications, e.g., pcg, the shared data is accessed in a less cache friendly way, and can not be fit into the cache. In this case, lots of shared data is not reused after brought into the cache. Instead, it has been evicted in the lower-level DRAM cache when required by other threads. For applications of this kind, a DD scheme is need to provide better performance. Finally, the face application on an 8MB cache, which exposes mainly streaming accesses to both private and shared data, is a special case. The DD scheme performs worse than MD and DH in this case. In the MD and DH schemes, the shared data footprint detected is smaller than that in
DD. However, this smaller detected shared data footprint happens to be able to fit in the 8MB cache by SA policy, while the larger detected shared footprint could not fit in without the help of SI policy. Thus we observe that SA-DD scheme performs no better than LRU. In fact, the correct way of handling this application is to enable the SI policy since both private and shared data are streaming.
The overhead of both streaming detection and sharing detection is very small. For our baseline 8 and 16 MB L2 cache, and 256MB DRAM cache, the additional storage for streaming detection is 0.19% for the L2 cache, and 0.19% for DRAM cache. The additional storage for sharing detection using DD scheme is 0.19% for L2 cache, and 0.77% for DRAM cache, assuming totally 16 cores. Thus the total storage overhead is less than 1%.

6. Related Work
Recent studies [11][16] show that emerging workloads have a large amount of data sharing and a large working set size and thus preferring the shared cache as a last-level cache design.
However, most of the recent shared cached partitioning proposals focus primarily on multi-programmed workloads. Stone et al. [24] study the optimal cache allocation between instruction and data streams. Suh et al. [25] investigate dynamic partitioning of shared cache for simultaneous executing different processes. They collect the marginal gain counters dynamically and use them to guide the OS schedule to reduce the overall miss rate. While [24] and [25] focus on reducing the overall miss rate, Kim et al. [13] and Iyer et al. [9] pursue cache fairness and quality of service as the main goal. Cooperative cache partitioning [6] further improves cache fairness by introducing multiple timesharing partitions and thus addressing the difficulty of satisfying multiple goals in a single spatial partition.
Utility-based cache partitioning [20] uses dynamic set sampling (DSS) to find the optimal partition of a shared cache among multiple applications running simultaneously.
While the above cache partitioning schemes change LRU replacement policy to achieve the optimal cache allocation for different applications, TADIP [10] modifies the cache insertion policy to achieve the same goal. TADIP directly extends DIP [19] to improve the overall throughput of multiple-program execution. DIP changes the cache insertion policy to handle the case when the working-set size of an application exceeds the cache capacity and LRU policy thrashes the cache. In such case, DIP allows a part of working set kept in the cache and prevents thrashing while the baseline LRU policy can be also used for a cache-fitting application.
The main difference of our approach is that we target the cache behavior of multi-threaded workloads and exploit the data sharing information to reduce the overall miss ratio. Compared to prior sharing-oblivious partitioning schemes, our approach combines management of both insertion policy and replacement policy to prevent cache thrashing and achieve better partitioning between shared data and private data.
Other researchers have examined using the data sharing information for cache management. Victim replication [27], Cooperative caching [5], and ASR [1] suggest modifying the replacement policy either to prefer the cache line with sharers or to keep the unique copy of data in the chip. The AMD Barcelona processor uses a sharing-aware replacement policy to manage the exclusive shared L3 cache [12]. Srikantaiah et al. [23] propose adaptive set pinning scheme. In their paper, they argue that the most of inter-processor misses are caused by a few hot blocks. To keep these hot blocks, they introduce the specialized storage, called pop cache and thus disallow the hot blocks to get evicted by regular L2 cache blocks.
Finally, researchers have started exploring a DRAM-stacking as an emerging technology to provide low-latency and high-bandwidth for many-core processors [4][26][17]. Black et al. [4] study the performance benefit and thermal impact of 3D DRAM stacking and show the feasibility of the technologies. Thoziyoor et al. [26] investigates the performance, bandwidth and energy trade-off of various stacked DRAM-based cache designs for many-core processors.

7. Conclusions
Motivated by the finding that the shared and private data have different data locality in multi-threaded workloads, this paper studies a Sharing-Aware Re-
placement (SA) and Streaming-Aware Insertion (SI) policy for managing a shared cache running multithreaded applications on multi-core chips. By using the low-overhead sampling mechanism, the SA and SI schemes can dynamically partition the cache between shared and private data and prevent streaming data from polluting the cache. Both SA and SI schemes leverage the emerging DRAM cache technique to gather information on streaming and sharing, and use dynamic set sampling technique to adapt to program characteristics and phase change. On the emerging multi-core workloads with 16 threads running, the SA+SI scheme is able to reduce the miss rate by 8.7% on 8MB caches and 20.1% on 16MB caches with LRU policy. Also our scheme outperforms other sharing-oblivious policy for shared cache, such as TADIP.

8. Acknowledgement
We acknowledge Christopher J. Hughes and YenKuang Chen for continued discussion and feedback throughout this work. Our thanks also go to Eric Li, Hao Feng, and Junmin Lin for their support with the benchmarks used in this work. We also thank Aamer Jaleel for sharing the cache simulator.

9. References
[1] B. M. Beckmann, M. R. Marty, and D. A. Wood, “ASR: Adaptive Selective Replication for CMP Caches”, Proc. of the 39th Annual Intl. Symp. on Microarchitecture, 2006, pp. 443-454. 
[2] C. Bienia, S. Kumar and K. Li, “PARSEC vs. SPLASH-2: A Quantitative Comparison of Two Multithreaded Benchmark Suites on Chip-Multiprocessors”, Proc. of the 2008 Intl. Symp. on Workload Characterization, 2008, pp. 47-56 
[3] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC Benchmark Suite: Characterization and Architectural Implications”, Tech. Report TR-811-08, Princeton University, Jan 2008. 
[4] B. Black, M. Annavaram, N. Brekelbaum, J. DeVale, L. Jiang, G. H. Loh, D. McCaule, P. Morrow, D. W. Nelson, D. Pantuso, P. Reed, J. Rupley, S. Shankar, J. P. Shen, and C. Webb, “Die Stacking (3D) Microarchitecture”, Proc. of the 39th Annual Intl. Symp. on Microarchitecture, 2006, pp. 469-479. 
[5] J. Chang and G. Sohi, “Cooperative Caching for ChipMultiprocessors”, Proc. of the 33rd Annual Intl. Symp. on Computer Architecture, 2006, pp. 264-276. 
[6] J. Chang and G. S. Sohi, “Cooperative Cache Partitioning for Chip Multiprocessor”, Proc. of the 21st Annual Intl. Conf. on Supercomputing, 2004, pp. 242-252. 
[7] P. Dubey, “Recognition, Mining and Synthesis Moves Computers to the Era of Tera”, Technology@IntelMagazine, Feb 2005.
[8] C. Hughes, R. Grzeszczuk, E. Sifakis, D. Kim, S. Kumar, A. Selle, J. Chhugani, M. Holliman and Y. Chen, “Physical Simulation for Animation and Visual Effects: Parallelization and Characterization for Chip Multiprocessors”, Proc. of the 34th Annual Intl. Conf. on Computer architecture, 2007, pp. 220-231 
[9] R. R. Iyer, “CQoS: A Framework for Enabling QoS in Shared Caches of CMP Platforms”, Proc. of the 18th Annual Intl. Conf. on Supercomputing, 2004, pp.257-266. 
[10] A. Jaleel, W. Hasenplaugh, M. Qureshi, J. Sebot, S. C. S. Jr, and J. Emer, “Adaptive Insertion Policies for Managing Shared Caches on CMPs”, Proc. of the 17th Intl. Conf. on Parallel Architectures and Compilation Techniques, 2008, pp. 208-219. 
[11] A. Jaleel, M. Mattina, and B. Jacob, “Last Level Cache (LLC) Performance of Data Mining Workloads on A CMP – A Case Study of Parallel Bioinformatics Workloads”, Proc. of the 12th Annual Intl. Symp. on High Perf. Computer Architecture, 2006, pp. 88-98. 
[12] D. Kanter, “Inside Barcelona: AMD's Next Generation”, available at http://www.realworldtech.com/page.cfm?ArticleID=RWT051 607033728&p=7 
[13] S. Kim, D. Chandra, and Y. Solihin, “Fair Cache Sharing and Partitioning in a Chip Multiprocessor Architecture”, Proc. of the 13th Intl. Conf. on Parallel Architectures and Compilation Techniques, 2004, pp. 111-122. 
[14] P. Kongetira, K. Aingaran, and K. Olukotun, “Niagara: A 32-way Multi-Threaded Sparc Processor”, IEEE Micro, 25(2), 2005, pp. 21-29. 
[15] S. Kumar, C. Hughes and A. Nguyen, “Carbon: Architectural Support for Fine-Grained Parallelism on Chip Multiprocessors”, Proc. of the 34th Annual Intl. Conf. on Computer architecture, 2007, pp. 162-173 
[16] W. Li, E. Li, A. Jaleel, J. Shan, Y. Chen, Q. Wang, R. R. Iyer, R. Illikkal, Y. Zhang, D. Liu, M. Liao, W. Wei, and J. Du, “Understanding the Memory Performance of DataMining Workloads on Small, Medium, and Large-Scale CMPs Using Hardware-Software Co-simulation”, Proc. of the IEEE Intl. Symp. on Performance Analysis of Systems and Software, 2007, pp. 35-43. 
[17] G. H. Loh. “3D-Stacked Memory Architectures for Multi-Core Processors”, Proc. of the 35th Annual Intl. Symp. On Computer Architecture, 2008, pp. 453-464.
[18] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wallace, V. J. Reddi, and K. Hazelwood, “Pin: building customized program analysis tools with dynamic instrumentation”, Proc. of ACM SIGPLAN Conf. on Programming Language Design and Implementation, 2005, pp. 190-200. 
[19] M. K. Qureshi, A. Jaleel, Y. N. Patt, S. C. S. Jr., and J. S. Emer, “Adaptive insertion policies for high performance caching”, Proc. of the 34th Annual Intl. Symp. on Computer Architecture, 2007, pp. 381-391. 
[20] M. K. Qureshi and Y. N. Patt, “Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches”, Proc. of the 39th Annual Intl. Symp. on Microarchitecture, 2006, pp. 423-432. 
[21] J. Rattner, “Cool Codes for Hot Chips: A Quantitative Basis for Multi-Core Design”, Hot Chips keynote presentation, 2006. 
[22] B. Sinharoy, R. N. Kalla, J. M. Tendler, R. J. Eickemeyer, and J. B. Joyner, “Power5 System Microarchitecture”, IBM Journal of Research and Development, 49(4-5), 2005, pp. 505–522. 
[23] S. Srikantaiah, M. T. Kandemir, and M. J. Irwin, “Adaptive Set Pinning: Managing Shared Caches in Chip Multiprocessors”, Proc. of the 13th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, 2008, pp. 135-144. 
[24] H. S. Stone, J. Turek, and J. L.Wolf, “Optimal Partitioning of Cache Memory”, IEEE Trans. Computers, 41(9), 1992, pp. 1054–1068. 
[25] G. E. Suh, S. Devadas, and L. Rudolp, “A New Memory Monitoring Scheme for Memory-Aware Scheduling and Partitioning”, Proc. of the 8th Annual Intl. Symp. on High Perf. Computer Architecture, 2002, pp. 117-128. 
[26] S. Thoziyoor, J. H. Ahn, A. Monchiero, J. B. Brockman, and N. P. Jouppi, “A Comprehensive Memory Modeling Tool and its Application to the Design and Analysis of Future Memory Hierarchies”, Proc. of the 35th Annual Intl. Symp. On Computer Architecture, 2008, pp. 51-62. 
[27] M. Zhang and K. Asanovic, “Victim Replication: Maximizing Capacity while Hiding Wire Delay in Tiled CMPs”, Proc. of the 32nd Annual Intl. Symp. on Computer Architecture, 2005, pp. 226-345.
