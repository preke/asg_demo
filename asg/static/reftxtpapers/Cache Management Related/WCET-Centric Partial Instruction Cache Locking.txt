WCET-Centric Partial Instruction Cache Locking
Huping Ding,Yun Liang,Tulika Mitra
d-huping@comp.nus.edu.sg,,tulika@comp.nus.edu.sg,,eric.liang@adsc.com.sg

Categories and Subject Descriptors C.3 [Special-purpose and Application-based Systems]: [Realtime and embedded systems]
General Terms Algorithm, Design, Performance
Keywords WCET, Partial Cache Locking

1. INTRODUCTION
Cache memories are often employed in embedded systems to hide the main memory access latency. Caches are quite effective in
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. DAC 2012, June 3-7, 2012, San Francisco, California, USA. Copyright 2012 ACM 978-1-4503-1199-1/12/06 ...$10.00.
improving the average-case performance due to the temporal and spatial locality of memory accesses in a program. For hard realtime systems, however, caches are problematic due to timing unpredictability — specially in the context of Worst-case Execution Time (WCET) estimation. WCET is an important metric for hard real-time systems. It is defined as the upper bound on the maximum execution time of the program on a particular hardware platform across all the possible inputs. In the presence of caches, it is challenging to determine the cache behavior for a memory access (hit or miss) for WCET estimation through static program analysis. If a memory access cannot be guaranteed as a cache hit, it is conservatively estimated to be a miss in WCET analysis. This leads to significant imprecision in WCET estimation.
In this paper, we focus on instruction caches, which are present in almost all embedded systems today. There exist many static program analysis techniques that model the instruction cache for tight WCET estimation [19, 10]. For example, Theiling et al. [19] model the cache states at each program point and classify the cache behavior (hit or miss) of a memory access based on the cache state. However, in the presence of complex control flow, cache modeling may fail to accurately determine the cache behavior for some memory accesses. Such unclassified accesses are conservatively assumed to be cache misses in WCET analysis due to the safety critical nature of hard real-time systems. This over-estimation can lead to serious over-dimensioning of the processor resources.
To improve timing predictability, modern embedded processors often feature cache locking mechanisms. Memory blocks can be locked in the cache using special lock instructions. Once a memory block is locked, it cannot be evicted from the cache under replacement policy. Thus, locking the entire cache resolves the problem of timing unpredictability. More importantly, by carefully choosing the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling techniques without locking [6, 14].
Embedded processors also provide the option of partial cache locking through two different mechanisms: way locking and line locking. In way locking, particular ways are entirely locked for all the cache sets. Way-locking is employed in several ARM processor series. Line locking allows different number of lines to be locked in different cache sets and is employed in Intel Xscale, ARM9 family and Blackfin 5xx family. In partial cache locking, the unlocked portion of the cache behaves as a normal cache. For example, if in a 4-way set associative cache, two cache ways are locked, then the other two cache ways serve as a 2-way set associative cache. Clearly, line locking is more flexible than way locking, which in turn is more flexible than full cache locking.
Recently, a heuristic [6] and an optimal solution [14] have been proposed to minimize the WCET via static instruction cache locking. These existing techniques make an implicit but important de-
cision of locking the entire cache. This crucial decision arises from the assumption that instruction cache modeling for WCET analysis is quite imprecise. By employing full cache locking, [6, 14] can completely bypass cache modeling in WCET analysis phase and thereby achieve tight WCET estimation. Indeed, as these techniques are oblivious to cache modeling, they assume the worst-case behavior with empty cache (where all the accesses are serviced from main memory) as the reference point and improve upon it through locking of memory blocks along the WCET path. In this context, it is guaranteed that locking the entire cache will provide maximumWCET reduction compared to the baseline empty cache. In other words, the cache locking problem becomes equivalent to the scratchpad memory allocation problem [5, 18].
In this paper, we argue (and experimentally validate) that aggressive full cache locking as proposed in [6, 14] may have substantial negative impact on WCET reduction. State-of-the-art instruction cache modeling techniques for WCET analysis are quite mature. Most memory accesses can thus be successfully classified as hit/miss through WCET analysis techniques. Consider a memory blockm originally classified as cache hit in a normal cache through static WCET analysis. But m is not selected for locking under full cache locking scenario. Thus m does not have any opportunity to reside in the cache and all its accesses incur cache misses. Now consider an alternative scenario where partial cache locking is employed. Againm is not selected for locking. However, as the cache has some unlocked lines, m may still be brought into the cache at runtime and the cache misses can be avoided.
In summary, full cache locking does not exploit the entire spectrum of opportunities presented by cache locking. Thus, in this paper we propose a partial cache locking technique that explores in conjunction with accurate cache modeling the entire spectrum of choices. Partial cache locking problem is more challenging compared to full cache locking as it requires careful cost-benefit analysis to decide between locking a cache line with a single memory block versus keeping it unlocked so that more than one memory blocks can benefit from it. This synergistic interaction between cache modeling and memory block selection for locking sets apart our technique from the state-of-the-art.
Motivating Example. We illustrate the benefit of partial cache locking over full cache locking with a concrete example shown in Figure 1. The program consists of four loops and we assume that all the memory blocks are mapped to the same cache set in a 2-way set associative cache.
Cache modeling with no locking: Let us first estimate the WCET via cache modeling with no locking. Theiling et al. [19] models the cache states at all program points. All the memory blocks in the first loop (m0, m1, m2) are cache misses in the worst case because alternate execution of the two program paths (P0 and P1) can lead to mutual eviction of the blocks. Thus, program path P1 with 2 cache miss is the worst case path in the first loop. For the other three loops, cache modeling techniques can easily determine that the first access is a cold miss and the subsequent accesses are cache hits via persistence analysis or virtual unrolling [15, 19]. Therefore, cache modeling estimates 23 cache misses in the worst case — 20 misses for the first loop and 3 misses for the other loops.
Full cache locking: Existing cache locking techniques [6, 14] first build the worst case path (e.g., (m1m2)10m3100m490m580) assuming that all accesses are serviced from the main memory (i.e., there is no cache). Now memory blocks are selected for locking along the worst-case path so as to improve the WCET until the cache is fully locked. Both cache locking techniques [6, 14] model the fact that the WCET path may change after locking some memory locks. For this example, the heuristic [6] and the optimal [14] approach return the same solution. m3 and m4 are chosen to be locked as they contribute most towards WCET reduction. After locking, we get 100 cache misses in total in the worst case — 20 misses in the first loop and 80 misses in the last loop. Thus, cache locking performs worse than cache modeling in this example.
Partial cache locking: Our partial locking technique can determine that it is beneficial to keep one cache line free so that accesses to m3, m4, and m5 can be cache hits after the first cold miss. It only chooses to lockm1 orm2 in the cache. Thus we get 13 cache misses in the worst case — 10 misses in the first loop and 3 cold misses for the other loops. Thus partial cache locking improves upon cache modeling and full locking.
From the example above, we first observe that full locking techniques [6, 14] are not guaranteed to perform better than cache modeling (with no locking) specially when some memory accesses can be easily classified as cache hits (m3, m4, m5 in our example). Locking these memory blocks with deterministic access pattern does not yield any benefit. On the other hand, if the cache is fully locked and these memory blocks with deterministic access pattern are not chosen for locking, it can have serious impact on theWCET.
Our partial locking mechanism integrates cache locking with cache modeling. We model the cache content at all program points and select the memory blocks for locking based on the cache state and their impact on the WCET. In particular, we use the concrete cache states or the abstract cache states to model the cache content. Concrete cache state captures the exact path behavior while abstract cache state is a compact representation that merges multiple concrete cache states together. For concrete cache state, we use integer linear programming (ILP) approach to optimally select the memory blocks for locking. As no cache locking and full cache locking are just two extreme instances of partial cache locking, partial locking is guaranteed to be equivalent to or better than them. To improve the efficiency, we also propose a heuristic partial locking strategy based on abstract cache states. Experimental results show that our partial cache locking techniques improve WCET substantially for a large number of embedded benchmark applications.

2. RELATED WORK
Cache locking is used to improve timing predictability in realtime systems. Puaut and Decotigny [16] explore static cache locking in multitasking real-time systems. Two content selection algorithms have been proposed in their work to minimize the utilization and inter-task interferences. Campoy et al. [4] employ genetic al-
gorithm to perform instruction cache locking. However, both [16] and [4] do not model the change in worst-case path after locking.
Falk et al. [6] perform cache locking by taking into account the change of worst-case path and achieve better WCET reduction. Their greedy algorithm computes the worst-case path and selects the procedure with maximum WCET reduction for locking. This process continues until the cache is fully locked. Liu et al. [14] present an optimal solution to minimize WCET via cache locking. However, their approach is optimal on the premise that the cache is fully locked. It may not be optimal towards minimizing WCET as shown in our motivating example. More importantly, they do not consider the cache mapping function at all in the locking algorithm. They simply assume that any memory block can be locked in any cache set (as if the cache is a scratchpad memory). After locking decisions are taken, they have to use code placement/layout technique [7, 12] that force the locked memory blocks to be mapped to the appropriate cache sets. This can lead to serious code size blowup, which has not been addressed.
Vera et al. [20] combine compile-time cache analysis and data cache locking in order to estimate a safe and tight worst-case memory performance. This work also assume full cache locking. Arnaud and Puaut [1] propose dynamic instruction cache locking for hard real-time systems. In their approach, the program is partitioned into regions, and static cache locking is performed for each region. In [17], cache locking is explored for predictable shared caches on multi-core systems. Cache locking is also shown to be quite effective for improving average-case execution time [13]. Finally, optimal on-chip scratchpad memory allocation to improve the WCET has been explored in [5, 18].

3. CACHE MODELING
Cache design depends on a few parameters: line (block) size L, which defines the unit of transfer of instructions or data between the cache and the main memory; number of setsK that the cache is divided into; associativityA, which determines the number of lines (blocks) in a set. Then the capacity of a cache is L × A ×K. We assume LRU (Least Recently Used) cache replacement policy.
Given a memory block m, it is mapped to only one cache set. Thus, the different cache sets are independent and can be modeled independently. In the following, we describe our modeling technique for one cache set. The same modeling techniques can be repeated for other cache sets. We use M to denote the set of memory blocks mapped to a cache set and use⊥ to indicate the absence of any memory block in a cache line.

3.1 Cache States
DEFINITION 1 (Concrete Cache State). A concrete cache state c is a vector 〈c[0], ..., c[A − 1]〉 of length A where c[i] ∈ M∪{⊥}. If c[i] = m, then m is the ith most recently used memory block in the cache set. We also define a special concrete cache state c⊥ = 〈⊥, ...,⊥〉 called the empty concrete cache state.
DEFINITION 2 (Concrete Cache State Hit). Given a concrete cache state c and a memory access m ∈M
c_hit(c,m) = { 1 if ∃i (0 ≤ i ≤ A− 1) s.t. c[i] = m 0 otherwise
We useΩ to denote the set of all possible concrete cache states of a program. Note that a program point can be reached via multiple paths and these paths may lead to different concrete cache states. We use P to denote the set of all possible concrete cache states at
a program point, i.e., P ∈ 2Ω. We can easily compute P at each program point through static program analysis as shown in [11].
Given the set of all possible cache states P at a program point and a memory access m ∈M ,
p_hit(P,m) = { 1 if ∀c ∈ P c_hit(c,m) = 1 0 otherwise
That is, an access m is a hit at a program point with the set of all possible concrete cache states P if and only if m is hit in all the concrete cache states of P .
Maintaining the set of all possible cache states may not be feasible (and scalable) for large programs with complex control flows where a program point can potentially have hundreds or even thousands of cache states. Thus we also employ abstract interpretation to compute the abstract cache state at every program point [19]. An abstract cache state is derived by joining all possible concrete cache states at a program point.
DEFINITION 3 (Abstract Cache State). An abstract cache state a is a vector 〈a[0], ...a[A− 1]〉 of length A where a[i] ∈ 2M .
An abstract cache state maps a cache line to a set of memory blocks. Must analysis and may analysis [19] are usually employed to compute abstract cache states for WCET analysis. Given a program point, must analysis determines the set of memory blocks that are guaranteed to be present in the cache, while may analysis determines the set of memory blocks that are never in the cache. Must analysis uses abstract cache states where the position of a memory block is an upper bound of its age. In may analysis, the lower bound of the age of a memory block is used as its position in the abstract cache state, in order to capture the set of all memory blocks that may be in the cache. Figure 2 shows the relationship between a set of concrete cache states and the corresponding abstract cache states.

4. CACHE LOCKING
In this paper, we consider static cache locking, where the selected memory blocks are locked into the cache before the program starts execution and remain unchanged throughout the execution. Furthermore, we consider line locking mechanism, where different number of lines can be locked in different cache sets. As discussed before, for our purposes, we can treat each cache set independently because the memory blocks mapped to different cache sets do not interfere. Each cache set can be considered as a fully associative cache containing A lines, where A is the associativity. Once a memory block is locked in a cache line, it can not be evicted from the cache. The remaining unlocked lines in the cache set serve as a fully associative cache with reduced capacity.
Note that the mapping of instructions to the cache sets depends on the code memory layout. Inserting additional code for cache locking may tamper this layout. To avoid this problem, we use the trampolines [2] approach. The extra code to fetch and lock the memory blocks in the cache are inserted at the end of the program as a trampoline. We leave a dummy NOP instruction at the entry point of the program that gets replaced by a call to this trampoline.
The main challenge is in selecting the memory blocks for locking so as to minimize the WCET. In the following, we propose two solutions. The first one is an optimal solution employing Integer Linear Programming (ILP) formulation based on concrete cache states and the second one is a heuristic approach based on abstract cache states.

4.1 Optimal solution with concrete cache states
The set of concrete cache states at any program point captures the exact set of cache states resulting from all possible program paths. Based on this accurate set of cache states, we formulate an ILP problem to optimally select the memory blocks for partial locking. In the following, we first show the ILP formulation for a loop and then extend it to the whole program.
4.1.1 ILP Formulation for Loop We represent the loop body as a Directed Acyclic Graph (DAG).
Each DAG is associated with a unique source and sink node. We compute the set of possible concrete cache states P at any point of the program through static program analysis [11]. Given the set of all possible cache states P and a memory block access m, p_hit(P,m) determines whether the access is a cache hit or miss before locking. Next, we proceed to determine the cache access behavior of m after locking.
For each memory block m, we define a 0-1 decision variable Lm, which indicates whether m is locked in the cache. Thus,
0 ≤ Lm ≤ 1 There are onlyA (associativity) cache lines available for locking in each cache set. Thus for each cache set i
∑
m∈Mi Lm ≤ A
where Mi is the set of memory blocks mapped to cache set i. The accesses to the locked memory blocks are cache hits. Let Locki denote the set of memory blocks locked in cache set i. For an unlocked memory block m mapped to cache set i (m ∈ Mi,m /∈ Locki), its access can be classified as hit or miss depending on the concrete cache states P at that program point and Locki.
For a concrete cache state c ∈ P , we define agecm as the age of the memory block m in c, where agecm = 0 (agecm = A− 1) if m is the most (least) recently accessed memory block in c. If m /∈ c (c_hit(c,m) = 0), then agecm = A. Thus, 0 ≤ agecm ≤ A. If m ∈ Mi and m /∈ Locki, then given a concrete cache state c, the access to m is cache hit if
⎛
⎝agecm + ∑
m′∈Locki∧agecm′>agecm Lm′
⎞
⎠ < A (1)
In other words, if a locked memory blockm′ ∈ Locki is younger than m in the cache state c, then locking m′ does not change the hit classification of m. However, if m′ ∈ Locki is older than m in cache state c (i.e., agecm′ > age c m), then lockingm′ essentially increases age of m by 1. If the number of such older memory blocks added to agecm exceeds the associativity, then m becomes a cache miss due to locking.
We define a 0-1 variable hcm, which specifies whether m is a cache hit in c after locking. Based on Equation 1,
hcm =
{ 1 if ( A− agecm − ∑
m′∈Locki∧agecm′>agecm Lm′
)
> 0
0 otherwise
However, the above equation is not linear. We substitute it with the equivalent linear equations as follows.
A− ∑ m′∈Locki∧agecm′>agecm Lm′ − agecm − U × hcm ≤ 0
A− ∑ m′∈Locki∧agecm′>agecm Lm′ − agecm + U − U × hcm > 0
where U is a large constant (U ≥ A). The set of concrete cache states P at a program point usually contains more than one concrete cache states (|P| > 1). Memory block access m is guaranteed as cache hit if and only if it is cache hit for every concrete cache state c ∈ P . We define a 0-1 variable hPm, which specifies whether m is a cache hit in P after locking.
hPm = { 1 if ∑ c∈P h c m = |P|
0 otherwise
We linearize the above equation as follows. ∑
c∈P hcm − hPm ≤ |P| − 1
∑ c∈P hcm − |P| × hPm ≥ 0
Finally, for each memory block access m, we define a 0-1 decision variable hitm, which specifies whether m is cache hit or miss after locking. Locked memory blocks are guaranteed to be cache hits. On the other hand, for an unlocked memory block m, we rely on its corresponding cache state P to determine the cache behavior.
hitm =
{
1 if Lm = 1 hPm otherwise
We linearize the above equation as follows.
hitm ≥ Lm, hitm ≥ hPm and hitm ≤ Lm + hPm Thus, the access latency of basic block B after cache locking is calculated as follows
TB = ∑ m∈B (miss_lat− (miss_lat− hit_lat)× hitm)
where miss_lat and hit_lat are the cache miss penalty and cache hit latency, respectively.
We also define a variableWB for each basic blockB in the loop, which represents the latency of the worst-case path rooted at basic block B in the DAG after cache locking. Then
WB = max B′∈imsucc(B)
{WB′ + TB}
where imsucc(B) is the set of immediate successors ofB in DAG. Therefore, for any outgoing edge from node B to node B′ (B → B′) in the DAG, we have the following constraint
WB ≥WB′ + TB Since there is no outgoing edge for the sink node of the loop, it is defined specially
Wsink = Tsink
Obviously, Wsrc will capture the latency of the worst-case acyclic path in the DAG (src is the source node of DAG). Let lb be the loop bound of this loop (maximum number of iterations of this loop). Then,Wsrc×lb is theWCET of this loop after cache locking. Thus, the optimal cache locking result for this loop can be obtained by minimizing Wsrc × lb (the objective function of ILP formulation).
4.1.2 Extension to the Whole Program In the previous section, we present an ILP formulation to obtain
the optimal cache locking for a loop. In order to obtain the ILP formulation for the whole program, we are required to start from the innermost loops of the program. We first generate the ILP formulation for the innermost loops, and then each innermost loop is treated as a dummy basic block of the outer loop. Therefore, we can construct the ILP formulation for the next level of loop. We continue this way until we reach the outmost loop in the program. Clearly, Wentry represents the WCET of the whole program under cache locking, where entry denotes the entry node of program. Finally, the locking overhead (e.g., the execution of the locking instructions) are included in the WCET of the whole program.

4.2 Heuristic with abstract cache states
In the previous section, we develop an optimal ILP formulation using concrete cache states. However, programs with complex control flow may have hundreds or even thousands of cache states at a program point. For such programs, maintaining all possible concrete cache states may not be feasible. Also ILP formulation may take very long to reach a solution specially for larger programs and larger associativity. Thus, we propose a heuristic approach based on abstract cache states. Abstract cache state is a more compact representation compared to the set of concrete cache states.
We first perform WCET analysis with cache modeling based on abstract interpretation [19]. Then we can easily determine cache hit/miss classification for each memory access based on the abstract cache states. As a by-product of the WCET analysis, we obtain the abstract cache states under must analysis at all program points. Meanwhile, we also collect the execution frequency of each basic block along the worse-case path. Then we iteratively lock some memory blocks on the worse-case path to improve the WCET.
Suppose memory block m is on the worst-case path. Let latm be the access latency of m according to the hit/miss classification in WCET analysis, and fm be its execution frequency along the worst-case path. By locking memory block m, all accesses to m will be cache hits. Therefore, we define the benefit of lockingm as
benefitm = (latm − hit_lat)× fm where hit_lat is the cache hit latency. Thus, locking a memory block guaranteed to be hit before locking does not give any benefit.
On the other hand, locking memory block m in cache may have negative impact for the memory blocks mapped to the same set as the associativity for this set is reduced by 1. Similar to concrete cache state, we define the age of a memory block m in abstract must cache state C as ageCm. When m ∈ C, 0 ≤ ageCm ≤ A − 1, where A is the associativity. Otherwise, we set its age to A.
Suppose we choose to lock memory blockm in the cache and its benefitm > 0. In other words, m is not in the abstract must cache state before locking. Then, locking m will downgrade the memory blockm′ from cache hit to cache miss if ageCm′ = A−1. Note that the associativityA here refers to the current associativity of the set. That is A refers to the original associativity of the cache minus the number of memory blocks locked in the set so far. Therefore, we define the cost of locking m as follows.
costm = ∑ m′∈Mi∧ageCm′=A−1 (miss_lat− hit_lat)× fm′
where as before m ∈Mi. Then, the overall gain of locking m is gainm = benefitm − costm
We compare different memory blocks in terms of their gain and select the most beneficial memory blockm to be locked. However,
gainm may not be the actual WCET reduction because the worstcase path may change after locking m. Thus, we update cache state for instructions mapped to the affected cache set and perform WCET analysis again to obtain the exact WCET after locking m. If the WCET is actually reduced, we lock m in the cache. We continually select memory blocks for locking until either there is no actual WCET improvement after locking any memory block or there is no gain in the cost-benefit analysis for any memory block m (i.e., gainm ≤ 0). Finally, the locking overhead is included. The detail algorithm is shown in Appendix A.

5. EXPERIMENTAL EVALUATION
In this section, we present the experimental evaluation of partial cache locking. We compare both the optimal and the heuristic solutions with static cache analysis [19] and the full cache locking approach proposed by Falk et al. [6].

5.1 Experimental Setup
We use the benchmarks from MRTC benchmark suite [8] as shown in Table 1. We compile our benchmarks for SimpleScalar PISA (Portable ISA) instruction set [3] — a MIPS like instruction set architecture — with gcc cross-compiler. The control flow graphs of these benchmarks are extracted and provided as input to our cache locking analysis. Our framework is built on top of the open-source WCET analysis tool Chronos [9]. The binary code size of each program is shown in the second column of Table 1. We perform all the experiments on 2.53GHz Intel Xeon CPU with 24GB memory. IBM CPLEX is used as the ILP solver.
We assume only one level of instruction cache in the architecture. In other words, an instruction access is either cache hit or it has to be fetched from memory. The cache hit latency is 1 cycle, while a cache miss takes 30 cycles. As we are modeling the instruction cache, we assume a simple in-order processor with unit-latency for all data memory references.

5.2 Partial Cache Locking vs. Static Analysis
Figure 3 shows the WCET improvement of partial cache locking over static analysis with no locking based on abstract interpretation [19]. The instruction cache is 4-way set associative with block size of 32 bytes, and its capacity is varied from 512B to 1KB (see Appendix for other settings).
Our partial cache locking technique significantly improves the WCET over static analysis with no locking for many benchmarks (e.g., cnt, crc and qurt) for different cache sizes. However, some benchmarks show limited improvement of WCET via partial cache locking, especially when the cache size is small. This is mainly due to the fact that locking memory blocks destroys the deterministic access pattern for some unlocked blocks. Therefore, our partial locking technique decides not to lock these memory blocks and the result of partial locking is close to that of static analysis.
For most of the benchmarks, the improvement increases as the cache size increases, because there is more space for locking and more memory blocks can be locked into the cache. However, for some benchmarks, the improvement decreases as cache size increases, for example fir. For fir, when the cache size increases, more memory accesses become deterministic, which can be successfully identified by static cache analysis. Thus, cache locking may not help to improve the WCET much compared to static cache analysis. Overall, more WCET improvement is observed as the cache size increases. On an average, 16% and 23% improvement are achieved for 512B and 1KB size cache, respectively.

5.3 Partial versus Full Cache Locking
There exist two full cache locking techniques as mentioned in Section 2 [14, 6]. Even though Liu et al. [14] show that their approach can achieve better WCET reduction compared to [6], it has several limitations. Liu et al. do not consider the cache mapping function in the locking algorithm. They simply assume that any memory block can be locked in any cache set (as if the cache is a scratchpad memory). After locking decisions are made, they employ code placement techniques that force the locked memory blocks to be mapped to the appropriate cache sets. This can lead to code size blowup, which has not been addressed in their work.
Thus we decide to compare our partial locking results with Falk et al.’s method [6] as both approaches do not require any subsequent code placement technique. We choose memory blocks as locking granularity instead of procedures originally used in Falk et al.’s method for a fair comparison. This choice of granularity does not change the greedy heuristic algorithm proposed in [6]. The instruction cache is 4-way associative with 32-byte blocks and capacity varying from 512B to 1KB (see Appendix for other settings).
The WCET improvement of partial cache locking over Falk et al.’s method is shown in Figure 4. Both optimal and heuristic partial locking approaches outperform Falk et al.’s method for different cache sizes. Our partial cache locking techniques usually lock part of the cache (see Appendix for detailed results). Thus, after locking, there are still some cache lines left for the unlocked memory blocks to exploit their locality of accesses. However, in Falk et al.’s method, the cache is fully locked and all the accesses to the
unlocked memory blocks are cache misses. On an average, partial cache locking improves the WCET by 61% and 45% over full cache locking for 512B and 1KB caches, respectively.

5.4 Optimal vs. Heuristic Approach
As shown in Figure 3 and 4, our heuristic approach obtains nearly the same results as the optimal solution. Table 1 presents the average analysis time of different algorithms for all the benchmarks. Clearly, our heuristic approach produces comparable results to the optimal solution while it is more efficient in analysis time.

6. CONCLUSION
In this paper, we propose partial cache locking for WCET reduction. We have proposed an optimal partial locking solution based on concrete cache states as well as a heuristic approach based on abstract cache states. Our partial cache locking significantly reduces the WCET compared to the static cache analysis and the state-of-the-art cache locking techniques that fully lock the cache. Our heuristic achieves comparable WCET reduction to the optimal solution but it is more efficient in terms of runtime. In the future, we will consider data cache locking and explore dynamic cache locking for further improvement.

7. ACKNOWLEDGMENTS
This work was partially supported by Singapore Ministry of Education Academic Research Fund Tier 2, MOE2009-T2-1-033.

8. REFERENCES
[1] A. Arnaud and I. Puaut. Dynamic instruction cache locking in hard real-timesystems. In RTNS, 2006. 
[2] B. Buck and J. K. Hollingsworth. An API for runtime code patching. Int. J.High Perform. Comput. Appl., 14(4), 2000. 
[3] D. Burger and T. M. Austin. The simplescalar tool set, version 2.0. SIGARCHComput. Archit. News, 25(3), 1997. 
[4] A. M. Campoy, I. Puaut, A. P. Ivars, and J. V. B. Mataix. Cache contentsselection for statically-locked instruction caches: An algorithm comparison. In ECRTS, 2005.
[5] H. Falk and J. C. Kleinsorge. Optimal static WCET-aware scratchpad allocation of program code. In DAC, 2009. 
[6] H. Falk, S. Plazar, and H. Theiling. Compile-time decided instruction cache locking using worst-case execution paths. In CODES+ISSS, 2007. 
[7] C. Guillon, F. Rastello, T. Bidault, and F. Bouchez. Procedure placement using temporal-ordering information: Dealing with code size expansion. J. Embedded Comput., 1(4), 2005. 
[8] J. Gustafsson, A. Betts, A. Ermedahl, and B. Lisper. The Mälardalen WCET benchmarks – past, present and future. In WCET, 2010. 
[9] X. Li, Y. Liang, T. Mitra, and A. Roychoudury. Chronos: A timing analyzer for embedded software. Science of Computer Programming, 69(1-3), 2007.
[10] Y.-T. S. Li, S. Malik, and A. Wolfe. Cache modeling for real-time software: beyond direct mapped instruction caches. In RTSS, 1996. 
[11] Y. Liang and T. Mitra. Cache modeling in probabilistic execution time analysis. In DAC, 2008. 
[12] Y. Liang and T. Mitra. Improved procedure placement for set associative caches. In CASES, 2010. 
[13] Y. Liang and T. Mitra. Instruction cache locking using temporal reuse profile. In DAC, 2010. 
[14] T. Liu, M. Li, and C. J. Xue. Minimizing WCET for real-time embedded systems via static instruction cache locking. In RTAS, 2009. 
[15] F. Martin, M. Alt, R. Wilhelm, and C. Ferdinand. Analysis of loops. In CC, 1998. 
[16] I. Puaut and D. Decotigny. Low-complexity algorithms for static cache locking in multitasking hard real-time systems. In RTSS, 2002. 
[17] V. Suhendra and T. Mitra. Exploring locking & partitioning for predictable shared caches on multi-cores. In DAC, 2008. 
[18] V. Suhendra, T. Mitra, A. Roychoudhury, and T. Chen. WCET centric data allocation to scratchpad memory. In RTSS, 2005. 
[19] H. Theiling, C. Ferdinand, and R. Wilhelm. Fast and precise WCET prediction by separated cache and path analyses. Real-Time Syst., 18(2/3), 2000. 
[20] X. Vera, B. Lisper, and J. Xue. Data cache locking for higher program predictability. SIGMETRICS Perform. Eval. Rev., 31(1), 2003.APPENDIX
