Exploratory Undersampling for Class-Imbalance Learning
Xu-Ying Liu,Jianxin Wu
liuxy@lamda.nju.edu.cn;,zhouzh@lamda.nju.edu.cn).,wujx@cc.

Index Terms—Class-imbalance learning, data mining, ensemble learning, machine learning, undersampling.
I. INTRODUCTION
IN MANY real-world problems, the data sets are typicallyimbalanced, i.e., some classes have much more instances than others. The level of imbalance (ratio of size of the majority class to minority class) can be as huge as 106 [41]. It is noteworthy that class imbalance is emerging as an important issue in designing classifiers [11], [23], [37].
Imbalance has a serious impact on the performance of classifiers. Learning algorithms that do not consider class imbalance tend to be overwhelmed by the majority class and ignore the minority class [10]. For example, in a problem with imbalance level of 99, a learning algorithm that minimizes error rate could decide to classify all examples as the majority class in order to achieve a low error rate of 1%. However, all minority class examples will be wrongly classified in this case. In problems where the imbalance level is huge, class imbalance must be carefully handled to build a good classifier.
Class imbalance is also closely related to cost-sensitive learning, another important issue in machine learning. Misclassi-
Manuscript received October 26, 2007; revised March 15, 2008 and June 24, 2008. First published December 16, 2008; current version published March 19, 2009. This work was supported in part by the National Science Foundation of China under Grants 60635030 and 60721002, by the Jiangsu Science Foundation under Grant BK2008018, and by the National High Technology Research and Development Program of China under Grant 2007AA01Z169. This paper was recommended by Associate Editor N. Chawla.
X.-Y. Liu and Z.-H. Zhou are with the National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China (e-mail: liuxy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn).
J. Wu is with the School of Interactive Computing, College of Computing, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: wujx@cc. gatech.edu).
Digital Object Identifier 10.1109/TSMCB.2008.2007853
fying a minority class instance is usually more serious than misclassifying a majority class one. For example, approving a fraudulent credit card application is more costly than declining a credible one. Breiman et al. [7] pointed out that training set size, class priors, cost of errors in different classes, and placement of decision boundaries are all closely connected. In fact, many existing methods for dealing with class imbalance rely on connections among these four components. Sampling methods handle class imbalance by varying the minority and majority class sizes in the training set. Cost-sensitive learning deals with class imbalance by incurring different costs for the two classes and is considered as an important class of methods to handle class imbalance [37]. More details about class-imbalance learning methods are presented in Section II.
In this paper, we examine only binary classification problems by ensembling classifiers built from multiple undersampled training sets. Undersampling is an efficient method for classimbalance learning. This method uses a subset of the majority class to train the classifier. Since many majority class examples are ignored, the training set becomes more balanced and the training process becomes faster. However, the main drawback of undersampling is that potentially useful information contained in these ignored examples is neglected. The intuition of our proposed methods is then to wisely explore these ignored data while keeping the fast training speed of undersampling.
We propose two ways to use these data. One straightforward way is to sample several subsets independently from N (the majority class), use these subsets to train classifiers separately, and combine the trained classifiers. Another method is to use trained classifiers to guide the sampling process for subsequent classifiers. After we have trained n classifiers, examples correctly classified by them will be removed from N . Experiments on 16 UCI data sets [3] show that both methods have higher Area Under the receiver operating characteristics (ROC) Curve (AUC), F-measure, and G-mean values than many existing class-imbalance learning methods.
The rest of this paper is organized as follows. Section II reviews related methods. Section III presents EasyEnsemble and BalanceCascade. Section IV reports the experiments. Finally, Section V concludes this paper.

II. RELATED WORK
As mentioned in the previous section, many existing classimbalance learning methods manipulate the following four components: training set size, class prior, cost matrix, and placement of decision boundary. Here, we pay special attention to two classes of methods that are most widely used: sampling
1083-4419/$25.00 © 2008 IEEE
and cost-sensitive learning. For other methods, we refer the readers to [37] for a more complete and detailed review.
Sampling is a class of methods that alters the size of training sets. Undersampling and oversampling change the training sets by sampling a smaller majority training set and repeating instances in the minority training set, respectively [15]. The level of imbalance is reduced in both methods, with the hope that a more balanced training set can give better results. Both sampling methods are easy to implement and have been shown to be helpful in imbalanced problems [37], [47]. Undersampling requires shorter training time, at the cost of ignoring potentially useful data. Oversampling increases the training set size and thus requires longer training time. Furthermore, it tends to lead to overfitting since it repeats minority class examples [9], [15]. Aside from the basic undersampling and oversampling methods, there are also methods that sample in more complex ways. SMOTE [9] added new synthetic minority class examples by randomly interpolating pairs of closest neighbors in the minority class. The one-sided selection procedures [25] tried to find a representative subset of majority class examples by only removing “borderline” and “noisy” majority examples. Some other methods combine different sampling strategies to achieve further improvement [1]. In addition, researchers have studied the effect of varying the level of imbalance and how to find the best ratio when a C4.5 tree classifier was used [38].
Cost-sensitive learning [14], [16] is another important class of class-imbalance learning methods. Although many learning algorithms have been adapted to accommodate class-imbalance and cost-sensitive problems, variants of AdaBoost appear to be the most popular ones. Many cost-sensitive boosting algorithms have been proposed [31]. A common strategy of these variants was to intentionally increase the weights of examples with higher misclassification cost in the boosting process. In [30], the initial weights of high cost examples were increased. It was reported that, however, the weight differences between examples in different classes disappear quickly when the boosting process proceeds [33]. Thus, many algorithms raised high cost examples’ weights in every iteration of the boosting process, for example, AsymBoost [33], AdaCost [17], CSB [31], DataBoost [21], and AdaUBoost [24], just to name a few. Another way to adapt a boosting algorithm to cost-sensitive problems is to change the weights of the weak classifiers in forming the final ensemble classifier, such as BMPM [22] and LAC [41]. Unlike the heuristic methods mentioned earlier, Asymmetric Boosting [28] directly minimized a cost-sensitive loss function in the statistical interpretation of boosting.
SMOTEBoost [12] is designed for class-imbalance learning, which is very similar to AsymBoost. Both methods alter the distribution for the minority class and majority class in separate ways. The only difference is how these distributions are altered. AsymBoost directly updates instance weights for the majority class and minority class differently in each iteration, while SMOTEBoost alters distribution by first updating instance weights for majority class and minority class equally and then using SMOTE to get new minority class instances.
Chan and Stolfo [8] introduced an approach to explore majority class examples. They split the majority class into several nonoverlapping subsets, with each subset having ap-
proximately the same number of examples as the minority class. One classifier was trained from each of these subsets and the minority class. The final classifier ensembled these classifiers using stacking [40]. However, when a data set is highly imbalanced, this approach requires a much longer training time than undersampling. Moreover, since the minority class examples are used by every classifier, stacking these classifiers will have a high probability of suffering from overfitting when the number of minority class examples is limited.
III. EasyEnsemble AND BalanceCascade
As was shown by Drummond and Holte [15], undersampling is an efficient strategy to deal with class imbalance. However, the drawback of undersampling is that it throws away many potentially useful data. In this section, we propose two strategies to explore the majority class examples ignored by undersampling: EasyEnsemble and BalanceCascade.

A. EasyEnsemble
Given the minority training set P and the majority training set N , the undersampling method randomly samples a subset N′ from N , where |N ′| < |N |. Usually, we choose |N ′| = |P| and therefore have |N ′| |N | for highly imbalanced problems. EasyEnsemble is probably the most straightforward way to further exploit the majority class examples ignored by undersampling, i.e., examples in N ⋂ N′. In this method, we independently sample several subsets N1,N2, . . . ,NT from N . For each subset Ni (1 ≤ i ≤ T ), a classifier Hi is trained using Ni and all of P . All generated classifiers are combined for the final decision. AdaBoost [29] is used to train the classifier Hi. The pseudocode for EasyEnsemble is shown in Algorithm 1.
Algorithm 1 The EasyEnsemble algorithm. 1: {Input: A set of minority class examples P , a set of majority class examples N , |P| < |N |, the number of subsets T to sample from N , and si, the number of iterations to train an AdaBoost ensemble Hi}
2: i ⇐ 0 3: repeat
4: i ⇐ i + 1 5: Randomly sample a subset Ni from N , |Ni| = |P|. 6: Learn Hi using P and Ni. Hi is an AdaBoost ensemble with si weak classifiers hi,j and corresponding weights αi,j . The ensemble’s threshold is θi, i.e.,
Hi(x) = sgn
⎛ ⎝
si∑ j=1 αi,jhi,j(x) − θi
⎞ ⎠ .
7: until i = T 8: Output: An ensemble
H(x) = sgn
⎛ ⎝
T∑ i=1 si∑ j=1 αi,jhi,j(x) − T∑ i=1 θi
⎞ ⎠ .
The idea behind EasyEnsemble is simple. Similar to the Balanced Random Forests [13], EasyEnsemble generates T balanced subproblems. The output of the ith subproblem is AdaBoost classifier Hi, an ensemble with si weak classifiers {hi,j}. An alternative view of hi,j is to treat it as a feature that is extracted by the ensemble learning method and can only take binary values [41]. Hi, in this viewpoint, is simply a linear classifier built on these features. Features extracted from different subsets Ni thus contain information of different aspects of the original majority training set N . Finally, instead of counting votes from {Hi}i=1,...,T , we collect all the features hi,j (i = 1, 2, . . . , T, j = 1, 2, . . . , si) and form an ensemble classifier from them.
The output of EasyEnsemble is a single ensemble, but it looks like an “ensemble of ensembles”. It is known that boosting mainly reduces bias, while bagging mainly reduces variance. Several works [19], [35], [36], [42] combine different ensemble strategies to achieve stronger generalization. MultiBoosting [35], [36] combines boosting with bagging/ wagging [2] by using boosted ensembles as base learners. Stochastic Gradient Boosting [19] and Cocktail Ensemble [42] also combine different ensemble strategies. It is evident that EasyEnsemble has benefited from the combination of boosting and a bagging-like strategy with balanced class distribution.
Both EasyEnsemble and Balanced Random Forests try to use balanced bootstrap samples; however, the former uses the samples to generate boosted ensembles, while the latter uses the samples to train decision trees randomly. Costing [43] also uses multiple samples of the original training set. Costing was initially proposed as a cost-sensitive learning method, while EasyEnsemble is proposed to deal with class imbalance directly. Moreover, the working style of EasyEnsemble is quite different from costing. For example, the costing method samples the examples with probability in proportion to their costs (rejection sampling). Since this is a probability-based sampling method, no positive example will definitely appear in all the samples (in fact, the probability of a positive example appearing in all the samples is small). While in EasyEnsemble, all the positive examples will definitely appear in all the samples. When the size of minority class is very small, it is important to utilize every minority class example.

B. BalanceCascade
EasyEnsemble is an unsupervised strategy to explore N since it uses independent random sampling with replacement. Our second algorithm, BalanceCascade, explores N in a supervised manner. The idea is as follows. After H1 is trained, if an example x1 ∈ N is correctly classified to be in the majority class by H1, it is reasonable to conjecture that x1 is somewhat redundant in N , given that we already have H1. Thus, we can remove some correctly classified majority class examples from N . As in EasyEnsemble, we use AdaBoost in this method. The pseudocode of BalanceCascade is described in Algorithm 2.
Algorithm 2 The BalanceCascade algorithm. 1: {Input: A set of minority class examples P , a set of majority class examples N , |P| < |N |, the number of subsets
T to sample from N , and si, the number of iterations to train an AdaBoost ensemble Hi}
2: i ⇐ 0, f ⇐ T−1 √
|P|/|N |, f is the false positive rate (the error rate of misclassifying a majority class example to the minority class) that Hi should achieve.
3: repeat 4: i ⇐ i + 1 5: Randomly sample a subset Ni from N , |Ni| = |P|. 6: Learn Hi using P and Ni. Hi is an AdaBoost ensemble with si weak classifiers hi,j and corresponding weights αi,j . The ensemble’s threshold is θi i.e.,
Hi(x) = sgn
⎛ ⎝
si∑ j=1 αi,jhi,j(x) − θi
⎞ ⎠ .
7: Adjust θi such that Hi’s false positive rate is f . 8: Remove from N all examples that are correctly classi-
fied by Hi. 9: until i = T 10: Output: A single ensemble
H(x) = sgn
⎛ ⎝
T∑ i=1 si∑ j=1 αi,jhi,j(x) − T∑ i=1 θi
⎞ ⎠ .
This method is called BalanceCascade since it is somewhat similar to the cascade classifier in [34]. The majority training set N is shrunk after every Hi is trained, and every node Hi is dealing with a balanced subproblem (|Ni| = |P|). However, the final classifier is different. A cascade classifier is the conjunction of all {Hi}i=1,...,T , i.e., H(x) predicts positive if and only if all Hi(x)(i = 1, 2, . . . , T ) predict positive. Viola and Jones [34] used the cascade classifier mainly to achieve fast testing speed. While in BalanceCascade, sequential dependence between classifiers is mainly exploited for reducing the redundant information in the majority class. This sampling strategy leads to a restricted sample space for the following undersampling process to explore as much useful information as possible. BalanceCascade is similar to EasyEnsemble in their structures. The main difference between them is the lines 7 and 8 of Algorithm 2. Line 8 removes the true majority class examples from N , and line 7 specifies how many majority class examples can be removed. At the beginning of the T th iteration, N has been shrunk T − 1 times, and therefore, its current size is |N | · fT−1 = |P|. Thus, after HT is trained and N is shrunk again, the size of N is smaller than |P|. We can stop the training process at this time.
There are other ways to combine weak classifiers in EasyEnsemble and BalanceCascade. A popular one is stacking [40]. It takes the outputs of other classifiers as input to train a generalizer. However, Ting and Witten [32] stated that the use of class probabilities is crucial for the successful application of stacked generalization in classification tasks. Furthermore, since minority class examples are used to train each weak classifier, stacking these classifiers is likely to suffer from overfitting when the number of minority class examples is
Chan and Stolfo’s method [8] (abbreviated as Chan) is closely related to EasyEnsemble and BalanceCascade. It splits the majority class into several nonoverlapping subsets, with each subset having similar size to the minority class. Classifiers trained from each majority class subset and the minority class are combined by stacking. The differences between Chan and the proposed methods are obvious and given as follows: 1) Chan uses all majority class examples, while EasyEnsemble and BalanceCascade use only part of them. When a data set is highly imbalanced, Chan requires a much longer training time than the proposed methods. However, the experimental results reveal that it is not necessary to use all majority class examples to achieve good performances. 2) Chan uses stacking to combine classifiers trained from each subset. As stated earlier, since the minority class is used repeatedly, stacking is likely to suffer from overfitting when the number of minority class examples is limited.
Both EasyEnsemble and BalanceCascade are very efficient. Their training time is roughly the same as that of undersampling when the same number of weak classifiers is used. Detailed analyses of training time and empirical running time are presented in Section IV-C.

IV. EXPERIMENTS
A. Evaluation Criteria
It is now well known that error rate is not an appropriate evaluation criterion when there are class imbalance or unequal costs. In this paper, we use F-measure, G-mean, and AUC [4] as performance evaluation measures. F-measure and G-mean are functions of the confusion matrix as shown in Table I. F-measure and G-mean are then defined as follows. Here, we take minority class as positive class
False Positive Rate (fpr) = FP
FP + TN
True Positive Rate (Acc+) = TP
TP + FN
True Negative Rate (Acc−) = TN
TN + FP
G-mean = √ Acc+ × Acc−
Precision = TP
TP + FP
Recall = TP
TP + FN = Acc+
F-measure = 2 × Precision × Recall
Precision + Recall .
(1)
AUC has proved to be a reliable performance measure for imbalanced and cost-sensitive problems [18]. Given a binary classification problem, an ROC curve depicts the performance of a method using the (fpr, tpr) pairs, as shown in Fig. 1. fpr is the false positive rate of the classifier, and tpr is the true positive rate (Acc+). AUC is the area below the curve (shaded region in Fig. 1). It integrates performance of the classification method over all possible values of fpr and is proved to be a reliable performance measure for imbalanced and cost-sensitive problems [18].
In our experiments, for ensemble classifiers in the form H(x) = sgn( ∑T i=1 αihi(x) − θ), we alter the value of θ from −∞ to ∞. In this way, we get a full range of (fpr, tpr) pairs and build an ROC curve from these data. We then use the Algorithm 3 in [18] to calculate the AUC score. Details of AUC can be found in [18].

B. Experimental Settings
We tested the proposed methods on 16 UCI data sets [3]. Information about these data sets is summarized in Table II.
For every data set, we perform a tenfold stratified cross validation. Within each fold, the classification method is repeated ten times considering that the sampling of subsets introduces randomness. The AUC, F-measure, and G-mean of this
cross-validation process are averaged from these ten runs. The whole cross-validation process is repeated for five times, and the final values from this method are the averages of these five cross-validation runs.
We compared the performance of 15 methods, including as follows.
1) CART: classification and regression trees [7]. It uses the entire data set (P and N ) to train a single classifier. 2) Bagging (abbreviated as Bagg). Bagging [5] uses the entire data set (P and N ). CART is used to train weak classifiers. The number of iterations is 40. 3) AdaBoost (abbreviated as Ada). AdaBoost uses the entire data set (P and N ). CART is used to train weak classifiers. The number of iterations is 40. 4) AsymBoost (abbreviated as Asym). AsymBoost is a typical cost-sensitive variant of AdaBoost.1 Let r = |N |/|P| be the imbalance level. At each iteration, the weight of every positive example is multiplied by T √ r, where T
is the number of iterations [33]. AsymBoost uses the entire data set (P and N ). CART is used to train weak classifiers. The number of iterations is 40. 5) SMOTEBoost (abbreviated as SMB). SMOTE adds synthetic minority class examples [9]. For data sets having nominal attributes, we use SMOTE-NC. Details for implementing SMOTE and SMOTE-NC can be found in [9]. SMOTEBoost uses SMOTE to get new minority class examples in each iteration. CART is used to train weak classifiers. The number of iterations is 40. The k nearest neighbor parameter of SMOTE is five. The amount of new data generated using SMOTE in each iteration is |P|. 6) Undersampling + AdaBoost (abbreviated as Under). A subset N′ is sampled (without replacement) from N , |N ′| = |P|. Then, AdaBoost is used to train a classifier using P and N′, since the problem is balanced after undersampling. CART is used to train weak classifiers. The number of iterations is 40. 7) Oversampling + AdaBoost (abbreviated as Over). A new minority training set is sampled (with replacement) from the original minority class, |P′| = |N |. Then, AdaBoost is used to train a classifier using P′ and N . CART is used to train weak classifiers. The number of iterations is 40. 8) SMOTE + AdaBoost (abbreviated as SMOTE). In our experiments, we first generate P′ using SMOTE, a set of synthetic minority class examples with |P′| = |P|. We sample a new majority training set N′ with |N ′| = 2|P| when |N | > 2|P|, and let N′ = N otherwise. Then, we use AdaBoost to train a classifier with P , P′, and N′. CART is used to train weak classifiers. The number of iterations is 40. The settings of SMOTE are the same as that of SMOTEBoost (k = 5). 9) Chan and Stolfo’s method + AdaBoost (abbreviated as Chan). It splits N into |N |/|P| nonoverlapping subsets. An AdaBoost classifier was trained from each of these subsets and P . Fisher Discriminant Analysis [20] is used as the stacking method. CART is used to
1It is also equivalent to the CSB2 algorithm in [31].
train weak classifiers. AdaBoost classifiers are trained for 40|P |/|N | iterations when |N |/|P | < 40; otherwise, only one iteration is allowed. 10) BalanceCascade (abbreviated as Cascade). CART is used to train weak classifiers. Number of subsets T = 4; number of rounds in each AdaBoost ensemble si = 10. 11) EasyEnsemble (abbreviated as Easy). CART is used to train weak classifiers. Number of subsets T = 4; number of rounds in each AdaBoost ensemble si = 10. 12) Random Forests (abbreviated as RF). Random Forests [6] uses bootstrap samples of training data to generate random trees and then form an ensemble. Here, we use RandomForest in WEKA [39], in which a random tree is a variant of REPTree, using random feature selection in the tree induction process, and not pruned. RF uses the entire data set (P and N ). The number of iterations is 40. 13) Undersampling + Random Forests (abbreviated as Under-RF). A subset N′ is sampled (without replacement) from N , |N ′| = |P|. Then, Random Forests is used to train a classifier using P and N′, The number of iterations is 40. 14) Oversampling + Random Forests (abbreviated as Over-RF). A new minority training set is sampled (with replacement) from the original minority class, |P′| = |N |. Then, Random Forests is used to train a classifier using P′ and N . The number of iterations is 40. 15) Balanced Random Forests (abbreviated as BRF). Balanced Random Forests is different from Random Forests in that it uses balanced bootstrap samples of training data. It is different from undersampling + Random Forests, because the latter preprocesses the training data and then learns a Random Forests classifier. Here, we use RandomTree in WEKA to train weak classifiers, which is the same weak classifier learning method used by RandomForest in WEKA. The number of iterations is 40.
The settings of CART are the same. In CART, pruning is used, and impure nodes must have at least ten examples to be split. CART and Ada are baseline methods. All other classifiers have 40 weak classifiers. In Chan, the amount of classifiers is also 40 since the imbalance levels of data sets in Table II are all lower than 40.

C. Analysis of Training Time
Random Forests series (RF, Under-RF, Over-RF, and BRF) use random decision trees, which train much faster than CART. Moreover, they are implemented in Java code, while the other methods are in Matlab code. Therefore, it is not fair to compare the running time of them directly. Here, we only analyze the training time of CART-based methods.
Since all methods use the same weak learner and have the same amount of weak classifiers, the training time of these methods mainly depends on the number of training examples.
From the descriptions in Section IV-B, Under uses the smallest number (2|P|) of examples and is the fastest among all methods. The proposed methods (Cascade and Easy) and Chan use the same number of weak classifiers as Under and use the same number of examples as Under to train every weak
classifier.2 These methods require additional time to sample or split subsets of N . However, this time is negligible. Thus, the proposed methods and Chan have approximately the same training time as Under. Note that the imbalance level of data sets used in the experiment happens to be lower than 40, so the number of weak classifiers in Chan can be the same with that in Cascade and Easy. However, when the data set is highly imbalanced (for example, the imbalance level is 1000), Chan will require extremely more training time than the proposed methods. Furthermore, Easy has a potential computational advantage since each undersampling process can be executed in parallel.
Both Ada and Asym use |P| + |N | examples. Since |N | > |P|, these methods are slower than Under. When the imbalance level is high, these methods have much longer training time than Under and the proposed methods.
In our experiments, SMOTE uses either 4|P| or 2|P| + |N | examples. SMB uses 2|P| + |N | examples, and both of them require to compute the distance between minority class examples. Thus, they are much slower than Under and the proposed methods. Over uses 2|N | examples, which has the largest training set. SMB and Over are the most expensive ones. For data sets with a large number of examples, e.g., letter, the time to train an oversampled or SMOTEBoost classifier is too long to be practical.
2Although different subsets of N are used in the training process, the number of active training examples is always 2|P| at all times.
CART uses |P| + |N | examples. CART trains only one classifier, so it indicates the time baseline.
Running times of these methods are recorded in Table III, on a computer with a 3.0-GHz Intel Xeon CPU. It shows that Chan, Easy, and Cascade are as efficient as Under. The most expensive ones are SMB and Over, followed by Ada and Asym, and then by SMOTE.

D. Results and Analyses
The average AUC of the compared methods are summarized in Tables IV and V. On car, ionosphere, letter, phoneme, sat, and wdbc, Ada achieves very high AUC values, which are all greater than 0.95. Applying class-imbalance learning methods on these data sets is not necessarily beneficial. On the other ten data sets, Ada’s AUC values are not high, and these data sets seem to suffer from class-imbalance problem. Therefore, we divide the 16 data sets into two groups. The first group contains 6 “easy” tasks, on which the AUC values of Ada are greater than 0.95. The second group contains 10 “hard” tasks, on which the AUC values of Ada are lower than 0.95. The AUC results are shown separately in Tables IV and V.3 The results of t-test (significance level at 0.05) of AUC are also shown separately in the upper and
3Note that the performance of Over and SMB on the data sets in the former group has not been obtained due to its large training time costs. CART gives discrete outputs, so its AUC is not available.
The results show that on “easy” tasks, all class-imbalance learning methods have lower AUC and F-measure than Ada, except that Asym has similar AUC and F-measure to it. While on “hard” tasks, class-imbalance learning methods generally have higher AUC and F-measure than Ada, including SMOTE, Chan, Cascade, and Easy. We argue that for tasks on which ordinary methods can achieve high AUC (e.g., ≥ 0.95), classimbalance learning is generally not helpful with AUC and F-measure. However, Easy and Cascade can be used to reduce the training time, while their average AUC values are close to that of Ada and Asym.
We are more interested in the results on “hard” tasks, where class-imbalance learning really helps. Compared with the results on “easy” tasks, they reveal more properties of classimbalance learning and the proposed methods.
Under is not performing well with AUC and F-measure. Its AUC and F-measure are lower than that of Ada and Asym on all “easy” tasks and lower than that of many other classimbalance learning methods on “hard” tasks. Our conjecture is that this is due to the information contained in the majority class which is ignored by Under. Both our proposed methods can improve upon Under, no matter on “easy” tasks or “hard” tasks. This result supports our argument that Easy and Cascade can effectively explore the majority class examples. Chan uses all the majority class examples, and it generally has higher AUC and F-measure than Under. However, the results show that on “hard” tasks, its AUC, F-measure, and G-mean are comparable to or slightly lower than that of Cascade, and they are lower than that of Easy on most of the data sets. This implies that using all majority class examples
Both Easy and Cascade attain higher average AUC, F-measure, and G-mean than almost all the other methods on “hard” tasks, except that Cascade is comparable to Chan with AUC and F-measure, and slightly worse than BRF and Under-RF with G-mean. However, Chan has much lower G-mean, and BRF and Under-RF have much lower AUC and F-measure than many other class-imbalance learning methods, while both Easy and Cascade are very robust with different performance measures. Easy and Cascade cannot only improve the AUC scores but also reduce the training time. They require approximately the same training time as Under and are faster than other methods. Considering both classification performance and training time, they are better than all other compared methods.
The results on “hard” tasks show that Cascade is inferior to Easy. The way Cascade explores the majority class examples might be responsible for this observation. In Cascade, the majority training set of Hi+1 is produced by Hi. Such a supervised cascading way of sampling might suffer from overfitting. In other words, the correctly predicted majority class examples that have been filtered out may be useful [27]. In particular, some examples that are deemed redundant and discarded in earlier rounds may be helpful in some later rounds, after some other examples have been discarded. Note that there are also situations in which Cascade is preferred. From the results on “easy” tasks, we can see that Cascade has higher AUC, F-measure, and G-mean than Easy on almost all data sets. This suggests that Cascade can focus on more useful data. In addition, note that Cascade is more favorable than Easy on data sets balance and wpbc. Both of these data sets have a very small minority class. In fact, if the number of
examples in a class is very small, there is a significant chance that the examples will scatter around broadly. It is difficult to get a representative subset by using undersampling alone. Focusing on more informative examples may be particularly helpful in this case. Moreover, Cascade is more suitable for highly imbalanced problems. For example, in the face detection problem described in [41], there are 5000 positive examples and 2284 million negative ones. The independent random sampling strategy of Easy requires T , the number of subsets, to be very large in order to catch all the information in N . Furthermore, the number of subsets is hard to decide since no prior information is available. Thus, Easy is computationally infeasible for this problem. However, for Cascade, it is much easier to set the iteration number since it is reasonable to set fp rate around 0.5. Therefore, T = 20 is sufficient for the face detection problem, since log2(2.284 × 109/5000) ≈ 19 (assuming a false positive rate of 0.5).

E. Analysis of the Ensemble Strategy
As stated earlier, since minority class examples are used to train each weak classifier in the proposed method, stacking these classifiers may cause overfitting when the number of minority class examples is limited. To verify this, the 16 data sets in Table II were used to compare stacking with the ensemble strategy used in Easy and Cascade.
The AUC values are summarized in Table XIII. Similar to the experiments in the previous section, the 16 data sets are divided into groups based on the performance of AdaBoost. When Cascade is used on “easy” tasks, stacking is inferior to the original ensemble strategy on three out of six data sets, while it is superior on only one data set. However, the difference between the two strategies is small. The same observation holds for Easy. On “hard” tasks, the performance of Cascade dominates that of stacking on all data sets. As for Easy, there is only one data set on which stacking is better. Generally speaking, there are significant differences between the performance of stacking and the current ensemble strategy used in our proposed methods.
Therefore, stacking is not very suitable for the case when minority class examples are used in each weak classifier. In such a case, stacking may cause overfitting. This is probably a major reason for Chan to be inferior to Easy.

F. Additional Remarks
We have the following remarks regarding the results in AUC, F-measure, and G-mean on both “easy” and “hard” tasks.
1) The proposed methods EasyEnsemble and BalanceCascade are more robust than many other class-imbalance learning methods. When class imbalance is not harmful, they do not cause serious degeneration of performance. When class imbalance is indeed harmful, they are better than almost all other methods we have compared with. 2) Class imbalance is not harmful for some tasks, and applying class-imbalance learning methods in such cases may lead to performance degeneration. A consequence of
this observation is that class-imbalance learning methods should only be applied to tasks which suffer from class imbalance. For this purpose, we need to develop some methods to judge whether a task suffers from class imbalance or not, before applying class-imbalance learning methods to it. 3) We observed that on tasks which do not suffer from class imbalance, AdaBoost and Bagging can improve the performance of decision trees significantly, while on tasks which suffer from class imbalance, they could not help and sometimes even deteriorate the performance. This might give us some clues on judging whether a task suffers from class imbalance or not, which will be studied in the future.

V. CONCLUSION
This paper extends our preliminary work [26] which proposed two algorithms EasyEnsemble and BalanceCascade for class-imbalance learning. Both algorithms are designed to utilize the majority class examples ignored by undersampling, while, at the same time, keeping its fast training speed. Both algorithms sample multiple subsets of the majority class, train an ensemble from each of these subsets, and combine all weak classifiers in these ensembles into a final output. Both algorithms make better use of the majority class than undersampling, since multiple subsets contain more information than a single one. The main difference is that EasyEnsemble samples independent subsets, while BalanceCascade uses trained classifiers to guide the sampling process for subsequent classifiers. Both algorithms have approximately the same training time as that of undersampling when the same number of weak classifiers is used.
Empirical results suggest that for problems on which ordinary methods achieve high AUC (e.g., ≥ 0.95), class-imbalance learning is not helpful. However, the proposed methods can be used to reduce training time. For problems where classimbalance learning methods really help, both EasyEnsemble and BalanceCascade have higher AUC, F-measure, and G-mean than almost all other compared methods, and the former is superior than the latter. However, since BalanceCascade removes correctly classified majority class examples in each iteration, it will be more efficient on highly imbalanced data sets. In addition, the comparison of Chan and our proposed methods reveals that it is not necessary to use all examples in the majority class.
In the current version of the proposed methods, we use αi,j returned by the weak learner directly. Further improvements are possible by learning αi,j , as shown in [22] and [41]. Note that both EasyEnsemble and BalanceCascade are ensemble methods. Therefore, while they provide strong generalization ability, they also inherit the weaknesses of ensemble methods. An apparent weakness is the lack of comprehensibility. Even when the base classifiers are comprehensible symbolic learners, ensembles are still black boxes. There are some research on this problem [44]–[46], and it is possible to use those research outputs to enhance the comprehensibility of EasyEnsemble and BalanceCascade.

ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers and the associate editor for their helpful comments and suggestions.

References
[1]G. Batista,R.C. Prati,M.C. MonardA study of the behavior of several methods for balancing machine learning training dataACM SIGKDD Explor. Newslett., vol. 6, no. 1, pp. 20–29, Jun. 2004.2004
[2]E. Bauer,R. KohaviAn empirical comparison of voting classification algorithms: Bagging, boosting, and variantsMach. Learn., vol. 36, no. 1/2, pp. 105–139, Jul./Aug. 1999.1999
[3]A.P. BradleyThe use of the area under the ROC curve in the evaluation of machine learning algorithmsPattern Recognit., vol. 30, no. 6, pp. 1145–1159, Jul. 1997.1997
[4]L. BreimanBagging predictorsMach. Learn., vol. 24, no. 2, pp. 123– 140, Aug. 1996.1996
[5]L. BreimanRandom forestMach. Learn., vol. 45, no. 1, pp. 5–32, Oct. 2001.2001
[6]L. Breiman,J. Friedman,R.A. Olshen,C.J. StoneClassification and Regression TreesBoca Raton, FL: CRC Press,1984
[7]P.K. Chan,S.J. StolfoToward scalable learning with non-uniform class and cost distributions: A case study in credit card fraud detectionProc. 4th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining, New York, 1998, pp. 164–168.1998
[8]N.V. Chawla,K.W. Bowyer,L.O. Hall,W.P. KegelmeyerSMOTE: Synthetic minority over-sampling techniqueJ. Artif. Intell. Res., vol. 16, pp. 321–357, 2002.2002
[9]N.V. Chawla,N. Japkowicz,A. KolczEditorial: Special issue on learning from imbalanced data setsACM SIGKDD Explor. Newslett., vol. 6, no. 1, pp. 1–6, Jun. 2004.2004
[10]N.V. Chawla,A. Lazarevic,L.O. Hall,K.W. BowyerSMOTE- Boost: Improving prediction of the minority class in boostingProc. 7th Eur. Conf. Principles Pract. Knowl. Discov. Databases, Cavtat- Dubrovnik, Croatia, 2003, pp. 107–119.2003
[11]C. Chen,A. Liaw,L. BreimanUsing random forest to learn imbalanced dataDept. Statistics, Univ. California, Berkeley, CA, Tech. Rep. 666, 2004.2004
[12]P. DomingosMetaCost: A general method for making classifiers costsensitiveProc. 5th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining, San Diego, CA, 1999, pp. 155–164.1999
[13]C. Drummond,R.C. HolteC4.5, class imbalance, and cost sensitivity: Why under-sampling beats over-samplingProc. Working Notes ICML Workshop Learn. Imbalanced Data Sets, Washington DC, 2003.2003
[14]C. ElkanThe foundations of cost-sensitive learningProc. 17th Int. Joint Conf. Artif. Intell., Seattle, WA, 2001, pp. 973–978.2001
[15]W. Fan,S.J. Stolfo,J. Zhang,P.K. ChanAdaCost: Misclassification cost-sensitive boostingProc. 16th Int. Conf. Mach. Learn.. Bled, Slovenia, 1999, pp. 97–105.1999
[16]T. FawcettROC graphs: Notes and practical considerations for researchersHP Labs, Palo Alto, CA, Tech. Rep. HPL-2003-4, 2003.2003
[17]J.H. FriedmanStochastic gradient boostingComput. Stat. Data Anal., vol. 38, no. 4, pp. 367–378, Feb. 2002.2002
[18]K. FukunagaIntroduction to Statistical Pattern RecognitionNew York: Academic,1990
[19]H. Guo,H.L. ViktorLearning from imbalanced data sets with boosting and data generation: The data boost-IM approachACM SIGKDD Explor. Newslett., vol. 6, no. 1, pp. 30–39, 2004.2004
[20]K. Huang,H. Yang,I. King,M.R. LyuLearning classifiers from imbalanced data based on biased minimax probability machineProc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog., Washington DC, 2004, pp. 558–563.2004
[21]G.J. Karakoulas,J. Shawe-TaylorOptimizing classifiers for imbalanced training setsProc. Adv. Neural Inf. Process. Syst. 11, 1999, pp. 253–259. 550 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 2, APRIL 20091999
[22]M. Kubat,S. MatwinAddressing the curse of imbalanced training sets: One-sided selectionProc. 14th Int. Conf. Mach. Learn., Nashville, TN, 1997, pp. 179–186.1997
[23]X.-Y. Liu,J. Wu,Z.-H. ZhouExploratory under-sampling for classimbalance learningProc. 6th IEEE Int. Conf. Data Mining, Hong Kong, 2006, pp. 965–969.2006
[24]F.-Z. MarcosOn the usefulness of almost-redundant information for pattern recognitionProc. Summer School Neural Netw., 2004, pp. 357–364.2004
[25]H. Masnadi-Shirazi,N. VasconcelosAsymmetric boostingProc. 24th Int. Conf. Mach. Learn., Corvallis, OR, 2007, pp. 609–619.2007
[26]R.E. SchapireA brief introduction to boostingProc. 16th Int. Joint Conf. Artif. Intell., Stockholm, Sweden, 1999, pp. 1401–1406.1999
[27]R.E. Schapire,Y. Singer,A. SinghalBoosting and Rocchio applied to text filteringProc. 4th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining, 1998, pp. 215–223.1998
[28]K.M. TingAn empirical study of MetaCost using boosting algorithmsProc. 11th Eur. Conf. Mach. Learn., Barcelona, Spain, 2000, pp. 413–425.2000
[29]K.M. Ting,I.H. WittenIssues in stacked generalizationJ. Artif. Intell. Res., vol. 10, pp. 271–289, 1999.1999
[30]P. Viola,M. JonesFast and robust classification using asymmetric AdaBoost and a detector cascadeProc. Adv. Neural Inf. Process. Syst. 14, T. G. Dietterich, S. Becker and Z. Ghahramani, Eds. Cambridge, MA: MIT, 2002, pp. 1311–1318.2002
[31]P. Viola,M. JonesRobust real-time face detectionInt. J. Comput. Vis., vol. 57, no. 2, pp. 137–154, May 2004.2004
[32]G.I. WebbMultiBoosting: A technique for combining boosting and waggingMach. Learn., vol. 40, no. 2, pp. 159–196, Aug. 2000.2000
[33]G.I. Webb,Z. ZhengMultistrategy ensemble learning: Reducing error by combining ensemble learning techniquesIEEE Trans. Knowl. Data Eng., vol. 16, no. 8, pp. 980–991, Aug. 2004.2004
[34]G.M. WeissMining with rarity: A unifying frameworkACM SIGKDD Explor. Newslett., vol. 6, no. 1, pp. 7–19, Jun. 2004.2004
[35]G.M. Weiss,F. ProvostLearning when training data are costly: The effect of class distributions on tree inductionJ. Artif. Intell. Res., vol. 19, pp. 315–354, 2003.2003
[36]I.H. Witten,E. FrankData Mining: Practical Machine Learning Tools and Techniques2005
[37]D.H. WolpertStacked generalizationNeural Netw., vol. 5, no. 2, pp. 241–260, 1992.1992
[38]J. Wu,S.C. Brubaker,M.D. Mullin,J.M. RehgFast asymmetric learning for cascade face detectionIEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 3, pp. 369–382, Mar. 2008.2008
[39]Y. Yu,Z.-H. Zhou,K.M. TingCocktail ensemble for regressionProc. 7th IEEE Int. Conf. Data Mining, Omeha, NE, 2007, pp. 721–726.2007
[40]B. Zadrozny,J. Langford,N. AbeCost-sensitive learning by costproportionate example weightingProc. 3rd IEEE Int. Conf. Data Mining, Melbourne, FL, 2003, pp. 435–442.2003
[41]Z.-H. Zhou,Y. JiangMedical diagnosis with C4.5 rule preceded by artificial neural network ensembleIEEE Trans. Inf. Technol. Biomed., vol. 7, no. 1, pp. 37–42, Mar. 2003.2003
[42]Z.-H. Zhou,Y. JiangNeC4.5: Neural ensemble based C4.5IEEE Trans. Knowl. Data Eng., vol. 16, no. 6, pp. 770–773, Jun. 2004.2004
[43]Z.-H. Zhou,Y. Jiang,S.-F. ChenExtracting symbolic rules from trained neural network ensemblesAI Commun., vol. 16, no. 1, pp. 3–15, May 2003.2003
