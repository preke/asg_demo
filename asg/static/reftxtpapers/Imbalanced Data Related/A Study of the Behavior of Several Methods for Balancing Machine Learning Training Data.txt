A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data
Gustavo E. A. P. A. Batista,Ronaldo C. Prati,Maria Carolina Monard
mcmonard}@icmc.usp.br

1. INTRODUCTION
Most learning systems usually assume that training sets used
for learning are balanced. However, this is not always the case in real world data where one class might be represented by a large number of examples, while the other is represented by only a few. This is known as the class imbalance problem and is often reported as an obstacle to the induction of good classifiers by Machine Learning (ML) algorithms. Generally, the problem of imbalanced data sets occurs when one class represents a circumscribed concept, while the other class represents the counterpart of that concept, so that examples from the counterpart class heavily outnumber examples from the positive class. This sort of data is found, for example, in medical record databases regarding a rare disease, were there is a large number of patients who do not have that disease; continuous fault-monitoring tasks where non-faulty examples heavily outnumber faulty examples, and others.
In recent years, there have been several attempts at dealing with the class imbalance problem in the field of Data Mining and Knowledge Discovery in Databases, to which ML is a substantial contributor. Related papers have been published in the ML literature aiming to overcome this problem. The ML community seems to agree on the hypothesis that the imbalance between classes is the major obstacle in inducing classifiers in imbalanced domains. However, it has also been observed that in some domains, for instance the Sick data set [3], standard ML algorithms are capable of inducing good classifiers, even using highly imbalanced training sets. This shows that class imbalance is not the only problem responsible for the decrease in performance of learning algorithms.
In [18] we developed a systematic study aiming to question whether class imbalances hinder classifier induction or whether these deficiencies might be explained in other ways. Our study was developed on a series of artificial data sets in order to fully control all the variables we wanted to analyze. The results of our experiments, using a discrimination-based inductive scheme, suggested that the problem is not solely caused by class imbalance, but is also related to the degree of data overlapping among the classes.
The results obtained in this previous work motivated the proposition of two new methods to deal with the problem of learning in the presence of class imbalance. These methods ally a known over-sampling method, namely Smote [5], with two data cleaning methods: Tomek links [22] and Wilson’s Edited Nearest Neighbor Rule [24]. The main motivation behind these methods is not only to balance the training
Sigkdd Explorations. Volume 6, Issue 1 - Page 20
data, but also to remove noisy examples lying on the wrong side of the decision border. The removal of noisy examples might aid in finding better-defined class clusters, therefore, allowing the creation of simpler models with better generalization capabilities.
In addition, in this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. We concluded that over-sampling methods are able to aid in the induction of classifiers that are more accurate than those induced from under-sampled data sets. This result seems to contradict results previously published in the literature. Two of our proposed methods performed well in practice, in particular for data sets with a small number of positive examples. It is worth noting that Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods.
The remainder of the paper is organized as follows: Section 2 discusses why learning from imbalanced data sets might be a difficult task. Section 3 describes the drawbacks of using accuracy (or error rate) to measure the performance of classifiers and discusses alternative metrics. Section 4 presents the methods employed in the experimental evaluation, including the three methods proposed by the authors. Section 5 discusses the methodology used in the experiments, as well as the results achieved. Finally, Section 6 presents the conclusions and outlines future research.

2. WHY LEARNING FROM IMBALANCED DATA SETS MIGHT BE DIFFICULT
Learning from imbalanced data sets is often reported as being a difficult task. In order to better understand this problem, imagine the situation illustrated in Figure 1. In Fig. 1(a) there is a large imbalance between the majority class (-) and the minority class (+), and the data set presents some degree of class overlapping. A much more comfortable situation for learning is represented in Fig. 1(b), where the classes are balanced with well-defined clusters.
In a situation similar to the one illustrated in Fig. 1(a), spare cases from the minority class may confuse a classifier like k-Nearest Neighbor (k-NN). For instance, 1-NN may incorrectly classify many cases from the minority class because the nearest neighbors of these cases are examples belonging to the majority class. In a situation where the imbalance is very high, the probability of the nearest neighbor of a minority class case is a case of the majority class is likely to be high, and the minority class error rate will tend to have high values, which is unacceptable.
Figure 1: Many negative cases against some spare positive cases (a) balanced data set with well-defined clusters (b).
Decision trees also experience a similar problem. In the presence of class overlapping, decision trees may need to create many tests to distinguish the minority class cases from majority class cases. Pruning the decision tree might not necessarily alleviate the problem. This is due to the fact that pruning removes some branches considered too specialized, labelling new leaf nodes with the dominant class on this node. Thus, there is a high probability that the majority class will also be the dominant class of those leaf nodes.

3. ON EVALUATING CLASSIFIERS IN IMBALANCED DOMAINS
The most straightforward way to evaluate the performance of classifiers is based on the confusion matrix analysis. Table 1 illustrates a confusion matrix for a two class problem having positive and negative class values. From such a matrix it is possible to extract a number of widely used metrics for measuring the performance of learning systems, such as Error Rate, defined as Err = FP+FN
TP+FN+FP+TN and
Accuracy, defined as Acc = TP+TN TP+FN+FP+TN = 1− Err.
However, when the prior class probabilities are very different, the use of such measures might lead to misleading conclusions. Error rate and accuracy are particularly suspicious performance measures when studying the effect of class distribution on learning since they are strongly biased to favor the majority class. For instance, it is straightforward to create a classifier having an accuracy of 99% (or an error rate of 1%) in a domain where the majority class proportion corresponds to 99% of the examples, by simply forecasting every new example as belonging to the majority class.
Another fact against the use of accuracy (or error rate) is that these metrics consider different classification errors to be equally important. However, highly imbalanced problems generally have highly non-uniform error costs that favor the minority class, which is often the class of primary interest. For instance, a sick patient diagnosed as healthy might be a fatal error while a healthy patient diagnosed as sick is considered a much less serious error since this mistake can be corrected in future exams.
Finally, another point that should be considered when studying the effect of class distribution on learning systems is that the class distribution may change. Consider the confusion matrix shown in Table 1. Note that the class distribution (the proportion of positive to negative examples) is the relationship between the first and second lines. Any performance metric that uses values from both lines will be inherently sensitive to class skews. Metrics such as accuracy and error rate use values from both lines of the confusion matrix. As class distribution changes, these measures will change as well, even if the fundamental classifier performance does not.
All things considered, it would be more interesting if we use a performance metric that disassociates the errors (or hits) that occurred in each class. From Table 1 it is possible to derive four performance metrics that directly measure the classification performance on positive and negative classes independently:
Sigkdd Explorations. Volume 6, Issue 1 - Page 21
False negative rate FNrate = FN
TP+FN is the percentage
of positive cases misclassified as belonging to the negative class;
False positive rate FPrate = FP
FP+TN is the percentage
of negative cases misclassified as belonging to the positive class;
True negative rate TNrate = TN
FP+TN is the percentage
of negative cases correctly classified as belonging to the negative class;
True positive rate TPrate = TP
TP+FN is the percentage
of positive cases correctly classified as belonging to the positive class.
These four performance measures have the advantage of being independent of class costs and prior probabilities. The aim of a classifier is to minimize the false positive and negative rates or, similarly, to maximize the true negative and positive rates. Unfortunately, for most real world applications there is a tradeoff between FNrate and FPrate, and similarly between TNrate and TPrate. ROC (Receiver Operating Characteristic) graphs [19] can be used to analyze the relationship between FNrate and FPrate (or TNrate and TPrate) for a classifier. Some classifiers, such as the Näıve Bayes classifier or some Neural Networks, yield a score that represents the degree to which an example is a member of a class. Such ranking can be used to produce several classifiers, by varying the threshold of an example pertaining to a class. Each threshold value produces a different point in the ROC space. These points are linked by tracing straight lines through two consecutive points to produce a ROC curve. For decision trees, we could use the class distributions at each leaf as a score or, as proposed in [9], by ordering the leaves by their positive class accuracy and producing several trees by re-labelling the leaves, one at a time, from all forecasting negative class to all forecasting positive class in the positive accuracy order.
A ROC graph characterizes the performance of a binary classification model across all possible trade-offs between the classifier sensitivity (TPrate) and false alarm (FPrate). ROC graphs are consistent for a given problem, even if the distribution of positive and negative examples is highly skewed. A ROC analysis also allows the performance of multiple classification functions to be visualized and compared simultaneously. The area under the ROC curve (AUC) represents the expected performance as a single scalar. The AUC has a known statistical meaning: it is equivalent to the Wilconxon test of ranks, and is equivalent to several other statistical measures for evaluating classification and ranking models [10]. In this work, we use the method proposed in [9] with Laplace correction for measuring the leaf accuracy to produce ROC curves. We also use the AUC as the main method for assessing our experiments.

4. METHODS
This section describes the notation used as well as our implementation of the k-NN algorithm, since this algorithm plays an important role in the behavior of the methods considered. Finally, an explanation of each balancing method is given.

4.1 Notation
In supervised learning, the inducer is fed with a data set E = {E1, E2, . . . EN}, in which each example Ei ∈ E has
an associated label. This label defines the class the example belongs to. Each example Ei ∈ E is a tuple Ei = (~xi, yi) in which ~xi is a vector of feature (or attribute) values of the example Ei, and yi is its class value. The objective of a supervised learning algorithm is to induce a general mapping of vectors ~x to values y. Thus, the learning system aims to construct a model y = f(~x), of an unknown function f , also known as concept function, that enables one to predict the y values for previously unseen examples. In practice, learning systems are able to induce a function h that approximates f , i.e., h(~x) ≈ f(~x). In this case, h is called the hypothesis of the concept function f .
In Table 2 a data set with N examples and M attributes is presented. Columns (A1, . . . AM ) represent the attributes and lines (E1, . . . EN ) represent the examples. For instance, line i in Table 2 refers to the ith example and the entry xij refers to the value of the jth attribute, Aj , of example i. For classification problems, the class-attribute Y is a qualitative attribute that may assume a set of NCl discrete values C = {C1, C2, . . . CNCl}.
As stated earlier, in this work we consider two-class problems where C1 = + represents the circumscribed concept class and C2 = − represents the counterpart of that concept. Furthermore, the examples from the negative class outnumber the examples from the positive class.
4.2 Our Implementation of the k-NN Algorithm
Several research papers use the Euclidean distance as a distance metric for the k-NN algorithm. However, this distance function might not be appropriate when the domain presents qualitative attributes. For those domains, the distance for qualitative attributes is usually calculated using the overlap function, in which the value 0 (if two examples have the same value for a given attribute) or the value 1 (if these values differ) are assigned. Our implementation of the k-NN algorithm uses the Heterogeneous Value Difference Metric (HVDM) distance function [25]. This distance function uses the Euclidean distance for quantitative attributes and the VDM distance [21] for qualitative attributes. The VDM metric provides a more appropriate distance function for qualitative attributes if compared with the overlap metric, since the VDM metric considers the classification similarity for each possible value of a qualitative attribute to calculate the distances between these values.
Another refinement to the basic k-NN algorithm is to weigh the contribution of each of the k neighbors according to their distance to the query example Eq, giving greater weight to closer neighbors. The vote of each neighbor is weighed according to the inverse square of its distance from Eq [17]. Given Ê = {Ê1, Ê2, . . . Êk}, the set of k nearest neighbors of Eq, according to the distance function d the final classification is given by Equation 1.
Sigkdd Explorations. Volume 6, Issue 1 - Page 22
h(Eq) = arg max c∈C k∑ i=1 ωiδ(c, f(Êi)) ωi = 1 d(Eq, Êi)2 (1)
and δ(a, b) = 1 if a = b otherwise δ(a, b) = 0.
As the balancing methods make severe use of distance computations, we implemented an indexing structure namely M-tree [6] to speed up the execution of k-NN queries. Mtree only considers relative distances of examples rather than their absolute positions in a multi-dimensional space, to organize and partition the metric space. In a metric space, example proximity is only defined by a distance function that satisfies the positivity, symmetry and triangle inequality postulates.

4.3 Methods
In this work, we evaluate ten different methods of under and over-sampling to balance the class distribution on training data. Two of these methods, random over-sampling and random under-sampling, are non-heuristic methods that were initially included in this evaluation as baseline methods. The evaluated methods are described next.
Random over-sampling is a non-heuristic method that aims to balance class distribution through the random replication of minority class examples. Random under-sampling is also a non-heuristic method that aims to balance class distribution through the random elimination of majority class examples.
Several authors agree that random over-sampling can increase the likelihood of occurring overfitting, since it makes exact copies of the minority class examples. In this way, a symbolic classifier, for instance, might construct rules that are apparently accurate, but actually cover one replicated example. On the other hand, the major drawback of random under-sampling is that this method can discard potentially useful data that could be important for the induction process. The remainder balancing methods use heuristics in order to overcome the limitations of the non-heuristic methods.
Tomek links Tomek links [22] can be defined as follows: given two examples Ei and Ej belonging to different classes, and d(Ei, Ej) is the distance between Ei and Ej . A (Ei, Ej) pair is called a Tomek link if there is not an example El, such that d(Ei, El) < d(Ei, Ej) or d(Ej , El) < d(Ei, Ej). If two examples form a Tomek link, then either one of these examples is noise or both examples are borderline. Tomek links can be used as an under-sampling method or as a data cleaning method. As an under-sampling method, only examples belonging to the majority class are eliminated, and as a data cleaning method, examples of both classes are removed. Condensed Nearest Neighbor Rule Hart’s Condensed Nearest Neighbor Rule (CNN) [11] is used to find a con-
sistent subset of examples. A subset Ê ⊆ E is consistent with E if using a 1-nearest neighbor, Ê correctly classifies the examples in E. An algorithm to create a subset Ê from E as an under-sampling method is the following [14]: First, randomly draw one majority class example and all examples from the minority class and put these examples in Ê. Afterwards, use a 1-NN over
the examples in Ê to classify the examples in E. Every misclassified example from E is moved to Ê. It is important to note that this procedure does not find the smallest consistent subset from E. The idea behind this implementation of a consistent subset is to eliminate the examples from the majority class that are distant from the decision border, since these sorts of examples might be considered less relevant for learning. One-sided selection One-sided selection (OSS) [14] is an under-sampling method resulting from the application of Tomek links followed by the application of CNN. Tomek links are used as an under-sampling method and removes noisy and borderline majority class examples. Borderline examples can be considered “unsafe” since a small amount of noise can make them fall on the wrong side of the decision border. CNN aims to remove examples from the majority class that are distant from the decision border. The remainder examples, i.e. “safe” majority class examples and all minority class examples are used for learning. CNN + Tomek links This is one of the methods proposed in this work. It is similar to the one-sided selection, but the method to find the consistent subset is applied before the Tomek links. Our objective is to verify its competitiveness with OSS. As finding Tomek links is computationally demanding, it would be computationally cheaper if it was performed on a reduced data set. Neighborhood Cleaning Rule Neighborhood Cleaning Rule (NCL) [15] uses the Wilson’s Edited Nearest Neighbor Rule (ENN) [24] to remove majority class examples. ENN removes any example whose class label differs from the class of at least two of its three nearest neighbors. NCL modifies the ENN in order to increase the data cleaning. For a two-class problem the algorithm can be described in the following way: for each example Ei in the training set, its three nearest neighbors are found. If Ei belongs to the majority class and the classification given by its three nearest neighbors contradicts the original class of Ei, then Ei is removed. If Ei belongs to the minority class and its three nearest neighbors misclassify Ei, then the nearest neighbors that belong to the majority class are removed. Smote Synthetic Minority Over-sampling Technique (Smote) [5] is an over-sampling method. Its main idea is to form new minority class examples by interpolating between several minority class examples that lie together. Thus, the overfitting problem is avoided and causes the decision boundaries for the minority class to spread further into the majority class space. Smote + Tomek links Although over-sampling minority class examples can balance class distributions, some other problems usually present in data sets with skewed class distributions are not solved. Frequently, class clusters are not well defined since some majority class examples might be invading the minority class space. The opposite can also be true, since interpolating minority class examples can expand the minority class clusters, introducing artificial minority class examples too deeply in the majority class space. Inducing a classifier under such a situation can lead to overfitting. In order to create better-defined class clusters, we propose applying Tomek links to the over-sampled training set as a data
Sigkdd Explorations. Volume 6, Issue 1 - Page 23
Smote + ENN The motivation behind this method is similar to Smote + Tomek links. ENN tends to remove more examples than the Tomek links does, so it is expected that it will provide a more in depth data cleaning. Differently from NCL which is an under-sampling method, ENN is used to remove examples from both classes. Thus, any example that is misclassified by its three nearest neighbors is removed from the training set.

5. EXPERIMENTAL EVALUATION
The main objective of our research is to compare several balancing methods published in the literature, as well as the three proposed methods, in order to verify whether those methods can effectively deal in practice with the problem of class imbalance. To make this comparison, we have selected thirteen data sets from UCI [3] which have different degrees of imbalance. Table 3 summarizes the data employed in this study. For each data set, it shows the number of examples (#Examples), number of attributes (#Attributes), number of quantitative and qualitative attributes, class attribute distribution and the majority class error. For data sets having more than two classes, we chose the class with fewer examples as the positive class, and collapsed the remainder as the negative class. As the Letter and Splice data sets have a similar number of examples in the minority classes, we created two data sets with each of them: Letter-a and Letter-vowel, Splice-ie and Splice-ei.
In our experiments, we used release 8 of the C4.5 symbolic learning algorithm to induce decision trees [20]. Firstly,
we ran C4.5 over the original (imbalanced) data sets and calculated the AUC for each data set using 10-fold crossvalidation. The results obtained in this initial experiment are shown in a graph in Figure 3.
Figure 3 plots the proportion of negative/positive examples versus the mean AUC values for the original data sets. If class imbalances can systematically hinder the performance of imbalanced data sets, then it would be expected that AUC decreases for highly imbalanced data sets. However, in spite of a large degree of imbalance the data sets Letter-a and Nursery obtained almost 100% AUC.
The results obtained in the UCI data sets seem to be compatible with previous work of the authors [18] conducted on a series of experiments with artificial domains, in which we varied the degree of overlapping between the classes. It was concluded that class imbalance, by itself, does not seem to be a problem, but when allied to highly overlapped classes, it can significantly decrease the number of minority class examples correctly classified. Domains with non-overlapping classes do not seem to be problematic for learning no matter the degree of imbalance. Moreover, in [12] Japkowicz performed several experiments on artificial data sets and concluded that class imbalances do not seem to systematically cause performance degradation. She concludes that the imbalance problem is a relative problem depending on both the complexity of the concept1 and the overall size of the training set.
The relationship between training set size and improper classification performance for imbalanced data sets seems to be that on small imbalanced data sets the minority class is poorly represented by an excessively reduced number of examples, that might not be sufficient for learning, especially when a large degree of class overlapping exists and the class is further divided into subclusters. For larger data sets, the effect of these complicating factors seems to be reduced, as the minority class is better represented by a larger number of examples. This trend is confirmed by the graph shown in Figure 4 which shows how the AUC is affected by the number of positive training examples in the data sets.
In a second stage, the over and under-sampling methods described in Section 4 were applied to the original data sets.
1Where the “concept complexity” corresponds to the number of subclusters into which the classes are subdivided.
Sigkdd Explorations. Volume 6, Issue 1 - Page 24
Smote, Random over-sampling, Random under-sampling and CNN methods have internal parameters that allow the user to set up the resulting class distribution obtained after the application of these methods. We decided to add/remove examples until a balanced distribution was reached. This decision is motivated by the results presented in [23], in which it is shown that when AUC is used as performance measure, the best class distribution for learning tends to be near the balanced class distribution.
The results obtained in our experiments are summarized in Tables 4 and 5. Table 4 shows the performance results for the original, as well as for the over-sampled data sets. Table 5 shows the results obtained for the under-sampled data sets. The performance results are reported in terms of AUC. The numbers between brackets are the corresponding standard deviations. As stated earlier, these results were obtained with 10-fold cross-validation. AUCs were measured over decision trees pruned with the default C4.5 pruning parameter setting (25% confidence level) and over unpruned decision trees.
Although some research papers state that pruning might be helpful with imbalanced data sets in some circumstances [4], other papers indicate that when target misclassification costs or class distributions are unknown, then pruning should be avoided [26; 2]. One reason to avoid pruning is that most pruning schemes, including the one used by C4.5, attempt to minimize the overall error rate. These pruning schemes
can be detrimental to the minority class, since reducing the error rate in the majority class, which stands for most of the examples, would result in a greater impact over the overall error rate. On the other hand, it still seems to be an openended question if pruning can lead to a performance improvement for decision trees grown over artificially balanced data sets. One argument against pruning is that if pruning is allowed to execute under these conditions, the learning system would prune based on false assumption, i.e., that the test set distribution matches the training set distribution [23].
Figure 5 shows a comparison of the effect of pruning decision trees on the original and balanced data sets. Line x = y represents when both pruned and unpruned decision trees obtain the same AUC. Plots above this line represent that unpruned decision trees obtained better results, and plots under this line the opposite. Figure 5 clearly shows that pruning rarely leads to an improvement in AUC for the original and balanced data sets.
In Tables 4 and 5 the results in bold indicate the best AUCs obtained for each data set considering pruned and unpruned decision trees independently. Note that all best results were obtained by the over-sampling methods. In order to facilitate the analysis of the results, Tables 6 and 7 present these results as a ranking of methods for pruned and unpruned decision trees respectively. The over-sampling methods are
Sigkdd Explorations. Volume 6, Issue 1 - Page 25
highlighted with a light gray color, and the results obtained with the original data sets with a dark gray color. Note that, in general, over-sampling methods are better ranked than the under-sampling methods. Hsu’s Multiple Comparison with the Best (MCB) test was performed in order to verify if significant differences exist, with 95% confidence level, among the best ranked method and the remaining methods. The results are also summarized in Tables 6 and 7, where methods marked with an asterisk obtained statistically inferior results when compared to the top ranked method.
Conversely, over-sampling methods in general and Random over-sampling in particular are well-ranked among the remainder methods. This result seems to diverge with several papers previously published in the literature. Drummond and Holte [8] report that when using C4.5’s default settings, over-sampling is surprisingly ineffective, often producing little or no change in performance in response to modifications of misclassification costs and class distribution. Moreover, they note that over-sampling prunes less and therefore generalizes less than under-sampling, and that a modification of the C4.5’s parameter settings to increase the influence of pruning and other overfitting avoidance factors can reestablish the performance of over-sampling. In our experiments, Random over-sampling did not produce overfitted decision trees even when these trees were left unpruned, as it can be confirmed by the higher AUC values obtained by this method for unpruned trees. In addition, under-sampling methods did not perform as well as over-sampling methods, even when heuristics to remove cases were considered in under-sampling.
Moreover, Domingos [7] reports that concerning concept learning problems, C4.5 Rules produces lower cost classifiers using under-sampling than over-sampling. Ling and Li [16] compare over and under-sampling for boosted C4.5 and report that under-sampling produces better lift index,
although extreme over-sampling performs almost as well. On the other hand, Japkowicz and Stephen [13] compare several methods of over and under-sampling on a series of artificial data sets and conclude that over-sampling is more effective than under-sampling at reducing error rate.
In our opinion, the good results obtained by over-sampling are not completely unexpected. As stated before, it seems that the loss of performance is directly related to the lack of minority class examples in conjunction with other complicating factors. Over-sampling is the class of methods that most directly attack the problem of the lack of minority class examples.
It is worth mentioning that two of our proposed methods, Smote + Tomek and Smote + ENN are generally ranked among the best for data sets with a small number of positive examples. Considering only data sets with less than 100 positive examples (in our experiments there are 6 of them: Flag, Glass, Post-operative, New-thyroid, E.Coli and Haberman), at least one of the proposed methods provided meaningful results for all 6 data sets for pruned trees – Table 6, and for 5 of the 6 data sets for unpruned trees – Table 7. This seems to indicate that these methods could be appropriate in domains having such conditions.
Since over-sampling methods, as well as unpruned decision trees obtained very good performance results, further analysis will focus on these results. In addition to classifier performance results, we also attempted to measure the syntactic complexity of the induced models. Syntactic complexity is given by two main parameters: the mean number of induced rules (branches) and the mean number of conditions per rule. Tables 8 and 9 respectively show the mean number of induced rules and the mean number of condition per rule for the over-sampling methods and the original data sets with unpruned decision trees. The best results are shown in bold, and the best results obtained by an over-sampling
Sigkdd Explorations. Volume 6, Issue 1 - Page 26
method, not considering the results obtained in the original data sets, are highlighted with a light gray color.
Figure 6 shows the results in Table 8 in graphical form,
where it can be observed that over-sampled data sets usually lead to an increase in the number of induced rules if compared to the ones induced with the original data sets. Comparing the mean number of rules obtained with the
Sigkdd Explorations. Volume 6, Issue 1 - Page 27
over-sampled data sets, Random over-sampling and Smote + ENN are the methods that provide a smaller increase in the mean number of rules. It was expected that the application of over-sampling would result in an increase in the mean number of rules, since over-sampling increases the total number of training examples, which usually generates larger decision trees. It can also be considered unexpected that Random over-sampling is competitive with Smote + Tomek and Smote + ENN in the number of induced rules, once Tomek and ENN were applied as data cleaning methods with the objective of eliminating noise examples and thus simplifying the induced decision trees.
The results presented in Table 9 are shown in a graph in
Figure 7 allowing a clearer comparison for the mean number of conditions per rule for the over-sampled data sets. The Smote + ENN method provided very good results. In fact, it was the best ranked in 10 data sets. Furthermore, this method was even able to obtain smaller values than those achieved by decision trees induced from the original data sets in 6 data sets. Moreover, considering only the over-sampled data sets, this method was the best ranked for another 4 data sets.

6. CONCLUSION AND LIMITATIONS
In this work we analyze the behavior of several over and under-sampling methods to deal with the problem of learn-
Sigkdd Explorations. Volume 6, Issue 1 - Page 28
ing from imbalanced data sets. Our results show that the over-sampling methods in general, and Smote + Tomek and Smote + ENN (two of the methods proposed in this work) in particular for data sets with few positive (minority) examples, provided very good results in practice. Moreover, Random over-sampling, frequently considered an unprosperous method provided competitive results with the more complex methods. As a general recommendation, Smote + Tomek or Smote + ENN might be applied to data sets with a small number of positive instances, a condition that is likely to lead to classification performance problems for imbalanced data sets. For data sets with larger number of positive examples, the Random over-sampling method which is computationally less expensive than other methods would produce meaningful results.
It should be noted that allocating half of the training examples to the minority class does not always provide optimal results [23]. We plan to address this issue in future research. Furthermore, some under-sampling methods, such as Tomek links and NCL, that do not originally allow the user to specify the resulting class distribution, must be improved to include this feature. Another natural extension to this work is to analyze the ROC curves obtained from the classifiers. This might provide us with a more in depth understanding of the behavior of balancing and cleaning methods.
Acknowledgements. We wish to thank the anonymous reviewers and Dorival Leão Pinto Júnior for their helpful comments. This research was partially supported by the Brazilian Research Councils CAPES and FAPESP.

7. REFERENCES
[1] Batista, G. E. A. P. A., Bazan, A. L., and Monard, M. C. Balancing Training Data for Automated Annotation of Keywords: a Case Study. In WOB (2003), pp. 35–43.
[2] Bauer, E., and Kohavi, R. An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants. Machine Learning 36 (1999), 105–139.
[3] Blake, C., and Merz, C. UCI Repository of Machine Learning Databases, 1998. http://www.ics.uci.edu/ ~mlearn/MLRepository.html.
[4] Chawla, N. V. C4.5 and Imbalanced Data Sets: Investigating the Effect of Sampling Method, Probabilistic Estimate, and Decision Tree Structure. In Workshop on Learning from Imbalanced Data Sets II (2003).
[5] Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. SMOTE: Synthetic Minority Over-sampling Technique. JAIR 16 (2002), 321–357.
[6] Ciaccia, P., Patella, M., and Zezula, P. M-tree: an Efficient Access Method for Similarity Search in Metric Spaces. In VLDB (1997), pp. 426–435.
[7] Domingos, P. MetaCost: A General Method for Making Classifiers Cost-Sensitive. In KDD (1999), pp. 155– 164.
[8] Drummond, C., and Holte, R. C. C4.5, Class Imbalance, and Cost Sensitivity: Why Under-sampling beats Over-sampling. In Workshop on Learning from Imbalanced Data Sets II (2003).
[9] Ferri, C., Flach, P., and Hernández-Orallo, J. Learning Decision Trees Using the Area Under the ROC Curve. In ICML (2002), pp. 139–146.
[10] Hand, D. J. Construction and Assessment of Classification Rules. John Wiley and Sons, 1997.
[11] Hart, P. E. The Condensed Nearest Neighbor Rule. IEEE Transactions on Information Theory IT-14 (1968), 515–516.
[12] Japkowicz, N. Class Imbalances: Are We Focusing on the Right Issue? In Workshop on Learning from Imbalanced Data Sets II (2003).
[13] Japkowicz, N., and Stephen, S. The Class Imbalance Problem: A Systematic Study. IDA Journal 6, 5 (2002), 429–449.
[14] Kubat, M., and Matwin, S. Addressing the Course of Imbalanced Training Sets: One-sided Selection. In ICML (1997), pp. 179–186.
[15] Laurikkala, J. Improving Identification of Difficult Small Classes by Balancing Class Distribution. Tech. Rep. A-2001-2, University of Tampere, 2001.
[16] Ling, C. X., and Li, C. Data Mining for Direct Mining: Problems and Solutions. In KDD (1998), pp. 73– 79.
[17] Mitchell, T. M. Machine Learning. McGraw-Hill, 1997.
[18] Prati, R. C., Batista, G. E. A. P. A., and Monard, M. C. Class Imbalances versus Class Overlapping: an Analysis of a Learning System Behavior. In MICAI (2004), pp. 312–321. LNAI 2972.
[19] Provost, F. J., and Fawcett, T. Analysis and Visualization of Classifier Performance: Comparison under Imprecise Class and Cost Distributions. In KDD (1997), pp. 43–48.
[20] Quinlan, J. R. C4.5 Programs for Machine Learning. Morgan Kaufmann, CA, 1988.
[21] Stanfill, C., and Waltz, D. Instance-based Learning Algorithms. Communications of the ACM 12 (1986), 1213–1228.
[22] Tomek, I. Two Modifications of CNN. IEEE Transactions on Systems Man and Communications SMC-6 (1976), 769–772.
[23] Weiss, G. M., and Provost, F. Learning When Training Data are Costly: The Effect of Class Distribution on Tree Induction. JAIR 19 (2003), 315–354.
[24] Wilson, D. L. Asymptotic Properties of Nearest Neighbor Rules Using Edited Data. IEEE Transactions on Systems, Man, and Communications 2, 3 (1972), 408–421.
[25] Wilson, D. R., and Martinez, T. R. Reduction Techniques for Exemplar-Based Learning Algorithms. Machine Learning 38, 3 (2000), 257–286.
[26] Zadrozny, B., and Elkan, C. Learning and Making Decisions When Costs and Probabilities are Both Unknown. In KDD (2001), pp. 204–213.Sigkdd Explorations. Volume 6, Issue 1 - Page 29
