Class Probability Estimates are Unreliable for Imbalanced Data (and How to Fix Them)
Byron C. Wallace,Issa J. Dahabreh
dahabreh}@brown.edu

Keywords-probability estimates, class imbalance
I. INTRODUCTION & MOTIVATION
Class probability estimates are intuitively interpretable, useful for estimating expected costs (of classification decisions), and in general offer more granular information than class predictions alone. They are especially useful when decisions have associated costs; we would like to incorporate uncertainty into decisions made in such contexts. Class imbalance is the scenario in which the number of instances from each class is (perhaps extremely) unequal. Imbalance is common in real-world learning tasks, e.g., in text classification and natural language problems [29] and medical applications [7]. In imbalanced scenarios, misclassification costs tend to be asymmetric: incorrectly classifying rare events is usually more costly than making mistakes in the other direction (consider, e.g., cancer screening). The problem of inducing classifiers over imbalanced datasets with asymmetric costs has been noted as key open problem in data mining [27], and has received a great deal of attention from the research community. Indeed, there are at least three surveys of methods for improving classification performance on imbalanced datasets [12], [13], [16].
Despite the attention that classification under imbalance has received, there has been little work investigating the reliability of class membership probability estimates for
imbalanced data. There has, however, been a substantial amount of work investigating attaining probability estimates from supervised learning in general; notably [21] and [28]. This is surprising because it is in such cases that probability estimates could potentially be of most use. In this work we focus on calibrated probability estimators, which transform raw scores from classifiers into probability estimates. We focus on calibrated methods because they are widely used, have desirable theoretical properties [8] and have been shown to achieve superior probability estimation performance [21]. Moreover, calibration is a general strategy that can be used to derive robust probability estimates from any classifier that provides a ‘raw’ output measuring confidence.
We demonstrate that calibrated probability estimators produce consistently biased estimates in imbalanced scenarios. Specifically, while such estimators tend to have good overall calibration, their estimates for minority instances are unreliable: low probabilities are wrongly assigned to truly positive minority examples. Such mistakes are especially problematic given the asymmetric costs common in imbalanced scenarios. Indeed, the motivation for attaining probability estimates is often to classify instances as belonging to the majority class only when we are quite sure of it (this is the basic idea behind cost-sensitive learning [4], [9]). But if the probability estimates for the minority instances are unreliable, then they are effectively useless for this purpose.
It is now well-appreciated that in classification tasks, accuracy is a poor measure of performance for imbalanced data [23]. Alternative metrics that emphasize good performance with respect to both classes are now widely accepted as more appropriate for such cases (e.g., sensitivity/specificity via ROC analysis, the g-mean) [15]. This is analogous to measures of overall calibration being uninformative with respect to probability estimation in imbalanced scenarios. We propose a new metric for measuring probability calibration under imbalance: the stratified Brier score. We use this metric to demonstrate the consistently poor probability estimates that calibrated classifiers provide for minority instances.
One may be tempted to simply use one of the existing techniques for handling imbalance for classification and then induce probability estimates as usual (i.e., via calibration).
1550-4786/12 $26.00 © 2012 IEEE DOI 10.1109/ICDM.2012.115
282695
But techniques for mitigating the effects of imbalance on classification do not improve class probability estimates for minority instances. This can be seen in Figure 1, which shows the results achieved using two popular methods for handling imbalance – undersampling/bagging and SMOTE [5] – both in terms of classification and probability estimation for minority instances over 16 imbalanced datasets. Using baseline SVM (i.e., not addressing imbalance at all) results in poor classifier sensitivity, as expected. Furthermore, the probability estimates for minority instances are entirely unreliable, as quantified by the Brier score calculated over minority instances (the Brier score is a popular metric for assessing the reliability of probability estimates [3] which we describe in Section II-A; a low Brier score implies good probability estimates). The Brier scores achieved using
baseline SVM are quite high, implying poor calibration for minority instances.
Both of the methods for handling imbalance shown (bagging classifiers induced on undersampled training datasets [25] and SMOTE [5]) improve the sensitivity of the model, as can be seen in the left-hand sub-plots. We then calibrated these classifiers (which were trained on balanced samples of the data) over the available training data and compared the goodness of their averaged probability estimates for minority instances to the standard (non-undersampled) estimates. The right-hand sub-plots show the Brier score calculated over the minority instances achieved with standard SVM and using the aforementioned methods to handle imbalance. In contrast to the case of classification, the latter models have no effect on probability estimates over minority instances. To recapitulate, probability estimates for the minority instances in imbalanced datasets are unreliable, and methods for mitigating class imbalance for classification do not automatically fix calibration. Rather, as we will show, imbalance must be handled during calibration to improve probability estimates for imbalanced data.
Our contributions are as follows. As far as we are aware, this is the first general exposition of the bias of probability estimates in imbalanced scenarios.1 This is an important and practical data mining problem. We introduce the stratified Brier score as an appropriate measure of calibration in imbalanced scenarios, and then demonstrate empirically the consistent bias of the probabilities attained from supervised learning algorithms via calibration, namely Support Vector Machines (SVMs) and boosted decision-trees. As we have already seen, methods for fixing this bias for classification improve model sensitivity but do not improve the probability estimates (Figure 1). We discuss theoretical reasons for this bias, and use these to motivate undersampling prior to model calibration as a method to mitigate bias and improve probability estimates with respect to the minority class. To reduce the variance of this undersampling strategy, we propose bagging estimators induced over balanced samples of the training data, and demonstrate the efficacy of this approach empirically on 16 imbalanced datasets. Finally, we coach the latter method in a Bayesian framework, and show that this provides an additional measurement of uncertainty that can be exploited.

II. PRELIMINARIES: ESTIMATING PROBABILITIES IN SUPERVISED LEARNING
The standard method for estimating probabilities in the supervised learning framework is to regress measurements correlated with (predicted) class labels output by a trained
1We note that Cieslak and Chawla have investigated the specific case of Probability Estimation Trees (PETs) for imbalanced data [6], and that Foster and Stine have considered the related task of variable selection for prediction under imbalance [11].
classifier against the true target labels [21], [22]. This process is called calibration. By convention these measurements are denoted by fi, where i indexes instances. This calibration squashes the (arbitrarily scaled) fi’s into the [0,1] range permissible for probabilities. When the sigmoid form is used (Equation 1), this method is referred to as Platt scaling [22]. It is perhaps the most popular way of obtaining probability estimates from classification models.
P (yi|fi) = 1 1 + exp{−β0 − β1fi} (1)
The fi’s may be any scalar that is predictive of class membership. We focus on two specific post-training calibration strategies; Platt calibration with SVMs and with boosted decision trees. We selected these methods because they have been shown to out-perform other supervised learning algorithms with respect to class probability estimation [20], [21].
In the case of SVMs, the fi is taken as the signed distance from the hyperplane w, i.e., fi = wTxi. This was the method originally proposed by Platt [22], and is now widely used [18]. Niculescu-Mizil and Caruana, meanwhile, have proposed attaining probabilities via calibrated boosted decision trees [20]. More precisely, recall that in boosting one induces a sequence of learners h0, h1, ... , hk over different distributions of the training set. These are in turn associated with a set of weights α0, α1, ... , αk reflecting the their estimated performance. A prediction is then taken as a function over these, i.e., as sign( ∑ j αjhj(x)). The natural value for fi is then the sum of the weighted class predictions over the ensemble, i.e., ∑ j αjhj(x).
Figure 2 displays the overall and stratified residual errors of probability estimates (obtained via Platt’s method) for the instances comprising a particular imbalanced dataset.2 Specifically, each sub-plot shows histograms of the absolute differences between the true (observed) labels and corresponding probability estimates, i.e., |yi−P̂{yi|xi}|. Density to the left therefore suggests good calibration, as this implies probability estimates largely agree with the observed labels. For example, if yi = 1 and P̂{yi|xi} = .99, the difference would be .01. Were the estimate .01, on the other hand, the difference would be .99.
The left-hand side of Figure 2 shows this histogram for all instances, corresponding to overall calibration. Over 80% of instances are in the left-most bin, implying that the estimator is well-calibrated, i.e., its estimates do not much diverge from the observed labels. But this ostensibly good calibration belies the unreliability of the probability estimates for the instances comprising the rare class. One can see this by looking at the middle plot, which is the same figure but includes only minority instances. In this case, the estimates diverge strikingly from the observed
2The proton beam dataset in Table I.
labels; indeed the model assigned a probability (of belonging to the minority class) of less than 20% to most of the minority instances. In other words, the probability estimates for instances comprising the minority class are completely unreliable (we demonstrate this on 16 datasets in Section V).

A. Metrics
Similar to the residual errors considered above, the Brier score [3] measures the fit of probability estimates to the observed data. In particular, it is the average squared difference between the observed label and the estimated probability. Formally, this is defined in Equation 2 – here we are assuming that y ∈ {0, 1}, and we are denoting by N the size of the sample with which the model is being assessed (the test set). The Brier score is defined as follows:
∑N i=0(yi − P̂{yi|xi})2
N (2)
Intuitively, this score is small when the probability estimates are near the true labels, and increases as they diverge. But there is a problem with the Brier score in the case of imbalanced datasets; calibration may be good overall, but poor for the rare class. Indeed, Figure 2 plots histograms reflecting each instance’s contribution to the Brier score. As we saw, this is low overall, but high for minority instances.

B. The Stratified Brier score
This phenomenon is analogous to the now wellappreciated idea that accuracy is a poor measure of classifier performance over imbalanced data [23]. Consider that in a task with a minority prevalence of 1% a classifier that naively predicts that every instance belongs to the majority class will achieve 99% accuracy. So too in the case of probability estimation; a model that predicts p=0 for every instance will look to be well-calibrated, according to most metrics, despite its manifest uselessness. In the case of classification, alternative metrics that emphasize good performance with respect to both classes are now widely accepted as more suitable for imbalanced data (e.g., the F -score, g-mean). But the corresponding problem for probability estimation – good overall calibration masking unreliable estimates for minority instances – has not been addressed. We propose a modification of the Brier score that stratifies the score by class. We view this as more appropriate for assessing calibration in imbalanced scenarios.
BS+ =
∑ yi=1
(yi − P̂{yi|xi})2 Npos
(3)
BS− =
∑ yi=0
(yi − P̂{yi|xi})2 Nneg
(4)
Taken together, the Equations 3 and 4 provide much more information than the overall Brier score because they provide
information regarding model calibration for instances drawn from both both classes. This is analogous to decomposing accuracy into sensitivity (recall) and specificity (or, similarly though not equivalently, precision).

III. THE BIAS OF PROBABILITY ESTIMATES FOR IMBALANCED DATA
Recall that the common approaches to inducing probability estimates in supervised machine learning rely on post-calibration, i.e., fitting a (usually sigmoidal) function to raw outputs [6], [20], [21]. Such strategies are theoretically motivated [6] and have been shown to produce reliable probability estimates [21]. The approach is also general: it allows, e.g., one to transform the raw, unscaled outputs of SVMs into interpretable probabilities.
As we will show in Section V, however, post-calibration begets biased estimators. The problem of parameter estimation bias in imbalanced regression scenarios has been considered by the econometrics community, notably by King and Zeng [17], who demonstrate that P̂{yi = 1} will be underestimated when yi = 1 is a rare class.
Intuitively, the problem arises due simply to the model having observed more points drawn from the majority class (fi’s corresponding to majority instances) than from the minority. The model thus naturally fits the distribution generating the majority instances better than the distribution characterizing the minority instances. From this perspective, the reason for poor performance with respect to calibration is similar to the explanation for degraded classification performance [25].
This intuition is best communicated graphically. Consider Figure 3, which depicts the fitted logistic for a simulated dataset. In this case, we assume fi’s (the predictor values) for each class are drawn from separate latent Gaussians for the two classes. (Note that this is in line with Hastie and
Tibshirani’s method of fitting fi’s from the respective classes to independent normals [14].) The fitted logistic is clearly biased with respect to the latent distributions; it is ‘pushed over’ toward the minority class, thus underestimating the conditional probability that yi = 1 for minority instances.
More formally, based on results due to McCullagh and Nelder [19], King and Zheng [17] derive a closed-form expression for finite-sample size bias in logistic regression. Following their example for illustration purposes, consider a special case in which the true coefficient of the predictor held constant (let β1 = 1); then we need only estimate β0. The predicted probability is:
p̂i = 1
(1 + exp{−β̂0 − fi}) (5)
And the expected bias in the estimate of β0 is:
E[β̂0 − β] ≈ π̃ − .5 nπ̃(1− π̃) (6)
Where π̃ denotes, roughly, the true minority prevalence. In the case of imbalanced data, π̃ << 0.5, implying that β̂0 will be an underestimate of β0. This results in an underestimation of the probability that yi = 1. Another intuition here is that the bias in the estimate is larger when imbalance is greater (because limπ̃→0 1π̃(1−π̃) =∞).3 This special case provides support for the empirical observation that imbalance is less of a problem when datasets are large: the bias in β̂0 is inversely proportional to the sample size [25].
These issues have long been recognized in the statistics and epidemiology literatures, and each community has developed distinct solutions to address the same problem. For example, there are several approaches that adjust for the small sample bias of logistic regression estimators (including the Haldane correction [26], Firth’s penalized maximum likelihood [10], and Bayesian logistic regression). By contrast, epidemiologists have avoided such concerns altogether by using (in the majority of investigations) balanced casecontrol studies, wherein the number of cases (the ‘positive’ class) is set to be equal to the number of controls (‘negative’ class) by design [2].

IV. OBTAINING BETTER PROBABILITY ESTIMATES FOR IMBALANCED DATA
From the above we can conclude that calibration in imbalanced scenarios will be biased, systemically affecting the conditional probability estimates for those instances comprising the minority class. This agrees with Figure 2 and our extensive empirical results (which we present in Section V). Moreover, this bias is introduced at the calibration step; thus handling imbalance during classifier induction using existing methods will not mitigate the issue. And, indeed, this agrees with Figure 1. What, then, can be done to mitigate bias (and improve estimations) in imbalanced scenarios?
We propose undersampling prior to calibration as a means to accomplish this. Specifically, this entails discarding majority instances (at random) from the training set and calibrating probability estimates (e.g., estimating β) on this balanced set. For classification, this simple approach works well [15] and is theoretically motivated [25]. It is analogous to the case-control sampling used in epidemiology that we mentioned above [2].
Intuitively, we can see the effect of undersampling on calibration by returning to the example introduced in Figure 3. The dotted line in Figure 4 shows the estimation when a logistic is fitted to a balanced sub-sample of the original dataset; contrast this with the solid line, which is the result
3This limit is undefined in general; here π̃ is coming from the positive side.
of fitting the entire sample, and it is clear that the former mitigates bias. Theoretically, it is easy to see that the prevalence term in Equation 6 drops out of the equation (we note, however, that this is only applicable to the special case derived by King [17]).

A. Bagging Probability Estimates
While undersampling will mitigate bias, it also introduces randomness: the particular majority instances sampled will greatly affect the estimate β̂. We can mitigate the variance inherent to this strategy via bagging [1]. Bagging is an ensemble strategy in which predictors are induced over bootstrap samples; the final prediction for a given instance is an aggregate of the predictions of the constituent models (bagging is an amalgam of ‘bootstrap’ and ‘aggregation’).
To bag probability estimates, we calibrate k models over corresponding balanced bootstrap samples. We then combine their outputs to form the estimated P̂{yi|xi} for a given xi. The easiest method of combination is a simple average (Equation 7).
P̂{yi|xi} = 1 k
k∑
j=1
P̂j{yi|fij} (7)
Note that we allow each constituent model to produce different scores for instance i (indexed by fij). This may be useful if we induce k classification models, each to be calibrated separately over different bootstrap samples of the training dataset. Indeed, we will take this approach in our empirical evaluation because models induced over undersampled samples of the dataset make better classifiers in imbalanced scenarios. Thus by undersampling before both classifier induction and estimator calibration, we can achieve good performance both with respect to classification and class probability estimation. This is only a tangential point, however, as we focus here only on the latter task. For this, calibrating models on balanced bootstrap samples using
the same fi’s performs as well as calibrating models over balanced samples with individual fij’s.
The simple average shown in Equation 7 has the practical advantage of being easy to implement and fast to run. Indeed, despite building an ensemble of models, this approach may actually reduce running time: inducing an SVM over an entire dataset often takes far longer than inducing several SVMs over small sub-samples thereof.4 That said, one may do better to weight the contribution of each constituent ensemble member by the certainty around their prediction, as in Equation 8 (where V ar(P̂j{yi|fij}) denotes the variance of the prediction).
P̂{yi|xi} = 1 z
k∑
j=1
1
V ar(P̂j{yi|fij}) P̂j{yi|fij} (8)
Where V ar(P̂j{yi|fij}) denotes the variance of the prediction and z is the following normalization constant:
z = k∑
j=1
1
V ar(P̂j{yi|fij}) (9)
It is natural to realize this weighting within the Bayesian framework by postulating a generative model and then sampling from the posterior probability estimates. Specifically, this can be done by assuming the following for each ensemble member j.
yi ∼ Bernoulli(pi) (10) logit(pi) = β0j + β1jfij (11)
That is, for each model’s predictive value fij , we assume that there exist coefficients that transform this value into the true probability. This is essentially the assumption made any time calibration is used. The estimate of the probability according to model j is then:
p̂ij = 1
1 + exp{−β̂0j − β̂1jfij} (12)
And, as before, we assume the true probability is an aggregate of the constituent estimates, or:
pi = 1
k
k∑
j=1
1
1 + exp{−β̂0j − β̂1jfij} (13)
We fit this model using uninformative priors over the βs. At the cost of added complexity, the Bayesian approach affords two major benefits. First, the uncertainty that each model has about its predictions is implicitly taken into account due to the sampling mechanism. A model uncertain regarding
4Recall that the training time of SVMs scales quadratically with the number of instances.
its β estimates will produce wide-ranging estimates during sampling, thus mitigating the contribution of their mean values. Conversely, a model with high certainty regarding its β̂s will repeatedly make similar estimates, shifting the overall estimate (Equation 13) toward its mean.
The second benefit that the Bayesian framework provides is that of an additional measure of uncertainty. Specifically, one can take into the account the empirical posterior distribution over the aggregated probability estimate; this, of course, would not be possible within the frequentist framework. We demonstrate the potential utility of exploiting this uncertainty in Section V-C.
Note that taking a simple average (as in Equation 7) can be interpreted as ignoring the confidence in the estimates of the constituent members.

V. EMPIRICAL RESULTS
The empirical results in this section demonstrate three things. First, standard supervised learning methods for probability estimation fare poorly on imbalanced data. More specifically, we analyze probability estimates attained via the two calibration methods reported to work best for supervised learning methods, namely (Platt-calibrated) SVMs and boosted decision trees [20], [21]. We show that while overall calibration performance is good, calibration with respect to the minority class (about which we are often most concerned) is often completely off.
Second, we demonstrate that undersampling prior to calibration greatly mitigates this problem on 16 datasets (described below), i.e., maintains reasonably good overall calibration while drastically improving calibration with respect to minority instances. Third, we show that bagging predictors induced over balanced bootstrap samples further boosts performance in this regard and reduces variance.

A. Datasets and Experimental Setup
We used 16 datasets with varying degrees of imbalance; 13 of these were taken from the UCI dataset repostiory, the other 3 from real-world biomedical text classification tasks. The datasets are summarized in Table I. These are the same datasets used by Wallace et al. in their work on handling imbalance in classification scenarios [25].
We split each of these into train and test sets, the former comprising 10% of the dataset size (N ). We induced probability estimators via calibration over the train sets using two distinct methods: SVMs and boosted decision-trees. These were selected due to their popularity and demonstrated performance in accurately estimating probabilities [21], [20]. To measure the probability estimation performance we recorded overall and stratified Brier scores (see Section II-B). We report the results achieved using standard calibration versus those achieved by undersampling prior to calibration. We also show the results of bagging 11 estimators calibrated over independently drawn balanced bootstrap samples of the training datasets. We repeated this procedure 10 times to assess the variance of the methods.

B. Results
Figure 5 describes results over the datasets in Table I using three methods for estimating probabilities via SVMs: standard Platt, undersampled and the undersampled/bagging methods proposed in Section IV. (The results are also summarized in Table II). The left and right sub-plots in Figure 5 correspond to the positive Brier score (BS+, defined in Equation 3) and the overall Brier score [3] (Equation 2). Each × represents a specific run: lines between these connect points generated from the same run, i.e., connect results for the three different methods on the same dataset, using the same test set. The black × are averages of the ten runs (lighter ×s are individual runs). The black lines depict the average difference in performance between methods on a given dataset (there are 16 in all).
Recall that the Brier score measures the divergence of probability estimates from observed labels; lower scores are thus better. Notice that the standard method of obtaining probabilities, Platt-calibrated SVMs, ostensibly slightly outperform the undersampled strategies according to the overall Brier score reported in the right sub-plot of Figure 5. But BS+ – the assessment of performance on the minority instances – tells a rather different story. Indeed, for half of the datasets the average BS+ achieved using the standard method is greater than .5; in these cases, estimators calibrated using the standard procedure assigned, on average, a probability of < .5 to the proposition that minority instances indeed belong to the minority class. Thus while overall estimation is good, the probabilities estimated for the truly positive (minority) instances are completely unreliable.
As the plot shows, undersampling prior to calibration sharply mitigates this issue. The BS+ has a clear downward
trend. In other words, undersampling greatly increases the quality of the probability estimates for minority instances. Furthermore, it is clear that while affected, this undersampling does not greatly sacrifice the overall calibration. Moreover, bagging undersampled estimators fares even better; we see a similar decrease in the BS+ with a lower hit in overall calibration. And, as expected, bagging reduces the variance of the performance. This can also be seen by inspecting the standard deviations (calculated over the 10 runs) presented in Table II.
The results for boosted decision trees tell a similar story. As in Figure 5, the left and right sub-plots in Figure 6 correspond to the positive and overall Brier scores, respectively. As in the case of SVMs, we see that the probability estimation method proposed for boosted decision trees in [20] is well-calibrated, overall, but provides unreliable estimates for minority instances. The undersampling and bagging methods that we have proposed in Section IV again drastically improve the estimates for minority instances while maintaining good overall calibration. In this case, bagging estimators improves calibration in addition to reducing variance (compared to a single undersampled estimator).
To explore whether dataset characteristics affect calibration performance (w.r.t. BS+) we evaluated associations between the logit transformed 1-p̂ values, the two techniques of interest (undersampled/bagged and standard SVM), and three dataset characteristics: prevalence, training set size, and dimensionality. Briefly, we used a linear mixed effects model for the logit-transformed 1-p̂ values (the transformation was chosen to improve model fit) that allows for different effects of undersampling and bagging by dataset and common effects of the characteristics of interest across datasets [24]. We plot the fitted lines for standard Platt and undersampled/bagged, over normalized dataset characteristics in Figure 7. The results show that the improvement in calibration performance from using the undersampled/bagged strategy is greater, compared to standard Platt, when prevalence is low. This is a statistically significant finding (p < 0.001),
and is what we would expect due to Equation 6. Although training set size and dimensionality did not reach statistical significance, the fitted lines suggest that the former may still have an effect (i.e., a lot of training data probably mitigates bias).

C. Exploiting the Bayesian Framework
As discussed in Section IV, bagging probability estimators within the Bayesian framework affords a few advantages over the simple (frequentist) averaging approach. One such advantage is the ability to take into account an additional level of uncertainty, namely the empirical posterior distribution around the estimated p̂i’s. For example, the median
of this estimate may be shy of .5, but the 95% credibility interval (or any other specified interval) may encompass .5. For certain applications this may be useful information. More generally, the ability to marginalize over this posterior distribution could produce, for example, informed estimates of the true cost.
Figure 8 shows the posterior distributions around four instances from the COPD dataset (one per column) of minority instances assigned point estimates < .5; these would, presumably, be classified (wrongly) as negatives. The top row shows these posterior distributions using the standard (non-undersampled) model, the bottom for the undersampled/bagged model we have proposed. In the former case,
the uncertainty is of no help: the estimates for each underestimated minority instance are well below .5. The posterior distributions obtained via the bagged model, however, are potentially useful. Barring the left-most example, all of these include .5, and indeed their mass hovers around it. One could easily exploit this additional uncertainty to be more cautious when making classification calls, or to obtain better estimates of expected costs.

VI. CONCLUSIONS
We have identified a major problem with supervised methods for estimating class probabilities in the case of imbalanced data: estimators systemically provide unreliable probability estimates for instances belonging to the minority class. We introduced a new metric, the stratified Brier score, to quantify this problem.
We demonstrated that methods that improve sensitivity (i.e., address classification under imbalance) do not improve
the poor probability estimates for minority instances. That is, calibration under imbalance is a problem independent of classification under imbalance. We discussed the theoretical underpinnings of the issue and proposed a novel solution, namely inducing probability estimators calibrated over balanced bootstrap samples of the training data. We empirically demonstrated that this simple approach mitigates the bias of the probability estimates, substantially improving the quality of the probability estimates for the minority class, without greatly sacrificing overall calibration. Finally, we demonstrated that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates.

References
[1]N.E. Breslow,N.E. DayStatistical methods in cancer research. Vol. 1. The analysis of case-control studies., volume 1. Distributed for IARC by WHOGeneva, Switzerland,1980
[2]G.W. BrierVerification of forecasts expressed in terms of probabilityMonthly weather review,1950
[3]N.V. ChawlaData mining for imbalanced datasets: An overviewData Mining and Knowledge Discovery Handbook,2010
[4]N.V. Chawla,K.W. Bowyer,L.O. Hall,W.P. KSMOTE: synthetic minority over-sampling techniqueJournal of Artificial Intelligence Research,2002
[5]D. Cieslak,N. ChawlaAnalyzing pets on imbalanced datasets when training and testing class distributions differAdvances in Knowledge Discovery and Data Mining,2008
[6]Gilles Cohen,Mélanie Hilario,Hugo Sax,Stéphane Hugonnet,Antoine GeissbuhlerLearning from imbalanced data in surveillance of nosocomial infectionArtificial Intelligence in Medicine,2006
[7]I. Cohen,M. GoldszmidtProperties and benefits of calibrated classifiers. Knowledge Discovery in Databases: PKDD2004
[8]C. ElkanThe foundations of cost-sensitive learningIn International Joint Conference on Artificial Intelligence,2001
[9]D. FirthBias reduction of maximum likelihood estimatesBiometrika, 80(1):27–38,1993
[10]D.P. Foster,R.A. StineVariable selection in data mining: Building a predictive model for bankruptcyJournal of the American Statistical Association,2004
[11]Xinjian Guo,Yilong Yin,Cailing Dong,Gongping Yang,Guantong ZhouOn the class imbalance problem2008
[12]H. Haibo,E.A. GarciaLearning from imbalanced dataIEEE Transactions on Knowledge and Data Engineering,2009
[13]T. Hastie,R. TibshiraniClassification by pairwise couplingThe annals of statistics,1998
[14]Jason Van Hulse,Taghi M. Khoshgoftaar,Amri NapolitanoExperimental perspectives on learning from imbalanced data2007
[15]Nathalie Japkowicz,Shaju StephenThe class imbalance problem: A systematic studyIntelligent Data Analysis,2002
[16]G. King,L. ZengLogistic regression in rare events dataPolitical analysis,2001
[17]H.T. Lin,C.J. Lin,R.C. WengA note on platts probabilistic outputs for support vector machinesMachine learning,2007
[18]P. McCullagh,J.A. NelderGeneralized Linear Models1987
[19]A. Niculescu-Mizil,R. CaruanaObtaining calibrated probabilities from boostingIn Proc. 21st Conference on Uncertainty in Artificial Intelligence2005
[20]A. Niculescu-Mizil,R. CaruanaPredicting good probabilities with supervised learningIn Proceedings of the 22nd international conference on Machine learning,2005
[21]J. PlattProbabilistic outputs for support vector machines and comparisons to regularized likelihood methodsAdvances in large margin classifiers,1999
[22]Foster ProvostMachine learning from imbalanced data sets 101In Proc. of the AAAI Workshop on Learning from Imbalanced Data Sets,2000
[23]S. Rabe-Hesketh,A. SkrondalMultilevel and longitudinal modeling using StataStata Corp,2008
[24]B.C. Wallace,K. Small,C.E. Brodley,T.ATrikalinos. Class imbalance, reduxIn Data Mining (ICDM),2011
[25]SD WalterSmall sample estimation of log odds ratios from logistic regression and fourfold tablesStatistics in medicine,1985
[26]Wu Yang,Xindong Wu10 challenging problems in data mining researchInternational Journal of Information Technology Decision Making,2006
[27]B. Zadrozny,C. ElkanTransforming classifier scores into accurate multiclass probability estimatesIn Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,2002
[28]J. Zhu,E. HovyActive learning for word sense disambiguation with methods for addressing the class imbalance problemIn Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2007
