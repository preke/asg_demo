Experimental Perspectives on Learning from Imbalanced Data
Jason Van Hulse,Taghi M. Khoshgoftaar,Amri Napolitano
jvanhulse@gmail.com,taghi@cse.fau.edu,anapoli1@fau.edu

1. Introduction
In many real-world classification domains, the vast majority of examples are from one of the classes. In binary classification, it is typically the minority (positive) class that the practitioner is interested in. Imbalance in the class distribution often causes machine learning algorithms to perform poorly on the minority class. In addition, the cost of misclassifying the minority class is usually much higher than the cost of other misclassifications. Therefore a natural question in machine learning research is how to improve upon the performance of classifiers when one class is relatively rare?
A common solution is to sample the data, either randomly or intelligently, to obtain an altered class distribution. Numerous techniques have been proposed (Section 3), although it is unclear which techniques work best. Some researchers have experimentally evaluated the use of sampling when learning from imbalanced data (e.g., (Drummond & Holte, 2003), (Kubat & Matwin, 1997), (Maloof, 2003), (Japkowicz, 2000), (Weiss & Provost, 2003), (Chawla et al., 2002), (Han et al.,
Appearing in Proceedings of the 24 th International Conference on Machine Learning, Corvallis, OR, 2007. Copyright 2007 by the author(s)/owner(s).
2005), (Monard & Batista, 2002)). The experimentation performed in this study, however, is more comprehensive than related work, which typically utilize one or two learners and a couple of datasets. In this work, we consider 35 different benchmark datasets (Section 2), seven sampling techniques (Section 3), and 11 commonly-used learning algorithms (Section 4). As explained in Section 5, a total of 1,232,000 classifiers were constructed in these experiments. In addition, we perform statistical analysis using analysis of variance (ANOVA) models to understand the statistical significance of the results. These components make our work very comprehensive, and dramatically increase the reliability of our conclusions. We strongly advocate robust, statistically valid, and reliable empirical work to understand the relative strengths and weaknesses of different techniques in realworld applications.

2. Experimental Datasets
The 35 datasets utilized in our empirical study are listed in Table 1. The percentage of minority examples varies from 1.33% (highly imbalanced) to almost 35% (only slightly imbalanced). The datasets also come from a wide variety of application domains, and 19 are from the UCI repository (Blake & Merz, 1998). The Mammography dataset was generously provided by Dr. Nitesh Chawla (Chawla et al., 2002). Fifteen datasets (some of which are proprietary) are from the domain of software engineering measurements. We have also considered datasets with diversity in the number of attributes, and datasets with both continuous and categorical attributes. The smallest dataset had 214 total examples (Glass-3), while the two largest datasets each contain 20,000 observations. Note that all datasets have, or were transformed to have, a binary class. We only consider binary classification problems in this work.

3. Sampling Techniques
This section provides a brief overview of the seven sampling techniques considered in this work: random
undersampling (RUS), random oversampling (ROS), one-sided selection (OSS), cluster-based oversampling (CBOS), Wilson’s editing (WE), SMOTE (SM), and borderline-SMOTE (BSM). RUS, ROS, WE, SM, and BSM require a parameter value to be set, so when it is important to specify this value, we often use notation such as ROS300, which denotes random oversampling with the parameter 300 (the meanings of the parameters in the context of each sampling techniques are explained below).
The two most common preprocessing techniques are random minority oversampling (ROS) and random majority undersampling (RUS). In ROS, instances of the minority class are randomly duplicated. In RUS, instances of the majority class are randomly discarded from the dataset.
In one of the earliest attempts to improve upon the performance of random resampling, Kubat and Matwin (Kubat & Matwin, 1997) proposed a technique called one-sided selection (OSS). One-sided selection attempts to intelligently undersample the majority class by removing majority class examples that are considered either redundant or ‘noisy.’
Wilson’s editing (Barandela et al., 2004) (WE) uses the kNN technique with k = 3 to classify each example in
the training set using all the remaining examples, and removes those majority class examples that are misclassified. Barandela et al. also propose a modified distance calculation, which causes an example to be biased more towards being identified with positive examples than negative ones.
Chawla et al. (Chawla et al., 2002) proposed an intelligent oversampling method called Synthetic Minority Oversampling Technique (SMOTE). SMOTE (SM) adds new, artificial minority examples by extrapolating between preexisting minority instances rather than simply duplicating original examples. The technique first finds the k nearest neighbors of the minority class for each minority example (the paper recommends k = 5). The artificial examples are then generated in the direction of some or all of the nearest neighbors, depending on the amount of oversampling desired.
Han et al. presented a modification of Chawla et al.’s SMOTE technique which they call borderline-SMOTE (Han et al., 2005) (BSM). BSM selects minority examples which are considered to be on the border of the minority decision region in the feature-space and only performs SMOTE to oversample those instances, rather than oversampling them all or a random subset.
Cluster-based oversampling (Jo & Japkowicz, 2004) (CBOS) attempts to even out the between-class imbalance as well as the within-class imbalance. There may be subsets of the examples of one class that are isolated in the feature-space from other examples of the same class, creating a within-class imbalance. Small subsets of isolated examples are called small disjuncts. Small disjuncts often cause degraded classifier performance, and CBOS aims to eliminate them without removing data.
RUS was performed at 5%, 10%, 25%, 50%, 75%, and 90% of the majority class. ROS, SM, and BSM were performed with oversampling rates 50%, 100%, 200%, 300%, 500%, 750%, and 1000%. When performing Wilson’s editing, we utilized both the weighted and unweighted (standard Euclidean) versions, and denote them WE-W and WE-E. A total of 31 combinations of sampling technique plus parameters were utilized. In addition, we built a classifier with no sampling, which we denote ‘NONE’. All of these sampling techniques were implemented in Java in the framework of the WEKA machine learning tool (Witten & Frank, 2005).

4. Learners
This section provides brief descriptions of the 11 classification algorithms along with an explanation of the parameters used in our experiments. These classifiers were considered since they are commonly-used in the machine learning community and in research on class imbalance.
All learners were built using WEKA, and changes to default parameter values were done only when experimentation showed a general improvement in the classifier performance across all datasets based on preliminary analysis.
Two different versions of the C4.5 (Quinlan, 1993) decision tree learner (denoted C4.5D and C4.5N) were constructed using J48 in WEKA. C4.5D uses the default WEKA parameter settings, while C4.5N uses no pruning and Laplace smoothing (Weiss & Provost, 2003). Two different K nearest neighbors classifiers (denoted IBk in WEKA) were constructed, using k = 2 and k = 5, and denoted 2NN and 5NN. The ‘distanceWeighting’ parameter was set to ‘Weight by 1/distance’ to use inverse distance weighting in determining how to classify an instance. For a Naive Bayes (NB) classifier, the parameters were left at the default values.
Two parameters were changed from the default values for the Multilayer perceptrons (MLP) learner. The ‘hiddenLayers’ parameter was changed to ‘3’ to define a network with one hidden layer containing three nodes, and the ‘validationSetSize’ parameter was changed to ‘10’ to cause the classifier to leave 10% of the training data aside to be used as a validation set to determine when to stop the iterative training process. Radial basis function networks (RBF) are another type of artificial neural network. The only parameter change for RBF (called ‘RBF Network’ in WEKA) was to set ‘numClusters’ to 10.
RIPPER (Repeated Incremental Pruning to Produce Error Reduction) is a rule-based learner. JRip is the implementation of RIPPER in WEKA, and the default parameters were used in all experiments. The logistic regression learner is denoted LR, and no changes to the default parameter values for this learner were made.
The random forest (RF) classifier (Breiman, 2001) utilizes bagging and the ‘random subspace method’ to construct randomized decision trees. The outputs of ensembles of these randomized, unpruned decision trees are combined to produce the ultimate prediction. No changes to the default parameters were made in our experiments. The support vector machine learner is called SMO in WEKA and denoted SVM in this study. For our experiments, the complexity constant ‘c’ was changed from 1.0 to 5.0, and the ‘buildLogisticModels’ parameter, which allows proper probability estimates to be obtained, was set to ‘true’ (Witten & Frank, 2005). In particular, the SVM learner used a linear kernel.

5. Experimental Design
The design of our experiments can be summarized as follows. For each of the 35 datasets, 20 different runs of five-fold cross validation (CV) were executed. For
each iteration of CV, the training dataset consisted of four folds, and the remaining fold served as a test dataset. Each of the 31 sampling techniques (and also no sampling) were applied to the training data, 11 different learners were constructed from the transformed dataset, and each of the learners was evaluated on the test dataset (based on CV).
In total, 20 five-fold CV runs times 35 datasets is 3500 different training datasets. 31 sampling techniques, plus no sampling, were applied to each of the 3500 training datasets, resulting in 32 × 3500 = 112, 000 tranformed datasets, each of which is used for learner construction. Since there are 11 learners, a total of 11 × 112, 000 = 1, 232, 000 classifiers were constructed and evaluated in our experiments.
To measure the performance of the classifiation algorithms, the area under the ROC curve (AUC), Kolmogorov-Smirnov statistic (K/S) (Hand, 2005), geometric mean (G), F-measure (F), accuracy (Acc), and true positive rate (TPR) were calculated. The last four performance measures utilize the implicit classification threshold of 0.5 (i.e., if the posterior probability of posi-
tive class membership is > 0.5, then the example is classified as belonging to the positive class). The first two, the AUC and K/S, measure the general ability of the classifier to separate the positive and negative classes.

6. Results
6.1. Experimental Data
The first set of results we present are for some of the individual learners separately. Due to space limitations we can only provide a small sampling of the results, however. First, the datasets were grouped into four categories based on severity of imbalance: those with π < 5%, 5% < π < 10%, 10% < π < 20%, and finally 20% < π (π is the percentage of examples belonging to the minority class). The reason for this categorization scheme is to capture differences in the performance of sampling techniques given different levels of imbalance. We focus primarily on the results from π < 10% for the learners SVM, RF, NB, C4.5N, and LR (Tables 2 to 6). Each of these tables shows the ordering of the sampling techniques, as measured by AUC and G,
along with a test of statistical significance. In Tables 2 to 6, the first nine rows are the results for datasets with π < 5%, while the second nine rows are for the datasets with 5% < π < 10%. The values for the performance measure (either AUC or G) in Tables 2 to 6 are averaged over all of the datasets with either π < 5% at the top of the table or 5% < π < 10% at the bottom of the table. For example, from Table 2, SVM with ROS1000 obtained an average AUC of 0.898 over the 20 CV runs of the eight datasets with π < 5%, and SVM with ROS1000 obtained an average AUC of 0.861 over the 20 CV runs of the 11 datasets with 5% < π < 10%.
For each learner and group of datasets, a one-factor analysis of variance (ANOVA) (Berenson et al., 1983) was constructed, where the factor was the sampling technique. Tukey’s Honestly Significant Difference (HSD) test (SAS Institute, 2004) is a statistical test comparing the mean value of the performance measure for the different sampling techniques. Two sampling techniques with the same block letter are not significantly different with 95% statistical confidence (all of the statistical tests in this work use 95% confidence level). Finally note that these tables show the parameter value for each of the seven types of sampling that achieved the optimal value. For example, from Table 2, ROS at 1000% obtained the highest average AUC (across all of the datasets with π < 5%) of 0.898, followed by RUS at 5%. Note that based on the average AUC over all datasets with π < 5%, ROS1000, RUS5, SM1000, and BSM1000 are not significantly different from one another (they all have the block letter ‘A’ in the HSD column) when used with the SVM classifier. Further, RUS5, SM1000, BSM1000, and CBOS are not significantly different from one another, since they have the block letter ‘B’ in the HSD column. We present the results only for these five learners and only these two performance measures due to space limitations. AUC and G were included because they represent one measure that is threshold dependent
(G) and one that is not (AUC). Note for example that sampling does not significantly improve the AUC obtained by NB (Table 4), however applying either RUS, SM, or BSM does significantly improve G.
Tables 7 to 10 present the sampling technique which results in the best AUC, G, K/S, and F measures for each learner and group of imbalance. If applying the sampling technique resulted in performance that was significantly better (with 95% statistical confidence) than that of using no sampling, then the technique is underlined.
Table 11 presents, over all 35 datasets, 11 learners, and six performance measures (AUC, K/S, G, F, Acc, and TPR), the number of times the rank of the sampling technique was 1, 2, . . . , 8. A rank of one means that the sampling technique, for a given dataset, learner, and performance measure, resulted in the highest value for the performance measure1. RUS resulted in the best performance 748 times (or 32.0% = 748/2340), followed by ROS (408 times). OSS and CBOS were rarely the best technique (66 or 2.8% for OSS and 86 or 3.7% for CBOS). Further CBOS resulted in the worst performance (rank 8, last column) 965 or 42.0% of the time, followed by no sampling, which was the worst 862 or 37.5% of the time.
1 Note that it is possible for ties to occur. Suppose, for example, that two sampling techniques obtained the best AUC. Both of these techniques would be given a rank of one, while the next best technique would be given a rank of three. Therefore the sum of the columns is not exactly equal for each rank.
Tables 12 and 13 display the ranking of each sampling technique separately for the four groups of imbalance (π < 5% at the top of Table 12 and 5% < π < 10% at the bottom, with 10% < π < 20% at the top of Table 13 and π > 20% at the bottom). Note that adding the individual cells of Tables 12 and 13 produces Table 11. Finally, Tables 14 to 16 show the rankings of the sampling techniques only for datasets with π < 5% and separately for each of the six performance measures, AUC, K/S, G, F, Acc, and TPR (adding the individual cells of Tables 14 to 16 produces the top half of Table 12).

6.2. Discussion of Results
Based on the experiments conducted in this work, a number of conclusions can be drawn. The utility of sampling depends on numerous factors. First, different types of sampling work best with different learners. RUS worked very well for C4.5D (not shown) and RF, while ROS works well with LR. Second, the value of sampling is heavily dependent on the performance measure being used. AUC and K/S, which are classification-threshold independent, generate different results than G, F, TPR, and Acc, which utilize the standard 0.5 threshold on the posterior probability. For numerous learners, such as NB, LR, 2NN, and 5NN (and to a slighly lesser extent RBF and MLP), none of the sampling techniques significantly improved the performance of the learner as measured by the AUC or K/S. However, when the performance is measured using the threshold-dependent measures, significant improvements for all learners are
obtained. For NB, for example, none of the sampling techniques improved the performance on datasets with π < 5% as measured by the AUC, however, relative to G, RUS, SM, and ROS significantly improved the performance (RUS, SM, and ROS achieved G > 80, while no sampling resulted in G = 60.72).
Further, consider Tables 7 to 10. Using the AUC, sampling significantly improved upon the performance of the classifier constructed with the unaltered data in only 15 of 44 scenarios (12 of the 15 occurrences were for datasets with π < 10%). For K/S, sampling improved the performance in 12 of the 44 scenarios. For G and F, however, in 42 and 34 of 44 scenarios, respectively, sampling significantly outperformed not using sampling.
RUS performed very well in our experiments, although for individual learners or datasets, other methods were sometimes better. Overall, however, RUS resulted in very good performance, being the best sampling technique 748 of 2340 times. ROS performed the second best overall, followed by SM and BSM. OSS and CBOS in particular performed very poorly, with the latter obtaining the worst overall ranking of the sampling techniques 965 of 2297 times. For datasets with more severe imbalance, RUS does even better, as can be seen from Tables 12 and 13, where RUS was the best technique 39.8% and 36.4% of the time for datasets with π < 5%
and 5% < π < 10%.
Finally, considering in more detail those datasets with π < 5% in Tables 14 to 16, RUS maintains a slight edge over ROS as the best sampling technique relative to the AUC, K/S, and F. Relative to G, RUS is clearly the best sampling technique. As would be expected, not using sampling typically results in the highest overall accuracy (Table 16), however since we are interested in detecting examples of the positive class, this measure is very misleading. We believe overall accuracy is not an appropriate measure, especially given imbalanced data, however it is presented because it is often used in related work. When considering the TPR, RUS is clearly the most successful.
One of the most important conclusions that can be drawn from these experiments is the inferior performance of the ‘intelligent’ sampling techniques, SM, BSM, WE, OSS, and CBOS (especially the last two). While these techniques seem to be promising solutions to
the problem of class imbalance, simpler techniques such as RUS or ROS often performed much better. CBOS and OSS especially performed very poorly in our experiments, very rarely being the best sampling technique and often being among the worst.

6.3. Threats to Validity
Two types of threats to validity are commonly discussed in empirical work (Wohlin et al., 2000). Threats to internal validity are unaccounted influences that may impact the results. Threats to external validity consider the generalization of the results outside the experimental setting, and what limits, if any, should be applied.
All experiments were conducted using WEKA (Witten & Frank, 2005), which is commonly used in machine learning research. Some enhancements were required to implement some of the sampling techniques, and all developed software was thoroughly tested. ANOVA analysis was performed using the SAS GLM procedure (SAS
Institute, 2004), and all assumptions for valid statistical inference were verified. Extensive care was taken to ensure the validity of our results.
External validity questions the reliability and generalizability of the experimental results. The comprehensive scope of our experiments greatly enhances the reliability of our conclusions, which is why we utilized 35 different real-world datasets. Performing numerous repetitions of cross validation greatly reduces the likelihood of anomalous results due to selecting a lucky or unlucky partition of the data. Building over one million learners in these experiments allows us to be confident in the reliability of our experimental conclusions.
One important consideration is the ‘free’ parameter associated with the four sampling techniques RUS, ROS, SM, and BSM. Prior work has suggested that no universal prior is optimal for tree construction (Weiss & Provost, 2003), so instead of only using one selected parameter (e.g., balanced classes), we tried numerous possibilities, but only from a limited selection - in other words, no attempt was made to optimize over all possible sampling percentages. Further, when utilizing sampling in practice, the user does have the ability to choose a value which produces good results, for example using cross validation. Our work has shown that in many cases, a sampling percentage which is more towards balanced is better than other choices, and future work should explore this further. In addition, as the sampling percentage varied near the one we deemed ‘best’, the results did not change dramatically. For example, RUS5 was the best technique for C4.5D for the datasets with π < 5% with respect to AUC (Table 7), but the AUC of RUS10 was very similar. Further, with OSS and CBOS, the techniques explicitly describe how to add/remove instances, so there was no ability to directly alter the level of sampling and we reported the single level of performance achieved. Therefore, we do not believe that the comparison of sampling techniques was unfairly biased towards those with a free parameter.

7. Conclusions
We have presented a comprehensive and systematic experimental analysis of learning from imbalanced data, using 11 learning algorithms with 35 real-world benchmark datasets from a variety of application domains. The objective of this research is to provide practical guidance to machine learning practitioners when building classifiers from imbalanced data, and to present to researchers some possible directions for future study. To our knowledge, no related work has attempted to empirically analyze class imbalance from such a wide scope, comparing learners, sampling techniques, and performance measures using many different datasets. Unfor-
tunately due to space limitations, we can only present a small fraction of our experimental results, however the data clearly demonstrate that sampling is often critical to improving classifier performance, especially optimizing threshold-dependent measures such as the geometric mean or TPR. Further, individual learners respond differently to the application of sampling. Much of the related work on class imbalance has focused on decision tree learners, however these results show that the observations made for decision trees will not carry over to neural networks, regression, or nearest neighbor classification algorithms. Future work may consider additional learners, e.g., different variations of neural network or SVM learners. Sampling can also be compared to costsensitive learning in future work. Alternative measures of classifier performance can also be analyzed. Future work should also consider sampling in the context of multi-class learning.

Acknowledgments
We are grateful to the current and former members of the Empirical Software Engineering and Data Mining and Machine Learning Laboratories at Florida Atlantic University for their reviews of this work. We also sincerely thank the anonymous reviewers for their helpful comments.

References
[1]R. Barandela,R.M. Valdovinos,J.S. Sanchez,F.J. FerriThe imbalanced training sample problem: Under or over sampling? In Joint IAPR International Workshops on Structural, Syntactic, and Statistical Pattern Recognition (SSPR/SPR’04)2004
[2]M.L. Berenson,D.M. Levine,M. GoldsteinIntermediate statistical methods and applications: A computer package approach1983
[3]C. Blake,C. MerzUCI repository of machine learning databases. http://www.ics.uci.edu/ mlearn/ MLRepository.htmlDepartment of Information and Computer Sciences,1998
[4]L. BreimanRandom forestsMachine Learning, 45, 5–32.2001
[5]N.V. Chawla,L.O. Hall,K.W. Bowyer,W.P. KegelmeyerSmote: Synthetic minority oversampling techniqueJournal of Artificial Intelligence Research,2002
[6]C. Drummond,R.C. HolteC4.5, class imbalance, and cost sensitivity: why under-sampling beats2003
[7]H. Han,W.Y. Wang,B.H. MaoBorderlinesmote: A new over-sampling method in imbalanced data sets learningIn International Conference on Intelligent Computing (ICIC’05). Lecture Notes in Computer Science2005
[8]D.J. HandGood practice in retail credit scorecard assessmentJournal of the Operational Research Society, 56, 1109–1117.2005
[9]N. JapkowiczLearning from imbalanced data sets: a comparison of various strategiesAAAI Workshop on Learning from Imbalanced Data Sets (AAAI’00) (pp. 10–15).2000
[10]T. Jo,N. JapkowiczClass imbalances versus small disjunctsSIGKDD Explorations,2004
[11]M. Kubat,S. MatwinAddressing the curse of imbalanced training sets: One sided selectionProceedings of the Fourteenth International Conference on Machine Learning (pp1997
[12]M. MaloofLearning when data sets are imbalanced and when costs are unequal and unknownProceedings of the ICML’03 Workshop on Learning from Imbalanced Data Sets.2003
[13]M.C. Monard,Batista,G.E.A.P. ALearning with skewed class distributions. Advances in Logic, Artificial Intelligence and Robotics (LAPTEC’022002
[14]SAS InstituteSAS/STAT user’s guideSAS Institute Inc.2004
[15]G.M. Weiss,F. ProvostLearning when training data are costly: the effect of class distribution on tree inductionJournal of Artificial Intelligence Research,2003
[16]I.H. Witten,E. FrankData mining: Practical machine learning tools and techniquesSan Francisco,2005
[17]C. Wohlin,P. Runeson,M. Host,M.C. Ohlsson,B. Regnell,A. WesslenExperimentation in software engineering: An introductionKluwer International Series in Software Engineering2000
