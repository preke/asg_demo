Robustness of Offline Signature Verification Based on Gray Level Features
Miguel A. Ferrer,J. Francisco Vargas,Aythami Morales,Aarón Ordóñez
mferrer@dsc.ulpgc.es).

Abstract
Several papers have recently appeared in the literature which propose pseudo-dynamic features for automatic static handwritten signature verification based on the use of gray level values from signature stroke pixels. Good results have been obtained using rotation invariant uniform local binary patterns LBP plus LBP and statistical measures from gray level co-occurrence matrices (GLCM) with MCYT and GPDS offline signature corpuses. In these studies the corpuses contain signatures written on a uniform white “nondistorting” background, however the gray level distribution of signature strokes changes when it is written on a complex background, such as a check or an invoice. The aim of this paper is to measure gray level features robustness when it is distorted by a complex background and also to propose more stable features. A set of different checks and invoices with varying background complexity is blended with the MCYT and GPDS signatures. The blending model is based on multiplication. The signature models are trained with genuine signatures on white background and tested with other genuine and forgeries mixed with different backgrounds. Results show that a basic version of local binary patterns (LBP) or local derivative and directional patterns are more robust than rotation invariant uniform LBP or GLCM features to the gray level distortion when using a support vector machine with histogram oriented kernels as a classifier.

I. INTRODUCTION
BIOMETRICS is playing an increasingly important role inpersonal identification and authentication systems. Several technologies that have been developed in this area are based on fingerprints, iris, face, voice, the handwritten signature, hand, etc. Handwritten signatures occupy a very special place in this wide set of biometric traits. The main reason is tradition: handwritten signatures have long been established as the most widespread means of personal verification. Signatures are generally accepted by governments and financial institutions as a legal means of verifying identity. Moreover, verification by signature analysis requires no invasive measurements and people are used to this event in their day to day activities A handwritten signature is the result of a complex process depending on the psychophysical state of the signer and the conditions under which the signing process occurs. Although complex theories have been proposed to model the psychophysical mechanisms underlying handwriting and the ink processes, signature verification is still an open challenge since a signature is usually judged to be genuine or a forgery on the basis of only a few reference specimens [1]. Two methods of signature verification stand out. One is an offline method that uses an optical scanner to obtain handwriting data from a signature written on paper. The other, which is generally more successful, is an online method, which, with a special device, measures the sequential data, such as handwriting speed and pen pressure. Although less successful than the online method, offline systems do have a significant advantage because they do not require access to special processing systems when the signatures are produced [2]. There are two main approaches for offline signature verification: static approaches and pseudo-dynamic approaches. The static one involves geometric measures of the signature while the pseudo-dynamic one tries to estimate dynamic information from the static image [3]. Three different approaches in the reconstruction of dynamic information from static handwriting records can be used: mathematical methods, which estimate the temporal order of stroke production; methods inspired by motor control theory, which recover temporal features on the basis of stroke geometries such as curvature; and methods analyzing stroke thickness and stroke intensity variations. Among the techniques that analyze the stroke thickness or stroke intensity variations, we highlight those that focus on the gray level distribution in the signature stroke. Ammar et al. [4] study the higher pressure points (HPPs) which indicate the regions where more effort has been made by the signer and are located as the darker zones of signature strokes. Mitra et al. in [5] propose analyzing both higher and lower pressure points. Lv et al. [6] divide the gray level values into 12 segments between the thresholds to segment the foreground and edge points. The percentage of pixels in each segment is added to the feature vector which also includes a stroke width distribution. Reference [7] proposes a radial and angular grid for a local rate of higher pressure points numbers versus number of pixels. Franke in [8] and [9] identify several ink-trace characteristics affected by the interaction of biomechanical writing and physical ink-deposition processes. Some authors thus suggest that the selected characteristic can be measured using texture features such as local binary pattern (LBP) which has already been applied in face and hand biometrics [10], [11]. Studies carried out after the initial LBP proposal have allowed the characterization carried out by this operator to improve, e.g., [12] and [13]. Specifically, [14] proposes the use of rotational invariant uniform local binary patterns LBP plus LBP and statistical measures from gray level co-occurrence matrices (GLCM) with theMCYT database and the first 100 signers of GPDS960GraySignature database. The results presented in [14] are summarized in Table I. An improved version of [14] that uses the mentioned texture features worked out from the detail matrices of signature wavelet transform was published in [15]. The major problem LBP has is its sensitivity when noise is present in the signature. Given that gradients are more stable than gray level features, the local derivative pattern (LDerivP) [16] and local directional pattern (LDP) [17] have been proposed. These patterns have been applied to face biometrics [18]. The LDP has also been applied to black and white signatures in [19] and to gray signatures in [20]. The previously mentioned developments occurred with databases using signatures from uniform and white backgrounds. It is well known that writing a signature on complex backgrounds changes the gray level distribution of signature strokes. This change suggests that the robustness of pseudo-dynamic features based on gray level should be assessed. The aim of this paper is to evaluate the dependence of the gray level based features [14] and propose strategies to improve their robustness to gray level distortion and segmentation errors due to complex backgrounds. All of the experimentation presented in this paper has been performed from a biometric point of view, which basically decides whether a signature is authentic or forged. We are aware that it is neither optimally informative nor practical for the forensic handwriting experts (FHEs) to carry out tasks such as detecting disguised signatures [21] or identifying the author of a forgery [22]. This paper proposes new methods that improve the automatic verification of signatures retrieved from checks and contributes to closing the gap between biometric state-of-the-art and FHEs requirements. The rest of the paper is organized as follows: Section II describes the signature and check databases, Section III presents the signature preprocessing and Section IV the signature features. Section V describes classifiers design while Section VI shows the experimental methodology, experiments and results. The paper closes with conclusions in Section VII.

II. DATABASE
Two publicly available offline signature corpuses were used. The first database is the offline subcorpus of theMCYT database
[3]. It includes 75 signers from four different Spanish sites. The corpus includes 15 genuine signatures and 15 simulated forgeries for each signer. Genuine signatures were acquired in two sessions. Forgers are given the signature images of clients to be forged and, after training with them several times, they are asked to imitate the shape. All signature data were acquired with the same inking pen and the same paper templates, over a similar pen tablet. The paper templates were scanned at 600 dpi. The second database is the GPDS960GraySignature corpus. This corpus contains 24 genuine signatures and 24 simulated forgeries from 881 individuals.1 The genuine signatures were taken in just one session. The repetitions of each genuine signature and forgery specimen were collected, allowing each user his or her own pen on sheets of white A4 paper. Each sheet provided two different box sizes for the signature: the first is 5 cm wide and 1.8 cm high and the second is 4.5 cm wide and 2.5 cm high. The forgeries were collected by forms with 15 boxes. Each forger form shows five images of different genuine signatures chosen randomly. The forger imitated each one three times for all five signatures. They were given unlimited time to learn the signatures and perform the forgeries. The forgers are not experts. The complete signing process was supervised by an operator. The sheets were scanned at 600 dpi with 256 gray levels, automatically segmented and manually supervised with the aim of reducing segmentation errors. The check database contains 20 images: 12 bank checks and 8 invoices with varying degrees of background complexity.2 Examples of the signing area of three checks with different background complexity are shown in Fig. 1.

A. Gray-Level Distortion at Database
TheMCYT and GPDS960Gray signatures were blended with the check database to obtain the synthetic signature database, that is, a new database with distorted gray levels. There are many different types of blending modes: darken, multiply, color or linear burn, lighten, color or linear dodge, etc. We used the multiply blend mode which multiplies the check image by the signature one. As we overlay gray level strokes, each stroke results in a new darker gray level. The pixels outside of the strokes are unaffected because the white signature background does not generate a change [23]. A description of the procedure follows: Let be an image from the MCYT or GPDS960GraySignature database and be the image of the check signing area (see Fig. 1), both of 256-level gray scale and by pixels. In order to ensure that the pixels outside of the strokes remain unchanged when blending the check and the signature, is converted to black and white with a fixed threshold equal to 222 (strokes 1The signatures of the GPDS960GraySignatures database are the same ones as those of the GPDS960Signatures corpus. In the first case, the signatures were scanned at 600 dpi and saved in gray scale while in the second case they were scanned at 300 dpi and saved in black and white. The GPDS960GraySignatures has just 881 signatures and 24 simulated forgeries per signer because several original sheets were unfortunately lost. The lost sheets are distributed all around the database. The signers of both databases share the same identifier. 2All the check images, the Matlab code to read the database and the program to blend the checks and signatures can be downloaded free of charge from http:// www.gpds.ulpgc.es/download/index.htm.
in white and the background in black) obtaining
if otherwise signature strokes (1)
The blended image is obtained by multiplying the pixels corresponding to the signature strokes as shown in (2) at the bottom of the page. An example of the result is shown in Fig. 2. Given that the checks vary in background complexity, the MCYT and GPDS960GraySignature databases were tripled in three databases, depending on the background complexity. In total, eight databases are used and are listed in Table II. The Gray level distortion Gd of each signature was calculated as follows:
Gd
(3)
The distribution of Gd for MCYT database with low, medium and high gray level distortion can be seen in Fig. 3.

III. SIGNATURE PREPROCESSING
A. Background Removal in Databases With No Gray Level Distortion
The background of the scanned signatures in MCYT and GPDS databases is well contrasted with the darker signature strokes so the signature images were binarized by posterization
as suggested in [14]. Let be a 256-level, gray scale signature image of the database, the gray level posterized image is then defined as
round round (4)
where round rounds the elements to the nearest integer. The authors in [14] suggest . To obtain the binarized signature (white strokes and black background) we apply a simple thresholding operation, as follows:
if otherwise. (5)
The binarized image displays hair-like artifacts which emerge from signature strokes, as seen in Fig. 4. These artifacts are eliminated as seen in (6), shown at the bottom of the next page, which correspond to or-exclusive operations. The previously specified operation converts the white pixels to black if the left and right pixels are black or if the upper and lower pixels are black. Fig. 4 shows an example of the above-mentioned operation on a signature stroke. The black andwhite image is used as amask to segment the original signature. The segmented signature is obtained as
if otherwise. (7)
An example of the previously described procedure is shown in Fig. 5.
if otherwise (signature strokes) (2)
Fig. 3. Distribution of the signature Gray level distortion for each database.
Fig. 4. Eliminating hair-like noise from signature strokes. (Left) signature
stroke detail of with noise. (right) Same signature stroke detail of
without noise.
Fig. 5. Segmentation procedure. Details of (a) original image with 256 gray levels, (b) binarized and noise reduced signature , (c) segmented signature with white background.

B. Background Removal in Databases With Gray Level Distortion
The posterization procedure is not useful with the databases of the signatures blended with the check images, because the background does not contain uniform character, lines and gray level textures. In this case, we used different background removal algorithms. Two methods for signature segmentation were considered: 1) segmentation of the database using the information from the original signature which allows the influence of the gray level distortion to be measured without segmentation error and 2) the
256 gray levels, (b) binarized signature , (c) segmented signature with gray level distortion, (d) segmented signature without gray level distortion. Circles identify some gray level differences.
use of automatic procedures to eliminate the background which includes segmentation error. 1) Signature Segmentation Using Original Signature: Let be a 256-level, gray scale signature image of the database without gray level distortion (MCYT or GPDS) and be the same signature converted to black and white as in (6). Let be the same signature as except belonging to a database with gray level distortion [see (2)]. The signature of is segmented as
GD1 if otherwise (8)
with the segmented signature. An example of the procedure can be seen in Fig. 6. The circles identify the major gray level distortion in the example. 2) Automatic Signature Segmentation on Complex Backgrounds: The second method for removing a complex background requires that the signature is segmented from the signed check without using the original signature. The chosen procedure binarizes the check by means of Otsu’s threshold. The resulting image contains the signatures strokes plus several lines and text from the check with noise. The binarized image is cleaned by removing the smaller objects. Two additional procedures follow: the first one eliminates the lines while the second one reduces the amount of residual text. The beginning and end of each line is detected by means of the Hough transform and, once the line width is detected, its pixels are turned to white except when the line crosses another object. Text reduction is performed by obtaining the centroids of all the objects and selecting those that are lined up: elimination occurs when at least four objects are aligned to a similar height. A minimum height is required so that low pressure signatures strokes which are similar to dotted lines are not erased. The result is shown in Fig. 7. The resulting signature is called and the signature automatically segmented with the gray level distorted is called obtained in a similar way as in (8).
if and and otherwise if and and otherwise
(6)
procedure (middle), after reducing the amount of residual text (right).
Let be a 256-level, gray scale signature image of the database without gray level distortion (MCYT or GPDS database) and the same signature converted to black and white. Let be the same signature as but belonging to database with gray level distortion and be the black and white signature automatically segmented. The segmentation error Se is measured as
(9)
with xor the or-exclusive operator. The averaged distribution of Se for all the databases is seen in Fig. 8. Observe that the average of Se is 0.016. An example of automatically segmented signatures with different Se can be seen in Fig. 9.

IV. GRAY LEVEL BASED SIGNATURE FEATURES
A. Local Patterns
The local binary pattern (LBP) operator is defined as a gray level invariant texture measure in a local neighborhood [24]. The original LBP operator labels the pixel of an image by thresholding the 3 3 neighborhood of each pixel and concatenating the results binomially to form a number. Assume that a given image is defined as . The LBP operator transforms the input image LBP to as follows:
LBP (10)
where
is the unit step function and is the 8-neighborhood around (see Fig. 10). The LBP can be extended to a generalized LBP [see (12)] by defining as the gray level of with equally spaced pixels on the circumference of a circle with radius [see (11)]
(11)
LBP (12)
Ojala et al. [25] also observed that in significant image areas certain local binary patterns appear frequently. These patterns are named as uniform LBP since they contain very few transitions from 0 to 1 or 1 to 0 in circular bit sequences. Therefore, they defined the uniform LBP as
LBP
with (13) and the rotation invariant uniform LBP operator as seen in (14), shown at the bottom of the page. An LBP can be considered as the concatenation of the binary gradient directions and is called a micropattern. The histogram of these micropatterns contains information on the distribution of the edges, spots, and other local figures in an image. Nevertheless, these variations of LBPs are still sensitive to random noise and nonmonotonic illumination variation. References [16] and [17] investigated the effectiveness of using high-order local patterns in which the th-order derivative direction variations are coded based on binary coding function. Zhang et al. provided calculations for the first order derivatives in [16] along or 135 as
(15)
LBP if LBP otherwise.
(14)
with as in Fig. 10. The local derivative pattern is defined as
LDerivP LDerivP (16)
with LDerivP the local derivative pattern in direction
LDerivP (17)
The previous definition can be generalized to th-order derivative. In a similar way, Jabid et al. [17] proposed a pattern called the local directional pattern (LDP) which is based on the eight directional edge response values computed by the Kirsh masks . The edge responses are worked out as
(18)
Given that the response values are not equally important in all directions, the LDP focuses on the most prominent directions. The LDP code is generated as
LDP (19)
where is the th most significant directional response of the sequence . Since in our case , only 3 bits are on in the LDP code which corresponds to 56 possible values. Since edge responses change less and are more stable than intensity values, LDerivP and LDP patterns are expected to provide more stable values in the presence of gray level distortion.

B. Histogram of Local Patterns
The gray level image is transformed to LBP LDP or LDerivP code matrices. Each code matrix contains information about the structure to which the pixel belongs: the stroke edge, stroke corners, stroke ends, inside the stroke or background, etc. We model the distribution of the local pattern by spatial histogram to avoid losing the location of the different structures inside the image. Therefore, the image is divided into a number of adjacent regions [18]. After conducting several experiments and testing a range of smaller and greater region sizes, the best equal error ratio performance was obtained when dividing the image into four equal vertical blocks and three equal horizontal blocks which overlapped by 60% [19]. Consequently, there are 12 blocks and for each block, we calculate the histograms and with , 45 , 90 , 135 . The histogram of LBP and LDerivP contains 255 bins while the histogram of LDP contains 55 bins. Note that the bin corresponding to the code of the background was not considered. An example of is seen in Fig. 11. The histograms of all the blocks are concatenated to avoid losing the spatial information obtaining the next vectors (nev-
V. VERIFIERS DESIGN

A. Histogram Similarity Measures
Many similarity measures for histogram matching have been proposed. The simplest one used to measure the similarity between two histograms is given in [26]
(20)
where is the histogram intersection statistic between histograms and . This measure is intuitively appealing because it calculates the common parts of two histograms. Its computational complexity is very low as it requires only simple operations [16]. It is also possible to use other measures for similarity, such as the chi-square distance defined as
(21)
B. Classifiers The previously mentioned measures are used for the nearest neighbor classifier, with the test sample or questioned his-
togram and the template histogram feature or unquestioned reference. Alternatively, we have also used a support vector machine (SVM) as a classifier. An SVM is a popular supervised machine learning technique which performs an implicit mapping into a higher dimensional feature space. This is the so-called kernel trick. After the mapping is completed it finds a linear separating hyperplane with maximal margin to separate data from this higher dimensional space. Least squares support vector machines (LS-SVM) are reformulations to standard SVMs which solve the indefinite linear systems generated within them. Robustness, sparseness, and weightings can be imposed to LS-SVMs where needed and a Bayesian framework with three levels of inference is then applied [27]. Though new kernels are being proposed, the most frequently used kernel functions are linear, polynomial, and radial basis function (RBF). This study tested the linear and RBF kernel. In the case of the RBF kernel, the concatenated histogram vector produces a very long feature vector for training a classifier with a few genuine samples. Discrete cosine transform (DCT) or principal component analysis (PCA) are two methods that were tested to reduce dimensionality. Another classical kernel that is used in computer vision and in this study is the kernel [28]. Several authors define the additive kernel as the negative of the distance. As such a kernel is only conditionally positive definite, here we have used as kernel:
(22)
which makes the additive kernel positive definite [28]. Finally, we also tested a generalized version of the histogram intersection (GHI) as a kernel [29], which has already been used in face biometrics [30] and is defined as
(23)
with a real number greater than 0. SVM or LS-SVM makes binary decision and multiclass classification possible by adopting the one-against-one or one-against all techniques. In our study we adopted the second technique. We carried out grid-search on the hyperparameters in the ten-fold cross validation for selecting the parameters on the training sequence. The parameters setting that produce the best cross-validation accuracy were picked.

VI. EXPERIMENTAL RESULTS
An automatic signature verifier should assess whether a questioned signature is an authentic signature normally used by the reference writer. A failure in this case is referred to as type I error (false rejection or miss). The system should also avoid the acceptance of questioned signatures which are the product of a forgery process. The second error is referred to as type II error (false acceptance or false alarm). Commonly, forgeries are divided into three different types: a fictitious signature which is
usually a genuine signature belonging to a different writer, a simple forgery for which the forger knows the writer’s name, and finally the simulated forgery which is a reasonable imitation of the genuine signature model [31]. In this paper, fictitious signatures and simulated forgery types were used. The signature model will be trained only with genuine and fictitious signatures.

A. Evaluation Methodology
Following the experimental protocol proposed in [32] for the MCYT database, the training set consists of five or ten randomly selected genuine signatures (depending on the experiment under consideration). The remaining genuine signatures are used for testing. When testing a specific signer, fictitious signatures test scores are computed by using the available test genuine samples from all the remaining users. Real impostor test scores are computed considering the simulated forgeries of each signer. As a result, we have or scores of genuine signatures, or test scores of fictitious signatures, and test scores for simulated forgeries. All the experiments are repeated ten times and the averaged results in term of equal error rate (EER) are provided. In the case of GPDS960Graysignature, we followed the same methodology as in the case of MCYT database. In order to compare the results of this database with those obtained with the MCYT database, we have performed the experiment with GPDS960Graysignature database three times: The first one using the first 75 users, the second one using the first 300 users and the last one with all of the users of the corpus.

B. Experiments
The experiments were designed to determine the influence of the gray level distortion and segmentation errors on the verification task. Therefore, the first experiment was aimed at showing the EER of different verifier configurations (nearest neighbor classifier with histogram intersection and Chi-square similarity measures and LS-SVMwith linear, RBF, histogram intersection and Chi-square kernels) with the different parameters proposed (LBP LBP LBP , LBP, LDP and LDerivP). Once the best combination is found, we measured the robustness of the designed verified against the multiple sessions, different ink type, increasing the number of training samples, increasing the number of users in the system and the influence of the automatic signature location and segmentation. Nearest Neighbor Classifiers: We used two dissimilarity measures for histograms: chi-square and histogram intersection. The results with Chi-square distance are given in Table III. Using histogram intersection, the EER lies around 19% and
TABLE VII
EER WITH LS-SVM CLASSIFIER AND KERNEL
TABLE VIII EER WITH LS-SVM CLASSIFIER AND GHI KERNEL
with kernel trained with ten samples. The improvement with respect to training with five samples is around 35% and 15% for fictitious signatures and simulated forgeries, respectively. The results with simulated forgeries are similar to those in Table I. Multisession Experiments: The results were obtained by randomly selecting the training sequence in the MCYT database. This database was recorded in two sessions. An evaluation of the robustness of the proposed system to signatures registered in different session is shown in Table X, including the results of training the classifier with five signatures randomly chosen from the first session and testing the verifier with signatures
MCYT3 databases and LS-SVM classifier with kernel. Left image testing with fictitious signatures, right image testing with simulated forgeries.
from the second session. As expected, the system reduces its performance by approximately 28% and 10% for fictitious signatures and simulated forgeries respectively, when compared to Table VII. Influence of Ink: All signers from the MCYT database used the same pen and signing surface. The influence of the pen and signing surface was evaluated using the GPDS960Gray database in which each signer and forger uses his own pencil and signs at his desk. The results of this experiment can be seen in Table XI. Observe that LBP and LDP are slightly worse when compared to the results from Table VII but LDerivP improves
by 14%. The verifier in this database also displays a more robust behavior against gray level distortion. The reason is that when training the user model with different inks, its model generalizes for more than just one ink case. Nevertheless, the influence of the ink is small because the background has been removed and the most relevant texture patterns belong to the signature stroke borders [19]. Increasing Number of Users: The GPDS960GraySignature database can also be used tomeasure the influence of the number of users over the proposed system. We tested the error with 300 users and with the entire database. Analyzing the results given in Table XII and XIII, in comparison with Table XI, reveals that going from 75 to 300 users worsen the results by 170% and going from 75 to 800 users worsen the result by 250% with fictitious signatures. The average decreases in accuracy with the simulated forgeries are 40% and 50%, respectively. Combining Parameters at Score Level: As in the case of other biometric technologies, the results are expected to improve if scores from different features are combined. Table XIV shows the results obtained when combining the proposed features in pairs and all together. As expected, the results improve but at the price of increasing the computational load. In the case of the fictitious signatures the improvement is greater than in the case of the simulated forgeries. Robustness Against Segmentation Errors: The previous results only include the errors due to the gray level distortion of the check background because we use the original signature to
segment it. Table XV shows the results for the automatic signature verifier using the automated segmentation procedure previously described. In this case, the featured signature includes additional noise due to the rest of the check lines and letters overlapped with the signature. In this case the performance worsens on average by 360% and 40% for fictitious signatures and simulated forgeries, respectively when compared with Table VII. Locating Signature on a Check: The results presented show the performance of the system when the signature location is known. In a bank application for biometric automatic check signature verification, only the signed check and an indubitable signature are available. In this section we programmed the algorithm proposed in [33] for signature position detection in a check which, combined with automatic segmentation, reveals information concerning the robustness of the whole procedure. The method proposed in [33] is based on an accumulative evidence technique, searching the region that maximizes a measure of correspondence with a given reference signature. This measure is based on the similarity of the slope marked out by each of the strokes in the signature. Table XVI shows the results for the automatic check signature verifier using the signature check location and the automated segmentation procedure. Fig. 15 shows DET plots that compare the system performance when the original signature and its position on the check is known, when we know where the signature in the check is located and when we do not use the previous information. Obviously, the last case cannot be applied to a real forensic casework, but it could be adapted to provide results in the form of likelihood ratios [34] and used as additional evidence in a forensic tool, e.g., the WANDA system [35]. Computational Load Consideration: The previous results have shown that the LDerivP features represent a good signature representation. The major problem of these features is their high dimension and high computational load. Table XVII displays the average time consumption of an Intel Core i5 CPU
the check location procedure. Classifier used: LS-SVM with kernel. Left image testing with fictitious signatures, right image testing with simulated forgeries.

VII. CONCLUSION
Histograms of local binary, local directional and local derivative patterns used for texture measures are proposed for offline automatic signature verification. These parameters were evaluated with different classifiers such as nearest neighbor and SVM. SVM was evaluated with different kernels such as the classical RBF and the histogram-oriented kernels GHI and kernels. The results show that the kernel provides the best results with local derivative patterns. These results improve those reported in [14]. The SVM with kernel has proved to be more robust against the gray level distortion when the signatures are mixed with bank checks. With this configuration, the robustness against gray level distortion is very similar with the basic LBP, LDP and LDerivP features. The proposed configuration is evaluated under different circumstances: changing the number of training signatures, multiple signing sessions, database with different inks, increasing the number of signers and combining different features at score
level. Results have also been provided when looking for the signature in the check and segmenting it automatically. In all the cases, the best results were obtained with the LDerivP parameters, which improve the results claimed in the established baseline, revealing very significant improvements with fictitious signatures.

References
[1]D. Impedovo,G. PirloAutomatic signature verification: The state of the artIEEE Trans. Syst. Man Cybern.—Part C: Applications Rev., vol. 38, no. 5, pp. 609–635, Sep. 2008.2008
[2]W. Hou,X. Ye,K. WangA survey of off-line signature verificationProc. Int. Conf. Intelligent Mechatronics Automation, Chengdu, China, Aug. 2004, pp. 536–541.2004
[3]J. Fierrez-Aguilar,N. Alonso-Hermira,G.Moreno-Marquez,J. Ortega-GarciaAn off-line signature verification system based on fusion of local and global informationProc. Workshop Biometric Authentication, Springer LNCS-3087, May 2004, pp. 298–306.2004
[4]M. Ammar,Y. Yoshida,T. FukumuraA new effective approach for automatic off-line verification of signatures by using pressure featuresProc. 8th Int. Conf. Pattern Recognition, 1986, pp. 566–569.1986
[5]A. Mitra,P. Kumar,C. ArdilAutomatic authentication of handwritten documents via low density pixel measurementsInt. J. Computational Intell., vol. 2, no. 44, pp. 219–223, 2005.2005
[6]H. Lv,W. Wang,C. Wang,Q. ZhuoOff-line Chinese signature verification based on support vector machinePattern Recognition Lett., vol. 26, pp. 2390–2399, 2005.2005
[7]J. Vargas,M. Ferrer,C. Travieso,J. AlonsoOff-line signature verification based on high pressure polar distributionProc. Int. Conf. Frontiers of Handwriting Recognition 2008, Montreal, Canada, 2008.2008
[8]K. Franke,S. RoseInk-deposition model: The relation of writing and ink deposition processesinProc. Ninth Int. Workshop IEEEComputer Soc. Frontiers in Handwriting Recognition, Washington, DC, 2004, pp. 73–178.2004
[9]K. Franke,O. Bünnemeyer,T. SyInk texture analysis for writer identificationProc. Eighth Int. Workshop IEEE Computer Soc. Frontiers in Handwriting Recognition, Washington, DC, 2002, pp. 268–273.2002
[10]T. Mäenpää,M. Pietikäinen,C.H. Chen,P.S.P. Wang,Eds.Handbook of pattern recognition and computer visionTexture Analysis With Local Binary Patterns, 3rd ed. Singapore: World Scientific, 2005, pp. 197–216.2005
[11]T. Ahonen,A. Hadid,M. PietikainenFace description with local binary patterns: Application to face recognitionIEEE Trans. Pattern Anal. Machine Intell., vol. 28, no. 12, pp. 2037–2041, Dec. 2006.2006
[12]S. Liao,M.W.K. Law,A.C.S. ChungDominant local binary patterns for texture classificationIEEE Trans. Image Process., vol. 18, no. 5, pp. 1107–1118, May 2009.2009
[13]Z. Guo,L. Zhang,D. ZhangA completed modeling of local binary pattern operator for texture classificationIEEE Trans. Image Process., vol. 19, no. 6, pp. 1657–1663, Jun. 2010.2010
[14]J.F. Vargas,M.A. Ferrer,C.M. Travieso,J.B. AlonsoOffline signature verification based on grey level information using texture featuresPattern Recognition, vol. 44, no. 2, pp. 375–385, Feb. 2011.2011
[15]J.F. Vargas,C.M. Travieso,J.B. Alonso,M.A. FerrerOff-line signature verification based on gray level information using wavelet transform and texture featuresProc. Int. Conf. Frontiers in Handwriting Recognition, Kolkata, India, Nov. 16–18, 2010, pp. 587–592.2010
[16]B. Zhang,Y. Gao,S. Zhao,J. LiuLocal derivative pattern versus local binary pattern: Face recognition with high-order local pattern descriptorIEEE Trans. Image Process., vol. 19, no. 2, pp. 533–544, Feb. 2010.2010
[17]T. Jabid,M.H. Kabir,O. ChaeLocal directional pattern (LDP)—A robust image descriptor for object recognitionProc. Seventh IEEE Int. Conf. Advanced Video Signal Based Surveillance, Boston, MA, Aug. 29–Sept. 1 2010, pp. 482–487.2010
[18]T. Jabid,M.H. Kabir,O. ChaeLocal directional pattern (LDP) for face recognitionDig. Technical Papers Int. Conf. Consumer Electronics, Las Vegas, NV, Jan. 9–13, 2010, pp. 329–330.2010
[19]M.A. Ferrer,F. Vargas,C.M. Travieso,J.B. AlonsoSignature verification using local directional pattern (LDP)Proc. IEEE Int. Carnahan Conf. Security Technology, San Jose, CA, Oct. 5–8, 2010, pp. 336–340. FERRER et al.: ROBUSTNESS OF OFFLINE SIGNATURE VERIFICATION BASED ON GRAY LEVEL FEATURES 9772010
[20]M.A. Ferrer,A. Morales,J.F. VargasOff-line signature verification using local patterns2 Congreso Nacional de Telecomunicaciones, Arequipa, Peru, May 2011.2011
[21]M. Liwicki,C.E. van den Heuvel,B. Found,andM.I.MalikForensic signature verification competition 4nsigcomp2010—Detection of simulated and disguised signaturesProc. 12th Int. Conf. Frontiers in Handwriting Recognition, Kolkata, India, Nov. 16–18, 2010, pp. 715–720.2010
[22]O. HiltonCan the forger be identified from his handwritingJ. Criminal Law, Criminology, Police Sci., vol. 43, pp. 547–555, 1952.1952
[23]T. Ojala,M. Pietikäinen,D. HarwoodA comparative study of texture measures with classification based on feature distributionPattern Recognition, vol. 29, no. 1, pp. 51–50, 1996.1996
[24]T. Ojala,M. Pietikainen,T. MaenpaaMultiresolution gray-scale and rotation invariant texture classification with local binary patternsIEEE Trans. Pattern Anal. Machine Intell., vol. 24, no. 7, pp. 971–987, 2002.2002
[25]M. Swain,D. BallardColor indexingInt. J. Computer Vision, vol. 7, pp. 11–32, 1991.1991
[26]A. Vedaldi,A. ZissermanEfficient additive kernels via explicit feature mapsProc. IEEE Conf. Computer Vision Pattern Recognition, Jun. 13–18, 2010, pp. 3539–3546.2010
[27]S. Maji,A.C. Berg,J. MalikClassification using intersection kernel support vector machines is efficientProc. IEEE Conf. Computer Vision Pattern Recognition, Jun. 23–28, 2008, pp. 1–8.2008
[28]J. Qiang-Rong,G. YuanFace recognition based on detail histogram intersection kernelProc. IEEE Int. Conf. Intelligent Computing Intelligent Systems, Shangai, China, Nov. 20–22, 2009, vol. 4, pp. 71–74.2009
[29]D. Bertolini,L.S. Oliveira,E. Justino,R. SabourinReducing forgeries in writer independent off-line signature verification through ensemble of classifiersPattern Recognition, vol. 43, no. 1, pp. 387–396, Jan. 2010.2010
[30]A. Gilperez,F. Alonso-Fernandez,S. Pecharroman,J. Fierrez,J. Ortega-GarciaOff-line signature verification using contour featuresProc. Int. Conf. Frontiers in Handwriting Recognition, Montreal, Canada, Aug. 19–21, 2008.2008
[31]J.L. Esteban,J.F. Vélez,Á. SánchezOff-line handwritten signature detection by analysis of evidence accumulationInt. J. Document Analysis Recognition, pp. 1–10, Oct. 20, 2011.2011
[32]J. Gonzalez-Rodriguez,J. Fierrez-Aguilar,D. Ramos-Castro,J. Ortega-GarciaBayesian analysis of fingerprint, face and signature evidences with automatic biometric systemsForensic Sci. Int., vol. 155, no. 2–3, pp. 126–140, Dec. 20, 2005.2005
