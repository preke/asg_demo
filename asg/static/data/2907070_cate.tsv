top_n_words	label	title	abstract	topic_word	topic_bigram	topic_trigram
sample set degree boundary minority add advantage date essential generate guide hotspot ideal light obvious participate people possible view new 	0	An improved SMOTE imbalanced data classification method based on support degree	Imbalanced data-set Classification has become a hotspot problem in Data Mining. The essential assumption of the traditional classification algorithms is that the distribution of the classes is balanced, therefore the algorithms used in Imbalanced data-set Classification cannot achieve an ideal effect. In view of imbalance date-set classification, we propose an oversampling method based on support degree in order to guide people to select minority class samples and generate new minority class samples. In the light of support degree, it is now possible to identify minority class boundary samples, then produce a number of new samples between the boundary samples and their neighbors, finally add the synthetic samples to the original data-set to participate in training and testing. Experimental results show that the method has an obvious advantage in dealing with imbalanced data-set. 	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
tree dn ensemble combined conventional bagging decision model 66 c4 collection considers data— designed distance disturbing generally generating hddt hellinger 	0	Disturbing neighbors ensembles of trees for imbalanced data	Disturbing Neighbors (DN ) is a method for generating classifier ensembles. Moreover, it can be combined with any other ensemble method, generally improving the results. This paper considers the application of these ensembles to imbalanced data: classification problems where the class proportions are significantly different. DN ensembles are compared and combined with Bagging, using three tree methods as base classifiers: conventional decision trees (C4.5), Hellinger distance decision trees (HDDT) —a method designed for imbalance data— and model trees (M5P) —trees with linear models at the leaves—. The methods are compared using two collections of imbalanced datasets, with 20 and 66 datasets, respectively. The best results are obtained combining Bagging and DN , using conventional decision trees. 	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
knowledge discovery raw understanding research review critical challenge complex data provide advance amount art availability continuous development direction due efficiently 	0	Learning from imbalanced data	With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
unlabeled active informativeness work available informative sample point arise during evolved hyper label many minimum promising risk separately such user 	0	Active learning from positive and unlabeled data	During recent years, active learning has evolved into a popular paradigm for utilizing user’s feedback to improve accuracy of learning algorithms. Active learning works by selecting the most informative sample among unlabeled data and querying the label of that point from user. Many different methods such as uncertainty sampling and minimum risk sampling have been utilized to select the most informative sample in active learning. Although many active learning algorithms have been proposed so far, most of them work with binary or multi-class classification problems and therefore can not be applied to problems in which only samples from one class as well as a set of unlabeled data are available. Such problems arise in many real-world situations and are known as the problem of learning from positive and unlabeled data. In this paper we propose an active learning algorithm that can work when only samples of one class as well as a set of unlabeled data are available. Our method works by separately estimating probability density of positive and unlabeled points and then computing expected value of informativeness to get rid of a hyper-parameter and have a better measure of informativeness. Experiments and empirical analysis show promising results compared to other similar methods. 	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
rusboost smoteboost boosting simpler alternative faster technique domain learner sampling 15 alleviate attractive comparably component conduct create evaluates given hybrid 	0	RUSBoost: A hybrid approach to alleviating class imbalance	Class imbalance is a problem that is common to many application domains. When examples of one class in a training data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classification models. Several techniques have been used to alleviate the problem of class imbalance, including data sampling and boosting. In this paper, we present a new hybrid sampling/boosting algorithm, called RUSBoost, for learning from skewed training data. This algorithm provides a simpler and faster alternative to SMOTEBoost, which is another algorithm that combines boosting and data sampling. This paper evaluates the performances of RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using 15 data sets from various application domains, four base learners, and four evaluation metrics. RUSBoost and SMOTEBoost both outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a simpler and faster technique. Given these experimental results, we highly recommend RUSBoost as an attractive alternative for improving the classification performance of learners built using imbalanced data.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
adaboost having error dataset higher prediction baboost bit certain decrease indicates little margin predict problematic proved unbalanced weight majority it 	0	An improved AdaBoost algorithm for unbalanced classification data	AdaBoost algorithm is proved to be a very efficient classification method for the balanced dataset with all classes having similar proportions. However, in real application, it is quite common to have unbalanced dataset with a certain class of interest having very small size. It will be problematic since the algorithm might predict all the cases into majority classes without loss of overall accuracy. This paper proposes an improved AdaBoost algorithm called BABoost (Balanced AdaBoost), which gives higher weights to the misclassified examples from the minority class. Empirical results show that the new method decreases the prediction error of minority class significantly with increasing the prediction error of majority class a little bit. It can also produce higher values of margin which indicates a better classification method	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
active yield efficient prediction achieves criterion diagnosis early entire medical necessitate observation pool providing stopping datasets way competitive concept smaller 	0	Learning on the border: Active learning in imbalanced data classification	This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly less number of observations of the target concept. Various real-world classification tasks, such as medical diagnosis, text categorization and fraud detection suffer from this phenomenon. The standard machine learning algorithms yield better prediction performance with balanced datasets. In this paper, we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes. We also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire dataset. The proposed method yields an efficient querying system and allows active learning to be applied to very large datasets. Our experimental results show that with an early stopping criteria, active learning achieves a fast solution with competitive prediction performance in imbalanced data classification.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
bi cost sensitive boosting algorithm applicable attainable attracts barrier concentrate drawback equal genetic interest lot misclassification most multiple optimum prevails 	0	Boosting for learning multiple classes with imbalanced class distribution	Classification of data with imbalanced class distribution has posed a significant drawback of the performance attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. This learning difficulty attracts a lot of research interests. Most efforts concentrate on bi-class problems. However, bi-class is not the only scenario where the class imbalance problem prevails. Reported solutions for bi-class applications are not applicable to multi-class problems. In this paper, we develop a cost-sensitive boosting algorithm to improve the classification performance of imbalanced data involving multiple classes. One barrier of applying the cost-sensitive boosting algorithm to the imbalanced data is that the cost matrix is often unavailable for a problem domain. To solve this problem, we apply Genetic Algorithm to search the optimum cost setup of each class. Empirical tests show that the proposed cost-sensitive boosting algorithm improves the classification performances of imbalanced data sets significantly.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
document learner evolutionary applying architecture carry classifying described discovers flexible goal guarantee independence manner modular multistrategy obtains parallel preclassified relies 	0	A multistrategy approach for digital text categorization from imbalanced documents	The goal of the research described here is to develop a multistrategy classifier system that can be used for document categorization. The system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample. The learners work in a parallel manner, where each learner carries out its own feature selection based on evolutionary techniques and then obtains a classification model. In classifying documents, the system combines the predictions of the learners by applying evolutionary techniques as well. The system relies on a modular, flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
svm feedback relevance subspace ab based random r bagging asymmetric sample 2 3 abrs cbir content dimension happens hyperplane image 	0	Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval	Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM’s optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
probability estimate calibration motivated uncertainty good approach affecting attained bag bagged bayesian bias bootstrap calibrated despite drastically estimator expert exploited 	0	Class probability estimates are unreliable for imbalanced data (and how to fix them)	Obtaining good probability estimates is imperative for many applications. The increased uncertainty and typically asymmetric costs surrounding rare events increases this need. Experts (and classification systems) often rely on probabilities to inform decisions. However, we demonstrate that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. To our knowledge, this problem has not previously been explored. Motivated by our exposition of this issue, we propose a simple, effective and theoretically motivated method to mitigate the bias of probability estimates for imbalanced data that bags estimators calibrated over balanced bootstrap samples. This approach drastically improves performance on the minority instances without greatly affecting overall calibration. We show that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates. 	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
adaptively ramoboost based characteristic minority 19 adaptive analysis—is assessed briefly efficiency explosive growth hypothesis idea iteration metrics—including ramo ranked shift 	0	Ramoboost: Ranked minority oversampling in boosting	In recent years, learning from imbalanced data has attracted growing attention from both academia and industry due to the explosive growth of applications that use and produce imbalanced data. However, because of the complex characteristics of imbalanced data, many real-world solutions struggle to provide robust efficiency in learning-based applications. In an effort to address this problem, this paper presents Ranked Minority Oversampling in Boosting (RAMOBoost), which is a RAMO technique based on the idea of adaptive synthetic data generation in an ensemble learning system. Briefly, RAMOBoost adaptively ranks minority class instances at each learning iteration according to a sampling probability distribution that is based on the underlying data distribution, and can adaptively shift the decision boundary toward difficult-to-learn minority and majority class instances by using a hypothesis assessment procedure. Simulation analysis on 19 real-world datasets assessed over various metrics—including overall accuracy, precision, recall, F-measure, G-mean, and receiver operation characteristic analysis—is used to illustrate the effectiveness of this method.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
test f interesting wrapper cost value average example approach minority 99 accurately aware causing composite computes costly cup disease economy 	0	Wrapper-based computation and evaluation of sampling methods for imbalanced datasets	Learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. When the imbalance is large, classification accuracy on the smaller class(es) tends to be lower. In particular, when a class is of great interest but occurs relatively rarely such as cases of fraud, instances of disease, and regions of interest in largescale simulations, it is important to accurately identify it. It then becomes more costly to misclassify the interesting class. In this paper, we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (SMOTE) to improve minority class accuracy. The f-value serves as the evaluation function. Experimental results show the wrapper approach is effective in optimization of the composite f-value, and reduces the average cost per test example for the datasets considered. We report both average cost per test example and the cost curves in the paper. The true positive rate of the minority class increases significantly without causing a significant change in the f-value. We also obtain the lowest cost per test example, compared to any result we are aware of for the KDD Cup-99 intrusion detection data set.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
instance prediction site cluster reduces high technique for ratio traditional ambiguity balancing deployed extremely false filter frequently functional genomic hence 	0	An unsupervised learning approach to resolving the data imbalanced issue in supervised learning problems in functional genomics	Learning from imbalanced data occurs very frequently in functional genomic applications. One positive example to thousands of negative instances is common in scientific applications. Unfortunately, traditional machine learning treats the extremely small instances as noise. The standard approach for this difficulty is balancing training data by resampling them. However, this results in high false positive predictions. Hence, we propose preprocessing majority instances by partitioning them into clusters. This greatly reduces the ambiguity between minority instances and instances in each cluster. For moderately high imbalance ratio and low in-class complexity, our technique gives better prediction accuracy than undersampling method. For extreme imbalance ratio like splice site prediction problem, we demonstrate that this technique serves as a good filter with almost perfect recall that reduces the amount of imbalance so that traditional classification techniques can be deployed and yield significant improvements over previous predictor. We also show that the technique works for subcellular localization and post-translational modification site prediction problems.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
loss ” paradigm affected recovered treatment imbalance experimental design expected distribution degree performance 10 30 artificially ass community decade dozen 	0	An experimental design to evaluate class imbalance treatment methods	In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition applications; therefore, these research communities have responded to such interest with literally dozens of methods and techniques. Surprisingly, there are still many fundamental open-ended questions such as “Are all learning paradigms equally affected by class imbalance?”, “What is the expected performance loss for different imbalance degrees?” and “How much of the performance losses can be recovered by the treatment methods?”. In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods. This experimental setup uses real data sets with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We employ such experimental design in a large-scale experimental evaluation with twenty-two data sets and seven learning algorithms from different paradigms. Our results indicate that the expected performance loss, as a percentage of the performance obtained with the balanced distribution, is quite modest (below 5%) for the most balanced distributions up to 10% of minority examples. However, the loss tends to increase quickly for higher degrees of class imbalance, reaching 20% for 1% of minority class examples. Support Vector Machine is the classifier paradigm that is less affected by class imbalance, being almost insensitive to all but the most imbalanced distributions. Finally, we show that the sampling algorithms only partially recover the performance losses. On average, typically about 30% or less of the performance that was lost due to class imbalance was recovered by random oversampling and SMOTE. 	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
svm improves ability clustering confusion drop failure flaw interference intrinsic k nn obviously prune randomness som subjective then instance factor 	0	A hybrid re-sampling method for SVM learning from imbalanced data sets	Support Vector Machine (SVM) has been widely studied and shown success in many application fields. However, the performance of SVM drops significantly when it is applied to the problem of learning from imbalanced data sets in which negative instances greatly outnumber the positive instances. This paper analyzes the intrinsic factors behind this failure and proposes a suitable re-sampling method. We re-sample the imbalance data by using variable SOM clustering so as to overcome the flaws of the traditional re-sampling methods, such as serious randomness, subjective interference and information loss. Then we prune the training set by means of K-NN rule to solve the problem of data confusion, which improves the generalization ability of SVM. Experiment results show that our method obviously improves the performance of the SVM on imbalanced data sets.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
learner deficiency subset train majority undersampling approximately balancecascade classified classimbalance consideration correctly easyensemble ignored main removed sequentially step trained under 	0	Exploratory undersampling for class-imbalance learning	Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. EasyEnsemble samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. BalanceCascade trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the ROC Curve, F-measure, and G-mean values than many existing classimbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods.	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']
feature metric evaluated selection best systematic studied small study area additionally candidate combat comparison conducted developed efficacy examined inside likelihood 	1	Combating the small sample class imbalance problem using feature selection	The class imbalance problem is encountered in real-world applications of machine learning and results in a classifier’s suboptimal performance. Researchers have rigorously studied the resampling, algorithms, and feature selection approaches to this problem. No systematic studies have been conducted to understand how well these methods combat the class imbalance problem and which of these methods best manage the different challenges posed by imbalanced data sets. In particular, feature selection has rarely been studied outside of text classification problems. Additionally, no studies have looked at the additional problem of learning from small samples. This paper presents a first systematic comparison of the three types of methods developed for imbalanced data classification problems and of seven feature selection metrics evaluated on small sample data sets from different applications. We evaluated the performance of these metrics using area under the receiver operating characteristic (AUC) and area under the precisionrecall curve (PRC). We compared each metric on the average performance across all problems and on the likelihood of a metric yielding the best performance on a specific problem. We examined the performance of these metrics inside each problem domain. Finally, we evaluated the efficacy of these metrics to see which perform best across algorithms. Our results showed that signal-tonoise correlation coefficient (S2N) and Feature Assessment by Sliding Thresholds (FAST) are great candidates for feature selection in most applications, especially when selecting very small numbers of features.	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']
measure or ordinal trivial evaluation choice parameter datasets attention standard argue assigning avoid behave coincide conceived consequence engineered genuinely involve 	1	Evaluation measures for ordinal regression	Ordinal regression (OR – also known as ordinal classification) has received increasing attention in recent times, due to its importance in IR applications such as learning to rank and product review rating. However, research has not paid attention to the fact that typical applications of OR often involve datasets that are highly imbalanced. An imbalanced dataset has the consequence that, when testing a system with an evaluation measure conceived for balanced datasets, a trivial system assigning all items to a single class (typically, the majority class) may even outperform genuinely engineered systems. Moreover, if this evaluation measure is used for parameter optimization, a parameter choice may result that makes the system behave very much like a trivial system. In order to avoid this, evaluation measures that can handle imbalance must be used. We propose a simple way to turn standard measures for OR into ones robust to imbalance. We also show that, once used on balanced datasets, the two versions of each measure coincide, and therefore argue that our measures should become the standard choice for OR. 	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']
feature sided chi selection cc ig indicative metric negative or combination membership g optimal e actual bayes consider control ensure 	1	Feature selection for text categorization on imbalanced data	A number of feature selection metrics have been explored in text categorization, among which information gain (IG), chi-square (CHI), correlation coefficient (CC) and odds ratios (OR) are considered most effective. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and nonmembership (e.g. negative features) by ignoring the signs of features. The former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data. In this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework. Using multinomial näıve Bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']
dominant highest index plain show theoretical metric accuracy analyzes estimate individual merit evaluate combining generalization measure domain known analysis new 	1	Theoretical analysis of a performance measure for imbalanced data	This paper analyzes a generalization of a new metric to evaluate the classification performance in imbalanced domains, combining some estimate of the overall accuracy with a plain index about how dominant the class with the highest individual accuracy is. A theoretical analysis shows the merits of this metric when compared to other well-known measures.	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']
capable phenomenon solving various hinder task suffer categorization active demonstrate text known world classification real we algorithm the problem imbalance 	1	Active learning for class imbalance problem	The class imbalance problem has been known to hinder the learning performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem.	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']
feature relief correlation selection fast threshold mining case compared bin card comparable credit dimensional filtering generated mass microarray nonexistent outperformed 	1	Fast: A roc-based feature selection metric for small samples and imbalanced data classification problems	The class imbalance problem is encountered in a large number of practical applications of machine learning and data mining, for example, information retrieval and filtering, and the detection of credit card fraud. It has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifier’s suboptimal performance. This is even more true when the imbalanced data are also high dimensional. In such cases, feature selection methods are critical to achieve optimal performance. In this paper, we propose a new feature selection method, Feature Assessment by Sliding Thresholds (FAST), which is based on the area under a ROC curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. FAST is compared to two commonly-used feature selection methods, correlation coefficient and RELevance In Estimating Features (RELIEF), for imbalanced data classification. The experimental results obtained on text mining, mass spectrometry, and microarray data sets showed that the proposed method outperformed both RELIEF and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred, the classification performance of the proposed method was significantly improved compared to correlation and RELIEF-based methods.	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']
threshold task cost moving study costsensitive hard multiclass network neural soft effective effect sensitive ensemble technique higher empirical training addressing 	1	Training cost-sensitive neural networks with methods addressing the class imbalance problem	This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and softensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that costsensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training costsensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']
cil outlier noise fsvms svms svm datasets sensitive fsvm normal handle fuzzy presence membership existing suffer proposed come conclude fsvmcil 	2	FSVM-CIL: Fuzzy support vector machines for class imbalance learning	Support vector machines (SVMs) is a popular machine learning technique, which works effectively with balanced datasets. However, when it comes to imbalanced datasets, SVMs produce suboptimal classification models. On the other hand, the SVM algorithm is sensitive to outliers and noise present in the datasets. Therefore, although the existing class imbalance learning (CIL) methods can make SVMs less sensitive to class imbalance, they can still suffer from the problem of outliers and noise. Fuzzy SVMs (FSVMs) is a variant of the SVM algorithm, which has been proposed to handle the problem of outliers and noise. In FSVMs, training examples are assigned different fuzzy-membership values based on their importance, and these membership values are incorporated into the SVM learning algorithm to make it less sensitive to outliers and noise. However, like the normal SVM algorithm, FSVMs can also suffer from the problem of class imbalance. In this paper, we present a method to improve FSVMs for CIL (called FSVM-CIL), which can be used to handle the class imbalance problem in the presence of outliers and noise. We thoroughly evaluated the proposed FSVM-CIL method on ten real-world imbalanced datasets and compared its performance with five existing CIL methods, which are available for normal SVM training. Based on the overall results, we can conclude that the proposed FSVMCIL method is a very effective method for CIL, especially in the presence of outliers and noise in datasets.	['fsvms', 'svms', 'svm']	['outliers_noise', 'input_points', 'cil_methods', 'svm_algorithm', 'algorithm_sensitive']	['algorithm_sensitive_outliers', 'fuzzy_svms_fsvms', 'learns_decision_surface', 'presence_outliers_noise', 'problem_outliers_noise']
input point fuzzy svms surface constributions distinct fully learns reformulate assigned fsvms apply different decision membership svm vector a support 	2	Fuzzy support vector machines	A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different constributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).	['fsvms', 'svms', 'svm']	['outliers_noise', 'input_points', 'cil_methods', 'svm_algorithm', 'algorithm_sensitive']	['algorithm_sensitive_outliers', 'fuzzy_svms_fsvms', 'learns_decision_surface', 'presence_outliers_noise', 'problem_outliers_noise']
classi er cost space duality frequency line o tw roc expected range representation point adv allowing demonstrates easier equivalen experimenter 	3	Explicitly representing expected cost: An alternative to ROC representation	This paper proposes an alternative to ROC representation, in which the expected cost of a classi er is represented explicitly . This expected cost representation maintains many of the adv an tagesof R OCrepresen tation, but is easier to understand. It allows the experimenter to immediately see the range of costs and class frequencies where a particular classi er is the best and quantitatively how much better it is than other classi ers. This paper demonstrates there is a point/line duality between the tw o represen tations.A point in ROC space representing a classi er becomes a line segment spanning the full range of costs and class frequencies. This duality produces equivalen t operations in the tw o spaces, allowing most techniques used in ROC analysis to be readily reproduced in the cost space.	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']
pr space curve roc dominates optimize area achievable algorithm’s connection convex corollary deep difference exists guaranteed hull incorrect interpolate linearly 	3	The relationship between Precision-Recall and ROC curves	Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm’s performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']
error rec cumulative model function target variable curve regression particularly use surface generalization distribution present important characteristic prediction analyzing compare 	3	Regression error characteristic surfaces	This paper presents a generalization of Regression Error Characteristic (REC) curves. REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems. REC curves provide useful information for analyzing the performance of models, particularly when compared to error statistics like for instance the Mean Squared Error. In this paper we present Regression Error Characteristic (REC) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable, i.e. the joint cumulative distribution function of the errors and the target variable. This provides a more detailed analysis of the performance of models when compared to REC curves. This extra detail is particularly relevant in applications with non-uniform error costs, where it is important to study the performance of models for specific ranges of the target variable. In this paper we present the notion of REC surfaces, describe how to use them to compare the performance of models, and illustrate their use with an important practical class of applications: the prediction of rare extreme values.	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']
learner sampling can do experimentation is objective perspective reduced showing subject suite improve related effectiveness optimize comprehensive change when address 	3	Experimental perspectives on learning from imbalanced data	We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the performance of learners built from imbalanced data? Is the effectiveness of sampling related to the type of learner? Do the results change if the objective is to optimize different performance metrics? We address these and other issues in this work, showing that sampling in many cases will improve classifier performance.	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']
dataset training suitable majority approach traditional away balance decreased distributed doesn’t evenly final got handling ignore maintaining an usually effectively 	3	An active under-sampling approach for imbalanced data classification	An active under-sampling approach is proposed for handling the imbalanced problem in this paper. Traditional classifiers usually assume that training examples are evenly distributed among different classes, so they are often biased to the majority class and tend to ignore the minority class. In this case, it is important to select the suitable training dataset for learning from imbalanced data. The samples of the majority class which are far away from the decision boundary should be got rid of the training dataset automatically in our algorithm, and this process doesn’t change the density distribution of the whole training dataset. As a result, the ratio of majority class is decreased significantly, and the final balance training dataset is more suitable for the traditional classification algorithms. Compared with other under-sampling methods, our approach can effectively improve the classification accuracy of minority classes while maintaining the overall classification performance by the experimental results. 	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']
sampling induced related smote aspect condition deal enn smallest two rule tree usually method our complex system random good provide 	3	A study of the behavior of several methods for balancing machine learning training data	There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']
