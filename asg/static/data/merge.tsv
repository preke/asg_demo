ref_title	ref_context	ref_entry	abstract	intro	ref_link	label	topic_word	topic_bigram	topic_trigram	description
Joint optimization of idle and cooling power in data centers while maintaining response time	[]	Faraz Ahmad, and T.N. Vijaykumar. 2010. Joint optimization of idle and cooling power in data centers while maintaining response time. ACM SIGPLAN Notices 45, 3 (March 2010) 243–256. DOI: http://dx.doi.org/10.1145/1735971.1736048	Server power and cooling power amount to a significant fraction of modern data centers’ recurring costs. While data centers provision enough servers to guarantee response times under the maximum loading, data centers operate under much less loading most of the times (e.g., 30-70% of the maximum loading). Previous serverpower proposals exploit this under-utilization to reduce the server idle power by keeping active only as many servers as necessary and putting the rest into low-power standby modes. However, these proposals incur higher cooling power due to hot spots created by concentrating the data center loading on fewer active servers, or degrade response times due to standby-to-active transition delays, or both. Other proposals optimize the cooling power but incur considerable idle power. To address the first issue of power, we propose PowerTrade, which trades-off idle power and cooling power for each other, thereby reducing the total power. To address the second issue of response time, we propose SurgeGuard to overprovision the number of active servers beyond that needed by the current loading so as to absorb future increases in the loading. SurgeGuard is a two-tier scheme which uses well-known over-provisioning at coarse time granularities (e.g., one hour) to absorb the common, smooth increases in the loading, and a novel fine-grain replenishment of the over-provisioned reserves at fine time granularities (e.g., five minutes) to handle the uncommon, abrupt loading surges. Using real-world traces, we show that combining PowerTrade and SurgeGuard reduces total power by 30% compared to previous low-power schemes while maintaining response times within 1.7%.	As mentioned in Section 1, spatial subsetting reduces server idle power but increases cooling power whereas inverse-temperature assignment reduces cooling cost but incurs high idle power. We propose a joint optimization, called PowerTrade, to reduce idle power and cooling power together so that the total power is reduced. Because PowerTrade builds on spatial subsetting and inverse-temperature assignment, we describe these schemes first.	https://doi.org/10.1145/1735971.1736048	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Ahmad, et al. [NO] proposes powertrade, which trades-off idle power and cooling power for each other, thereby reducing the total power. 
Energy efficient allocation of virtual machines in cloud data centers	[]	Anton Beloglazov, and Rajkumar Buyya. 2010. Energy efficient allocation of virtual machines in cloud data centers. 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing (CCGrid’10). 577–578. DOI:http://dx.doi.org/10.1109/CCGRID.2010.45	Rapid growth of the demand for computational power has led to the creation of large-scale data centers. They consume enormous amounts of electrical power resulting in high operational costs and carbon dioxide emissions. Moreover, modern Cloud computing environments have to provide high Quality of Service (QoS) for their customers resulting in the necessity to deal with power-performance trade-off. We propose an efficient resource management policy for virtualized Cloud data centers. The objective is to continuously consolidate VMs leveraging live migration and switch off idle nodes to minimize power consumption, while providing required Quality of Service. We present evaluation results showing that dynamic reallocation of VMs brings substantial energy savings, thus justifying further development of the proposed policy. Keywords-Energy efficiency; Cloud computing; Energy consumption; Green IT; Resource management; Virtualization; Allocation of virtual machines; Live migration of virtual machines.	"The proposed heuristics have been evaluated by simulation using CloudSim toolkit <NO>. The simulated data center comprises 100 heterogeneous physical nodes. Each node is modeled to have one CPU core with performance equivalent to 1000, 2000 or 3000 MIPS, 8 GB of RAM and 1 TB of storage. Users submit requests for provisioning of 290 heterogeneous VMs that fill the full capacity of the data center. For the benchmark policies we simulated a Non Power Aware policy (NPA) and DVFS that adjusts the voltage and frequency of CPU according to current utilization. We present results obtained using ST policy and the best two-threshold policy, MM policy. Besides that, the policies have been evaluated with different values of the thresholds.
The simulation results presented in Table I show that dynamic reallocation of VMs according to current utilization of CPU brings higher energy savings compared with static allocation policies. MM policy achieves the best energy savings: by 83%, 66% and 23% less energy consumption relatively to NPA, DVFS and ST policies respectively with thresholds 30-70% and ensuring percentage of SLA violations of 1.1%; and by 87%, 74% and 43% with thresholds 50-90% and 6.7% of SLA violations. MM policy leads to more than 10 times fewer VM migrations than ST. The results show the flexibility of the proposed algorithms, as the thresholds can be adjusted according to SLA requirements. Strict SLA (1.11%) allow achievement of the energy consumption of
1.48 KWh. However, if SLA are relaxed (6.69%), the energy consumption is further reduced to 1.14 KWh.
In this work we have proposed and evaluated heuristics for dynamic reallocation of VMs to minimize energy consumption, while providing reliable QoS. The obtained results show that the technique of dynamic reallocation of VMs and switching off the idle servers brings substantial energy savings and is applicable to real-world Cloud data centers. For the future work, we propose to investigate the consideration of multiple system resource in reallocation decisions, such as network interface and disk storage, as these resources also significantly contribute to the overall energy consumption. Other interesting directions for the future work are investigation of setting the utilization thresholds dynamically according to a current set of VMs allocated to a host, leveraging multi-core CPU architectures, and decentralization of the optimization algorithms to improve scalability and fault tolerance. Besides the reduction of operational and establishment costs, the work has social significance as it decreases carbon dioxide footprints and energy consumption by modern IT infrastructures.
REFERENCES
<NO> R. Brown et al., “Report to congress on server and data center energy efficiency: Public law 109-431,” Lawrence Berkeley National Laboratory, 2008.
<NO> R. Buyya, C. S. Yeo, and S. Venugopal, “Market-oriented cloud computing: Vision, hype, and reality for delivering it services as computing utilities,” in Proceedings of HPCC’08. IEEE CS Press, Los Alamitos, CA, USA, 2008.
<NO> D. Kusic, J. O. Kephart, J. E. Hanson, N. Kandasamy, and G. Jiang, “Power and performance management of virtualized computing environments via lookahead control,” Cluster Computing, vol. 12, no. 1, pp. 1–15, 2009.
<NO> S. Srikantaiah, A. Kansal, and F. Zhao, “Energy aware consolidation for cloud computing,” Cluster Computing, vol. 12, pp. 1–15, 2009.
<NO> R. Buyya, R. Ranjan, and R. N. Calheiros, “Modeling and simulation of scalable cloud computing environments and the CloudSim toolkit: Challenges and opportunities,” in Proceedings of HPCS’09. IEEE Press, NY, USA, 2009.
578"	https://doi.org/10.1109/CCGRID.2010.45	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Beloglazov, et al. [NO] proposes an efficient resource management policy for virtualized cloud data centers. 
Understanding the impact of multi-core architecture in cluster computing: A case study with Intel dual-core system	[]	Lei Chai, Qi Gao, and Dhabaleswar K. Panda. 2007. Understanding the impact of multi-core architecture in cluster computing: A case study with Intel dual-core system. 7th IEEE International Symposium on Cluster Computing and the Grid (CCGRID’07). IEEE, 471–478. DOI:http://dx.doi.org/10.1109/ CCGRID.2007.119.	Multi-core processors are growing as a new industry trend as single core processors rapidly reach the physical limits of possible complexity and speed. In the new Top500 supercomputer list, more than 20% processors belong to the multi-core processor family. However, without an indepth study on application behaviors and trends on multicore clusters, we might not be able to understand the characteristics of multi-core cluster in a comprehensive manner and hence not be able to get optimal performance. In this paper, we take on these challenges and design a set of experiments to study the impact of multi-core architecture on cluster computing. We choose to use one of the most advanced multi-core servers, Intel Bensley system with Woodcrest processors, as our evaluation platform, and use benchmarks including HPL, NAMD, and NAS as the applications to study. From our message distribution experiments, we find that on an average about 50% messages are transferred through intra-node communication, which is much higher than intuition. This trend indicates that optimizing intranode communication is as important as optimizing internode communication in a multi-core cluster. We also observe that cache and memory contention may be a potential bottleneck in multi-core clusters, and communication middleware and applications should be multi-core aware to alleviate this problem. We demonstrate that multi-core aware algorithm, e.g. data tiling, improves benchmark execution time by up to 70%. We also compare the scalability of a multi-core cluster with that of a single-core cluster and find that the scalability of the multi-core cluster is promising. ∗This research is supported in part by DOE’s Grants#DE-FC0206ER25749 and #DE-FC02-06ER25755; NSF’s Grants #CNS-0403342 and #CNS-0509452; grants from Intel, Mellanox, Cisco systems, Linux Networx and Sun Microsystems; and equipment donations from Intel, Mellanox, AMD, Apple, Appro, Dell, Microway, PathScale, IBM, SilverStorm and Sun Microsystems.	"Multi-core means to integrate two or more complete computational cores within a single chip <NO>. The motivation is the fact that scaling up processor speed results in dramatic rise in power consumption and heat generation. In addition, it becomes more difficult to increase processor speed nowadays that even a little increase in performance will be costly. Realizing these factors, computer architects have proposed multi-core processors that speed up application performance by dividing the workload among multiple processing cores instead of using one “super fast” single processor. Multi-core processor is also referred to as Chip Multiprocessor (CMP). Since a processing core can
be viewed as an independent processor, in this paper we use processor and core interchangeably.
Most processor vendors have multi-core products, e.g. Intel Quad- and Dual-Core Xeon, AMD Quad- and DualCore Opteron, Sun Microsystems UltraSPARC T1 (8 cores), IBM Cell, etc. There are various alternatives in designing cache hierarchy organization and memory access model. Figure 1 illustrates two typical multi-core system designs. The left box shows a NUMA <NO> based dual-core system in which each core has its own L2 cache. Two cores on the same chip share the memory controller and local memory. Processors can also access remote memory, although local memory access is much faster. The right box shows a bus based dual-core system, in which two cores on the same chip share the same L2 cache and memory controller, and all the cores access the main memory through a shared bus.
Multi-core processors have been deployed in cluster computing. In a multi-core cluster, there are three levels of communication as shown in Figure 1. The communication between two processors on the same chip is referred to as intra-CMP communication in this paper. The communication across chips but within a node is referred to as inter-CMP communication. And the communication between two processors on different nodes is referred to as inter-node communication. Multi-core clusters impose new challenges in software design, both at the middleware level and application level."	https://doi.org/10.1109/CCGRID.2007.119	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Chai, et al. [NO] might not is able to understand the characteristics of multi-core cluster in a comprehensive manner and hence not be able to get optimal performance. 
Cloud computing: Issues and challenges	[]	Tharam Dillon, Chen Wu, and Elizabeth Chang. 2010. Cloud computing: Issues and challenges. 24th IEEE International Conference on Advanced Information Networking and Applications (AINA). 27–33. DOI:http://dx.doi.org/10.1109/AINA.2010.187	Many believe that Cloud will reshape the entire ICT industry as a revolution. In this paper, we aim to pinpoint the challenges and issues of Cloud computing. We first discuss two related computing paradigms Service-Oriented Computing and Grid computing, and their relationships with Cloud computing We then identify several challenges from the Cloud computing adoption perspective. Last, we will highlight the Cloud interoperability issue that deserves substantial further research	"What is Cloud Computing? Although many formal definitions have been proposed in both academia and industry, the one provided by U.S. NIST (National Institute of Standards and Technology) <NO> appears to include key common elements widely used in the Cloud Computing community:
Cloud computing is a model for enabling convenient, ondemand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction <NO>.
This definition includes cloud architectures, security, and deployment strategies. In particular, five essential elements of cloud computing are clearly articulated:
On-demand self-service: A consumer with an instantaneous need at a particular timeslot can avail computing resources (such as CPU time, network storage, software use, and so forth) in an automatic (i.e. convenient, self-serve) fashion without resorting to human interactions with providers of these resources.
Broad network access: These computing resources are delivered over the network (e.g. Internet) and used by various client applications with heterogeneous platforms (such as mobile phones, laptops, and PDAs) situated at a consumer's site.
Resource pooling. A cloud service provider’s computing resources are 'pooled' together in an effort to serve multiple consumers using either the multi-tenancy or the virtualization model, ""with different physical and virtual resources dynamically assigned and reassigned according to consumer demand"" <NO>. The motivation for setting up such a pool-based computing paradigm lies in two important factors: economies of scale and specialization. The result of a pool-based model is that physical computing resources become 'invisible' to consumers, who in general do not have control or knowledge over the location, formation, and originalities of these resources (e.g. database, CPU, etc.) . For example, consumers are not able to tell where their data is going to be stored in the Cloud.
Rapid elasticity. For consumers, computing resources become immediate rather than persistent: there are no up-front commitment and contract as they can use them to scale up whenever they want, and release them once they finish to scale down. Moreover, resources provisioning appears to be infinite to them, the consumption can rapidly rise in order to meet peak requirement at any time.
Measured Service. Although computing resources are pooled and shared by multiple consumers (i.e. multi-tenancy), the cloud infrastructure is able to use appropriate mechanisms
1550-445X/10 $26.00 © 2010 IEEE DOI 10.1109/AINA.2010.187
27
to measure the usage of these resources for each individual consumer through its metering capabilities."	https://doi.org/10.1145/1735971.1736048	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Dillon, et al. [NO] aims to pinpoints the challenges and issues of cloud computing. 
Energy-aware ant colony based workload placement in clouds	[]	Eugen Feller, Louis Rilling, and Christine Morin. 2011. Energy-aware ant colony based workload placement in clouds. Proceedings of IEEE/ACM 12th International Conference on Grid Computing. 26–33. IEEE, DOI:http://dx.doi.org/10.1109/Grid.2011.13	With increasing numbers of energy hungry data centers energy conservation has now become a major design constraint. One traditional approach to conserve energy in virtualized data centers is to perform workload (i.e., VM) consolidation. Thereby, workload is packed on the least number of physical machines and over-provisioned resources are transitioned into a lower power state. However, most of the workload consolidation approaches applied until now are limited to a single resource (e.g., CPU) and rely on simple greedy algorithms such as First-Fit Decreasing (FFD), which perform resource-dissipative workload placement. Moreover, they are highly centralized and known to be hard to distribute. In this work, we model the workload consolidation problem as an instance of the multidimensional bin-packing (MDBP) problem and design a novel, nature-inspired workload consolidation algorithm based on the Ant Colony Optimization (ACO). We evaluate the ACO-based approach by comparing it with one frequently applied greedy algorithm (i.e., FFD). Our simulation results demonstrate that ACO outperforms the evaluated greedy algorithm as it achieves superior energy gains through better server utilization and requires less machines. Moreover, it computes solutions which are nearly optimal. Finally, the autonomous nature of the approach allows it to be implemented in a fully distributed environment. Keywords-Ant Colony Optimization, Combinatorial Optimization, Green Cloud Computing, Multidimensional Bin Packing, Swarm Intelligence, Virtualization	"Ant Colony Optimization (ACO) is a meta-heuristic, which was initially introduced as Ant Systems (AS) in 1992 within the PhD thesis of the Italian researcher Marco Dorigo <NO>. Initially, it was developed to solve the Traveling Salesman Problem (TSP). However, since then it has been successfully adapted to solve many other complex combinatorial optimization problems (e.g., vehicle routing, quadratic assignment, dynamic job scheduling, graph coloring and bin packing).
The main inspiration to develop this system was the natural food-discovery behavior of real ants. Because of the limited abilities of the ants to see and hear their environment they have developed a form of indirect communications (also called Stigmergy) by use of a chemical substance referred as
pheromone. This substance is deposited by each ant on the path it traverses and evaporates after a certain period of time. Other ants can smell the concentration of this substance and tend to favor paths probabilistically according to the amount of pheromone deposited on them. Surprisingly, after some time the entire ant colony converges towards the shortest path to the food source. This behavior was studied by biologists in numerous controlled experiments <NO> and can be explained as follows. At the beginning, when starting from the nest the ants choose a random path to follow. However, on the shortest path to the food source the ants will return faster. Thereby, this path will have a stronger pheromone concentration thus being more attractive for subsequent ants to follow it. When time passes, pheromone concentration on the shortest paths will continue to increase, while on the longer ones it will keep falling, making them less and less attractive.
When applied on combinatorial optimization problems such as TSP or the Bin-Packing Problem (BPP), artificial ants act as a multi-agent system and construct a complex solution based on indirect low-level communication. Thereby, several parts of the algorithm need to be defined in order to imitate real ants. Similarly, as real ants do, a decision on which path or item to choose next needs to be taken. Therefore, a probabilistic decision rule has to be defined which will be used by the algorithm to guide the ants choice towards the optimal solution. Furthermore, unlike real ants, a memory is necessary for each ant, which will be used to keep track of the local solution constructed so far. Finally, a pheromone update mechanism is required in order to: (1) simulate pheromone evaporation, (2) deposit pheromone either on the visited paths (i.e., TSP) or on the selected item-bin pairs (i.e., BPP), respectively. Thereby, a decision needs to be taken on which ant will perform the pheromone updates. This can be either done after each ants move, by the iterations best ant, or the best-so-far ant. We will describe our design choices in Section IV."	https://doi.org/10.1109/Grid.2011.13	0	['algorithm', 'consolidation', 'optimization']	['relocation_costs', 'v_man', 'consolidation_algorithm', 'energy_conservation', 'first_fit']	['total_power_consumption', 'ant_colony_optimization', 'definition_cloud_computing', 'multidimensional_bin_packing', 'placement_task_assignment']	Feller, et al. [NO] models the workload consolidation problem as an instance of the multidimensional bin-packing (mdbp) problem and design a novel, nature-inspired workload consolidation algorithm based on the ant colony optimization (aco). 
The cost of a cloud: Research problems in data center networks	[]	Albert Greenberg, James Hamilton, David A. Maltz, and Parveen Patel. 2008. The cost of a cloud: Research problems in data center networks. ACM SIGCOMM Computer Communication Review 39, 1 (2008) 68–73. DOI:http://dx.doi.org/10.1145/1496091.1496103	The data centers used to create cloud services represent a significant investment in capital outlay and ongoing costs. Accordingly, we first examine the costs of cloud service data centers today. The cost breakdown reveals the importance of optimizing work completed per dollar invested. Unfortunately, the resources inside the data centers often operate at low utilization due to resource stranding and fragmentation. To attack this first problem, we propose (1) increasing network agility, and (2) providing appropriate incentives to shape resource consumption. Second, we note that cloud service providers are building out geo-distributed networks of data centers. Geo-diversity lowers latency to users and increases reliability in the presence of an outage taking out an entire site. However, without appropriate design and management, these geo-diverse data center networks can raise the cost of providing service. Moreover, leveraging geo-diversity requires services be designed to benefit from it. To attack this problem, we propose (1) joint optimization of network and data center resources, and (2) new systems and mechanisms for geo-distributing state.	"It is natural to ask why existing solutions for the enterprise data center do not work for cloud service data centers. First and foremost, the leading cost in the enterprise is operational staff. In the data center, such costs are so small (under 5% due to automation), that we safely omit them from Table 1. In a well-run enterprise, a typical ratio of IT staff members to servers is 1:100. Automation is partial <NO>, and human error is the cause of a large fraction of performance impacting problems <NO>. In cloud service data centers, automation is a mandatory requirement of scale, and it is accordingly a foundational principle of design <NO>. In a well run data center, a typical ratio of staff members to servers is 1:1000. Automated, recovery-oriented computing techniques cope successfully with the vast majority of problems that arise <NO>.
There are additional differences between the enterprise and the cloud service data center environments including:
Large economies of scale. The size of cloud scale data centers (some now approaching 100,000 severs) presents an opportunity to leverage economies of scale not present in the enterprise data centers, though the up front costs are high.
Scale Out. Enterprises often optimize for physical space and number of devices, consolidating workload onto a small number of high-price “scale-up” hardware devices and servers. Cloud service data centers “scale-out” — distributing workload over large numbers of low cost servers and hardware.
That said, enterprises are also moving toward the cloud. Thus, we expect innovation in cloud service data centers to benefit the enterprise, through outsourcing of computing and storage to cloud service providers <NO>, and/or adapting and scaling down technologies and business models from cloud service providers."	https://doi.org/10.1145/1496091.1496103	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Greenberg, et al. [NO] first examines the costs of cloud service data centers today. 
Server consolidation algorithms with bounded migration cost and performance guarantees in cloud computing	[]	Yufan Ho, Pangfeng Liu, and Jan-Jan Wu. 2011. Server consolidation algorithms with bounded migration cost and performance guarantees in cloud computing. 4th IEEE International Conference on Utility and Cloud Computing (UCC). 154–161. DOI:http://doi.ieeecomputersociety.org/10.1109/UCC.2011.30.	Consolidation of virtual machines is essential to achieve energy optimization in cloud computing environments. As virtual machines dynamically enter and leave a cloud system, it becomes necessary to relocate virtual machines among servers. However, relocation of virtual machines introduces run-time overheads and consumes extra energy, thus an careful planning for relocation is necessary. We model the relocation problem as a modified bin packing problem and propose a new server consolidation algorithm that guarantees server consolidation with bounded relocation costs. We also conduct a detailed analysis on the complexity of the server consolidation problem, and give a upper bound on the cost of relocation. Finally, we conduct simulations and compare our server consolidation algorithm with other relocation methods, like First Fit and Best Fit method. The experiment results suggest an interesting trade-off between server consolidation quality and relocation cost. Our algorithm is able to trade about 1% in server consolidation quality for a reduction about 50% in relocation cost, when compared with other well known bin packing algorithms. We also note that the relocation cost incurred in our method is much less than the theoretical bound we provided. The reason is that we overestimate the amount of relocation from theoretical analysis, and the actual amount of relocation found from experiments is much less than the worst-case bound from theoretical analysis. Keywords-Energy Optimization; Server Consolidation; VM Relocation;	"Data centers provide essential computing power for the operations of business, scientific organizations, and academic institutes. However, recent data centers consume significant amount of energy to sustain the system. As energy consumption and associated costs become increasingly
significant, energy management in data center becomes more and more important. As a result, researchers have been investigating mechanism for reducing resource requirements (e.g., number of servers) to decrease energy consumption in cloud data centers.
Server consolidation is a common method for energy optimization in cloud computing environments. There have been numerous works regarding energy conservation by server consolidation in data centers. Srikantaiah et al. <NO> investigate the inter-relationship between energy consumption, resource utilization, and performance of consolidated workloads. The study reveals the energy performance tradeoffs for consolidation and shows that optimal operating points exist.
Li et al. <NO> propose EnaCloud, which places applications dynamically on servers, with consideration of energy efficiency in a cloud platform. The application placement is abstracted as a bin packing problem, and an energyaware heuristic algorithm is proposed to get an appropriate solution.
Beloglazov and Buyya <NO> propose a resource management policy for virtualized cloud data centers. They propose and evaluate heuristics for dynamic reallocation of virtual machines to minimize energy consumption, while providing reliable quality of services. The heuristics are based on the idea of setting upper and lower utilization thresholds for hosts and keeping total utilization of CPU by all virtual machines between these thresholds.
Liao et al. <NO> use live migration to transfer loads among servers on a multi-layer ring-based overlay to reduce power consumption. Active servers are organized into 3-layer rings, and each ring consists of servers that have similar workloads within a specific load interval, so that the inner ring has the servers with the heavy loads, the outer ring has the the servers with light workloads, and the middle ring has the rest of the servers. They move servers, along the workloads at the servers, from the outer and the inner rings toward the middle ring. The idea is to reduce power consumption without affecting the performance of servers.
Hanson et al. <NO> propose AMP, an autonomic manager in charge of the power states of servers. AMP is designed to work in conjunction with other autonomic managers, and is responsible for cutting excess power use and preserving the lifespan of servers it manages. AMP considers broader metrics in its decision, which may include power consumption, server lifespan, and interactions with other application managers. In contrast this paper focuses on computation power as a measurement of resources that a server can provide, and a task can consume. The computation power could be a function of standard metrics like CPU, memory, and disk storage. This focus of a single objective function enables us to analyze the complex trade-off among various standard metrics, so that we can analyze the relocation cost and the quality of consolidation simultaneously.
There are also theoretical works for the server consolidation problem in the literature. The consolidation problems in homogeneous server environments are mostly modeled as a bin packing problem. The bin packing problem is a combinatorial NP-hard problem, and several well-known approximation algorithms have been developed. Let N be the number of bins in optimal solution of a bin packing problem. Dósa and György <NO> proves that the bound of a First Fit Decreasing method is 1.222N + 0.667 and this bound is tight. Xia and Tan <NO> present bounds for the First Fit algorithm. They prove the bound of a First Fit method is 1.7N + 0.7, and the absolute performance ratio of First Fit is at most 1.714.
The online bin packing problem is also studied in the literature. Lee and Lee <NO> give an algorithm for online bin packing with a worst-case performance ratio of less than 1.636. Babel <NO> presents two online algorithms for online bin packing problems with cardinality constraints.
To achieve the goal of server consolidation in dynamic environments, we must relocate virtual machines among servers. In this paper, we focus on reducing the relocation costs introduced by relocation of virtual machines, while still guaranteeing solution quality. The relocation is to relocate virtual machines in such a way that the performance guarantee of the First Fit is still valid after relocation. Note that the First Fit method not only produces a theoretically good solution in the worst case, but also a good solution in the average case. As a result we choose to maintain a First Fit ordering when we relocate virtual machines."	https://doi.org/10.1109/UCC.2011.30	0	['algorithm', 'consolidation', 'optimization']	['relocation_costs', 'v_man', 'consolidation_algorithm', 'energy_conservation', 'first_fit']	['total_power_consumption', 'ant_colony_optimization', 'definition_cloud_computing', 'multidimensional_bin_packing', 'placement_task_assignment']	Ho, et al. [NO] models the relocation problem as a modified bin packing problem and propose a new server consolidation algorithm that guarantees server consolidation with bounded relocation costs. 
Priority-based scheduling for large-scale distribute systems with energy awareness	[]	Masnida Hussin, Young C. Lee, and Albert Y. Zomaya. 2011. Priority-based scheduling for large-scale distribute systems with energy awareness. IEEE 9th International Conference on Dependable, Autonomic and Secure Computing (DASC’11). 503–509. DOI:http://dx.doi.org/10.1109/DASC.2011.96	Large-scale distributed computing systems (LDSs), such as grids and clouds are primarily designed to provide massive computing capacity. These systems dissipate often excessive energy to both power and cool them. Concerns over greening these systems have prompted a call for scheduling policies with energy awareness (e.g., energy proportionality). The dynamic and heterogeneous nature of resources and tasks in LDSs is a major hurdle to be overcome for energy efficiency when designing scheduling policies. In this paper, we address the problem of scheduling tasks with different priorities (deadlines) for energy efficiency exploiting resource heterogeneity. Specifically, our investigation for energy efficiency focuses on two issues: (1) balancing the workload in the way utilization is maximized and (2) power management by controlling execution of tasks on processor for ensuring the energy is optimally consumed. We form a hierarchical scheduler that exploits the multi-core architecture for effective scheduling. Our scheduling approach exploits the diversity of task priority for proper load balancing across heterogeneous processors while observing energy consumption in the system. Simulation experiments prove the efficacy of our approach; and the comparison results indicate our scheduling policy helps improve energy efficiency of the system. Keywords– energy efficiency, dynamic scheduling, task priority.	"Energy management has been an active area of research over the years for computing components. Processors are the major energy consumer <NO>, thus, accounting its energy consumption is necessary for energy-efficient computing. There are numerous research efforts on energy efficiency that use scheduling algorithms to optimize energy consumption (e.g., <NO>). Since the performance of LDSs greatly influences processing time, heuristics are most popularly adopted in the scheduling model. This is because heuristic methods tend to produce competitive solutions with lower time complexity and react competently in the highly heterogeneous environment. Generally, resource manager or scheduler can contribute much to the overall energy efficiency of the system. It has the ability to work within the processing requirements/ constraints that put forth by the system users. The processing constraints must be effectively handled particularly in the present of dynamic computing; otherwise it may lead to load imbalance, over-provisioning of resources, and system unreliability. The schedulers in <NO> adaptively deal with processing constraints for reliable execution while minimizing energy consumption. The work in <NO> developed a software framework to implement and evaluate various scheduling techniques to save energy with the minimal performance impact. The EASY backfilling policy is proposed in <NO> to increase resource utilization by continuously monitoring the load in the system. It adaptively selects a certain number of resources that need to be put into ‘sleep mode.’ In <NO> a meta-scheduler is used to select the most energy efficient resource site. The scheduler decides the time slot in which tasks should be executed at the minimum CPU frequency for saving energy. A task consolidation technique is proposed in <NO> aiming to maximize resource utilization. The approach contributes for promising energy-saving capability as the energy consumption is significantly reduced when the task is consolidated with one or more tasks. These approaches have demonstrated the effectiveness in minimizing energy consumption while still meeting certain performance goals. However, the efficacy of these approaches in dealing with system dynamicity is limited to a certain level. The scope of energy efficiency also should be stretched further incorporating dynamicity and heterogeneity of both resources and tasks. For the case of large-scale dynamic environment, it is much beneficial for a scheduler to
measure its performance and adapt accordingly. The scheduler typically, strives for minimizing response time and ensuring fairness among the running tasks in the system. The impending widespread usage of multiple processor cores appear to be an excellent opportunity for realizing performance and energy benefits <NO>. To seek for the optimal performance, the scheduler needs to actively adapt of the multi-core processors topologies while being aware of task characteristics. Due to the fact that cores in a processor are in a very close proximity to each other <NO>, load balancing can be done very effectively reducing idle time of processors. Dynamic scheduling can be of a very effective approach for proper load balancing while tracking energy consumption. While most previous energy efficiency solutions deal with homogenous resources and/or adopt static scheduling policy, our scheduling approach in this work is explicitly taking into account processing priority with heterogeneous resources."	https://doi.org/10.1109/DASC.2011.96	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Hussin, et al. [NO] addresses the problem of scheduling tasks with different priorities (deadlines) for energy efficiency exploiting resource heterogeneity. 
Virtual machine scheduling for multicores considering effects of shared on-chip last level cache interference	[]	Shin-gyu Kim, Hyeonsang Eom, and Heon Y. Yeom. 2012. Virtual machine scheduling for multicores considering effects of shared on-chip last level cache interference. International Green Computing Conference (IGCC). IEEE, 1–6. DOI:http://dx.doi.org/ 10.1109/IGCC.2012.6322250	As the cloud markets grow, the cloud providers are faced with new challenges such as reduction of power consumption and guaranteeing service level agreements (SLAs). One reason for these problems is the use of server consolidation policy based on virtualization technologies for maximizing the efficiency of resource usage. Because current virtualization technologies do not ensure performance isolation among active virtual machines (VMs), it is required to consider resource usage pattern of VMs to improve total throughput and quality of service. In this paper, we propose a virtual machine scheduler for multicore processors, which exploits the lastlevel cache (LLC) reference ratio. Specifically, we focus on the performance impact of contention in a shared LLC. We have found that the ratio of the number of LLC references to that of instructions (LLC reference ratio) is highly associated with the amount of cache demand, and a Performance-Maximizing VM (PMV) scheduling algorithm can be devised by using the ratio. We show that our PMV scheduler is effective by evaluation for various workloads. Keywords-Virtual machine consolidation; Shared resource interference; Multicore processor	In this section, we describe a cache hierachy in modern multicore processors and how to characterize cache demand of a workload by LLC reference ratio.	https://doi.org/10.1109/IGCC.2012.6322250	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Kim, et al. [NO] proposes a virtual machine scheduler for multicore processors, which exploits the lastlevel cache (llc) reference ratio. 
Energy-aware scheduling for infrastructure clouds	[]	Thomas Knauth, and Christof Fetzer. 2012. Energy-aware scheduling for infrastructure clouds. 4th International Conference on Cloud Computing Technology and Science (CloudCom’12). IEEE, 58–65. DOI:http://dx.doi.org/10.1109/CloudCom.2012.6427569	More and more data centers are built, consuming ever more kilo watts of energy. Over the years, energy has become a dominant cost factor for data center operators. Utilizing lowpower idle modes is an immediate remedy to reduce data center power consumption. We use simulation to quantify the difference in energy consumption caused exclusively by virtual machine schedulers. Besides demonstrating the inefficiency of wide-spread default schedulers, we present our own optimized scheduler. Using a range of realistic simulation scenarios, our customized scheduler OptSched reduces cumulative machine uptime by up to 60.1%. We evaluate the effect of data center composition, run time distribution, virtual machine sizes, and batch requests on cumulative machine uptime. IaaS administrators can use our results to quickly assess possible reductions in machine uptime and, hence, untapped energy saving potential.	"In this paper, we make the following contributions: • Building on our previous initial study <NO> on the use-
fulness of timed instances, we extend the evaluation to include more varied settings such at heterogeneous hosts, heterogeneous instance sizes, run time distributions, and batch requests. • We provide an extensive (re)-evaluation of the energy saving potential of timed instances. • Based on the changed simulation parameters, we adapt the original optimizing scheduler to achieve even better results in the new environment. • We share with the scientific community our simulator and workload generator. Other researchers can thus repeat and verify our results easily. The code and other artifacts can be found at http://bitbucket.org/tknauth/cloudcom2012/"	https://doi.org/10.1109/CloudCom.2012.6427569	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Knauth, et al. [NO] uses simulation to quantify the difference in energy consumption caused exclusively by virtual machine schedulers. 
Fundamentals of green communications and computing: Modeling and simulation	[]	Murat Kocaoglu, Derya Malak, and Ozgur Akan. 2012. Fundamentals of green communications and computing: Modeling and simulation. Computer 45, 9 (2012), 40–46. DOI:http://dx.doi.org/10.1109/ MC.2012.248	"A bot tom-up approach to minimizing energy dissipation starts with establishing a fundamental energy dissipation limit, which shows the true gap between current and optimum energy savings and which can serve as a springboard for developing techniques to achieve that limit. Unfortunately, for point-to-point communications, such an approach applies only in certain scenarios; using information-theory approaches to derive energy dissipation limits is largely impractical for large-scale networks, and even some small networks, because incorporating the dynamics of practical networks from an informationtheory perspective is difficult.
As an alternative way to find the minimum energy consumption in larger networks, we propose combining an architecture that uses Internet layers with a physical layer operating at the fundamental limit of energy dissipation. We can then view energy consumption as the fundamental energy consumed per information bit. For large networks, this layered approach is more feasible than an analysis based on information theory.
Our exploration of this approach highlighted some novel implementation challenges, particularly how to address the effects of constraints on the upper layers— constraints that stem from requiring minimal energy consumption at the physical layer. It also revealed new directions and open issues for green network simulation and standardization.
A s society becomes more aware of how carbon diox-ide (CO2) emissions are affecting the environment, the information and communications technology (ICT) community is investigating how to ensure that communication systems consume less energy and thus collectively have a substantially smaller carbon footprint. To that end, researchers have been working on novel techniques to reduce the energy dissipation of point-to-point communication links and computer networks.
Although modifying existing ICT systems can lead to incremental energy savings, the primary purpose of lowering energy dissipation is to reduce the CO2 emission rate. The relationship between these two ideas is more complex than many organizations realize. The ICT community has long assumed that energy savings will reduce CO2 emissions,1 but the amounts of emitted CO2 and dissipated energy are not always linearly related. In some scenarios, optimum CO2 savings in fact do not stem from the lowest possible energy consumption. Minimizing the total energy consumption of links over all possible network
A layered architecture incorporates the concept of minimum energy consumption for communication links and computer networks with multiple terminals, where emission-reduction approaches based on information theory are impractical."	"As the “Green Communication Networks” sidebar describes, ICT researchers have many ideas about how to reduce a network’s energy dissipation. Significantly reducing the carbon footprint of ICT systems is not easy, particularly if the solution is to use only energy-efficient techniques and algorithms to improve existing network components. For the past decade, ICT researchers have actively sought to satisfy the demand for high performance and high bit rates, optimizing communication links and networks to maximize data rate within power and energy constraints.
To continue meeting demand and satisfy the need for lower energy consumption, the ICT community must take a more revolutionary approach to developing green ICT solutions: optimizing ICT systems with the objective of minimizing energy consumption while using data rate and reliability as constraints could significantly reduce ICT’s adverse effects on Earth’s atmosphere. A cornerstone of this new order is the creation of fundamental energy limits that are independent of a particular technology."	https://doi.org/10.1109/MC.2012.248	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Kocaoglu, et al. [NO] proposes combines an architecture that uses internet layers with a physical layer operating at the fundamental limit of energy dissipation. 
EnaCloud: An energy-saving application live placement approach for cloud computing environments	['<NO> propose EnaCloud, which places applications dynamically on servers, with consideration of energy efficiency in a cloud platform.']	Bo Li, Jianxin Li, Jinpeng Huai, Tianyu Wo, Qin Li, and Liang Zhong. 2009. EnaCloud: An energy-saving application live placement approach for cloud computing environments. IEEE International Conference on Cloud Computing (CLOUD’09). 17–24. DOI:http://dx.doi.org/10.1109/CLOUD.2009.72	With the increasing prevalence of large scale cloud computing environments, how to place requested applications into available computing servers regarding to energy consumption has become an essential research problem, but existing application placement approaches are still not effective for live applications with dynamic characters. In this paper, we proposed a novel approach named EnaCloud, which enables application live placement dynamically with consideration of energy efficiency in a cloud platform. In EnaCloud, we use a Virtual Machine to encapsulate the application, which supports applications scheduling and live migration to minimize the number of running machines, so as to save energy. Specially, the application placement is abstracted as a bin packing problem, and an energy-aware heuristic algorithm is proposed to get an appropriate solution. In addition, an over-provision approach is presented to deal with the varying resource demands of applications. Our approach has been successfully implemented as useful components and fundamental services in the iVIC platform. Finally, we evaluate our approach by comprehensive experiments based on virtual machine monitor Xen and the results show that it is feasible.	"The infrastructure of cloud computing environment <NO><NO> is usually composed of hundreds or even thousands of server nodes. The nodes can be categorized into two types – computing nodes and storage nodes. We assume that all the data and files are stored in storage nodes, which are running network file system. Each computing node consists of processor, memory, as well as network interfaces. For simplicity, we assume all the computing nodes are
homogenous and the resource capacity of every server is 1 unit. The nodes are interconnected by high-speed LAN such as Infiniband. Each computing node runs a virtual machine monitor (VMM) and hosts one or more VMs. Each VM encapsulates an application or a component of the application. The application and the underlying operating system, which are encapsulated by a VM, are referred to as workload. We assume each workload has a predefined resource requirement when being submitted to our system. We use the term open box for a server node that is running VMs. The idle server node without running VMs is referred to as close box."	https://doi.org/10.1109/CLOUD.2009.72	1	['application', 'proposed', 'platform']	['resource_provisioning', 'provision_approach', 'application_placement', 'computing_platforms', 'consolidation_virtual']	['as_energy_efficiency', 'ability_power_subsystems', 'abstracted_bin_packing', 'abstraction_nearly_unlimited', 'acceptable_qos_penalty']	Li, et al. [NO] proposes a novel approach named enacloud, which enables application live placement dynamically with consideration of energy efficiency in a cloud platform. 
Energy-efficient resource provisioning with SLA consideration on cloud computing	[]	Jian-Sheng Liao, Chi-Chung Chang, Yao-Lun Hsu, Xiao-Wei Zhang, Kuan-Chou Lai, and ChingHsien Hsu. 2012. Energy-efficient resource provisioning with SLA consideration on cloud computing. 41st International Conference on Parallel Processing Workshops (ICPPW’12), 206–211. DOI:http://dx.doi.org/10.1109/ICPPW.2012.31	Cloud computing aims at reducing energy consumption and maximizing resource efficiency without violating service level agreement (SLA). To address these important issues, this study proposes an energy-efficient resource provisioning technology with SLA consideration for virtual machine scheduling. According to SLAs, the resource manager could consolidate virtual machines onto the physical machine for meeting customers’ SLA requests. Experimental results show that the proposed approach outperforms other proposed ones in power consumption. Keywords-component; Cloud Computing; Virtualization; Energy Saving; Resource Provisioning; Service Level Agreement	"where ""$6/ ""$'is require time of executing a job with sufficient resources. !77-0! $ is time of allocating sufficient resources to this job. 9: is the penalty of violating SLA.
Figure 4(a) shows a demonstration of applying the round-robin approach to allocate limited resources to VMs. In this demonstration, the round-robin manner distributes virtual machines to different hosts in order to balance load. However, the number of hosts could be reduced for saving energy. When the VMs are consolidated onto fewer hosts, as shown in Figure 4(b), the unused hosts could be turned off and save energy.
In order to obtain the optimal host of allocating VMs, the number of unused processors in a host is defined as
(?@9 )*?@9 # A ) @?@9 # B C?@9
and the amount of the unused memory size is defined as
(? )*? # A ) @? # B C?
and the amount of unused storage disk is defined as
(?C D )*?C D # A ) @?C D # B C?C D
Therefore, the optimal host of allocating an incoming VM is the host with the minimal value, as show in following.
EFGH/%?0I/JK""?0I/LM! !7?0I/J N O /%?,$,JK""?,$,L M! !7?,$,J P O /%?& QJK""?& QL M! !7?& QJ RS (8)
where ?@9 is the requirement of the incoming virtual machine. N is the percentage of overall energy consumption donate by CPU, P is the percentage of overall energy consumption donate by memory, and R is the percentage of overall energy consumption donate by storage disk. Based on the literature <NO>, N is 58%, P is 28% and R is 14%, as shown in Figure 5.
This study proposes a SLA-Based Resource Constraint VM Scheduling approach to allocate resources to the incoming VMs without violating SLAs, as shown in the following algorithm. When a user submits a request to allocate a VM to a suitable host, the proposed approach assigns a priority and checks running hosts for determining whether the remaining resources are enough for executing the incoming VM (steps 9-12). If the running hosts couldn’t satisfy the incoming VM request, a new host is turn-on for this VM (steps 13-16). If there is more than one of hosts with sufficient resources, the proposed approach selects the host with the minimal value obtained from equation (8) (step 17-18).
Algorithm SLA-Based Resource Constraint VM Scheduling
1. Input HostList, VmList, CurrentHostList
2. Output allocation of VMs
3. HostList {Host1, Host2, …, HostN};
4. VmList {Vm1, Vm2, …, VmN};
5. CurrentHostList null;
6. OptimalHost null; 7. for Vm from Vm1 to VmN in VmList do{ 8. if(CurrentHostList != null){ 9. for Host from Host1 to HostN in
CurrentHostList{ 10. if(Host.GetUnusedResource >
Vm.GetResource) 11. EnoughHostList += Host;"	https://doi.org/10.1109/ICPPW.2012.31	1	['application', 'proposed', 'platform']	['resource_provisioning', 'provision_approach', 'application_placement', 'computing_platforms', 'consolidation_virtual']	['as_energy_efficiency', 'ability_power_subsystems', 'abstracted_bin_packing', 'abstraction_nearly_unlimited', 'acceptable_qos_penalty']	Cloud computing aims at reducing energy consumption and maximizing resource efficiency without violating service level agreement (SLA) [NO].
Server consolidation in clouds through gossiping	[]	Moreno Marzolla, Ozalp Babaoglu, and Fabio Panzieri. 2011. Server consolidation in clouds through gossiping. International Symposium on World of Wireless, Mobile and Multimedia Networks (WoWMoM’11), IEEE, 1–6. DOI:http://dx.doi.org/10.1109/WoWMoM.2011.5986483	The success of Cloud computing, where computing power is treated as a utility, has resulted in the creation of many large datacenters that are very expensive to build and operate. In particular, the energy bill accounts for a significant fraction of the total operation costs. For this reason a significant attention is being devoted to energy conservation techniques, for example by taking advantage of the built-in power saving features of modern hardware. Cloud computing offers novel opportunities for achieving energy savings: Cloud systems rely on virtualization techniques to allocate computing resources on demand, and modern Virtual Machine (VM) monitors allow live migration of running VMs. Thus, energy conservation can be achieved through server consolidation, moving VM instances away from lightly loaded computing nodes so that they become empty and can be switched to low-power mode. In this paper we present V-MAN, a fully decentralized algorithm for consolidating VMs in large Cloud datacenters. V-MAN can operate on any arbitrary initial allocation of VMs on the Cloud, iteratively producing new allocations that quickly converge towards the one maximizing the number of idle hosts. V-MAN uses a simple gossip protocol to achieve efficiency, scalability and robustness to failures. Simulation experiments indicate that, starting from a random allocation, V-MAN produces an almost-optimal VM placement in just a few rounds; the protocol is intrinsically robust and can cope with computing nodes being added to or removed from the Cloud.	"The problem of migrating running processes from one processor to another has been initially considered for balancing workload in distributed multiprocessor systems <NO>. Recent advances in virtualization technologies allow entire running VMs to be transferred across different physical hosts <NO>. This opportunity is being investigated for different purposes, mostly related with various Quality of Service aspects.
Stage and Setzer <NO> describe a network-aware migration scheduler which takes into consideration the workload type of each VM. The migration takes into explicit consideration the network topology and the bandwidth requirements to move VM images within a given deadline. Wood et al. <NO> describe Sandpiper, a system which automatically identifies performance bottlenecks, identifies a new VM allocation which removes them and finally initiate the required migrations to instantiate the new allocation. Sandpiper is OS and application independent, relying on monitoring disk and network usage inside the Xen VM monitor.
Some recent works considered distributed (hierarchical) approaches for energy management in large datacenters. Bennani and Menasce <NO> present a hierarchical approach addressing the problem of dynamically redeploying servers in a continuously varying workload scenario. In this case, servers are grouped according to an application environmental logic, and a so-called local controller that is in charge of managing a set of servers. Das et al. <NO> present a multiagent system approach to the problem of green performance
in data center. As for aforementioned papers, the framework is based on a hierarchy, according to which a resource arbiter assigns resources to the application managers, which in turn become in charge of managing physical servers. Srikantaiah et al. <NO> study the impact of consolidation of multiple workloads with different resource usage on performance, energy usage, and resource utilization. This is not achieved by migrating applications, but rather by consolidating the workload so that each server receives a “balanced mix” of requests. Finally, Barbagallo et al. <NO> describe a bioinspired algorithm based on the scout-worker migration method, in which some entities (the scouts) are allowed to move from one physical node to another in order to cooperatively identify a suitable destination for VMs (the workers) which are migrated.
V-MAN uses a fully decentralized approach with no shared data structures or central controllers. Also, V-MAN does not require any instrumentation of either VMs or hosted applications, V-MAN does not rely on a small subset of special entities (e.g., the scouts of <NO>): instead, all servers cooperate to identify a new VM allocation, and this ensures that V-MAN is capable of scaling with the size of the datacenter."	https://doi.org/10.1109/WoWMoM.2011.5986483	0	['algorithm', 'consolidation', 'optimization']	['relocation_costs', 'v_man', 'consolidation_algorithm', 'energy_conservation', 'first_fit']	['total_power_consumption', 'ant_colony_optimization', 'definition_cloud_computing', 'multidimensional_bin_packing', 'placement_task_assignment']	Marzolla, et al. [NO] present v-man, a fully decentralized algorithm for consolidating vms in large cloud datacenters. 
A tale of clouds: Paradigm comparisons and some thoughts on research issues	[]	Lijun Mei, Wing Kwong Chan, and T.H. Tse. 2008. A tale of clouds: Paradigm comparisons and some thoughts on research issues. Asia-Pacific Services Computing Conference (APSCC’08). IEEE, 464– 469. DOI:http://dx.doi.org/10.1109/APSCC.2008.168	Cloud computing is an emerging computing paradigm. It aims to share data, calculations, and services transparently among users of a massive grid. Although the industry has started selling cloud-computing products, research challenges in various areas, such as UI design, task decomposition, task distribution, and task coordination, are still unclear. Therefore, we study the methods to reason and model cloud computing as a step toward identifying fundamental research questions in this paradigm. In this paper, we compare cloud computing with service computing and pervasive computing. Both the industry and research community have actively examined these three computing paradigms. We draw a qualitative comparison among them based on the classic model of computer architecture. We finally evaluate the comparison results and draw up a series of research questions in cloud computing for future exploration.	"This section reviews the preliminaries of cloud computing, service computing, and pervasive computing.
2.1. Cloud computing As we have introduced in Section 1, a computing cloud is a massive network of nodes. Thus, scalability should be a quality feature of the computing cloud. It has at least two dimensions, namely horizontal cloud scalability and vertical cloud scalability (adapted from <NO>).
Horizontal cloud scalability is the ability to connect and integrate multiple clouds to work as one logical cloud. For instance, a cloud providing calculation services (calculation cloud) can access a cloud providing storage services (storage cloud) to keep intermediate results. Two calculation clouds can also integrate into a larger calculation cloud.
Vertical cloud scalability is the ability to improve the capacity of a cloud by enhancing individual existing nodes in the cloud (such as providing a server with more physical memory) or improving the bandwidth that connects two nodes. In addition, to meet increasing market demand, a node can be gradually upgraded from a single power machine to a data center. Scalability should be transparent to users. For instance, users may store their data in the cloud without the need to know where it keeps the data or how it accesses the data.
For simplicity, we will refer to horizontal and vertical cloud scalability, respectively, as horizontal scalability and vertical scalability in this paper.
2.2. Service computing Service computing (or service-oriented computing) is an emerging paradigm to model, create, operate, and manage business services. In this paradigm, services publish themselves in public registries, discover peer services, and bind to the latter services to form service compositions using standardized protocols <NO>. To create a service composition, engineers may use a specification, such as WS-BPEL <NO>, to model the collaborative need in workflows. To carry out individual workflow steps, software developers may use Web services, the most
popular way to fulfill service-oriented architecture in the industry. A set of service-oriented applications over the Web services thus creates a network of services.
We briefly describe a service-oriented network <NO> to facilitate the comparison in the rest of the paper. An element in such a network is a service registry, service consumer, or service provider. A service provider registers itself in a service registry. A service consumer first discovers the service from a registry, and then binds to the service. A service provider may register itself to more than one registry. A registry may also associate its registered services to other registries, and acts as a service itself. Such a treatment on a registry provides a generic view among elements in service-oriented modeling.
2.3. Pervasive computing Pervasive computing (or ubiquitous computing) <NO><NO> is another emerging computing paradigm. Software (often referred as pervasive software) can be embedded in a constantly changing computing environment. Therefore, pervasive software users do not need to be concerned about how to adjust the software to adapt to the surrounding computing environment. A well-developed environment will enable users to use pervasive software everywhere without extra effort.
To understand and react to a user, applications use environmental features, known as contexts, extensively. Sensors can capture these contexts. To allow ubiquitous support to end users, smart sensors are placed around users to preserve different information, such as the locations, contexts, and user-relevant data.
Figure 2 shows a pervasive computing example. Sensors, mobile phones and PDAs, desktop computer, and servers are interconnected logically to form an application.
Suppose a nomadic user at the top left corner of Figure 2 moves from using a laptop to using a desktop computer. The laptop and the desktop computer both serve as UI portals to the tuple space maintained by the pervasive software. The remarked information from various display portals (such as the PDAs on the right-hand part) may need adapting. For example, a desktop computer may be equipped with a high-definition webcam. Thus, a presentation display portal may display the contents with a camera image kept in the tuple space of the application when using a laptop."	https://doi.org/10.1109/APSCC.2008.168	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Mei, et al. [NO] studies the methods to reason and model cloud computing as a step toward identifying fundamental research questions in this paradigm. 
The current state of understanding of the energy efficiency of cloud computing	[]	Francis Owusu, and Colin Pattinson. 2012. The current state of understanding of the energy efficiency of cloud computing. IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom’12). 1948–1953. DOI:http://dx.doi.org/10.1109/TrustCom.2012.270	Cloud computing has been hailed as the achievement of the longheld dream of computing as a utility and has the potential to transform a large part of the Information and Communication Technology (ICT) industry. Cloud computing is both a business and an economic model which has been gaining popularity since 2006 and it is currently the most talked about technology in the ICT industry. Because it views hardware and software as commodities, the cloud is an example of a disruptive technology. It offers enterprises the opportunity to reduce hardware and software cost and the potential reduction of maintenance and support staff. Data centers and cloud computing services providers hope that the widespread adoption of the cloud will bring them more profit and they are actively promoting the technology. The cloud has had its share of controversy; ranging from the definition of cloud computing to its energy efficiency. This paper discusses one area of controversy; the energy efficiency of cloud computing. We outline previous contributions to the discussion of energy efficiency of cloud computing, provide a working definition of cloud computing and discuss its importance, which will grow as the technology matures and becomes well known.	"Arif wrote that cloud computing has evolved through a number of phases including grid and utility computing, and application service provision (ASP) <NO>. Arif wrote Salesforce.com pioneered the concept of delivering enterprise applications via a website in 1999 followed by Amazon Web Services for storage and computation in 2002. Cloud computing started gaining general popularity in 2006 when Amazon launched its Elastic Compute Cloud (EC2) as a commercial web service, allowing small businesses and individuals to rent computing resources. After the introduction of Web 2.0, Google and others started to offer browser-based enterprise applications such as Google Apps in 2009. Other technologies enabling the evolution of cloud computing include virtualization and broadband. The many definitions of cloud computing has generated some controversy because the technology is new and not well understood resulting in confusion among IT professionals and vendors. This confusion was captured in “Twenty one experts define cloud computing” when these experts gave twenty one different definitions for cloud computing <NO>.
978-0-7695-4745-9/12 $26.00 © 2012 IEEE DOI 10.1109/TrustCom.2012.270
1948
Gartner lamented that the contrasting views on cloud computing are causing confusion then added their own definition by defining cloud computing as ""a style of computing where scalable and elastic IT-enabled capabilities are provided 'as a service' to external customers using Internet technologies” <NO>. IDC defines it as ""an emerging IT development, deployment and delivery model, enabling realtime delivery of products, services and solutions over the Internet"" <NO>. Accenture defines cloud computing as “the dynamic provisioning of IT capabilities, whether hardware, software, or services from a third party over the network” <NO>. Kenneth K. Chellappa gave the first academic definition of cloud computing as “a computing paradigm where the boundaries of computing will be determined by economic rationale rather than technical limits” <NO>. Ambrust et al writes that “Cloud computing refers to both the applications delivered as services over the Internet and the hardware and systems software in the data centers that provide those services” <NO>. Although there are many different definitions for cloud computing, there is a broad consensus on the definition by the National Institute of Standards and Technology (NIST). They define cloud computing as: “A model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model promotes availability and is composed of five essential characteristics, three service models, and four deployment models”. This paper accepts the NIST definition but with the understanding that “network” comprises the telecommunication networks and the Internet. The hope of many is that a standard definition will be found and adopted as the technology matures.
III. COMPONENTS OF CLOUD COMPUTING AND THEIR ENERGY EFFICIENCY
Cloud computing can be considered as a system with three main components."	https://doi.org/10.1145/1735971.1736048	0	['algorithm', 'consolidation', 'optimization']	['relocation_costs', 'v_man', 'consolidation_algorithm', 'energy_conservation', 'first_fit']	['total_power_consumption', 'ant_colony_optimization', 'definition_cloud_computing', 'multidimensional_bin_packing', 'placement_task_assignment']	Owusu, et al. [NO] discusses one area of controversy ; the energy efficiency of cloud computing. 
Variation-aware server placement and task assignment for data center power minimization	[]	Ali Pahlavan, Mahmoud Momtazpour, and Maziar Goudarzi. 2012. Variation-aware server placement and task assignment for data center power minimization. 2012 IEEE 10th International Symposium	Size and number of data centers are fast growing all over the world and their increasing total power consumption is a worldwide concern. Moreover, increase in the amount of process variation in nanometer technologies and its effect on total power consumption of servers has made it inevitable to move toward variation-aware power reduction strategies. This paper formulates a variation-aware joint server placement and task assignment method using Integer Linear Programming (ILP) to minimize total power consumption of data centers. We first determine the optimum placement of servers in the data center racks based on total power consumption of each server and the data center recirculation model obtained by Computational Fluid Dynamics (CFD) simulations. Then, we dynamically consolidate the ON servers in chassis and racks such that the use of powergreedy servers is minimized. Experimental results reveal up to 14.85% and an average of 8.92% power saving at different server utilization rates with respect to conventional methods. Keywords-Data center, Power reduction, Process variation, Server placement, Task assignment.	"The authors in <NO> proposed a method to minimize the total energy consumption of data center while considering thermal management for their reliable function. They have used CFDbased simulations in order to evaluate their task scheduling methods such as Minimal Computing Energy (MCE) which is the best one among others. The aforementioned algorithm tries to minimize the total number of running servers and turn off all other idle ones. The algorithm first assigns the tasks to the nodes having the lowest inlet temperature. In another CFDbased work <NO> an analysis of a data center with temperature variation has been presented. The authors have obtained static provisioning for an arbitrary distribution of cooling resources that will lead to a reference state. They try to minimize the inlet temperature by dividing the workload on other available systems.
CFD-based techniques are often time consuming and complex. This issue will not lead to tackling online scheduling problem efficiently <NO>. Therefore, fast thermal evaluation models are developed such as <NO>, <NO>. In <NO>, the goal is to reduce the peak inlet temperature in order to obtain the lowest power consumption of cooling system using heat recirculation model. As a result, 20% to 30% cooling power saving will occur in different data center utilization rates. In <NO> fast prediction of temperature distribution is done using distributed sensors to reduce energy consumption in high performance data centers considering recirculation properties. So, this method is suitable for real time and online management. The authors in <NO> demonstrated a thermal aware resource management method considering heat transfer properties and workloads having thermal features. Their scheduling algorithm will result in reduced power consumption without performance degradation.
Reference <NO> presented the chassis consolidation technique as a mathematical optimization problem and a heuristic algorithm. Optimization problem has been solved via ILP. Their experiments show that they gain 13% power saving for different utilization rates in comparison with the technique lacking consolidation.
None of the above works considers process variation effects on total power consumption of high performance servers. As will be explained below, variability effects in nanometer-scale technologies has caused dramatic variations in leakage and total power consumption of processor cores especially in high performance processors <NO>. Such variation effects are visible in today processors and are expected to further rise with technology scaling when further approaching atomic scales. Process variation effects are already studied in highperformance multiprocessor and embedded systems <NO>, <NO>, <NO>, but to the best of our knowledge such effects have not been previously considered at data center scales."	https://doi.org/10.1109/ISPA.2012.29	0	['algorithm', 'consolidation', 'optimization']	['relocation_costs', 'v_man', 'consolidation_algorithm', 'energy_conservation', 'first_fit']	['total_power_consumption', 'ant_colony_optimization', 'definition_cloud_computing', 'multidimensional_bin_packing', 'placement_task_assignment']	Pahlavan, et al. [NO] formulates a variation-aware joint server placement and task assignment method using integer linear programming (ilp) to minimize total power consumption of data centers. 
A quantitative analysis of cooling power in container-based data centers	[]	Amer Qouneh, Chao Li, and Tao Li. 2011. A quantitative analysis of cooling power in container-based data centers. IEEE International Symposium on Workload Characterization (IISWC). 61–71. DOI:http://doi.ieeecomputersociety.org/10.1109/IISWC.2011.6114197	Cooling power is often represented as a single taxed cost on the total energy consumption of the data center. Some estimates go as far as 50% of the total energy demand. However, this view is rather simplistic in the presence of a multitude of cooling options and optimizations. In response to the rising cost of energy, the industry introduced modular design in the form of containers to serve as the new building block for data centers. However, it is still unclear how efficient they are compared to raised-floor data centers and under what conditions they are preferred. In this paper, we provide comparative and quantitative analysis of cooling power in both container-based and raised-floor data centers. Our results show that a container achieves 80% and 42% savings in cooling and facility powers respectively compared to a raised-floor data center and that savings of 41% in cooling power are possible when workloads are consolidated onto the least number of containers. We also show that cooling optimizations are not very effective at high utilizations; and that a raised-floor data center can approach the efficiency of a container at low utilizations when employing a simple cooling optimization.	The efficiency of the cooling architecture greatly affects the recurring costs of data centers. In this section we describe the operation and characteristics of both raised-floor and container-based data centers. We also provide background on calculating the cooling power.	https://doi.org/10.1109/IISWC.2011.6114197	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Qouneh, et al. [NO] provides comparative and quantitative analysis of cooling power in both container-based and raised-floor data centers. 
Energy-efficient application-aware online provisioning for virtualized clouds and data centers	[]	Ivan Rodero, Juan Jaramillo, Andres Quiroz, Manish Parashar, Francesc Guim, and Stephen Poole. 2010. Energy-efficient application-aware online provisioning for virtualized clouds and data centers. International Green Computing Conference. 31–45. DOI:http://dx.doi.org/10.1109/GREENCOMP.2010.5598283	As energy efficiency and associated costs become key concerns, consolidated and virtualized data centers and clouds are attractive computing platforms for dataand computeintensive applications. These platforms provide an abstraction of nearly-unlimited computing resources through the elastic use of pools of consolidated resources, and provide opportunities for higher utilization and energy savings. Recently, these platforms are also being considered for more traditional high-performance computing (HPC) applications that have typically targeted Grids and similar conventional HPC platforms. However, maximizing energy efficiency, costeffectiveness, and utilization for these applications while ensuring performance and other Quality of Service (QoS) guarantees, requires leveraging important and extremely challenging tradeoffs. These include, for example, the tradeoff between the need to efficiently create and provision Virtual Machines (VMs) on data center resources and the need to accommodate the heterogeneous resource demands and runtimes of these applications. In this paper we present an energy-aware online provisioning approach for HPC applications on consolidated and virtualized computing platforms. Energy efficiency is achieved using a workload-aware, just-right dynamic provisioning mechanism and the ability to power down subsystems of a host system that are not required by the VMs mapped to it. We evaluate the presented approach using real HPC workload traces from widely distributed production systems. The results presented demonstrated that compared to typical reactive or predefined provisioning, our approach achieves significant improvements in energy efficiency with an acceptable QoS penalty. Keywords-Autonomic Computing; Cloud Computing; Energy Efficiency; Virtualization; Data Center; Resource Provisioning	"In our previous work <NO>, we investigated decentralized online clustering. We also presented autonomic mechanisms for VM provisioning to improve resource utilization <NO>. This approach focused on reducing the over-provisioning that occurs because of the difference between the virtual resources allocated to VM instances and those contained in individual job requests. In particular, we used DOC to efficiently characterize dynamic, rather than generic (such as Amazon’s EC2 VM types <NO>), classes of resource requirements that can be used for proactive VM provisioning. To address the inaccuracies in client resource requests that lead to over-provisioning, we explored the use of workload modeling techniques and their application to the highly varied workloads of cloud environments.
In the VM provisioning mechanism that we proposed, as with most predictive approaches, the flow of arriving jobs was divided into time periods that we call analysis windows. During each window, an instance of the clustering algorithm was run with the jobs that arrived during that window, producing a number of clusters or VM classes. At the same time, each job was assigned to an available VM as it arrived if one was provisioned with sufficient resources to meet its requirements. The provisioning was done based on the most recent analysis results from the previous analysis window. For the first window, the best option was to reactively create VMs for incoming jobs. However, the job descriptions were sent to the processing node network and by the end of the analysis window each node could quickly determine if a cluster existed in its particular region. If so, the node could locally trigger the creation of new VMs for the jobs in the next analysis window with similar resource requirements. According to <NO>, the time required to create batches of VM in a cloud infrastructure does not differ significantly
from the time for creating a single VM instance. Thus, the VMs for each class could be provisioned within the given time window. In order to match jobs to provisioned VMs, the cluster description could be distributed in the node network using the range given by the space occupied by the cluster in the information space. Thus, when a new job arrived, it would be routed to a node that holds descriptors for VMs that had close resource requirements."	https://doi.org/10.1109/GREENCOMP.2010.5598283	1	['application', 'proposed', 'platform']	['resource_provisioning', 'provision_approach', 'application_placement', 'computing_platforms', 'consolidation_virtual']	['as_energy_efficiency', 'ability_power_subsystems', 'abstracted_bin_packing', 'abstraction_nearly_unlimited', 'acceptable_qos_penalty']	Rodero, et al. [NO] presents an energy-aware online provisioning approach for hpc applications on consolidated and virtualized computing platforms. 
Virtualization: A survey on concepts, taxonomy and associated security issues	[]	Jyotiprakash Sahoo, Subasish Mohapatra, and Radha Lath. 2010. Virtualization: A survey on concepts, taxonomy and associated security issues. 2nd International Conference on Computer and Network Technology (ICCNT’10). IEEE. 222–226. DOI:http://dx.doi.org/10.1109/ICCNT.2010.49	Virtualization is a term that refers to the abstraction of computer resources. The purpose of virtual computing environment is to improve resource utilization by providing a unified integrated operating platform for users and applications based on aggregation of heterogeneous and autonomous resources. More recently, virtualization at all levels (system, storage, and network) became important again as a way to improve system security, reliability and availability, reduce costs, and provide greater flexibility. This paper explains the basics of system virtualization and addresses pros and cons of virtualization along with taxonomy and challenges. Keywords— Virtualization; hypervisor; VMM; Security; Threats.	"Virtualization was first developed in 1960’s by IBM Corporation, originally to partition large mainframe computer into several logical instances and to run on single physical mainframe hardware as the host. This feature was invented because maintaining the larger mainframe computers became cumbersome. The scientist realized that this capability of partitioning allows multiple processes and applications to run at the same time, thus increasing the efficiency of the environment and decreasing the maintenance overhead. Although the main focus of this paper is to provide an overview of security vulnerabilities in a virtual environment. It
is worth mentioning some of the security benefits that comes together with virtualization. Two primary benefits offered by any virtualization technology are
• Resource sharing - Unlike in non-virtualized environment where all the resources are dedicated to the running programs, in virtualized environment the VMs shares the physical resources such as memory, disk and network devices of the underlying host.
• Isolation - One of the key issues in virtualization provides isolation between virtual machines that are running on the same physical hardware. Programs running in one virtual machine cannot see programs running in another virtual machine."	https://doi.org/10.1109/ICCNT.2010.49	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	Sahoo, et al. [NO] explains the basics of system virtualization and addresses pros and cons of virtualization along with taxonomy and challenges. 
Low-power Amdahl-balanced blades for data intensive computing	[]	Alexander S. Szalay, Gordon C. Bell, Howie Huang, Andreas Terzis, and Alainna White. 2010. Low-power Amdahl-balanced blades for data intensive computing. ACM SIGOPS Operating Systems Review 44, 1 (2010), 71–75. DOI:http://dx.doi.org/10.1145/1740390.1740407	Enterprise and scientific data sets double every year, forcing similar growths in storage size and power consumption. As a consequence, current system architectures used to build data warehouses are about to hit a power consumption wall. In this paper we propose an alternative architecture comprising large number of so-called Amdahl blades that combine energy-efficient CPUs with solid state disks to increase sequential read I/O throughput by an order of magnitude while keeping power consumption constant. We also show that while keeping the total cost of ownership constant, Amdahl blades offer five times the throughput of a state-of-theart computing cluster for data-intensive applications. Finally, using the scaling laws originally postulated by Amdahl, we show that systems for data-intensive computing must maintain a balance between low power consumption and per-server throughput to optimize performance per Watt.	"Data-Intensive Computing. Scientific data sets are approaching petabytes today; enterprise data warehouses routinely store and process even more data. Most analyses performed over these datasets (e.g., data mining, regressions, aggregates and statistics) need to look at a large fraction of the stored data. Thereby, sequential read throughput is becoming the most relevant metric to measure the performance of data-intensive systems. Given that the relevant data sets do not fit in main memory, they have to be stored and retrieved from disks. For this reason, understanding the scaling behavior of hard disks is critical for predicting the
performance of existing data-intensive systems as data sets continue to grow.
Over the last decade the rotation speed of large disks used in disk arrays has only changed by a factor of three, from 5,400 RPM to 15,000 RPM, while disk sizes have increased by a factor of 1,000. Likewise, seek times have improved only modestly over the same time period because they are limited by mechanical strains on the disk’s heads. As a result, random access times have only improved slightly. Moreover, the sequential I/O rate continues to grow with the square root of disk capacity since it depends on the disk platter density <NO>.
As a concrete example of the trends described above, the sequential I/O throughput of commodity SATA drives is 60- 80 MB/sec today, compared to 20 MB/sec ten years ago. However, considering the vast increase in disk capacity this modest increase in throughput has effectively turned the hard disk to a serial device: reading a terabyte disk at this rate requires 4.5 hours. Therefore, the only way to increase aggregate I/O throughput is to read in parallel from more smaller disks. In fact, modern data warehouse systems, such as the GrayWulf cluster described next, aggressively use this approach to improve application performance.
GrayWulf. The GrayWulf system <NO> represents a state-ofthe-art architecture for data-intensive applications, having won the Storage Challenge at SuperComputing 2008. Focusing primarily on sequential I/O performance, each GrayWulf server consists of 30 locally attached 750 GB SATA drives, connected to two Dell PERC/6 controllers in a Dell 2950 server with 24 GB of memory and two four-core Intel Xeon processors clocked at 2.66 GHz. The raw read performance of this system is 1.5 GB/s, translating to 15,000 seconds (4.2 hours) to read all the disks. Such a building block costs approximately $12,000 and offers a total storage capacity of 22.5TB. Its power consumption is 1,150 W.
The GrayWulf cluster consists of 50 such servers and this parallelism linearly increases the aggregate bandwidth to 75 GB/sec, the total amount of storage to more than 1.1 PB and the power consumption to 56 kW. However, the time to read all the disks remains 4.2 hours, independent of the number of servers.
Doubling the storage capacity of the GrayWulf cluster, while maintaining its per-node current throughput, would require using twice as many servers, thereby doubling its power consumption. Alternatively, one could divide the same amount of data over twice as many servers to double the system’s throughput at the cost of doubling its power consumption.
At this rate, the cost of building and operating these ever expanding facilities is becoming a major roadblock not only for universities but even for large corporations <NO>. Thus tackling the next generation of data-intensive computations in a power-efficient fashion requires a radical departure from existing approaches."	https://doi.org/10.1145/1740390.1740407	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	S. et al. [NO] proposes an alternative architecture comprising large number of so-called amdahl blades that combine energy-efficient cpus with solid state disks to increase sequential read i/o throughput by an order of magnitude while keeping power consumption constant. 
System-level virtualization for high performance computing	[]	Geoffroy Valle, Thomas Naughton, Christian Engelmann, Hong Ong, and Stephen L. Scott. 2008. System-level virtualization for high performance computing. 16th IEEE Euromicro Conference on Parallel, Distributed and Network-Based Processing (PDP’08). 636–643. DOI:http://dx.doi.org/	System-level virtualization has been a research topic since the 70’s but regained popularity during the past few years because of the availability of efficient solution such as Xen and the implementation of hardware support in commodity processors (e.g. Intel-VT, AMD-V). However, a majority of system-level virtualization projects is guided by the server consolidation market. As a result, current virtualization solutions appear to not be suitable for high performance computing (HPC) which is typically based on large-scale systems. On another hand there is significant interest in exploiting virtual machines (VMs) within HPC for a number of other reasons. By virtualizing the machine, one is able to run a variety of operating systems and environments as needed by the applications. Virtualization allows users to isolate workloads, improving security and reliability. It is also possible to support nonnative environments and/or legacy operating environments through virtualization. In addition, it is possible to balance work loads, use migration techniques to relocate applications from failing machines, and isolate fault systems for repair. This document presents the challenges for the implementation of a system-level virtualization solution for HPC. It also presents a brief survey of the different approaches and ∗ORNL’s research sponsored by the Laboratory Directed Research and Development Program of Oak Ridge National Laboratory (ORNL), managed by UT-Battelle, LLC for the U. S. Department of Energy under Contract No. DE-AC05-00OR22725. techniques to address these challenges.	In this section we present the challenges for the implementation of a system-level virtualization solution: a hypervisor for HPC, virtual system environments, high availability and fault tolerance, system management and administration, resource management, and I/O & storage. We also present a brief survey for each of these challenges.	https://doi.org/10.1109/PDP.2008.85	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	This document presents the challenges for the implementation of a system-level virtualization solution for HPC [NO].
Dynamic heterogeneity and the need for multicore virtualization	[]	Philip M. Wells, Koushik Chakraborty, and Gurindar S. Sohi. 2009. Dynamic heterogeneity and the need for multicore virtualization. ACM SIGOPS Operating Systems Review 43, 2 (2009), 5–14. DOI:http://dx.doi.org/10.1145/1531793.1531797	As the computing industry enters the multicore era, exponential growth in the number of transistors on a chip continues to present challenges and opportunities for computer architects and system designers. We examine one emerging issue in particular: that of dynamic heterogeneity, which can arise, even among physically homogeneous cores, from changing reliability, power, or thermal conditions, different cache and TLB contents, or changing resource configurations. This heterogeneity results in a constantly varying pool of hardware resources, which greatly complicates software’s traditional task of assigning computation to cores. In part to address dynamic heterogeneity, we argue that hardware should take a more active role in the management of its computation resources. We propose hardware techniques to virtualize the cores of a multicore processor, allowing hardware to flexibly reassign the virtual processors that are exposed, even to a single operating system, to any subset of the physical cores. We show that multicore virtualization operates with minimal overhead, and that it enables several novel resource management applications for improving both performance and reliability.	Several proposals have exalted the benefits of designing processors with statically heterogeneous cores — cores that are designed to have different physical characteristics in order to capitalize on different engineering trade-offs (e.g., <NO>). Future multicore chips will similarly contain dynamically heterogeneous cores as well — cores which exhibit different, and rapidly changing, execution characteristics, even though they may be physically homogeneous in design.	https://doi.org/10.1145/1531793.1531797	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	M. et al. [NO] examines one emerges issue in particular : that of dynamic heterogeneity, which can arise, even among physically homogeneous cores, from changing reliability, power, or thermal conditions, different cache and tlb contents, or changing resource configurations. 
Survivable virtual infrastructure mapping in virtualized data centers	[]	Jielong Xu, Jian Tang, Kevin Kwiat, Weiyi Zhang, and Guoliang Xue. 2012. Survivable virtual infrastructure mapping in virtualized data centers. IEEE 5th International Conference on Cloud Computing. 196– 203. DOI:http://doi.ieeecomputersociety.org/10.1109/CLOUD.2012.100	In a virtualized data center, survivability can be enhanced by creating redundant Virtual Machines (VMs) as backup for VMs such that after VM or server failures, affected services can be quickly switched over to backup VMs. To enable flexible and efficient resource management, we propose to use a service-aware approach in which multiple correlated VMs and their backups are grouped together to form a Survivable Virtual Infrastructure (SVI) for a service or a tenant. A fundamental problem in such a system is to determine how to map each SVI to a physical data center network such that operational costs are minimized subject to the constraints that each VM’s resource requirements are met and bandwidth demands between VMs can be guaranteed before and after failures. This problem can be naturally divided into two sub-problems: VM Placement (VMP) and Virtual Link Mapping (VLM). We present a general optimization framework for this mapping problem. Then we present an efficient algorithm for the VMP subproblem as well as a polynomial-time algorithm that optimally solves the VLM subproblem, which can be used as subroutines in the framework. We also present an effective heuristic algorithm that jointly solves the two subproblems. It has been shown by extensive simulation results based on the real VM data traces collected from the green data center at Syracuse University that compared with the First Fit Descending (FFD) and single shortest path based baseline algorithm, both our VMP+VLM algorithm and joint algorithm significantly reduce the reserved bandwidth, and yield comparable results in terms of the number of active servers.	"VM management has attracted research attention from both industry and academia due to its potential for reducing operation costs of data centers. A few commercial software tools
(such as VMware Capacity Planner <NO>) have been developed to determine VMP according to resources at hosting servers such as CPU, memory, etc. The problems of determining how to place VMs with the objective of minimizing server power consumption have been studied in <NO>, <NO>, <NO>. In <NO>, Li et al. proposed a power efficient approach named EnaCloud, which uses application scheduling and VM live migration to minimize the number of running servers. In <NO>, mathematical programming formulations were presented for various VMP problems and heuristic algorithms were presented to solve them. Extensive simulations were conducted based on a large set of real server load data from a data center. Unlike <NO>, the VMP problem with bandwidth demand constraints was formulated into a stochastic (instead of deterministic) bin packing problem and an online approximation algorithm was presented in <NO>.
In <NO>, Verma et al. presented the design, implementation and evaluation of a power-aware application placement controller, pMapper, in virtual server clusters. They presented multiple ways to formulate the cost-aware VMP problem and present simple and practical algorithms to solve them. In <NO>, Meng et al. , for the first time, considered VMP with the objective of minimizing the communication cost. The problem was showed to be NP-hard. The authors designed a two-tier heuristic algorithm to solve it. In <NO>, the authors introduced a computationally efficient scheme, AppAware, to incorporate inter-VM dependencies and the underlying network topology into VM migration decisions. Using simulations, they showed that it decreases network traffic by up to 81% compared to a well-known method that is not application-aware.
The mapping problem studied in this paper shares some similarities with the NV problems, which have been studied in recent works <NO>, <NO>, <NO>, <NO>. In <NO>, the authors developed heuristic algorithms for two versions of the problem: Virtual Network (VN) assignment without reconfiguration and VN assignment with reconfiguration. In <NO>, Yu et al. presented heuristic algorithms to solve an NV problem which allows the substrate (physical) network to split a virtual link over multiple substrate paths and employ path migration to periodically re-optimize the utilization of the physical network. In <NO>, a fast VN mapping algorithm based on subgraph isomorphism detection was presented, which maps nodes and links during the same stage. The authors of <NO>, for the first time, presented heuristic algorithms to solve a survivable NV problem, in which a certain percentage of bandwidth of each link is reserved to support survivability.
The closest work is a very recent paper <NO>, in which an NV architecture called SecondNet was designed and evaluated for virtualized data centers. SecondNet introduces a centralized resource allocation algorithm for bandwidth guaranteed virtual to physical mapping. Moreover, the proposed approach was implemented using source routing and the Multi-Protocol Label Switching (MPLS) <NO>.
The differences between our work and these related works are summarized as follows: 1) The commercial software tools <NO> and most of the recent works on VM management <NO>, <NO>, <NO> did not consider bandwidth demands between VMs. 2) as mentioned above, our mapping problem
is different from the NV problems <NO>, <NO>, <NO>, <NO> since multiple VMs in an SVI are allowed to be placed on a common server. 3) Survivability has not been addressed in NV works <NO>, <NO>, <NO> or other closely related works <NO>, <NO>, <NO>, <NO>. 4) Unlike heuristic algorithms presented in <NO> which cannot provide any performance guarantees, our algorithms can ensure sufficient link bandwidth for failover traffic after any single server failure."	https://doi.org/10.1109/CLOUD.2012.100	0	['algorithm', 'consolidation', 'optimization']	['relocation_costs', 'v_man', 'consolidation_algorithm', 'energy_conservation', 'first_fit']	['total_power_consumption', 'ant_colony_optimization', 'definition_cloud_computing', 'multidimensional_bin_packing', 'placement_task_assignment']	Xu, et al. [NO] proposes to uses a service-aware approach in which multiple correlated vms and their backups are grouped together to form a survivable virtual infrastructure (svi) for a service or a tenant. 
Analysis of virtualization technologies for high performance computing environments	[]	Andrew J. Younge, Robert Henschel, James T. Brown, Gregor von Laszewski, Judy Qiu, and Geoffrey C. Fox. 2011. Analysis of virtualization technologies for high performance computing environments. IEEE International Conference on Cloud Computing (CLOUD). 9–16. DOI:http://dx.doi.org/10.1109/ CLOUD.2011.29	As Cloud computing emerges as a dominant paradigm in distributed systems, it is important to fully understand the underlying technologies that make Clouds possible. One technology, and perhaps the most important, is virtualization. Recently virtualization, through the use of hypervisors, has become widely used and well understood by many. However, there are a large spread of different hypervisors, each with their own advantages and disadvantages. This paper provides an in-depth analysis of some of today’s commonly accepted virtualization technologies from feature comparison to performance analysis, focusing on the applicability to High Performance Computing environments using FutureGrid resources. The results indicate virtualization sometimes introduces slight performance impacts depending on the hypervisor type, however the benefits of such technologies are profound and not all virtualization technologies are equal. From our experience, the KVM hypervisor is the optimal choice for supporting HPC applications within a Cloud infrastructure.	"While the use of virtualization technologies has increased dramatically in the past few years, virtualization is not specific
978-0-7695-4460-1/11 $26.00 © 2011 IEEE DOI 10.1109/CLOUD.2011.29
9
to the recent advent of Cloud computing. IBM originally pioneered the concept of virtualization in the 1960’s with the M44/44X systems <NO>. It has only recently been reintroduced for general use on x86 platforms. Today there are a number of public Clouds that offer IaaS through the use of virtualization technologies. The Amazon Elastic Compute Cloud (EC2) <NO> is probably the most popular Cloud and is used extensively in the IT industry to this day. Nimbus <NO>, <NO> and Eucalyptus <NO> are popular private IaaS platforms in both the scientific and industrial communities. Nimbus, originating from the concept of deploying virtual workspaces on top of existing Grid infrastructure using Globus, has pioneered scientific Clouds since its inception. Eucalyptus has historically focused on providing an exact EC2 environment as a private cloud to enable users to build an EC2-like cloud using their own internal resources. Other scientific Cloud specific projects exist such as OpenNebula <NO>, In-VIGO <NO>, and Cluster-on-Demand <NO>, all of which leverage one or more hypervisors to provide computing infrastructure on demand. In recent history, OpenStack <NO> has also come to light from a joint collaboration between NASA and Rackspace which also provide compute and storage resources in the form of a Cloud.
While there are currently a number of virtualization technologies available today, the virtualization technique of choice for most open platforms over the past 5 years has typically been the Xen hypervisor <NO>. However more recently VMWare ESX <NO> 1, Oracle VirtualBox <NO> and the Kernelbased Virtual Machine (KVM) <NO> are becoming more commonplace. As these look to be the most popular and featurerich of al virtualization technologies, we look to evaluate
1Due to the restrictions in VMWare’s licensing agreement, benchmark results are unavailable.
all four to the fullest extent possible. There are however, numerious other virtualizaton technologies also available, including Microsoft’s Hyper-V <NO>, Parallels Virtuozzo <NO>, QEMU <NO>, OpenVZ <NO>, Oracle VM <NO>, and many others. However, these virtualization technologies have yet to seen widespread deployment within the HPC community, at least in their current form, so they have been placed outside the scope of this work.
In recent history there have actually been a number of comparisons related to virtualization technologies and Clouds. The first performance analysis of various hypervisors started with, unsurprisingly, the hypervisor vendors themselves.
VMWare published their performance analysis in <NO> as did the Xen developers in an their first paper <NO>. The Xen paper compares Xen, XenoLinux, and VMWare across a number of SPEC and normalized benchmarks, resulting in a conflict between the two works. From here, a number of more unbiased reports originated, concentrating on server consolidation and web application performance <NO>, <NO>, <NO> with fruitful yet sometimes incompatible results. A feature base survey on virtualization technologies <NO> also illustrates the wide variety of hypervisors that currently exist. Furthermore, there has been some investigation into the performance within HPC, specifically with InfiniBand performance of Xen <NO> and rather recently with a detailed look at the feasibility of the Amazon Elastic Compute cloud for HPC applications <NO>, however both works concentrate only on a single deployment rather than a true comparison of technologies.
As these underlying hypervisor and virtualization implementations have evolved rapidly in recent years along with virtualization support directly on standard x86 hardware, it is necessary to carefully and accurately evaluate the performance implications of each system. Hence, we conducted an investigation of several virtualization technologies, namely Xen, KVM, VirtualBox, and in part VMWare. Each hypervisor is compared alongside one another with bare-metal as a control and (with the exeption of VMWare) run through a number of High Performance benchmarking tools."	https://doi.org/10.1109/CLOUD.2011.29	2	['processor', 'center', 'virtualization']	['multi_core', 'cooling_power', 'core_cluster', 'energy_dissipation', 'energy_consumption']	['multi_core_cluster', 'floor_data_centers', 'raised_floor_data', 'data_centers_operate', 'minimum_energy_consumption']	J. et al. [NO] provides an in-depth analysis of some of today ’ s commonly accepted virtualization technologies from feature comparison to performance analysis, focusing on the applicability to high performance computing environments using futuregrid resources. 
Cache index-aware memory allocation	[]	Y. Afek, D. Dice, and A. Morrison. 2011. Cache index-aware memory allocation. Proc. of the ISMM. ACM, 55–64.	Poor placement of data blocks in memory may negatively impact application performance because of an increase in the cache conflict miss rate [18]. For dynamically allocated structures this placement is typically determined by the memory allocator. Cache index-oblivious allocators may inadvertently place blocks on a restricted fraction of the available cache indexes, artificially and needlessly increasing the conflict miss rate. While some allocators are less vulnerable to this phenomena, no general-purpose malloc allocator is index-aware and methodologically addresses this concern. We demonstrate that many existing state-of-the-art allocators are index-oblivious, admitting performance pathologies for certain block sizes. We show that a simple adjustment within the allocator to control the spacing of blocks can provide better index coverage, which in turn reduces the superfluous conflict miss rate in various applications, improving performance with no observed negative consequences. The result is an index-aware allocator. Our technique is general and can easily be applied to most memory allocators and to various processor architectures. Furthermore, we can reduce inter-thread and inter-process conflict misses for processors where threads concurrently share the level-1 cache such as the Sun UltraSPARC-T2TMand Intel “Nehalem” by coloring the placement of blocks so that allocations for different threads and processes start on different cache indexes.	"The default SolarisTMlibc allocator uses a single global heap protected by one mutex. Memory is allocated from the operating system by means of the sbrk system call. The global free list is organized as a splay tree <NO> ordered by size and allocation requests are serviced via a best-fit policy. The heap is augmented by a small set of segregated free lists of bounded capacity, allowing many common requests to operate in constant-time. This results in an allocator with excellent heap density, reasonable single-threaded latency, but poor scalability. Furthermore, applications using the libc allocator may be subject to excessive allocator-induced false sharing, where blocks allocated to different threads happen to abut in the midst of a cache line.
Modern state-of-the-art allocators include Hoard <NO>, CLFMalloc <NO>, LFMalloc <NO>, libumem <NO>, jemalloc <NO> and tcmalloc <NO>. They are broadly categorized as segregated free-list <NO> allocators as they maintain distinct free lists based on block size. Such allocators round requested allocation sizes up to the nearest size-class where a size-class is simply an interval of block sizes and without ambiguity we can refer to a size-class by its upper bound. The set of size-classes forms a partition on the set of possible allocation sizes. The choice of size-classes is largely arbitrary and defined at the whim of the implementor, although a step size of 1.2x between adjacent size-classes is common <NO> as the worst-case internal fragmentation is constrained to 20%.
We will use Hoard as a representative example of modern allocator design. Hoard uses multiple heaps to reduce contention. Specifically, Hoard attempts to diffuse contention and improve scalability by satisfying potentially concurrent malloc requests from multiple local heaps – this strategy also mitigates the allocatedinduced false sharing problem. Each heap consists of an array of references to superblocks, with one slot for each possible size-class. A superblock is simply an array of blocks of a certain size class. Superblocks are all the same size, a multiple of the system page size, and are allocated from the system via the mmap interface which allocates virtual address pages and associates physical pages to those addresses. Mmap is used instead of the more traditional sbrk operator as pages allocated through mmap may later be returned to the system, if desired, through munmap. The superblock is the fundamental unit of allocator for Hoard. Each superblock has a local singly-linked free list threaded through the free blocks and maintained in LIFO order to promote TLB and data cache locality. A small superblock header at the base of the array contains the head of the superblock-local free list. Superblocks and heaps are opaque to the application that uses the allocator. The Hoard implementation
places superblocks on highly aligned addresses. The free operator then uses address arithmetic – simple masking – on the block address to locate the header of the enclosing superblock, which in turn allows the operator to quickly push the block onto the superblock’s free list. As such, in-use blocks do not require a header field. If a superblock becomes depleted it can be detached from a heap and moved to a global heap. The local heap can be reprovisioned from either the global heap, assuming a superblock with sufficient free space is available, or by allocating a new superblock from the system. Superblocks can circulate between various local heaps and the global heap, but will be associated with at most one local heap at any one time. Allocator metadata is minimal, consisting of the heap structures and superblock headers. The implementation associates a malloc request with a heap by hashing the identity of the current thread. To reduce collisions Hoard overprovisions the number of heaps to be twice the number of processors. Concurrency control is provided by per-heap locks. Hoard’s malloc operator first quantizes the requested size to an appropriate size-class, identifies a heap, locks the heap, locates a superblock of the appropriate size-class in that heap, unlinks a block from that superblock’s free list, unlocks the heap, and finally returns the address of the block’s data area. As Hoard employs segregated free lists (segregated by size), in the common case finding a free block of a given size is a simple constant-time operation. Given this allocation policy the returned addresses for a given size-class may be regular in a manner that results in interblock cache index conflicts and excessive conflict misses if a group of blocks of a size-class are accessed frequently by the application. More generally, array-based superblock allocators coupled with inopportune index-oblivious block sizes can easily result in patterns of block addresses that map to only a few cache indices.
Superblock-based allocators of this design allow for good scaling although their footprint is often somewhat larger than that of libc as they attempt to diffuse contention by distributing requests over multiple heaps. Latency varies but usually reflects path length through malloc and free and metadata access costs, which are properties of the implementation and not fundamental to the category of segregated free list allocators. CLFMalloc is structurally similar to Hoard, differing mostly in the policy by which it associates malloc requests with heap instances and in that CLFMalloc is lock-free. Libumem and tcmalloc use a central heap but diffuse contention via multiple local free lists. In the case of tcmalloc the central heap uses segregated free lists which are populated by allocating runs of pages and then splitting those pages into contiguous arrays of the desired size-class."	https://doi.org/10.1145/1993478.1993486	3	['locking', 'locked', 'core']	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Afek, et al. [NO] demonstrates that many existing state-of-the-art allocators are index-oblivious, admitting performance pathologies for certain block sizes. 
Performance evaluation of cache replacement policies for the SPEC CPU2000 benchmark suite	[]	Hussein Al-Zoubi, Aleksandar Milenkovic, and Milena Milenkovic. 2004. Performance evaluation of cache replacement policies for the SPEC CPU2000 benchmark suite. Proc. of the 42nd ACM-SE. ACM, New York, NY, 267–272.	Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set. In our quest for replacement policy as close to optimal as possible, we thoroughly explored the design space of existing replacement mechanisms using SimpleScalar toolset and SPEC CPU2000 benchmark suite, across wide range of cache sizes and organizations. In order to better understand the behavior of different policies, we introduced new measures, such as cumulative distribution of cache hits in the LRU stack. We also dynamically monitored the number of cache misses, per each 100000 instructions. Our results show that the PLRU techniques can approximate and even outperform LRU with much lower complexity, for a wide range of cache organizations. However, a relatively large gap between LRU and optimal replacement policy, of up to 50%, indicates that new research aimed to close the gap is necessary. The cumulative distribution of cache hits in the LRU stack indicates a very good potential for way prediction using LRU information, since the percentage of hits to the bottom of the LRU stack is relatively high.	"The LRU mechanism uses a program’s memory access patterns to guess that the cache line which has been accessed most recently will, most likely, be accessed again in the near future, and the cache line that has been “least recently used” should be replaced by the cache controller. An example of how the LRU stack is maintained is shown in Figure 1. Although the LRU replacement heuristic is relatively efficient, it does require a number of bits to track when each block is accessed, and relatively a complex logic. Another problem with the LRU heuristic is that each time the cache hit or miss occurs the block comparison and LRU stack shift operations require time and power.
To reduce the cost and complexity of the LRU heuristic, Random policy can be used, but potentially at the expense of performance. Random replacement policy chooses its victim randomly from all the cache lines in the set. An obvious way to implement it is with a simple Linear Feedback Shift Register (LFSR). Round Robin (or FIFO) replacement heuristic simply replaces the cache lines in a sequential order, replacing the oldest block in the set. Each cache memory set is accompanied with a circular counter which points to the next cache block to be replaced; the counter is updated on every cache miss.
PLRU schemes employ approximations of the LRU mechanism to speed up operations and reduce the complexity of implementation <NO>, <NO>. Due to the approximations, the least recently accessed cache memory is not always the location to be replaced. Here, we will discuss two implementations, a treebased and a MRU-based. In the tree-based PLRU replacement heuristic nway-1 bits are used to track the accesses to the cache blocks, where nway represents the number of cache blocks (ways) in a set. Figure 2 illustrates tree-based PLRU (PLRUt) using a 4-way cache memory as an example. The track bits B0, B1, B2 form a decision binary tree. The track bit B1 indicates whether two lower cache blocks CL0 and CL1 (B1=1), or 2 higher cache blocks CL2 and CL3 (B1 = 0) have been recently used. The track bit B0 determines further which one of two blocks CL0 (B0=1) or CL1 (B0=0) has been recently used; bit B2 keeps the access track between cache lines CL2 and CL3. On a cache miss, bit B1 determines where to look for the least recently block (2 lower cache lines or 2 higher cache lines). Bit
B0 or B2 determines the least recently used block. On a cache hit, the tree bits are set according to this policy.
The other implementation of the PLRU heuristic is based on using the most recently used (MRU) bits (PLRUm). In this case each cache block is assigned an MRU bit, stored in the tag table. The MRU bit for each cache block is set to a “1” each time a cache hit occurs on the cache block, indicating that the cache block has recently been used. When the cache controller is forced to replace a cache block, it examines the MRU bit for each cache block looking for a “0”. When it finds a “0”, the cache controller replaces that cache block and then sets the MRU bit to a “1”. A problem could occur if the MRU bits for all cache memory blocks are set to a “1”. If this happens, all the blocks are unavailable for replacement causing a deadlock. To prevent this type of deadlock, all the MRU bits in the set are cleared except the MRU bit being accessed when a potential overflow situation is detected. An example in Figure 3 illustrates the MRU-based PLRU.
Table 1 gives storage requirements and corresponding actions taken on a cache hit and a cache miss for all replacement
policies discussed. Random policy guarantees the minimal hardware cost, while the LRU hardware cost increases dramatically for caches with associativity larger than 8. In 2-way cache organizations PLRUt policy requires only one track bit, and shows the same performance as other LRU-based polices, hence it is an obvious choice for 2-way caches. For caches with higher associativity, PLRUt and PLRUm have roughly the same complexity. If we take the number of transitions in track bits as a qualitative measure of power consumption and assume the same number of misses, it is clear that PLRUm shows slightly better characteristics than PLRUt, and both are much better than LRU."	https://doi.org/10.1145/986537.986601	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Al-Zoubi, et al. [NO] thoroughly explores the design space of existing replacement mechanisms using simplescalar toolset and spec cpu2000 benchmark suite, across wide range of cache sizes and organizations. 
Efficient shared cache management through sharing-aware replacement and streaming-aware insertion policy	[]	Yu Chen, Wenlong Li, Changkyu Kim, and Zhizhong Tang. 2009. Efficient shared cache management through sharing-aware replacement and streaming-aware insertion policy. Proc. of the 24th IEEE IPDPS’09. 1–11.	Multi-core processors with shared caches are now commonplace. However, prior works on shared cache management primarily focused on multi-programmed workloads. These schemes consider how to partition the cache space given that simultaneously-running applications may have different cache behaviors. In this paper, we examine policies for managing shared caches for running single multi-threaded applications. First, we show that the shared-cache miss rate can be significantly reduced by reserving a certain amount of space for shared data. Therefore, we modify the replacement policy to dynamically partition each set between shared and private data. Second, we modify the insertion policy to prevent streaming data (data not reused before eviction) from promoting to the MRU position. Finally, we use a low-overhead sampling mechanism to dynamically select the optimal policy. Compared to LRU policy, our scheme reduces the miss rate on average by 8.7% on 8MB caches and 20.1% on 16MB caches respectively.	In this section, we show that the shared cache blocks in multi-threaded applications can affect the cache behavior significantly, thus rendering the new opportunity to improve cache performance over prior shareoblivious cache management policies.	https://doi.org/10.1109/IPDPS.2009.5161016	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Chen, et al. [NO] examines policies for managing shared caches for running single multi-threaded applications. 
Fully associative cache partitioning with don’t care bits for real-time applications	[]	Ali Chousein, and Rabi N. Mahapatra. 2005. Fully associative cache partitioning with don’t care bits for real-time applications. SIGBED Review 2, 2 (April 2005), 35–38.	The usage of cache memories in time-critical applications has been limited as caches introduce unpredictable execution behavior. Cache partitioning techniques have been developed to reduce the impact of unpredictability owing to context switch effects. However, partitioning reduces the cache size available for each task resulting in capacity related cache misses. This paper introduces a fully associative cache architecture for multi-tasking applications where effective partition sizes are increased by tag compression in the cache. The proposed scheme uses a few don’t care cells in its least significant bits of the tag to aggregate multiple tag entries into a single entry. The experimental results indicate that the proposed scheme is context switch resilient when eight different real-time benchmarks use the cache concurrently. Further, this cache architecture requires less time and less energy to perform tag table search compared to contemporary fully associative caches of the same size.	"The proposed fully associative cache architecture employs TCAM cells in the last significant L bits of the tag entries to compact tag table. Each TCAM cell can store don’t care state (x) in addition to the regular 0 and 1 binary states. This don’t care state is used as a wild bit to aggregate multiple entries in the tag table to single entry to achieve tag table compaction. This scheme is different from traditional tag compression schemes <NO>. As an example, the compaction can be effective up to eight times in reducing the tag table size with three don’t care bits in the tag when program locality behaves favorably. Due to tag table compaction, it is possible to build large sized caches with fewer number of tag entries. This in turn provides larger working cache area for each task when partitioning takes place and improves the miss ratio performance.
The overhead to maintain the above compaction comes with a price of additional hardware but no additional time overhead. The aggregation process that is responsible for tag compaction is neither on the critical path of cache access nor needs extensive hardware. The details of the aggregation module are not discussed in this paper. The decoding of the compacted tag entry is done concurrently with the tag table search and does not affect the critical path of the cache access. The decoder is implemented using offthe-shelf de-multiplexers."	https://doi.org/10.1145/1121788.1121799	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Chousein, et al. [NO] introduces a fully associative cache architecture for multi-tasking applications where effective partition sizes are increased by tag compression in the cache. 
A survey of hard real-time scheduling for multiprocessor systems	[]	Robert I. Davis, and Alan Burns. 2011. A survey of hard real-time scheduling for multiprocessor systems. Computing Surveys 43, 4, Article 35 (Oct. 2011), 44 pages.	This survey covers hard real-time scheduling algorithms and schedulability analysis techniques for homogeneous multiprocessor systems. It reviews the key results in this field from its origins in the late 1960s to the latest research published in late 2009. The survey outlines fundamental results about multiprocessor real-time scheduling that hold independent of the scheduling algorithms employed. It provides a taxonomy of the different scheduling methods, and considers the various performance metrics that can be used for comparison purposes. A detailed review is provided covering partitioned, global, and hybrid scheduling algorithms, approaches to resource sharing, and the latest results from empirical investigations. The survey identifies open issues, key research challenges, and likely productive research directions.	"Systems are referred to as real-time when their correct behavior depends not only on the operations they perform being logically correct, but also on the time at which they are performed. For example in avionics, flight control software must execute within a fixed time interval in order to accurately control the aircraft. In automotive electronics there are tight time constraints on engine management and transmission control systems that derive from the mechanical systems that they control.
Guaranteeing real-time performance while making the most effective use of the available processing capacity requires the use of efficient scheduling policies or algorithms supported by accurate schedulability analysis techniques. These analysis techniques need to be capable of analyzing the worst-case behavior of the application under a given scheduling policy, thus providing proof, subject to a set of assumptions about application behavior, that timing constraints will always be met during operation of the system.
Research into uniprocessor real-time scheduling can trace its origins back to the late 1960s and early 1970s with significant research effort and advances made in the 1980s and 1990s. The interested reader is referred to Audsley et al. <NO> and Sha et al. <NO>, which provide historical accounts of the most important advances in the field of uniprocessor scheduling during those decades. Today, although there is still significant scope for further research, uniprocessor real-time scheduling theory can be viewed as reasonably mature, with a large number of key results documented in textbooks such as those by Burns and Wellings <NO>, Buttazzo <NO>, and Liu <NO>, and successfully transferred into industrial practice.
Multiprocessor real-time scheduling theory also has it origins in the late 1960s and early 1970s. Liu <NO> noted that multiprocessor real-time scheduling is intrinsically a much more difficult problem than uniprocessor scheduling:
“Few of the results obtained for a single processor generalize directly to the multiple processor case; bringing in additional processors adds a new dimension to the scheduling problem. The simple fact that a task can use only one processor even when several processors are free at the same time adds a surprising amount of difficulty to the scheduling of multiple processors.”
The seminal paper of Dhall and Liu <NO> heavily influenced the course of research in this area for two decades. During the 1980s and 1990s, conventional wisdom was that global approaches to multiprocessor scheduling, where tasks may migrate from one processor to another, suffered from the so-called “Dhall effect,” and were therefore inferior to partitioned approaches, with a fixed allocation of tasks to processors. Research efforts therefore focused almost exclusively on partitioned approaches. It was not until Phillips et al. <NO> showed that the Dhall effect was more of a problem with high-utilization tasks than it was with global scheduling algorithms that there was renewed interest in global scheduling algorithms.
In the late 1990s silicon vendors such as IBM and AMD began research into the development of multicore processors, with IBM releasing the first nonembedded dual-core processor, the POWER4, in 2001. This trend away from increasing processing capacity via ever higher clock speeds toward increasing performance via multiple processor cores became evident to the real-time systems research community. This resulted in significant research effort being focused on the problem of real-time multiprocessor scheduling. While markedly more articles have been published in this area since 2000 than before, and significant progress has been made, there are still many open questions and research challenges that remain.
This article presents a survey of multiprocessor real-time scheduling algorithms and schedulability analysis techniques, from the origins of the field in the late 1960s up to the latest research published at the end of 2009. The aim of the survey is to provide a classification of existing research, both providing a perspective on the area and identifying significant open issues and future research directions."	https://doi.org/10.1145/1978802.1978814	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	I. et al. [NO] covers hard real-time scheduling algorithms and schedulability analysis techniques for homogeneous multiprocessor systems. 
Analysis of probabilistic cache related pre-emption delays	['For reasons of space and clarity, here we only present results for the first two experiments for the simple FAC benchmark, results for the other experiments and for the BS, FDCT, FIBCALL, FIR, JFDCTINT, and INSERTSORT benchmarks can be found in the technical report <NO>.', 'Detailed results can be found in the technical report <NO>.', 'The appendix of the technical report <NO> contains the results of the four experiments for the BS, FDCT, FIBCALL, FIR, JFDCTINT, and INSERTSORT benchmarks.', 'Meanwhile, Probabilistic Timing Analysis (PTA) techniques <NO> rely on caches with a time-randomised behaviour in which hit and miss events have an associated probability for every cache access.', 'Probabilistic Timing Analysis (PTA) <NO> provides WCET estimates with an associated exceedance probability, called probabilistic WCET (pWCET) estimates.']	R.I. Davis, L. Santinelli, S. Altmeyer, C. Maiza, and L. Cucu-Grosjean. 2013. Analysis of probabilistic cache related pre-emption delays. Proc. of the 25th ECRTS. IEEE, 168–179.	This paper integrates analysis of probabilistic cache related pre-emption delays (pCRPD) and static probabilistic timing analysis (SPTA) for multipath programs running on a hardware platform that uses an evict-on-miss random cache replacement policy. The SPTA computes an upper bound on the probabilistic worst-case execution time (pWCET) of the program, which is an exceedance function giving the probability that the execution time of the program will exceed any given value on any particular run. The pCRPD analysis determines the maximum effect of a pre-emption on the pWCET. The integration between SPTA and pCRPD updates the pWCET to account for the effects of one or more pre-emptions at any arbitrary points in the program. This integration is a necessary step enabling effective schedulability analysis for probabilistic hard real-time systems that use pre-emptive or co-operative scheduling. The analysis is illustrated via a number of benchmark programs.	"Temporal analysis of probabilistic real-time systems where at least one parameter, e.g. execution time, is described by a random variable, was first investigated by Lehoczky in 1990 <NO> who extended queuing theory under real-time hypotheses. This work was improved upon in 2002 by Zhu et al. <NO>; however, the main limitation remained the use of the same probability law for the execution times of all tasks, which is not always realistic. Gardner et al. in 1999 <NO> and Tia et al. in 1995 <NO> also considered execution times as random variables with special assumptions made about the critical instant. Schedulability analysis for real-time systems with probabilistic execution times was given by Diaz et al. in 2002 <NO> and refined by Lopez et al. in 2008 <NO>; however, the analysis was difficult to use in practice for computational reasons. Improvements based on re-sampling of random variables were proposed by Maxim et al. in 2012 <NO>.
In 2009, Quinones et al. <NO> investigated the use of random cache replacement policies as a means of obtaining real-time performance less dependent on execution history. In 2012, Cucu-Grosjean et al. <NO> and Cazorla et al. <NO> introduced SPTA for single-path programs, assuming an evict-on-access random cache replacement policy.
For deterministic systems, the integration of cache related pre-emption delays into schedulability analysis for fixed priority pre-emptive scheduling has been considered by (i) analysing the effect of the pre-empting task (Busquets-Mataix et al. in 1996 <NO>), (ii) analysing the effect on the pre-empted
1In this paper, we use ’program’ and ’task’ interchangeably.
1068-3070/13 $26.00 © 2013 IEEE DOI 10.1109/ECRTS.2013.27
168
task (Lee et al. in 1998 <NO>), or (iii) a combination of both (Altmeyer et al. in 2011 <NO> and 2012 <NO>).
In this paper, we build on the idea of using random cache replacement policies in hard real-time systems proposed in <NO>, and the SPTA for the evict-on-access random cache replacement policy introduced in <NO> and <NO>; however, we assume an evict-on-miss policy because its performance dominates that of evict-on-access in terms of the pWCET distributions (exceedance functions) obtained. We extend previous work on SPTA for single path programs given in <NO> and <NO>, both integrating analysis of pCRPD, and introducing for the first time a method of analysing multipath programs.
Section II presents our system model, terminology and notation. In Section III we provide SPTA for single-path programs assuming an evict-on-miss random cache replacement policy. In Section IV we derive analysis of pCRPD, based on the effect on the pre-empted program. In section V we extend our SPTA and pCRPD analysis to multi-path programs. Section VI applies our analysis to a number of benchmarks, while Section VII concludes with a summary and discussion of future work."	https://doi.org/10.1109/ECRTS.2013.27	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Davis, et al. [NO] integrates analysis of probabilistic cache related pre-emption delays (pcrpd) and static probabilistic timing analysis (spta) for multipath programs running on a hardware platform that uses an evict-on-miss random cache replacement policy. 
WCET-directed dynamic scratchpad memory allocation of data	['Scratchpads <NO> can be used to store data as well as instructions.', 'This subset may be static <NO> or dynamically updated during execution <NO>, but the forms of data and programs that can be supported are restricted.', 'scratchpad allocation algorithms only support statically allocated or stack data <NO>, while non time-predictable algorithms can only support dynamic data if whole-program pointer analysis can identify every memory operation that could access each variable <NO>.', 'Thirdly, whole-program pointer analysis is not required: previous work required such analysis <NO> or eliminated pointers entirely <NO>.', '<NO> or Deverge and Puaut <NO> Latency/ CPU Bus System CPU clock freq/ freq/ cycles MHz MHz', 'While straightforward, this approach will not be taken by time-predictable scratchpad allocation algorithms as proposed by Suhendra <NO> or Deverge <NO> because the objects are dynamically allocated.', 'Sometimes, this can be determined by pointer analysis at compile time, and this is part of the purpose of the restrictions applied by <NO>.', 'Dynamic allocation algorithms <NO> make better use of scratchpad space because the set of objects stored in scratchpad can change during execution, matching the requirements of the current function.', 'The restrictions applied by <NO> make it easy to determine the size of each object used by the program offline, because there is a 1-1 relationship between the variables in the source code and the objects they represent.', 'Scratchpad allocation algorithms demonstrate that a subset of the data used by a program can be allocated to the scratchpad in order to minimize the estimated WCET <NO>.', 'ti can be specified by the programmer, but generating this value is really a task for a scratchpad allocation algorithm: existing algorithms <NO> already specify methods for doing this.', 'For example, the algorithm specified by Deverge <NO> may be extended by:', 'This idea has been successfully applied within tasks for SPM <NO>, <NO>.', '<NO>, <NO>) could have done some of the required work.']	J.-F. Deverge, and I. Puaut. 2007. WCET-directed dynamic scratchpad memory allocation of data. Proc. of the 19th Euromicro Conference on Real-Time Systems (ECRTS’07). 179–190.	Many embedded systems feature processors coupled with a small and fast scratchpad memory. To the difference with caches, allocation of data to scratchpad memory must be handled by software. The major gain is to enhance the predictability of memory accesses latencies. A compile-time dynamic allocation approach enables eviction and placement of data to the scratchpad memory at runtime. Previous dynamic scratchpad memory allocation approaches aimed to reduce average-case program execution time or the energy consumption due to memory accesses. For real-time systems, worst-case execution time is the main metric to optimize. In this paper, we propose a WCET-directed algorithm to dynamically allocate static data and stack data of a program to scratchpad memory. The granularity of placement of memory transfers (e.g. on function, basic block boundaries) is discussed from the perspective of its computation complexity and the quality of allocation.	"On many programs, a large amount of data accesses are dynamic; the target address of load-store instructions may change for each execution. Table 1 gives a twodimensional classification of data storage and load-store accesses from <NO>. The storage type defines the location of a given data. Static data, stack data and heap data are respectively stored in global, heap and stack sections of the program memory space layout. Literals are compiler-generated constants stored in the code section; these data are used to reduce the size of the program code.
19th Euromicro Conference on Real-Time Systems (ECRTS'07) 0-7695-2914-3/07 $20.00 © 2007
The access type defines the way a data is accessed. Scalar access types are accesses to a unique data address for statics and to a relative address to stack frame base addresses for stack data. The access type of a load-store instruction is regular if this instruction is accessing to multiple elements of a unique array with a constant stride (classified as linear address sequence accesses in <NO>). Irregular accesses include accesses to (possibly multiple) data through pointers and are still independent to the input data. Lastly, input dependent accesses include any accesses with addresses computed at runtime from unknown input data (mentioned as indirect address sequence accesses in <NO>).
In the next section, we will motivate the need for a method to analyze any data memory accesses of programs."	https://doi.org/10.1109/ECRTS.2007.37	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Deverge, et al. [NO] proposes a wcet-directed algorithm to dynamically allocate static data and stack data of a program to scratchpad memory. 
WCET-Centric partial instruction cache locking	[]	Huping Ding, Yun Liang, and T. Mitra. 2012. WCET-Centric partial instruction cache locking. Proc. of the 49th ACM/IEEE DAC. 412–420.	Caches play an important role in embedded systems by bridging the performance gap between high speed processors and slow memory. At the same time, caches introduce imprecision in Worst-case Execution Time (WCET) estimation due to unpredictable access latencies. Modern embedded processors often include cache locking mechanism for better timing predictability. As the cache contents are statically known, memory access latencies are predictable, leading to precise WCET estimate. Moreover, by carefully selecting the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling without locking. Existing static instruction cache locking techniques strive to lock the entire cache to minimize the WCET. We observe that such aggressive locking mechanisms may have negative impact on the overall WCET as some memory blocks with predictable access behavior get excluded from the cache. We introduce a partial cache locking mechanism that has the flexibility to lock only a fraction of the cache. We judiciously select the memory blocks for locking through accurate cache modeling that determines the impact of the decision on the program WCET. Our synergistic cache modeling and locking mechanism achieves substantial reduction in WCET for a large number of embedded benchmark applications.	"Cache locking is used to improve timing predictability in realtime systems. Puaut and Decotigny <NO> explore static cache locking in multitasking real-time systems. Two content selection algorithms have been proposed in their work to minimize the utilization and inter-task interferences. Campoy et al. <NO> employ genetic al-
gorithm to perform instruction cache locking. However, both <NO> and <NO> do not model the change in worst-case path after locking.
Falk et al. <NO> perform cache locking by taking into account the change of worst-case path and achieve better WCET reduction. Their greedy algorithm computes the worst-case path and selects the procedure with maximum WCET reduction for locking. This process continues until the cache is fully locked. Liu et al. <NO> present an optimal solution to minimize WCET via cache locking. However, their approach is optimal on the premise that the cache is fully locked. It may not be optimal towards minimizing WCET as shown in our motivating example. More importantly, they do not consider the cache mapping function at all in the locking algorithm. They simply assume that any memory block can be locked in any cache set (as if the cache is a scratchpad memory). After locking decisions are taken, they have to use code placement/layout technique <NO> that force the locked memory blocks to be mapped to the appropriate cache sets. This can lead to serious code size blowup, which has not been addressed.
Vera et al. <NO> combine compile-time cache analysis and data cache locking in order to estimate a safe and tight worst-case memory performance. This work also assume full cache locking. Arnaud and Puaut <NO> propose dynamic instruction cache locking for hard real-time systems. In their approach, the program is partitioned into regions, and static cache locking is performed for each region. In <NO>, cache locking is explored for predictable shared caches on multi-core systems. Cache locking is also shown to be quite effective for improving average-case execution time <NO>. Finally, optimal on-chip scratchpad memory allocation to improve the WCET has been explored in <NO>."	https://doi.org/10.1145/2228360.2228434	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Ding, et al. [NO] observes that such aggressive locking mechanisms may have negative impact on the overall wcet as some memory blocks with predictable access behavior get excluded from the cache. 
Coordinated control of multiple prefetchers in multi-core systems	[]	Eiman Ebrahimi, Onur Mutlu, Chang Joo Lee, and Yale N. Patt. 2009. Coordinated control of multiple prefetchers in multi-core systems. Proc. of the 42Nd MICRO. ACM, 316–326.	Aggressive prefetching is very beneficial for memory latency tolerance of many applications. However, it faces significant challenges in multi-core systems. Prefetchers of different cores on a chip multiprocessor (CMP) can cause significant interference with prefetch and demand accesses of other cores. Because existing prefetcher throttling techniques do not address this prefetcher-caused inter-core interference, aggressive prefetching in multi-core systems can lead to significant performance degradation and wasted bandwidth consumption. To make prefetching effective in CMPs, this paper proposes a low-cost mechanism to control prefetcher-caused inter-core interference by dynamically adjusting the aggressiveness of multiple cores’ prefetchers in a coordinated fashion. Our solution consists of a hierarchy of prefetcher aggressiveness control structures that combine per-core (local) and prefetcher-caused intercore (global) interference feedback to maximize the benefits of prefetching on each core while optimizing overall system performance. These structures improve system performance by 23% while reducing bus traffic by 17% compared to employing aggressive prefetching and improve system performance by 14% compared to a state-of-the-art prefetcher aggressiveness control technique on an eight-core system.	We briefly describe relevant previous research on prefetcher aggressiveness control, since our proposal builds on this prior work. We also describe the shortcomings of these prefetcher control mechanisms and provide insight into the potential benefits of reducing prefetcher-caused inter-core interference using coordinated control of multiple prefetchers.	https://doi.org/10.1145/1669112.1669154	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Ebrahimi, et al. [NO] proposes a low-cost mechanism to control prefetcher-caused inter-core interference by dynamically adjusting the aggressiveness of multiple cores ’ prefetchers in a coordinated fashion. 
Compile-time decided instruction cache locking using worst-case execution paths	['The algorithms in <NO> and <NO> are compared with the polynomial time optimal algorithms proposed in this paper.', 'The heuristic algorithm proposed in <NO> performs close to our optimal solutions.', 'The algorithms in <NO> and <NO> are compared with the optimal algorithms proposed in this paper.', 'EFT is similar to the EFG proposed by <NO>.', 'This EFG is different from EFG in <NO>, where there are values associated with edges.', '<NO> obtains the same results as the optimal solutions calculated by our algorithms.', 'That is because the heuristic algorithm <NO> considers the cost saving and function size at the same time and it can achieve optimal solutions for some applications.', '<NO> take the changing of worse-case execution path into', 'The algorithms under comparison are the static I-Cache locking algorithm in <NO>, Algorithm SLEFGS and Algorithm DLEFGS.', '4 and 5 that for some multi-task sets, the static algorithm in <NO> obtains close results as the', 'That is because the heuristic algorithm in <NO> considers the cost saving and node size at the same time and it can achieve approximate solutions for some applications.', 'More importantly, by carefully choosing the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling techniques without locking <NO>.', 'Recently, a heuristic <NO> and an optimal solution <NO> have been proposed to minimize the WCET via static instruction cache locking.', 'By employing full cache locking, <NO> can completely bypass cache modeling in WCET analysis phase and thereby achieve tight WCET estimation.', 'In this paper, we argue (and experimentally validate) that aggressive full cache locking as proposed in <NO> may have substantial negative impact on WCET reduction.', 'Full cache locking: Existing cache locking techniques <NO> first build the worst case path (e.', 'Both cache locking techniques <NO> model the fact that the WCET path may change after locking some memory locks.', 'For this example, the heuristic <NO> and the optimal <NO> approach return the same solution.', 'From the example above, we first observe that full locking techniques <NO> are not guaranteed to perform better than cache modeling (with no locking) specially when some memory accesses can be easily classified as cache hits (m3, m4, m5 in our example).', '<NO> perform cache locking by taking into account the change of worst-case path and achieve better WCET reduction.', '3 Partial versus Full Cache Locking There exist two full cache locking techniques as mentioned in Section 2 <NO>.', '<NO> show that their approach can achieve better WCET reduction compared to <NO>, it has several limitations.', '’s method <NO> as both approaches do not require any subsequent code placement technique.', 'This choice of granularity does not change the greedy heuristic algorithm proposed in <NO>.', 'Instruction cache locking has been primarily employed in hard real time systems for better timing predictability <NO>.']	Heiko Falk, Sascha Plazar, and Henrik Theiling. 2007. Compile-time decided instruction cache locking using worst-case execution paths. Proc. of the 5th IEEE/ACM CODES+ISSS. ACM, 143–148.	Caches are notorious for their unpredictability. It is difficult or even impossible to predict if a memory access results in a definite cache hit or miss. This unpredictability is highly undesired for real-time systems. The Worst-Case Execution Time (WCET) of a software running on an embedded processor is one of the most important metrics during real-time system design. The WCET depends to a large extent on the total amount of time spent for memory accesses. In the presence of caches, WCET analysis must always assume a memory access to be a cache miss if it can not be guaranteed that it is a hit. Hence, WCETs for cached systems are imprecise due to the overestimation caused by the caches. Modern caches can be controlled by software. The software can load parts of its code or of its data into the cache and lock the cache afterwards. Cache locking prevents the cache’s contents from being flushed by deactivating the replacement. A locked cache is highly predictable and leads to very precise WCET estimates, because the uncertainty caused by the replacement strategy is eliminated completely. This paper presents techniques exploring the lockdown of instruction caches at compile-time to minimize WCETs. In contrast to the current state of the art in the area of cache locking, our techniques explicitly take the worst-case execution path into account during each step of the optimization procedure. This way, we can make sure that always those parts of the code are locked in the I-cache that lead to the highest WCET reduction. The results demonstrate that WCET reductions from 54% up to 73% can be achieved with an acceptable amount of CPU seconds required for the optimization and WCET analyses themselves.	"The papers <NO> present different algorithms for static I-cache lockdown. These publications are very close to the work presented in this paper. In <NO>, the authors present two heuristics for cache contents selection. The first one tries to minimize CPU utilization by locking an I-cache. The second algorithm minimizes interference between tasks. In <NO>, an additional genetic algorithm for cache locking is proposed. All three algorithms have in common that they do not consider changing WC-paths at all. Instead, the WC-path is determined only once before the optimization process takes place. After that, optimization is done along this single WC-path. The authors admit that their approach is nonoptimal. Due to the consideration of only one WC-path, we call such techniques “single-path analyses” in the following.
In <NO>, the techniques of <NO> for I-cache locking are extended to deal with changing WC-paths. However, the way how WC-paths are recomputed is not detailed. The authors use a parameter N trading off accuracy of WC-path recomputation with runtime consumption. Since runtimes for WC-path recomputation are still very high, the authors are unable to provide results for some of their benchmarks. In contrast, the techniques presented here scale much better so that we can present results for very large benchmarks.
The work of <NO> is complementary to this paper since it presents a D-cache locking algorithm for WCET minimization. Using an extended version of reuse vectors, a system of cache miss equations is set up describing where data reuse translates to locality. These equations can be solved statically and define a set of data to be locked in the D-cache. Data dependencies in the CFG which can not be analyzed statically are handled by a heuristic that locks data which is likely to be accessed. In this paper on D-cache locking, the WC-path is not considered – neither implicitly nor explicitly.
In terms of predictability, locked caches behave similar to software-controlled scratchpad memories. In the past, sev-
eral papers were published exploiting scratchpads for energy dissipation minimization. In <NO>, the influence of scratchpads on WCET prediction is studied. Even though WCET is targeted in that work, the selection algorithm deciding which objects to be placed in the scratchpad is not WCETaware. Instead, a selection algorithm for energy reduction is employed, and the effect of this energy reduction strategy on WCET is evaluated afterwards. Hence, that work is not a true WCET-aware optimization and does not consider WC-paths at all."	https://doi.org/10.1145/1289816.1289853	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Falk, et al. [NO] presents techniques exploring the lockdown of instruction caches at compile-time to minimize wcets. 
Using bypass to tighten WCET estimates for multi-core processors with shared instruction caches	['We also assume that loads to the lines that have not been locked in the L1 cache bypass the L1 cache (as in a previous research work <NO>).']	Damien Hardy, Thomas Piquet, and Isabelle Puaut. 2009. Using bypass to tighten WCET estimates for multi-core processors with shared instruction caches. Proc. of the 30th RTSS. IEEE, 68–77.	Multi-core chips have been increasingly adopted by the microprocessor industry. For real-time systems to exploit multicore architectures, it is required to obtain both tight and safe estimates of worst-case execution times (WCETs). Estimating WCETs for multi-core platforms is very challenging because of the possible interferences between cores due to shared hardware resources such as shared caches, memory bus, etc. This paper proposes a compile-time approach to reduce shared instruction cache interferences between cores to tighten WCET estimations. Unlike [28], which accounts for all possible conflicts caused by tasks running on the other cores when estimating the WCET of a task, our approach drastically reduces the amount of inter-core interferences. This is done by controlling the contents of the shared instruction cache(s), by caching only blocks statically known as reused. Experimental results demonstrate the practicality of our approach.	"A multi-core architecture is assumed. Each core has a private first-level (L1) instruction cache, followed by instruction cache levels with at least one shared cache. The caches are setassociative. Each level of the cache hierarchy is non-inclusive:
− A piece of information is searched for in the cache of level if and only if a cache miss occurred when searching it in the cache of level −1. Cache of level 1 is always accessed. − Except if the bypass mechanism presented in Section 4 is used, every time a cache miss occurs at cache level , the entire cache block containing the missing piece of information is always loaded into the cache of level . − There are no actions on the cache contents (i.e. invalidations, lookups/modifications) other than the ones mentioned above.
Our study concentrates on instruction caches; it is assumed that the shared caches do not contain data. This study can be seen as a first step towards a general solution for shared caches. It can also push the use of separate shared instruction and data caches instead of unified ones1.
Our method assumes a LRU (Least Recently Used) cache replacement policy. The access time variability to main memory and shared caches, due to bus contention, is supposed to
1Unified caches could be partitioned at boot time for instance in a A-way instruction cache and a B-way data cache.
be bounded and known, by using for instance Time Division Multiple Access (TDMA) like in <NO> or other predictable bus arbitration policies <NO>.
Figure 1 illustrates two different supported architectures.
Regarding scheduling, it is assumed that a job does not migrate between cores at run-time. Migrations are allowed between job instances only. No further assumption is made on task scheduling, implying that any part of an interfering task may be executed simultaneously with the analysed task and may thus pollute the shared cache(s). This assumption was made in a first approach to keep WCET estimation and schedulability independent activities, as traditionally done when temporally validating real-time software. We do not attempt to explore joint WCET estimation and scheduling, which is left for future work. Tasks are independent (i.e. do not synchronize with each other), but might share code, such as libraries (see paragraph 3.2).
The focus in this paper is to estimate the WCET of a hardreal time task running on a core, in isolation from the tasks running on the same core, but suffering indirect interferences because of cache sharing from tasks running on the other cores. The computation of cache-related preemption delay due to intra-core interferences is considered out of the scope of this paper."	https://doi.org/10.1109/RTSS.2009.34	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Hardy, et al. [NO] proposes a compile-time approach to reduce shared instruction cache interferences between cores to tighten wcet estimations. 
CAMA: A predictable cache-aware memory allocator	['Similar techniques can be applied in software <NO> or in hardware <NO> with different granularities.']	J. Herter, P. Backes, F. Haupenthal, and J. Reineke. 2011. CAMA: A predictable cache-aware memory allocator. Proc. of the 2011 ECRTS. 23–32.	General-purpose dynamic memory allocation algorithms strive for small memory fragmentation and good average-case response times. Hard real-time settings, in contrast, place different demands on dynamic memory allocators: worst-case response times are more important than averagecase response times. Furthermore, predictable cache behavior is a prerequisite for timing analysis to derive tight bounds on a program’s execution time. This paper proposes a novel algorithm that meets these demands. It guarantees constant response times, does not cause unpredictable cache pollution, and allocations are cache-set directed, i.e., allocated memory is guaranteed to be mapped to a given cache set. The latter two are necessary to enable a subsequent precise static cache analysis.	"The remainder of this paper is organized as follows. We elaborate on the problems that dynamic memory allocation imposes on the determination of WCET bounds in Section II and present related work in Section III. In Section IV, we
2011 23rd Euromicro Conference on Real-Time Systems
1068-3070/11 $26.00 © 2011 IEEE DOI 10.1109/ECRTS.2011.11
23
describe our cache-aware memory allocation algorithm and its implementation. For a set of real-time applications, we compare worst-case execution time bounds and memory consumption of CAMA to that of TLSF, and, in the case of memory consumption, to that of Doug Lea’s allocator in Section V."	https://doi.org/10.1109/ECRTS.2011.11	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Herter, et al. [NO] proposes a novel algorithm that meets these demands. 
CQoS: A framework for enabling QoS in shared caches of CMP platforms	['This is a key advantage of our technique over some other cache partitioning schemes <NO> since cache allocation is not limited in the absence of contention.', '<NO> proposed a scheme for QoS support in shared CMP caches.', 'As expected, recent studies <NO><NO><NO><NO><NO><NO> have indicated that contention for critical platform resources (e.', 'RELATED WORK Previous research on partitioning for cache/memory architectures can be found in <NO><NO><NO><NO><NO><NO><NO> The primary optimization goal for the techniques presented in most of these papers has been either fairness <NO><NO><NO> or improving overall throughput <NO>.', 'Although QoS has been studied for a long time in real-time environments <NO>, networking <NO> and multimedia <NO>, it has only been recently introduced to the CMP architecture<NO>.', 'Iyer <NO> described the need for a priority-based framework to introduce QoS in CMP caches.', 'Similar cache partitioning mechanisms have been presented before <NO><NO><NO><NO>.', 'Iyer <NO> in a recent publication presented a framework for enabling QoS in shared caches.', 'The work on heterogeneous cache regions by Iyer <NO> is based on the use of multiple caches each with different cache parameters and the optimal assignment of applications to appropriate caches with suitable cache parameters.', 'Similar techniques can be applied in software <NO> or in hardware <NO> with different granularities.', 'There are many prior studies <NO><NO><NO><NO><NO><NO> that investigate the optimal shared cache partitioning for multiprogrammed workloads.', '<NO> pursue cache fairness and quality of service as the main goal.', 'The majority of work required modifications to hardware and falls into one of two camps: performance aware cache modification (most commonly cache-partitioning) <NO> or performance-aware DRAM controller memory scheduling [Loh 2008; Suh et al.', 'The effect of simultaneously executing threads competing for space in the LLC has been explored extensively by many different researchers <NO>.', 'The fact that a thread’s instruction retirement rate depends on the other threads in the workload makes performance on CMPs unpredictable which can be an even bigger problem than reduced throughput <NO>.', 'To incorporate the decisions made by the partitioning algorithm, the baseline LRU policy is augmented to enable way partitioning <NO><NO><NO>.', 'Mechanisms to facilitate static and dynamic partitioning of cache is described in detail in <NO>.']	Ravi Iyer. 2004. CQoS: A framework for enabling QoS in shared caches of CMP platforms. Proc. of the 18th ICS. ACM, 257–266.	Cache hierarchies have been traditionally designed for usage by a single application, thread or core. As multi-threaded (MT) and multi-core (CMP) platform architectures emerge and their workloads range from single-threaded and multithreaded applications to complex virtual machines (VMs), a shared cache resource will be consumed by these different entities generating heterogeneous memory access streams exhibiting different locality properties and varying memory sensitivity. As a result, conventional cache management approaches that treat all memory accesses equally are bound to result in inefficient space utilization and poor performance even for applications with good locality properties. To address this problem, this paper presents a new cache management framework (CQoS) that (1) recognizes the heterogeneity in memory access streams, (2) introduces the notion of QoS to handle the varying degrees of locality and latency sensitivity and (3) assigns and enforces priorities to streams based on latency sensitivity, locality degree and application performance needs. To achieve this, we propose CQoS options for priority classification, priority assignment and priority enforcement. We briefly describe CQoS priority classification and assignment options -ranging from user-driven and developer-driven to compiler-detected and flow-based approaches. Our focus in this paper is on CQoS mechanisms for priority enforcement -these include (1) selective cache allocation, (2) static/dynamic set partitioning and (3) heterogeneous cache regions. We discuss the architectural design and implementation complexity of these CQoS options. To evaluate the performance trade-offs for these options, we have modeled these CQoS options in a cache simulator and evaluated their performance in CMP platforms running network-intensive server workloads. Our simulation results show the effectiveness of our proposed options and make the case for CQoS in future multi-threaded/multi-core platforms since it improves shared cache efficiency and increases overall system performance as a result.	"The typical architecture of a processor shown in Figure 1. As shown in the figure typically consists of a number of compu L1 (and perhaps L2 caches). While the las be made up of private caches per proce shown that shared caching is more desira and design point of view. Our focus is on last-level cache as it is critical shared reso keep the compute cores busy executing an against the memory wall.
Conventional cache management relies o by only one memory access stream at a However, as processors, systems and a more complex, we need to consider the di access streams that allocate data into th consider the potential for different memo be broadly classified into the following ca
(a) Multi-Threaded Applications: The simplest case is where the last-level cache is being used by multiple threads of the same application. In this scenario, if the threads are sharing and communicating a lot of data between each other, the conventional management of shared caches will work reasonably well. However, if the threads perform entirely different types of transactions concurrently (for instance – HTTP transactions in a web server application), then the performance of the concurrent transactions is dependent not only on its own locality, but also the nature of memory accesses generated by the other transactions. In such scenarios, the transactions that are of higher importance (e.g. secure payment transactions) should be prioritized higher than those of lower importance (e.g. browsing transactions).
(b) Multiple Heterogeneous Applications: When multi-tasking multiple applications in a CMP platform, it is likely that threads of one application and another get scheduled on to the same microprocessor, thereby sharing the last-level cache. This computing model is particularly gaining relevance/importance as virtual machines <NO> start to proliferate in data centers as a mechanism to reduce server sprawl. In such scenarios, different applications will definitely exhibit different memory access properties and therefore should be handled differently in terms of cache space allocated. The notion of cache space prioritization between is important here for high efficiency.
(c) Specialized Cores in CMPs: As application and network processing tends to frequently execute some common communication layers (TCP/IP, SSL for instance) or computing components (data encryption, compression, CRC, XML parsing, etc), architects are considering replacing one or two of the CPU d Many Different Applications, VMs Threads, Cores, Specialized Engines, Devices, Assists, Data Prefetch/Forwar
P Processors
CHES in a CMP platform is
, a CMP microprocessor te cores with individual t level cache (LLC) can ssor, studies <NO> have ble from a performance the performance of this urce that is intended to d the last line of defense
n the cache being used ny given point in time. pplications become far fferent types of memory e shared LLC. Let us ry access streams as can tegories:
cores with specialized cores for such components. In such scenarios, the processing and memory access flows generated by the cores on the CMP will definitely have different properties and can be best handled with that knowledge.
(d) Sharing Caches between Cores and Devices: With the imminent potential of computing appearing in I/O devices (and possibly management controllers) in the system and the integration of I/O links into the CPU, researchers and architects are evaluating the benefits of using cache space to speed up the processing on the device. In such scenarios, it becomes important to perhaps partition the cache space dynamically between the cores and the devices.
(e) Memory Latency Helpers: In current processors, prefetching is employed to overlap computation with memory access. Excessive prefetching <NO> is known to cause problems such as cache pollution and deteriorate application performance. In addition to prefetching <NO> initiated by the CPU, researchers have considered memory-side prefetching, data forwarding <NO> and direct placement of network data into CPU cache <NO>. To reduce the amount of pollution caused by these memory latency helpers, it is important to prioritize cache space usage between demand activity and prefetching/forwarding activity.
Several other considerations need to be kept in mind when considering prioritizing cache space utilization. Other than the basic priority of the application, it also needs to be kept in mind that providing higher cache space to a higher priority application does not always guarantee higher cache performance. This depends heavily on the application’s memory access characteristics (i.e. locality properties). As a result, locality and
“user-defined” application priority need to be considered in unison to form the cache priority of the memory access stream. Other aspects to consider are the dynamic changes to the priority of an application or memory access stream. Since processing tends to go through several phases, it is important that the priority assignment mechanism be designed to allow dynamic changes. These issues motivated us to develop a framework that introduced the notion of quality of service in caches. In the next section, we will discuss the basic ideas behind this and present several potential design and implementation options to enable this in future systems."	https://doi.org/10.1145/1006209.1006246	2	['workload', 'priority', 'platform']	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Iyer. et al. [NO] presents a new cache management framework (cqos) that (1) recognizes the heterogeneity in memory access streams, (2) introduces the notion of qos to handle the varying degrees of locality and latency sensitivity and (3) assigns and enforces priorities to streams based on latency sensitivity, locality degree and application performance needs. 
QoS policies and architecture for cache/memory in CMP platforms	['The fact that a thread’s instruction retirement rate depends on the other threads in the workload makes performance on CMPs unpredictable which can be an even bigger problem than reduced throughput <NO>.', 'We note that HPAC is orthogonal to techniques that provide fairness in shared resources <NO>.']	Ravi Iyer, Li Zhao, Fei Guo, Ramesh Illikkal, Srihari Makineni, Don Newell, Yan Solihin, Lisa Hsu, and Steve Reinhardt. 2007. QoS policies and architecture for cache/memory in CMP platforms. Proc. of the 2007 ACM SIGMETRICS. ACM, 25–36.	As we enter the era of CMP platforms with multiple threads/cores on the die, the diversity of the simultaneous workloads running on them is expected to increase. The rapid deployment of virtualization as a means to consolidate workloads on to a single platform is a prime example of this trend. In such scenarios, the quality of service (QoS) that each individual workload gets from the platform can widely vary depending on the behavior of the simultaneously running workloads. While the number of cores assigned to each workload can be controlled, there is no hardware or software support in today’s platforms to control allocation of platform resources such as cache space and memory bandwidth to individual workloads. In this paper, we propose a QoS-enabled memory architecture for CMP platforms that addresses this problem. The QoS-enabled memory architecture enables more cache resources (i.e. space) and memory resources (i.e. bandwidth) for high priority applications based on guidance from the operating environment. The architecture also allows dynamic resource reassignment during run-time to further optimize the performance of the high priority application with minimal degradation to low priority. To achieve these goals, we will describe the hardware/software support required in the platform as well as the operating environment (O/S and virtual machine monitor). Our evaluation framework consists of detailed platform simulation models and a QoS-enabled version of Linux. Based on evaluation experiments, we show the effectiveness of a QoSenabled architecture and summarize key findings/trade-offs.	In this section, we motivate QoS needs by describing disparate threads and the shared resource problem.	https://doi.org/10.1145/1254882.1254886	2	['workload', 'priority', 'platform']	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Iyer, et al. [NO] enters the era of cmp platforms with multiple threads/cores on the die, the diversity of the simultaneous workloads running on them is expected to increase. 
A coordinated approach for practical OS-level cache management in multi-core real-time systems	['While LLC offers high potential for average performance improvement, it challenges worst-case execution time (WCET) estimation, which has made LLC to be studied in the last years by the real-time community <NO>.', 'Software cache partitioning <NO> and hardware cache partitioning <NO> have been used so far to control inter-task interaction in the LLC.', 'Software cache partitioning is done through memory coloring <NO>.']	Hyoseung Kim, Arvind Kandhalu, and Ragunathan Rajkumar. 2013. A coordinated approach for practical OS-level cache management in multi-core real-time systems. Proc. of the 25th ECRTS. 80–89.	Many modern multi-core processors sport a large shared cache with the primary goal of enhancing the statistic performance of computing workloads. However, due to resulting cache interference among tasks, the uncontrolled use of such a shared cache can significantly hamper the predictability and analyzability of multi-core real-time systems. Software cache partitioning has been considered as an attractive approach to address this issue because it does not require any hardware support beyond that available on many modern processors. However, the state-of-the-art software cache partitioning techniques face two challenges: (1) the memory co-partitioning problem, which results in page swapping or waste of memory, and (2) the availability of a limited number of cache partitions, which causes degraded performance. These are major impediments to the practical adoption of software cache partitioning. In this paper, we propose a practical OS-level cache management scheme for multi-core real-time systems. Our scheme provides predictable cache performance, addresses the aforementioned problems of existing software cache partitioning, and efficiently allocates cache partitions to schedule a given taskset. We have implemented and evaluated our scheme in Linux/RK running on the Intel Core i7 quad-core processor. Experimental results indicate that, compared to the traditional approaches, our scheme is up to 39% more memory space efficient and consumes up to 25% less cache partitions while maintaining cache predictability. Our scheme also yields a significant utilization benefit that increases with the number of tasks.	"We discuss related work on cache interference and describe the assumptions and notation used in our paper.
1Intel Core i7 processors are used in not only desktop/server machines but also aerospace and defense embedded systems <NO>. In fact, Intel’s Embedded Systems Division is rather big inside Intel."	https://doi.org/10.1109/ECRTS.2013.19	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Kim, et al. [NO] proposes a practical os-level cache management scheme for multi-core real-time systems. 
Multi-level unified caches for probabilistically time analysable real-time systems	['A similar two-level cache hierarchy has been shown to be MBPTA compliant <NO> though it was deployed in a single-core architecture with no inter-task interferences.']	L. Kosmidis, J. Abella, R. Quinones, and F.J. Cazorla. 2013. Multi-level unified caches for probabilistically time analysable real-time systems. Proc. of the 34th RTSS. IEEE, 360–371.	Caches are key resources in high-end processor architectures to increase performance. In fact, most highperformance processors come equipped with a multi-level cache hierarchy. In terms of guaranteed performance, however, cache hierarchies severely challenge the computation of tight worstcase execution time (WCET) estimates. On the one hand, the analysis of the timing behaviour of a single level of cache is already challenging, particularly for data accesses. On the other hand, unifying data and instructions in each level, makes the problem of cache analysis significantly more complex requiring tracking simultaneously data and instruction accesses to cache. In this paper we prove that multi-level cache hierarchies can be used in the context of Probabilistic Timing Analysis and tight WCET estimates can be obtained. Our detailed analysis (1) covers unified data and instruction caches, (2) covers different cache-write policies (write-through and write back), write allocation policies (write-allocate and non-write-allocate) and several inclusion mechanisms (inclusive, non-inclusive and exclusive caches), and (3) scales to an arbitrary number of cache levels. Our results show that the probabilistic WCET (pWCET) estimates provided by our analysis technique effectively benefit from having multi-level caches. For a two-level cache configuration and for EEMBC benchmarks, pWCET reductions are 55% on average (and up to 90%) with respect to a processor with a single level of cache.	This section provides some background on PTA as well as the cache characteristics and assumptions in this paper.	https://doi.org/10.1109/RTSS.2013.43	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Kosmidis, et al. [NO] of a single level of cache is already challenging, particularly for data accesses. 
Instruction cache locking using temporal reuse profile	['Cache locking is also shown to be quite effective for improving average-case execution time <NO>.']	Yun Liang, and T. Mitra. 2010. Instruction cache locking using temporal reuse profile. Proc. of the 47th ACM/IEEE DAC. 344–349.	The performance of most embedded systems is critically dependent on the average memory access latency. Improving the cache hit rate can have significant positive impact on the performance of an application. Modern embedded processors often feature cache locking mechanisms that allow memory blocks to be locked in the cache under software control. Cache locking was primarily designed to offer timing predictability for hard real-time applications. Hence, the compiler optimization techniques focus on employing cache locking to improve worst-case execution time. However, cache locking can be quite effective in improving the average-case execution time of general embedded applications as well. In this paper, we explore static instruction cache locking to improve average-case program performance. We introduce temporal reuse profile to accurately and efficiently model the cost and benefit of locking memory blocks in the cache. We propose an optimal algorithm and a heuristic approach that use the temporal reuse profile to determine the most beneficial memory blocks to be locked in the cache. Experimental results show that locking heuristic achieves close to optimal results and can improve the cache miss rate by up to 24% across a suite of real-world benchmarks. Moreover, our heuristic provides significant improvement compared to the state-of-the-art locking algorithm both in terms of performance and efficiency.	"Instruction cache locking has been primarily employed in hard real time systems for better timing predictability <NO>. In hard real time systems, worst case execution time (WCET) is an essential input to the schedulability analysis of mutli-tasking real time systems. It is difficult to estimate a safe but tight WCET in the presence of complex micro-architectural features such as caches. By statically locking instructions in the cache, WCET becomes more predictable. Locking has also been applied to shared caches in multi-cores in <NO>.
In this paper, our aim is to improve average-case execution time for general embedded systems through locking. Data cache locking mechanism based on the length of the reference window for each data-access instruction is proposed in <NO>. However, they do not model the cost/benefit of locking and there is no guarantee of performance improvement. Recently, Anand and Barua proposed an instruction cache locking algorithm for improving average-case execution time in <NO>. This work is most related to ours. However, our approach differs in two important aspects. First, Anand and Barua’s approach relies on simulation, while we use an accurate cache modeling. More concretely, in Anand and Barua’s method, two detailed trace simulations are employed in each iteration where an iteration locks one memory block in the cache. In contrast, we require only one profiling step. Secondly, in our approach, the cost/benefit of cache locking is modeled precisely using temporal reuse profiles of memory blocks. However, in their method, cache locking benefit is approximated by locking dummy blocks to keep the number of simulations reasonable. Thus our work improves over <NO> both in terms of performance and efficiency.
In this work, we introduce temporal reuse profile to model cache behavior. Previously, reuse distance has been proposed for the same purpose <NO>. Reuse distance is defined as the number of distinct data accesses between two consecutive references to the same address and it accurately models cache behavior of a fully associative cache. However, to precisely model the effect of cache locking, we need the content instead of the number (size) of the distinct data accesses between two consecutive references. TRP records both the reuse content and their frequencies."	https://doi.org/10.1145/1837274.1837362	3	['locking', 'locked', 'core']	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Liang, et al. [NO] explores static instruction cache locking to improve average-case program performance. 
Instruction cache locking for real-time embedded systems with multi-tasks	[]	Tiantian Liu, Minming Li, and C.J. Xue. 2009. Instruction cache locking for real-time embedded systems with multi-tasks. Proc. of the 15th IEEE RTCSA. 494–499.	Modern processors often provide cache locking capability which can be applied statically and dynamically to manage cache in a predictable manner. The selection of instructions to be locked in the instruction cache (I-Cache) has dramatic influence on the performance of multi-task real-time embedded systems. This paper focuses on using cache locking techniques on a shared I-Cache in a real-time embedded system with multi-tasks to minimize its worst-case utilization (WCU) which is one of the most important criteria for designing realtime embedded systems. We analyze the static and dynamic strategies to perform I-Cache locking and propose different algorithms which utilize the foreknowing information of the real-time embedded applications. Experiments show that the proposed algorithms can reduce WCU further compared to previous techniques. Design suggestions on which strategy should be utilized under different situations are also induced from the experimental results.	"There is N periodic tasks Ti, 1 ≤ i ≤ N with each period peri. The tasks are scheduled in the processor using some scheduling methods. WCU <NO> presents the busy proportion of a processor used in task computation. The following equation gives this ratio:
WCU = ∑
i
WCET (Ti) peri
(1)
In this paper, an Execution Flow Tree (EFT ) is used for modelling a real-time program.
Definition 1: An EFT is a weighted tree EFT = (V, E), where V represents the set of nodes and E represents the set of edges. Node v ∈ V represents a code block of the program and has three attributes: w(v) is the processing time when v is not in the cache, w′(v) is the processing time when v is in the cache, and s(v) is the size of v. Edge e(v, u) denotes a program control flow from node v to node u. It can be a sequential flow, a loop flow or a branch flow.
Algorithm EFT CON in <NO> constructs an EFT from an executable program code. An example of EFT is shown in Fig. 1. Fig. 1(a) is a code segment of the benchmark “Audio beam former” <NO> and Fig. 1(b) is its corresponding EFT . For simplicity, the EFT ’s call functions are not presented recursively in this example. Algorithm EFT CON does recursively process the subroutines. There is a procedure Duplicate() in Algorithm EFT CON. If a node v has an indegree(v) at least 2, Duplicate() instantiates the structure starting from v by indegree(v) times, which ensures the output to be a tree. For example, node 31, 32 and 33 in Fig. 1 (b) are the duplicated nodes introduced by Duplicate().
To calculate the WCU, we first need to obtain the WCET of each task. For an EFT , WCET is the length of its longest rootleaf path. For a root-leaf path Px = (px0px1 . . . px num(Px)) in an EFT where px0, px1, . . . , px num(Px) ∈ V and num(Px) is the number of edges on this path, Len(Px) represents the execution time needed if this path is chosen to execute in runtime. It can be calculated as: Len(Px) = ∑num(Px) y=0 (1 − δ(pxy)) ·w(pxy)+ δ(pxy) ·w′(pxy), where δ(pxy) = 1 if node
pxy is put in the I-Cache and δ(pxy) = 0 otherwise. Then the WCET (Ti) can be calculated by the following equation:
WCET (Ti) = max px∈Ti num(Px)∑ y=0 ((1 − δ(pxy)) · w(pxy) +
δ(pxy) · w′(pxy)) (2)"	https://doi.org/10.1109/RTCSA.2009.59	3	['locking', 'locked', 'core']	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Liu, et al. [NO] focuses on using cache locking techniques on a shared i-cache in a real-time embedded system with multi-tasks to minimize its worst-case utilization (wcu) which is one of the most important criteria for designing realtime embedded systems. 
Minimizing WCET for real-time embedded systems via static instruction cache locking	['More importantly, by carefully choosing the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling techniques without locking <NO>.', 'Recently, a heuristic <NO> and an optimal solution <NO> have been proposed to minimize the WCET via static instruction cache locking.', 'By employing full cache locking, <NO> can completely bypass cache modeling in WCET analysis phase and thereby achieve tight WCET estimation.', 'In this paper, we argue (and experimentally validate) that aggressive full cache locking as proposed in <NO> may have substantial negative impact on WCET reduction.', 'Full cache locking: Existing cache locking techniques <NO> first build the worst case path (e.', 'Both cache locking techniques <NO> model the fact that the WCET path may change after locking some memory locks.', 'For this example, the heuristic <NO> and the optimal <NO> approach return the same solution.', 'From the example above, we first observe that full locking techniques <NO> are not guaranteed to perform better than cache modeling (with no locking) specially when some memory accesses can be easily classified as cache hits (m3, m4, m5 in our example).', '<NO> present an optimal solution to minimize WCET via cache locking.', '3 Partial versus Full Cache Locking There exist two full cache locking techniques as mentioned in Section 2 <NO>.', '<NO> show that their approach can achieve better WCET reduction compared to <NO>, it has several limitations.']	Tiantian Liu, Minming Li, and C.J. Xue. 2009. Minimizing WCET for real-time embedded systems via static instruction cache locking. Proc. of the 15th IEEE RTAS. 35–44.	Cache is effective in bridging the gap between processor and memory speed. It is also a source of unpredictability because of its dynamic and adaptive behavior. Worst-case execution time (WCET) of an application is one of the most important criteria for real-time embedded system design. The unpredictability of instruction miss/hit behavior in the instruction cache (I-Cache) leads to an unnecessary overestimation of the real-time application’s WCET. A lot of modern processors provide cache locking capability. Static ICache locking locks function/instruction blocks of a program into the I-Cache before program execution. In this way, a more precise estimation of WCET can be achieved. The selection of functions/instructions to be locked in the I-Cache has dramatic influence on the performance of the real-time application. This paper focuses on the static I-Cache locking problem to minimize WCET for real-time embedded systems. We formulate the problem using an Execution Flow Tree (EFT ) and a linear programming model. For a subset of the problems with certain properties, corresponding polynomial time optimal algorithms are proposed. We prove that the general problem is an NP-Hard problem. We also show that for a subset of the general problem with certain patterns, optimal solutions can be achieved in polynomial time. Experimental results show that our algorithms can reduce the WCET of applications further compared to current best known techniques.	"A lot of works have been done regarding the predictability and performance issues of caches in real-time embedded systems. Some of them consider the I-Cache locking problem.
Puaut et al. have done a series of studies about I-Cache locking. In <NO>, they propose two polynomial greedy algorithms to select instruction locking contents. These two algorithms have different metrics in selecting tasks and program lines. One aims at minimizing the worst-case CPU utilization, while the other aims at minimizing the interferences between tasks. Experiments show that their algorithms improve these two metrics compared with a static cache analysis method <NO>. Their candidates are selected in a single set of tasks, which are the tasks along the worst-cast execution path without using cache. In <NO>, they compared their reference-based algorithm in <NO> with a genetic algorithm for cache contents selection in <NO>. Experiments show that both algorithms perform closely with respect to worst-case system utilization. The genetic algorithm performs slightly better than the reference-based algorithm with respect to the average slack time of tasks, while it has a higher time complexity.
The genetic algorithm mentioned above is proposed by Campoy et al. in <NO>. This genetic idea is inspired by Darwin’s
theory of evolution and is adopted by many researchers to solve optimization problems in a variety of research fields. They initially operate on a population of potential solutions, which are individual function/instruction blocks in the ICache locking problem. At each generation, a new set of approximations is created by selecting individuals according to their levels of fitness in the problem domain and breeding them together using cross-over operators borrowed from natural genetics. In the I-Cache locking selection problem, the fitness level is weighted by the average response time of all tasks and the cross-over operators are done by randomly selecting/reducing/increasing/modifying the locked program lines. This process leads to evolutions of individuals that are better suited to their environment than their ancestors. Finally an approximately optimal solution is achieved. This method is noted for its high time complexity. Also the cross-over operators need to be further discussed.
Falk et al. point out in <NO> that Puaut et al.’s algorithms do not consider the changing of worst-case path (WC-Path) after a function node is selected to be locked in I-Cache. Falk et al. take the changing of WC-Path into account during each step of the optimization procedure. They use an Execution Flow Graph (EFG) to model a program, where its nodes represent the function blocks and edges represent the control flows. They propose an algorithm for WC-Path construction with a complexity of O((|V |+ |E|) log |V |) and adopt greedy strategy to choose a node x with a maximal g(x) in each step. Here, g(x) is defined as g(x) = wx−w ′ x
sx ∗w(∗,x), where wx is
the WCET of x if x is placed in memory, w′x is the WCET of x if x is in the I-Cache, sx is the size of x in bytes, and w(∗,x) is the execution frequency of x over all contexts on the current WC-Path. This greedy algorithm can not guarantee the optimal solutions.
Asaduzzaman et al. <NO> also aim at minimizing the WCET. They believe WCET has direct connection with cache miss rate. So they propose a greedy algorithm which picks the function node with the maximal miss rate in each selecting step. However, the relation between WCET and miss rate is vague and has not been proved by persuasive statement.
Previous works have not given a formal formulation of I-Cache locking selection problem. No previous work has analyzed whether there exist polynomial algorithms to obtain the optimal solutions. Most of the previous proposed heuristic algorithms use greedy strategy. Also, they ignore the problem that the selected functions may conflict when they are filled into cache according to the cache mapping strategies. In this paper, we explicitly formulate the I-Cache locking selection problem and propose polynomial algorithms to obtain optimal solutions for a subset of the programs with certain properties. We prove the general problem is NP-Hard and also identify that a subset of the general problems which exhibit special patterns can be solved optimally in polynomial time. The algorithms in <NO> and <NO> are compared with the optimal algorithms proposed in this paper. The experimental results show that the proposed algorithms can reduce WCETs further for real-time applications effectively under different cache sizes."	https://doi.org/10.1109/RTAS.2009.11	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Liu, et al. [NO] focuses on the static i-cache locking problem to minimize wcet for real-time embedded systems. 
Soft-OLP: Improving hardware cache performance through software-controlled object-level partitioning	[]	Qingda Lu, Jiang Lin, Xiaoning Ding, Zhao Zhang, Xiaodong Zhang, and P. Sadayappan. 2009. Soft-OLP: Improving hardware cache performance through software-controlled object-level partitioning. Proc. of the 18th PACT. 246–257.	Performance degradation of memory-intensive programs caused by the LRU policy’s inability to handle weaklocality data accesses in the last level cache is increasingly serious for two reasons. First, the last-level cache remains in the CPU’s critical path, where only simple management mechanisms, such as LRU, can be used, precluding some sophisticated hardware mechanisms to address the problem. Second, the commonly used shared cache structure of multi-core processors has made this critical path even more performance-sensitive due to intensive inter-thread contention for shared cache resources. Researchers have recently made efforts to address the problem with the LRU policy by partitioning the cache using hardware or OS facilities guided by run-time locality information. Such approaches often rely on special hardware support or lack enough accuracy. In contrast, for a large class of programs, the locality information can be accurately predicted if access patterns are recognized through small training runs at the data object level. To achieve this goal, we present a system-software framework referred to as Soft-OLP (Software-based Object-Level cache Partitioning). We first collect per-object reuse distance histograms and inter-object interference histograms via memory-trace sampling. With several low-cost training runs, we are able to determine the locality patterns of data objects. For the actual runs, we categorize data objects into different locality types and partition the cache space among data objects with a heuristic algorithm, in order to reduce cache misses through segregation of contending objects. The object-level cache partitioning framework has been implemented with a modified Linux kernel, and tested on a commodity multi-core processor. Experimental results show that in comparison with a standard L2 cache managed by LRU, Soft-OLP significantly reduces the execution time by reducing L2 cache misses across inputs for a set of single- and multi-threaded programs from the SPEC CPU2000 benchmark suite, NAS benchmarks and a computational kernel set.	"Here we use the conjugate gradient (CG) program from the NAS benchmarks as a motivating example to illustrate the problem. As shown in Fig. 1, CG spends most of its running time on a sparse matrix-vector multiplication w = a ·p,
where a is a sparse matrix, rowstr and colidx are row and column index arrays and w and p are dense vectors. There are also code pieces not shown in Fig. 1 due to the space limitation for this paper. These code pieces access arrays iv,v,acol,arow, x,r,q,z,aelt in addition to the arrays shown in Fig. 1. In CG, the majority of the memory accesses are on arrays a, p and colidx. Although vector p has high temporal reuse in the matrix-vector multiplication code, depending on its size, its elements may get repeatedly evicted from cache before their next uses, due to the streaming accesses on arrays a and colidx. As the result of this thrashing effect caused by accessing arrays a and colidx, CG often reveals a streaming access pattern and has a very high miss rate in cache. Without special code/data treatment based on domain knowledge, general compiler optimizations, such as loop tiling, cannot be applied in this case because of the irregular nature of this program — there is indirection in array accesses.
The caching inefficiency problem occurs in CG because the conventional LRU cache replacement policy does not distinguish strong- and weak-locality accesses and thus is unable to treat them differently. Since cache replacement decisions are made at the whole-system level, any data reuse with a reuse distance greater than the cache size cannot be hit in the cache. The CG case is an example of variable locality strengths among different data objects, which can not be distinguished and handled properly by LRU. If we allow the cache space to be partitioned between data objects, we would be able to allocate variable cache sizes to different objects based on their locality strengths, well utilizing the limited cache space and minimizing cache misses. With CG, there are different ways to reduce and even completely eliminate capacity misses on strong-locality array p without increasing cache misses on the other objects. One approach is to protect p in an exclusive cache space and leave the remaining cache capacity for the remaining data objects. Alternatively, we can divide the cache such that the minimum cache quota is given to weak-locality arrays colidx and a. This optimization is not limited to single-thread performance. Even when the code is augmented with OpenMP directives, with a shared cache the object-level partitioning decisions should still reduce capacity misses, since memory accesses from different processor cores collectively reveal the same pattern as with sequential execution. If we allocate a very small cache quota for arrays colidx and a and co-schedule CG with other programs, it no longer exhibits a streaming access pattern that significantly interferes with its co-runners, so
that high throughput can be achieved with judicious interthread cache partitioning. In this paper, we focus on objectlevel cache partitioning and defer the combination of interobject and inter-thread cache partitioning to future work.
III. OVERVIEW OF THE APPROACH
The CG example in Fig. 1 demonstrates the benefits of partitioning the cache space at the object level. In this paper the term object is defined as an allocated region of data storage and used interchangeably with variable. Note that this definition is not equivalent to its usage in objectoriented programming. We partition the last-level cache space among global and heap objects for high-performance scientific applications. There are two reasons for this decision. First, high-performance scientific applications often have relatively regular memory access patterns and high data reuse ratios, which makes object-level cache partitioning possible and profitable. Second, in these programs, the majority of the memory accesses and cache misses are on a limited number of global and heap objects. In order to partition the last-level cache space among data objects, we need to answer the following questions: (1) How can we capture data reuse patterns at the object level, across cache configurations and program inputs? (2) How can we capture the interference among the data objects that share and compete for cache space? (3) How can we identify critical objects as partitioning candidates? (4) How can we make quick object-level partitioning decisions with a different program input? (5) What system support is needed to enforce cache partitioning decisions?
To answer the above questions, we propose a system framework called Soft-OLP that detects a program’s data reuse patterns at the object level, through memory profiling and pattern recognition, and enforces partitioning decisions at run time with operating system support. This proposed framework consists of the following steps and is summarized in Fig. 2.
1) Profile Generation. For a given program and several small training inputs, we capture memory accesses in an object-relative form through binary instrumentation. We obtain per-object reuse distance histograms and inter-object interference histograms for data objects. These histograms are program profiles with
training inputs that are to be used to predict the program’s data access and reuse patterns. 2) Profile Analysis. Based on program profiles from training runs, we detect the patterns of the program’s perobject data reuse, object sizes and access frequencies as polynomial functions, using a pattern recognition algorithm based on the work in <NO>. 3) Cache Partitioning Decision Making and Enforcement. When the program is scheduled to run with an actual input, we predict its per-object reuse distance histograms and inter-object interference information with detected access patterns. We then categorize data objects as being “hot”, “hog”, “cold” or “other”. Using this classification, we follow a heuristic algorithm to make an object-level cache partitioning decision so that “hog” objects do not prevent us from exploiting the locality of “hot” objects and the contention between “hot” objects is alleviated. Such a partitioning decision is finally enforced on commodity CMPs with an OS kernel that supports page coloring <NO>, <NO> at the object level.
IV. OBJECT-LEVEL PROGRAM LOCALITY PROFILE
With a given input, we model a program’s data locality at the object level with a locality profile. An object-level program locality profile has two components: an objectrelative locality profile consisting of per-object reuse distance histograms and an inter-object interference profile including inter-object interference histograms."	https://doi.org/10.1109/PACT.2009.35	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Lu, et al. [NO] presents a system-software framework referred to as soft-olp (software-based object-level cache partitioning). 
Real-time cache management framework for multi-core architectures	[]	R. Mancuso, R. Dudko, E. Betti, M. Cesati, M. Caccamo, and R. Pellizzoni. 2013. Real-time cache management framework for multi-core architectures. Proc. of the 19th IEEE RTAS. 45–54.	Multi-core architectures are shaking the fundamental assumption that in real-time systems the WCET, used to analyze the schedulability of the complete system, is calculated on individual tasks. This is not even true in an approximate sense in a modern multi-core chip, due to interference caused by hardware resource sharing. In this work we propose (1) a complete framework to analyze and profile task memory access patterns and (2) a novel kernel-level cache management technique to enforce an efficient and deterministic cache allocation of the most frequently accessed memory areas. In this way, we provide a powerful tool to address one of the main sources of interference in a system where the last level of cache is shared among two or more CPUs. The technique has been implemented on commercial hardware and our evaluations show that it can be used to significantly improve the predictability of a given set of critical tasks.	In this section we will briefly discuss existing profiling and cache management techniques and we will explain how our cache allocation strategy differs from existing related work.	https://doi.org/10.1109/RTAS.2013.6531078	2	['workload', 'priority', 'platform']	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Mancuso, et al. [NO] proposes (1) a complete framework to analyze and profile task memory access patterns and (2) a novel kernel-level cache management technique to enforce an efficient and deterministic cache allocation of the most frequently accessed memory areas. 
A comparison of memory allocators for real-time applications	[]	Miguel Masmano, Ismael Ripoll, and Alfons Crespo. 2006. A comparison of memory allocators for real-time applications. Proc. of the 4th JTRES. ACM, 68–76.	Real-Time applications can require dynamic storage management. However this feature has been sistematically avoided due to the general belief about the poor performance of allocation and deallocation operations in time and space. Actually, the use of Java technologies in real-time require to analyse in detail the performance of this feature due to its intensive use. In a previous paper, the authors proposed a new dynamic storage allocator that perform malloc and free operations in constant time (O(1)) with a very high efficiency. In this paper, we compare the behaviour of several allocators under ”real-time” loads measuring the temporal cost and the fragmentation incurred by each allocator. In order to compare the temporal cost of the allocators, two parameters have been considered: number of instructions and processor cycles. To measure the fragmentation, we have calculated the relation between the maximum memory used by the each allocator relative to the point of the maximum amount of memory used by the load. Additionally, we have measured the impact of delayed deallocation in a similar way a periodic garbage collector server will do. The results of this paper show that TLSF allocator obtains the best resuts when both aspects, temporal and spatial are considered.	"This section presents a brief descrition of the allocators used in the evaluation. Allocators can be classified attending to the policy used (first fit, best fit, good fit, etc.) and the mechanism implemented (doubly linked lists, segregated lists, bitmaps, etc.) based on the work of Wilson et al.<NO>
In order to perform the evaluation presented in this paper, we have selected some of the allocatores more representatives taking into account considerations like:
Representatives of very well known policies. First-fit and Best-fit are two of the most representative sequential fit allocators. First-fit allocator is used in all comparisons. It does not provide good results in terms of time and fragmentation but it is a reference. Best-fit provides very good results on fragmentation but bad results in time. Both of them are usually implemented with a doubly linked list. The pointers which implement the list are emb]dded inside the header of each free block. First-fit allocator searches the free list and selects the first block whose size is equal or greater than the requested size, whereas Best-fit goes further to select the block which best fits the request.
Widely used in several environments. Doug Lea’s allocator <NO> is the most representative of hybrid allocator and it is used in Linux systems and several environments. It is a combination of several mechanisms. This allocator uses a single array of lists, where the first 48 indexes are lists of blocks of an exact size (16 to 64 bytes) called “fast bins” . The remaining part
of the array contains lists of segregated lists, called “bins”. Each of these segregated lists are sorted by block size. A mapping function is used to quickly locate a suitable list. DLmalloc uses the delayed coalescing strategy, that is, the deallocation operation does not coalesce blocks. Instead a massive coalescing is done when the allocator can not serve a request.
Labelled as ”real-time” allocators. Binary-Buddy and Half-fit are good-fit allocators that provide excellent results in time reponse. However, the fragmentation produced by these allocators is known to be non negligible.
Buddy systems <NO> are a particular case of Segregated free lists. Being H the heap size, there are only log2(H) lists since the heap can only be split in powers of two. This restriction yields efficient splitting and merging operations, but it also causes a high memory fragmentation. There exist several variants of this method <NO> such as Binary-buddy, Fibonacci-buddy, Weighted buddy and Double-buddy.
The Binary-buddy <NO> allocator is the most representative of the Buddy Systems allocators, which besides has always been considered as a real-time allocator. The initial heap size has to be a power of two. If a smaller block is needed, then any available block can only be split into two blocks of the same size, which are called buddies. When both buddies are again free, they are coalesced back into a single block. Only buddies are allowed to be coalesced. When a small block is requested and no free block of the requested size is available, a bigger free block is split one or more times until one of a suitable size is obtained.
Half-fit <NO> uses bitmaps to find free blocks rapidly without having to perform an exhaustive search. Halffit groups free blocks in the range [2i, 2i+1[ in a list indexed by i. Bitmaps to keep track of empty lists jointly with bitmap processor instructions are used to speed-up search operations. When a block of size r is required, the search for a suitable free block starts on i, where i = ⌊log2(r−1)⌋+1 (or 0 if r = 1). Note that the list i always holds blocks whose sizes are equal to or larger than the requested size. If this list is empty, then the next non-empty free list is used instead. If the size of the selected free block is larger than the requested one, the block is split in two blocks of sizes r and r′. The remainder block of size r′ is re-inserted in the list indexed by i′ = ⌊log2(r ′)⌋.
New real-time allocator. TLSF (Two-Level Segregated Fit) <NO> is a bounded-time, good-fit allocator. TLSF implements a combination of segregated and bitmap fits mechanisms. The use of bitmaps allow to implement fast, bounded-time mapping and searching functions. TLSF data structure can be represented as a two-dimension array. The first dimension splits free blocks in size-ranges a power of two apart from each other, so that first-level index i refers to free blocks of sizes in the range [2i,2i+1[. The second dimension splits each first-level range linearly in a number of ranges of an equal width. The number of such ranges, 2L, should not exceed the number of bits of the underlying architecture, so that a one-word bitmap
can represent the availability of free blocks in all the ranges. TLSF uses word-size bitmaps and processor bit instructions to find a suitable list in constant time. The range of sizes of the segregated lists has been chosen so that a mapping function can be used to locate the position of the segregated list given the block size, with no sequential or binary search. Also, ranges have been spread along the whole range of possible sizes in such a way that the relative width (the length of the range) of the range is similar for small blocks than for large blocks. In other words, there are more lists used for smaller blocks than for larger blocks.
One important aspect is the theoretical temporal cost (complexity) of each allocator. Table 1 summarises these costs for each allocator.
In <NO>, the worst-case or bad-case2 scenario of each allocator has been analysed and detailed. For each allocator, a sinthetic load was generated to conduct it to its worst-case allocating and deallocating scenarios. Once these scenarios were reached, we measured the number of instructions performed by the allocation or deallocation operations. Table 2 shows a summary of these results.
These results can slightly change depending on the compiler version and the optimisation options used."	https://doi.org/10.1145/1167999.1168012	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Masmano, et al. [NO] compares the behaviour of several allocators under ” real-time ” loads measuring the temporal cost and the fragmentation incurred by each allocator. 
Hardware support for WCET analysis of hard real-time multicore systems	['<NO> have proposed TDMA-based bus and L2 cache access to improve predictability on multi-core architectures.', 'Such hybrid execution of application tasks has been considered in recent research <NO>.', 'While LLC offers high potential for average performance improvement, it challenges worst-case execution time (WCET) estimation, which has made LLC to be studied in the last years by the real-time community <NO>.', 'Software cache partitioning <NO> and hardware cache partitioning <NO> have been used so far to control inter-task interaction in the LLC.', 'Hardware cache partitioning assigns different cache ways or cache banks to the different co-running tasks such in a way that contention is prevented <NO>.', 'hardware cache (way) partitioning <NO>.', 'However, we believe the scheduling algorithm and analysis techniques in this paper is a necessary step towards completely avoiding interference between tasks running on multicores, and can be integrated with techniques of performance isolation on other shared resources, for instance, the work in <NO> to avoid interference caused by the shared on-chip bus.', '<NO> formulated cache allocation as a MILP problem to minimize the total CPU utilization of Paolieri’s new multi-core architecture <NO>.', 'be bounded and known, by using for instance Time Division Multiple Access (TDMA) like in <NO> or other predictable bus arbitration policies <NO>.', 'DL1 and IL1 caches are connected to the UL2 through fully-dedicated bidirectional buses, whose access latency can be bounded using the technique presented in <NO>.']	Marco Paolieri, Eduardo Quiñones, Francisco J Cazorla, Guillem Bernat, and Mateo Valero. 2009. Hardware support for WCET analysis of hard real-time multicore systems. ACM SIGARCH Computer Architecture News, Vol. 37. ACM, 57–68.	The increasing demand for new functionalities in current and future hard real-time embedded systems like automotive, avionics and space industries is driving an increase in the performance required in embedded processors. Multicore processors represent a good design solution for such systems due to their high performance, low cost and power consumption characteristics. However, hard real-time embedded systems require time analyzability and current multicore processors are less analyzable than single-core processors due to the interferences between different tasks when accessing shared hardware resources. In this paper we propose a multicore architecture with shared resources that allows the execution of applications with hard real-time and non hard real-time constraints at the same time, providing time analizability for the hard real-time tasks so that they can meet their deadlines. Moreover our architecture proposal provides high-performance for the non hard real-time tasks.	"One of the design goals of our architecture is that it can be easily analyzed by current measurement based WCET tools with no modifications. In this section we provide some background on real-time scheduling and the analysis tool we use in this paper.
In real-time systems, for each task, the scheduler knows three main parameters: The period, the deadline and the Worst-Case Execution Time (WCET). If the task is periodic, its period is the interval at which new instances of that task are ready for execution. The deadline is the time before a task instance must be complete. For simplicity, the deadline is often set equal to the period. This means that a task has to be executed before its next instance arrives into the system. The WCET is a safe estimation of the upper bound time required to execute any instance of the task. In single-core systems the WCET of a task is computed assuming that the task has full access to processor resources.
One of the current approaches to analyze WCET in singlecore processors is measurement-based WCET analysis<NO>. In this paper we use RapiTime<NO>, a commercial tool developed by Rapita Systems Ltd.3, that estimates the WCET using a measurement-based technique. This tool is widely used in the avionics, telecommunications, space, and automotive industries. RapiTime uses on-line testing to measure the execution time of sub-paths between instrumentation points in the code. Moreover, by contrast, offline static analysis is the best way to determine the overall structure of the code and the paths through it. RapiTime therefore uses path
3 www.rapitasystems.com
analysis techniques to build up a precise model of the overall code structure and determine which combinations of subpaths form complete and feasible paths through the code. Finally RapiTime combines the measurement and control flow analysis information to compute measurement based worst-case execution time estimations in a way that captures accurately the execution time variation on individual paths due to hardware effects<NO>."	https://doi.org/10.1145/1555815.1555764	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Paolieri, et al. [NO] proposes a multicore architecture with shared resources that allows the execution of applications with hard real-time and non hard real-time constraints at the same time, providing time analizability for the hard real-time tasks so that they can meet their deadlines. 
Architectural support for operating system-driven CMP cache management	['that its blocks are not evicted by demand based replacement policies like LRU (the code for the hog program is available in the technical report <NO>).', 'Rafique (23) proposes an OS scheme that consists of a hardware cache quota management mechanism, an OS interface and a set of OS level quota orchestration policies for greater flexibility in policies.', 'The majority of work required modifications to hardware and falls into one of two camps: performance aware cache modification (most commonly cache-partitioning) <NO> or performance-aware DRAM controller memory scheduling [Loh 2008; Suh et al.', 'The effect of simultaneously executing threads competing for space in the LLC has been explored extensively by many different researchers <NO>.']	Nauman Rafique, Won-Taek Lim, and Mithuna Thottethodi. 2006. Architectural support for operating system-driven CMP cache management. Proc. of the 15th PACT. ACM, 2–12.	The role of the operating system (OS) in managing shared resources such as CPU time, memory, peripherals, and even energy is well motivated and understood [23]. Unfortunately, one key resource—lower-level shared cache in chip multi-processors—is commonly managed purely in hardware by rudimentary replacement policies such as least-recentlyused (LRU). The rigid nature of the hardware cache management policy poses a serious problem since there is no single best cache management policy across all sharing scenarios. For example, the cache management policy for a scenario where applications from a single organization are running under “best effort” performance expectation is likely to be different from the policy for a scenario where applications from competing business entities (say, at a third party data center) are running under a minimum service level expectation. When it comes to managing shared caches, there is an inherent tension between flexibility and performance. On one hand, managing the shared cache in the OS offers immense policy flexibility since it may be implemented in software. Unfortunately, it is prohibitively expensive in terms of performance for the OS to be involved in managing temporally fine-grain events such as cache allocation. On the other hand, sophisticated hardware-only cache management techniques to achieve fair sharing or throughput maximization have been proposed. But they offer no policy flexibility. This paper addresses this problem by designing architectural support for OS to efficiently manage shared caches with a wide variety of policies. Our scheme consists of a hardware cache quota management mechanism, an OS interface and a set of OS level quota orchestration policies. The hardware mechanism guarantees that OS-specified quotas are enforced in shared caches, thus eliminating the need for (and the performance penalty of) temporally fine-grained OS intervention. The OS retains policy flexibility since it can tune the quotas during regularly scheduled OS interventions. We demonstrate that our scheme can support a wide range of policies including policies that provide (a) passive performance differentiation, (b) reactive fairness by miss-rate equalization and (c) reactive performance differentiation.	Our cache management scheme consists of three essential components: a hardware quota enforcement mechanism, an interface between hardware and OS, and a set of OS level policies. We first describe two hardware quota enforcement mechanisms in Section 2.1, their advantages and limitations. Section 2.2 describes the OS interface of our scheme and finally Section 2.3 explains how the OS can use this interface to implement different policies.	https://doi.org/10.1145/1152154.1152160	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Rafique, et al. [NO] addresses this problem by designing architectural support for os to efficiently manage shared caches with a wide variety of policies. 
Reconfigurable caches and their application to media processing	['The other kinds of work <NO> on this topic dynamically adjust the cache for performance benefits.']	Parthasarathy Ranganathan, Sarita Adve, and Norman P. Jouppi. 2000. Reconfigurable caches and their application to media processing. Proc. of the 27th ISCA. ACM, 214–224.	High performance general-purpose processors are increasingly being used for a variety of application domains { scienti c, engineering, databases, and more recently, media processing. It is therefore important to ensure that architectural features that use a signi cant fraction of the on-chip transistors are applicable across these di erent domains. For example, current processor designs often devote the largest fraction of on-chip transistors (up to 80%) to caches. Many workloads, however, do not make e ective use of large caches; e.g., media processing workloads which often have streaming data access patterns and large working sets. This paper proposes a new recon gurable cache design. This design enables the cache SRAM arrays to be dynamically divided into multiple partitions that can be used for di erent processor activities. These activities can bene t applications that would otherwise not use the storage allocated to large conventional caches. Our design involves relatively few modi cations to conventional cache design, and analysis using a modi cation of the CACTI analytical model shows a small impact on cache access time. We evaluate one representative use of recon gurable caches { instruction reuse for media processing. We nd this use gives IPC improvements ranging from 1.04X to 1.20X in simulation across eight media processing benchmarks. 	"Some of the possible applications for recon gurable caches are discussed below. We speci cally discuss how these applications are relevant to the domain of media processing; however, codes from other domains can bene t from these as well.
Hardware optimizations using lookup tables or bu ers. Several hardware optimizations have been proposed that re-
quire maintaining lookup tables or bu ers, where the effectiveness of the optimization improves signi cantly with larger table sizes. For example, value prediction, memoization, and instruction reuse have recently been studied to exploit redundancy in computation in the SPEC benchmarks <NO>. Other optimizations that require large lookup tables or bu ers include coherence prediction, memory disambiguation prediction, compression-based branch prediction, hardware prefetching (where lookup tables are used to store information for address prediction), and dynamic optimizations triggered by performance information collected and stored in tables at runtime. Several of these techniques have been reported to have the capacity to perform better with larger lookup table spaces <NO>. The lookup tables and bu ers for these optimizations could be implemented in a partition of a recon gurable cache instead of using other valuable chip area. Section 5 studies one such technique, instruction reuse, with recon gurable caches to address the computation bottleneck in media processing workloads.
Software and hardware prefetched data. Software and hardware prefetching are widely used techniques to hide memory latency. However, if the prefetched data is fetched too far in advance, it can pollute the cache replacing other useful data or be replaced before use by a demand access, eliminating any performance bene ts. On the other hand, prefetches that occur too late do not fully hide the latency. Therefore, prefetching techniques need to strike a careful balance when scheduling the prefetches, but are often unsuccessful in doing so. With recon gurable caches, a separate partition can be used to prefetch data early while avoiding the problem of cache pollution or replacement of prefetched data. Such an application of recon gurable caches could be particularly useful with media processing benchmarks which often have streaming behavior <NO>.
Compiler or application controlled memory. A partition of a recon gurable cache could be con gured as compiler or application controlled memory. As discussed in <NO>, the compiler could use such memory as a scratch area for spill code to achieve performance bene ts. Alternatively, this area can be used by system code or device drivers as a separately addressable bu er area. Such a use may also be bene cial in ensuring real-time requirements of media applications with general-purpose processors. Many DSP processors hardwire their on-chip SRAM to be used as memory (as opposed to caches) to ensure predictability of memory latencies <NO>. Cache line locking (as in the Cyrix MediaGX processor <NO>) or controlled cache line replacement (as with malleable caches <NO>) can provide the same functionality."	https://doi.org/10.1145/339647.339685	2	['workload', 'priority', 'platform']	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Ranganathan, et al. [NO] proposes a new recon gurable cache design. 
Predictable task migration for locked caches in multi-core systems	[]	Abhik Sarkar, Frank Mueller, and Harini Ramaprasad. 2011. Predictable task migration for locked caches in multi-core systems. Proc. of the LCTES’11. ACM, 131–140.	Locking cache lines in hard real-time systems is a common means of achieving predictability of cache access behavior and tightening as well as reducing worst case execution time, especially in a multitasking environment. However, cache locking poses a challenge for multi-core hard real-time systems since theoretically optimal scheduling techniques on multi-core architectures assume zero cost for task migration. Tasks with locked cache lines need to proactively migrate these lines before the next invocation of the task. Otherwise, cache locking on multi-core architectures becomes useless as predictability is compromised. This paper proposes hardware-based push-assisted cache migration as a means to retain locks on cache lines across migrations. We extend the push-assisted migration model with several cache migration techniques to efficiently retain locked cache lines on a bus-based chip multi-processor architecture. We also provide deterministic migration delay bounds that help the scheduler decide which migration technique(s) to utilize to relocate a single or multiple tasks. This information also allows the scheduler to determine feasibility of task migrations, which is critical for the safety of any hard real-time system. Such proactive migration of locked cache lines in multi-cores is unprecedented to our knowledge.	"In the past decade, there has been considerable research on cache line locks in the context of multi-tasking real-time systems. Static and dynamic cache locking algorithms for instruction caches have been proposed to improve system utilization in <NO>. Data cache locking techniques that pin data when cache behavior is hard to analyze statically have been proposed <NO>. Past work presented techniques for cache locking that provides comparable performance to scratchpad allocation <NO>. Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>. Of course, cache locking can also be used in conjunction with private L2 caches. Multi-cores certainly make cache locking even more attractive in terms of real-time predictability.
Choffnes et al. propose migration policies for multi-core fairshare scheduling <NO>. Their technique strives to minimize migration costs while ensuring fairness among the tasks by maintaining balanced scheduling queues as new tasks are activated. The work is in the context of soft real-time systems while ours focuses on hard real-time. Calandrino et al. propose scheduling techniques that account for co-schedulability of tasks with respect to cache behavior <NO>. Their approach is based on organizing tasks with the same period into groups of cooperating tasks. While their method improves cache performance in soft real-time systems, they do not specifically address issues related to task migration. Li et al. discuss migration policies that facilitate efficient operating system scheduling in asymmetric multi-core architectures <NO>. Their work focuses on fault-and-migrate techniques to handle resource-related faults in heterogeneous cores and does not consider real-time constraints. Eisler et al. <NO> develop a cache capacity increasing scheme for multi-cores that scavenges unused neighboring cache lines. They consider “migration” of cache lines amounting to distribution of data in caches while we focus on task migration combined with data migration mechanisms that keep data local to the target core and retains the locks in caches across migration.
Acquaviva et al. <NO> assess the cost of task migration for soft real-time systems. They assume private memory and different operating system instances per core on a low-end processor. In contrast, we assume private caches with a single operating system instance, which more accurately reflects contemporary embedded multi-cores <NO>. Their focus is on task replication and re-creation across different memory spaces while our work focuses on task migration within part shared, part private memory spaces. Hardy et al. have recently proposed static cache analysis techniques to quantify cache-related migration delay cost on multi-cores by estimating re-use of cache lines that cause cache misses <NO>. Our methodology focuses upon eliminating migration delay to support cache line locking in multi-cores and providing support for deterministic migration delay for locked cache lines."	https://doi.org/10.1145/1967677.1967696	3	['locking', 'locked', 'core']	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Sarkar, et al. [NO] proposes hardware-based push-assisted cache migration as a means to retain locks on cache lines across migrations. 
Static task partitioning for locked caches in multi-core real-time systems	[]	Abhik Sarkar, Frank Mueller, and Harini Ramaprasad. 2012. Static task partitioning for locked caches in multi-core real-time systems. Proc. of the CASES’12. ACM, 161–170.	Locking cache lines in hard real-time systems is a common means to ensure timing predictability of data references and to lower bounds on worst-case execution time, especially in a multi-tasking environment. Growing processing demand on multi-tasking real-time systems can be met by employing scalable multi-core architectures, like the recently introduced tile-based architectures. This paper studies the use of cache locking on massive multi-core architectures with private caches in the context of hard real-time systems. In shared cache architectures, a single resource is shared among all the tasks. However, in scalable cache architectures with private caches, conflicts exist only among the tasks scheduled on one core. This calls for a cache-aware allocation of tasks onto cores. Our work extends the cache-unaware First Fit Decreasing (FFD) algorithm with a Naive locked First Fit Decreasing (NFFD) policy. We further propose two cache-aware static scheduling schemes: (1) Greedy First Fit Decreasing (GFFD) and (2) Colored First Fit Decreasing (CoFFD). This work contributes an adaptation of these algorithms for conflict resolution of partially locked regions. Experiments indicate that NFFD is capable of scheduling high utilization task sets that FFD cannot schedule. Experiments also show that CoFFD consistently outperforms GFFD resulting in lower number of cores and lower system utilization. CoFFD reduces the number of core requirements from 30% to 60% compared to NFFD. With partial locking, the number of cores in some cases is reduced by almost 50% with an increase in system utilization of 10%. Overall, this work is unique in considering the challenges of future multicore architectures for real-time systems and provides key insights into task partitioning with locked caches for architectures with private caches.	"In the past decade, there has been considerable research promoting locked caches in the context of multi-tasking realtime systems. Static and dynamic cache locking algorithms for instruction caches have been proposed to improve system utilization in <NO>. Several methods have been developed to lock program data that is hard to analyze statically <NO>. Further techniques have been developed for cache locking that provide performance comparable to that obtained with scratchpad allocation <NO>. Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>. This trend is a strong proponent of cache locking as a viable solution in future real-time system designs on multi-cores.
Choffnes et al. have proposed migration policies for multicore fair-share scheduling <NO>. Their technique strives to minimize migration costs while ensuring fairness among the tasks by maintaining balanced scheduling queues as new tasks are activated. Calandrino et al. propose scheduling techniques that account for co-schedulability of tasks with respect to cache behavior <NO>. Their approach is based on organizing tasks with the same period into groups of cooperating tasks. All these methods improve cache performance in soft real-time systems. Li et al. discuss migration policies that facilitate efficient operating system scheduling in asymmetric multicore architectures <NO>. Their work focuses on fault-and-migrate techniques to handle resource-related faults in heterogeneous cores and does not operate in the context of real-time systems. Eisler et al. <NO> develop a cache capacity increasing scheme for multicores that scavenges unused neighboring cache lines.
Paolieri et al. <NO> have proposed TDMA-based bus and L2 cache access to improve predictability on multi-core architectures. Their work focuses on supporting hard real-time applications on multi-cores but assumes shared L2 caches with contention due to accesses by different tasks. Ouyang et al. <NO> have proposed extending Quality of Service support to mesh-based interconnects but their study is limited to the on-chip network traffic."	https://doi.org/10.1145/2380403.2380434	3	['locking', 'locked', 'core']	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Sarkar, et al. [NO] studies the use of cache locking on massive multi-core architectures with private caches in the context of hard real-time systems. 
Adaptive set pinning: Managing shared caches in chip multiprocessors	['<NO> propose adaptive set pinning scheme.', 'The effect of simultaneously executing threads competing for space in the LLC has been explored extensively by many different researchers <NO>.']	S. Srikantaiah, M. Kandemir, and M.J. Irwin. 2008. Adaptive set pinning: Managing shared caches in chip multiprocessors. Proc. of the 13th ASPLOS. ACM, 135–144.	As part of the trend towards Chip Multiprocessors (CMPs) for the next leap in computing performance, many architectures have explored sharing the last level of cache among different processors for better performance–cost ratio and improved resource allocation. Shared cache management is a crucial CMP design aspect for the performance of the system. This paper first presents a new classification of cache misses – CII: Compulsory, Inter-processor and Intra-processor misses – for CMPs with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a CMP. We then propose a novel approach, called set pinning, for eliminating inter-processor misses and reducing intra-processor misses in a shared cache. Furthermore, we show that an adaptive set pinning scheme improves over the benefits obtained by the set pinning scheme by significantly reducing the number of off–chip accesses. Extensive analysis of these approaches with SPEComp 2001 benchmarks is performed using a full system simulator. Our experiments indicate that the set pinning scheme achieves an average improvement of 22.18% in the L2 miss rate while the adaptive set pinning scheme reduces the miss rates by an average of 47.94% as compared to the traditional shared cache scheme. They also improve the performance by 7.24% and 17.88% respectively.	"The motivation for our classification springs from the example transactions depicted in Figure 1. Consider a CMP with two processors, P1 and P2 and a fully associative shared L2 cache. Exam-
Replaced (P1)Referenced (P1)
Inter Processor MissNever Referenced
Referenced (P2)
Cache Hit
Replaced (P2)
Cache Hit Intra Processor Miss
Intra Processor Miss
Cold Miss
Cold Miss
P1 Ref P2 Ref P1 Replacement P2 Replacement Any
Figure 2. State diagram representing a memory element’s life cycle in the shared cache. Non–compulsory misses are classified based on the processor responsible for evicting the referenced block. A non–compulsory miss is classified as Intra-processor miss if it was evicted by the same processor that brought it into the cache and Inter-processor miss if it was evicted by other processors.
ples (a) and (b) in Figure 1 show two possible types of transactions that could result in a miss in a shared cache. Example (a) depicts a traditional capacity miss where the same processor P1 is responsible for both first reference and eviction of the memory element X. Example (b) also depicts a miss by P1, that occurs to a memory element X. The difference here is that X was brought into the cache by an earlier reference by P1, but was evicted only because of a reference to a different memory element Y that mapped to the same cache block as X by P2.
Clearly, we would fail to understand the inherent differences in the cause for such misses, by classifying both of these misses as “capacity misses” (as in the 3C miss classification). The same is true with conflict misses. We classify the misses similar to that shown in (a) of Figure 1 as Intra-processor misses and ones similar to that shown in (b) as Inter-processor misses. Thus our classification, CII, classifies the cache misses in a CMP with a shared cache
into compulsory misses, intra-processor misses and inter-processor misses.
In order to present a more formal understanding of the CII classification, we can represent the life cycle of a memory element as shown in the state diagram in Figure 2. This diagram depicts the life cycle of a memory element in the shared cache during the execution of a program accessing it, assuming the program is executing on a dual core CMP. The same idea is easily extensible to any number of processors. As seen from Figure 2, the memory element under consideration is initially in the Never Referenced state. The first access by P1 or P2 causes a compulsory (cold) miss and the memory element enters the Referenced state for the first time in the life cycle. Any subsequent references (by any processor) to a memory element in the Referenced state leads to a cache hit. Further, a replacement of the cache block takes the memory element into the Replaced state. We tag the memory element with the id of the processor which replaced the element. For instance, a memory element which is evicted from the cache as a result of a reference from P1 is in Replaced(P1) state. It is evident that all non-compulsory cache misses to a memory element occur when it is in the Replaced state. Our classification of the non-compulsory misses is based on whether the cache miss is occurring because of the block being replaced (at an earlier point in time) by the same processor or a different processor. This is deciphered by comparing the processor facing the miss with the tag of the memory element in the Replaced state.
It is important to note that the classification of non-compulsory misses into intra-processor misses and inter-processor misses in the CII classification is orthogonal to the classification of the same as capacity and conflict misses. For instance, the examples discussed with reference to Figure 1, in case of a fully associative cache, represent (a) capacity miss that is also an intra-processor miss and (b) capacity miss that is also an inter-processor miss. Conflict misses can also be classified as intra-processor misses and interprocessor misses by the CII classification. Our CII classification is more expressive; and more importantly, it is able to model the interactions between transactions of multiple processors at the level of the shared cache.
We measured the distribution of various classes of misses in the CII classification. Figure 3 plots the distribution of compulsory, inter-processor and intra-processor misses in our base system configuration (see Section 5 for a detailed description of our baseline configuration). The black portion of the stacked bars represents the inter-processor misses, the spotted portion (in the middle) represents intra-processor misses and the striped portion represents the compulsory misses. On an average, 40.3% of the misses are interprocessor misses, 24.6% of the misses are intra-processor misses and the remaining 35.1% are compulsory misses.
Characterization of CII classification. We vary L2 cache size, L2 cache associativity and the number of processors individually from the baseline configuration in order to measure their impact on the distribution of various classes of CII cache misses. Graphs (a) and (b) of Figure 4 plot the normalized miss rates of various benchmarks by varying the size and associativity of the shared L2 cache. The miss rates are normalized to the total L2 cache miss rates of the respective benchmarks in the base configuration. Although the number of inter-processor and intra-processor misses in the L2 cache tend to decrease with increasing associativity and the size of the L2 cache, the contribution of inter-processor misses to the non–compulsory misses (as a percentage of non–compulsory misses) increases and that of intra-processor misses decreases as associativity and size increase. We also studied the impact of the number of processors on the CII classification. In order to have a fair comparison (the private L1 caches of a different number
of processors lead to a different number of total L2 accesses) after varying the number of processors, we plot the variation in distribution of CII misses with the number of processors in Graph (c) of Figure 4. The contribution of inter-processor misses also increases with increasing number of processors. This is clearly due to the increased interaction between memory transactions of a higher number of processors in the shared L2 cache.
Reducing off–chip accesses is the key to a successful shared cache management scheme in a CMP with large shared L2/L3 cache (16). The effect of compulsory misses can be reduced by hiding their latency. This can be achieved by prefetching data into the cache before it is accessed. There have been many recent studies for reducing memory bandwidth and the number of off–chip accesses through hardware/software data prefetching (17; 24; 27; 34). The focus of this paper is on developing techniques to reduce inter-processor and intra-processor misses."	https://doi.org/10.1145/1346281.1346299	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Srikantaiah, et al. [NO] first presents a new classification of cache misses – cii : compulsory, inter-processor and intra-processor misses – for cmps with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a cmp. 
Exploring locking & partitioning for predictable shared caches on multi-cores	['Another study on cache locking for shared caches has assumed locking individual cache lines <NO>.', 'Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>.', 'They can be grouped in three main categories: cache partitioning, cache locking, and page coloring <NO>.', 'In <NO>, cache locking is explored for predictable shared caches on multi-core systems.', 'Locking has also been applied to shared caches in multi-cores in <NO>.', 'Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>.', 'A very different approach for multi-cores with shared instruction caches is proposed in <NO> and is based on the combined use of cache locking, i.', 'In contrast to <NO>, our approach does neither lock nor partition the shared instruction cache(s).', 'Experiments would be required to assess the respective merits of <NO> compared with our approach.']	Vivy Suhendra, and Tulika Mitra. 2008. Exploring locking & partitioning for predictable shared caches on multi-cores. Proc. of the 45th DAC. ACM, 300–303.	Multi-core architectures consisting of multiple processing cores on a chip have become increasingly prevalent. Synthesizing hard realtime applications onto these platforms is quite challenging, as the contention among the cores for various shared resources leads to inherent timing unpredictability. This paper proposes the use of shared cache in a predictable manner through a combination of locking and partitioning mechanisms. We explore possible design choices and evaluate their effects on the worst-case application performance. Our study reveals certain design principles that strongly dictate the performance of a predictable memory hierarchy.	"We consider a multi-core architecture consisting of identical cores. The on-chip memory is configured as two-level caches with a shared L2 cache, which is the focus of this work. We assume that the cache coherence is implemented in hardware, and that the caches support locking and set-based partitioning <NO> that allocates a number of sets (rows) to each task. This paper focuses on instruction cache, though our technique is equally applicable to data caches.
We adopt the classic real-time system model where a set of independent tasks {T1, T2, . . . , TK} execute periodically. Each task Ti is associated with a period pi, which defines its deadline, and a worst-case execution time ci. We choose partitioning <NO> strategy for homogeneous multiprocessor scheduling. In a partitioning strategy, once a task is allocated to a processor, it is executed exclusively on that processor. Any uniprocessor scheduling algorithm can then be applied on each processor. Partitioning strategy has the advantage of lower overhead compared to global strategy that allows migration of a task to a different processor at runtime.
López et al. <NO> show that the earliest deadline first (EDF) scheduling policy with First Fit (FF) allocation is an optimal partitioning approach with respect to utilization bounds. Our framework applies this policy. FF assigns a task to the first processor that can accept it. A task set is EDF-schedulable on uniprocessor if U ≤ 1, where U is the utilization of a task set {T1, T2, . . . , TK} given by U = ∑K i=1 ci pi
. The system utilization of a Q-core multiprocessor is Usystem = UQ . We measure the performance of a task set on a multiprocessor by the system utilization: the lower, the better.
We separate the treatment of the private L1 caches and the shared L2 cache, in order to observe the shared cache behavior while abstracting out the effects of the L1 caches. As our focus is on the shared cache, we choose a simple static locking scheme for L1. The private L1 cache attached to a core is utilized only by the tasks executing on that core; for each, we adopt the cache content selection algorithm for multitasking systems <NO>. The chosen blocks for L1 will be excluded during content selection for the L2 cache.
The shared L2 cache opens up the opportunity to combine different locking and partitioning schemes as shown in Figure 1(e). For cache locking, we can choose a static scheme (cache content remains unchanged throughout execution) or a dynamic scheme (cache content can be reloaded at runtime) For cache partitioning, we have the choice of (1) no partition, where a cache block may be occupied by any task, scheduled on any core; (2) task-based partition, where each task is assigned a portion of the cache; or (3) core-based partition, where each core is assigned a portion of the cache, and each task scheduled on that core may occupy the whole portion while it is executing. From these, the {dynamic locking, no partition} combination must be ruled out, because dynamic locking strictly requires a dedicated partition. Further, both the {static locking, no partition} (SN) and the {static locking, task-based partition} (ST) schemes lock the cache contents chosen from all tasks in the application throughout execution, but SN offers more flexibility by not enforcing a concrete boundary. Thus ST is either inferior or at most as good as SN; we eliminate ST from our evaluation.
Figure 1(a–d) illustrates the four eligible possibilities, applied on a multi-core with 2 processing elements (PE1, PE2) and 4 independent tasks (T1, . . . T4). The scheduler assigns T1, T2 to PE1 and T3, T4 to PE2. T1 and T4 are each divided into two regions for dynamic cache locking. We assume a 2-way set-associative shared L2 cache with 8 sets.
Static Locking, No Partition (SN). This is the simplest scheme where the cache content is kept unchanged throughout application runtime (Figure 1(a)). A cache block can be assigned to any task irrespective of the processor it is scheduled on. This scheme offers maximum flexibility; however, its performance is restricted if the code size of all the tasks together far exceeds the L2 cache size. For static locking, we apply the cache content selection algorithm presented in <NO>, which minimizes the system utilization.
Static Locking, Core-based Partition (SC). On a system with preemptive scheduling, only memory blocks belonging to the “active"" tasks are useful at any time. When a task Ti is preempted by Ti′ on one of the cores, we can replace Ti’s memory blocks in the cache with those of Ti′ ’s. This comes at the cost of reloading the cache at every preemption. This scheme requires the cache to be partitioned among the cores, as each core has an active task at any point of time and the cores invoke preemptions independently. However, when a task runs on a core, it can occupy the entire partition for that core. In Figure 1(b), PE1 gets the first four sets; PE2 gets the rest. Initially T1 occupies PE1’s partition and T4 occupies PE2’s partition. When T2 preempts T1, it loads and locks PE1’s partition with its own content. We adapt a dynamic programming based optimal partitioning algorithm <NO> here.
Dynamic Locking, Task-based Partition (DT). Dynamic locking allows more memory blocks to fit in the cache via runtime load and lock. The overhead is the cache reload cost every time the execution of a task moves from one region to another. As different tasks have different region formations, the cache is first partitioned among the tasks. Each task then performs dynamic locking within its partition. Figure 1(c) shows the scheme at work for T1 and T4. In contrast to SC, reloading is performed intra-task. No intertask reloading is required as the partitioning prevents interference among the tasks, thus preemptions incur no cache reload overhead. However, if the application comprises a large number of tasks, such rigid partitioning might not be effective. DT also suffers from the same drawback as SN: tasks occupy cache blocks even when they are inactive (preempted). We employ the dynamic locking algorithm in <NO> here.
Dynamic Locking, Core-based Partition (DC). In this most complex scheme, reloading is supported within a task in addition to reloading at preemption (see Figure 1(d)). Initially, the cache is loaded with region 1 of T1 and region 1 of T4. As time progresses, T1’s execution (on PE1) moves to region 2, which then replaces the content of PE1’s portion of the cache. Later on, a preemption brings into the cache the content associated with T2. However, when T1 resumes, it again brings in region 2 into the cache."	https://doi.org/10.1145/1391469.1391545	3	['locking', 'locked', 'core']	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Suhendra, et al. [NO] proposes the use of shared cache in a predictable manner through a combination of locking and partitioning mechanisms. 
Dynamic allocation for scratch-pad memory using compile-time decisions	['Approaches for static <NO> and dynamic <NO> allocations have been designed to automatically place code and data on scratchpad memories.', 'Previous approaches for dynamic scratchpad allocation <NO> have focused on the optimization of the average case.', '<NO> proposes to restrict memory transfer operations to interesting program points, such as functions, conditionals or loops entries/exits with high execution frequencies in a flexible way.', 'Moreover, <NO> associates execution timestamps to program points in order to capture program execution context.', '<NO> has shown that such a mechanism has a minor impact on program performance.', '<NO> also addresses major implementations issues on static data and stack data relocation for dynamic scratchpad memory allocation.', 'This idea has been successfully applied within tasks for SPM <NO>, <NO>.', '<NO>, <NO>) could have done some of the required work.']	S. Udayakumaran, A. Dominguez, and R. Barua. 2006. Dynamic allocation for scratch-pad memory using compile-time decisions. ACM Transactions on Embedded Computing Systems (TECS) 5, 2 (May 2006).	In this research, we propose a highly predictable, low overhead, and, yet, dynamic, memoryallocation strategy for embedded systems with scratch pad memory. A scratch pad is a fast compilermanaged SRAM memory that replaces the hardware-managed cache. It is motivated by its better real-time guarantees versus cache and by its significantly lower overheads in energy consumption, area, and overall runtime, even with a simple allocation scheme. Primarily scratch pad allocation methods are of two types. First, software-caching schemes emulate the workings of a hardware cache in software. Instructions are inserted before each load/store to check the software-maintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption, and SRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of algorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation schemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that never changes at runtime cannot achieve the full locality benefits of a cache. We propose a dynamic allocation methodology for global and stack data and program code that; (i) accounts for changing program requirements at runtime, (ii) has no software-caching tags, (iii) requires no runtime checks, (iv) has extremely low overheads, and (v) yields 100% predictable memory access times. In this method, data that is about to be accessed frequently is copied into the scratch pad using compiler-inserted code at fixed and infrequent points in the program. Earlier data is evicted if necessary. When compared to a provably optimal static allocation, results show that our scheme reduces runtime by up to 39.8% and energy by up to 31.3%, on average, for our benchmarks, depending on the SRAM size used. The actual gain depends on the SRAM size, but our results show that close to the maximum benefit in runtime and energy is achieved for a substantial range of small SRAM sizes commonly found in embedded systems. Our comparison with a direct mapped cache shows that our method performs roughly as well as a cached architecture.	"Memory systems generally are organized using a variety of devices, which serve different purposes. Devices like SRAM and ROM are fast, but expensive. On the other hand, devices like DRAM and tape drives are slower, but, being cheaper, can be used to provide capacity. Designing a memory system, therefore, involves using small amounts of faster devices like SRAM along with slower devices like DRAM to obtain satisfactory performance, while keeping a check on the overall dollar cost.
In desktops, the usual approach to adding SRAM is to configure it as a hardware cache. The cache dynamically stores a subset of the frequently used data. Caches have been a big success for desktops, a trend that is likely to continue in the future. Using noncached SRAM or scratch-pads is usually not feasible for desktops; one reason is the lack of binary code portability. Scratch-pad code is not portable across different sizes of scratch-pad because all existing compiletime methods for data1 allocation to scratch-pad (including the method in this paper) require that the scratch-pad size be known; otherwise, they cannot reason about what variables will fit in the scratch-pad. This contrasts with cache allocation, which is decided only at runtime and, hence, does not require compiletime knowledge of the size of cache. Binary portability is valuable for desktops, where independently distributed binaries must work on any cache size. However, in embedded systems, software is configured along with the hardware in the factory and rarely changes thereafter. Thus embedded system designers can afford to customize the SRAM to a particular size to reap the additional cost savings from customization.
For embedded systems, the serious overheads of caches are less defensible. An alternative that is, instead, prevalent is to use compiler-managed SRAM or scratch-pad. Studies <NO> have shown that scratch-pad’s use 34% lesser area and consume 40% lower power than a cache of the same capacity. These savings are significant, since the on-chip cache typically consumes 25–50% of the processor’s area and energy consumption, a fraction that is increasing with time <NO>. Given the power, cost, performance, and real-time advantages of scratch-pad, it is not surprising that scratch-pads are the most common form of SRAM in embedded CPUs today. Some examples of processors with scratch-pad memory are Intel IXP network processor, ARMv6, IBM 440 and 405, Motorola’s MCORE and 6812, and TI TMS-370. Trends in recent embedded designs indicate that the dominance of scratch-pad will likely consolidate further in the future <NO>.
Although many embedded processors with a scratch-pad exist, using the scratch-pad effectively has been a challenge. Central to the effectiveness of caches is their ability to maintain, at each time during program execution, the subset of data that is frequently used at that time in fast memory. The contents of cache constantly change during runtime to reflect the changing working set of data across time. Unfortunately, two of the existing allocation
1We use the terms data and program objects to broadly refer to both program code and program data. approaches for scratch-pad—program annotations and the recent compilerdriven approaches <NO>—are static allocators, i.e., they do not change the contents of scratch-pad at runtime. This is a serious limitation. For example, consider the following thought experiment. Let a program consist of three successive loops, the first of which makes repeated references to array A; the second to B; and the third to C. If only one of the three arrays can fit within the scratch-pad, then any static allocation suffers DRAM accesses for two out of three loops. In contrast, a dynamic strategy can fit all three arrays in the scratch-pad at different times. Although this example is oversimplified, it intuitively illustrates the benefits of dynamic allocation.
This research presents a new compiler method for allocating three types of program objects—global variables, stack variables, and program code—to scratch-pad that is able to change the allocation at runtime and avoid the overheads of runtime methods. A preliminary version of our method published in Udayakumaran and Barua <NO> was the first such method to allocate global and stack data using whole program analysis. Our method (i) accounts for changing program requirements at runtime, (ii) has no tags like that used by runtime methods, (iii) requires no runtime checks per load/store, (iv) has extremely low overheads, and (v) yields 100% predictable memory access times.
Our method is outlined as follows. The compiler analyzes the program to identify locations we call program points where it may be beneficial to insert code to copy a variable from DRAM into the scratch-pad. It is beneficial to copy a variable into scratch-pad if the latency gain from having it in scratchpad rather than DRAM is greater than the cost of its transfer. A profile-driven cost model estimates these benefits and costs. The compiler ensures that the program data allocated to scratch-pad fits at all times by occasionally evicting existing variables in scratch-pad to make space for incoming variables. In other words, just like in a cache, data is moved back and forth between DRAM and scratch-pad, but under compiler control, and with no additional overhead.
Key components of our method are as follows: (i) to reason about the contents of scratch-pad across time, it helps to attach a concept of time to the abovedefined program points. Toward this end, we introduce a new data structure called the Data-Program Relationship Graph (DPRG), which associates a timestamp with each program point. As far as we know, this is the first time that a static data structure to represent program execution time has been defined. (ii) A detailed cost model is presented to estimate the runtime cost of any proposed data transfer at a program point. (iii) A compile-time heuristic is presented that uses the cost model to decide which transfers minimize the runtime. The well-known data-flow concept of liveness analysis <NO> is used to eliminate unnecessary transfers—probably dead variables2 are not
2In compiler terminology, a variable is dead at a point in the program if the value in it is not used beyond this point, although the space could be. A dead variable becomes live later if it is written to with subsequently used data. As a special case it is worth noting that every uninitialized variable is dead at the beginning of the program. It becomes live only when written to first. Further, a variable may have more than one live range separated by times when it is dead. copied back to DRAM, nor are newly alive variables in this region copied in from DRAM to SRAM.3 In programs, where the final results (only global) need to be left in the memory itself, this optimization can be turned off, in which case the benefits would be reduced.4 This optimizations also needs to be turned off for segments shared between tasks.
We observe three desirable features of our algorithm: (i) No additional transfers beyond those required by a caching strategy are done; (ii) data that is accessed only once is not brought into the scratch-pad, unlike in caches, where the data is cached and potentially useful data evicted. This is particularly beneficial for streaming multimedia codes where use-once data is common. (iii) Data that the compiler knows to be dead is not written out to DRAM upon eviction, unlike in a cache, where the caching mechanism writes out all evicted data.
Our method is clearly profile-dependent; that is, its improvements are dependent upon how representative the profile data set really is. Indeed, all existing scratch-pad allocation methods, whether compiler-derived or programmerspecified, are inherently profile-dependent. This cannot be avoided since they all need to predict which data will be frequently used. Further our method does not require the profile data to be like the actual data in all respects—so long as the relative reuse trends between variables are similar in the profile and actual data, good allocation decisions will be made, even if the reuse factors are not identical. A regions gain may even be higher with nonprofile data if its data reuse is more than in the profile data."	https://doi.org/10.1145/1151074.1151085	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Udayakumaran, et al. [NO] proposes a highly predictable, low overhead, and, yet, dynamic, memoryallocation strategy for embedded systems with scratch pad memory. 
Molecular Caches: A caching structure for dynamic creation of application-specific Heterogeneous cache regions	[]	K. Varadarajan, S.K. Nandy, V. Sharda, A. Bharadwaj, R. Iyer, S. Makineni, and D. Newell. 2006. Molecular Caches: A caching structure for dynamic creation of application-specific Heterogeneous cache regions. Proc. of the 39th MICRO. 433–442.	CMPs enable simultaneous execution of multiple applications on the same platforms that share cache resources. Diversity in the cache access patterns of these simultaneously executing applications can potentially trigger inter-application interference, leading to cache pollution. Whereas a large cache can ameliorate this problem, the issues of larger power consumption with increasing cache size, amplified at sub-100nm technologies, makes this solution prohibitive. In this paper, in order to address the issues relating to power-aware performance of caches, we propose a caching structure that addresses the following: 1. Definition of application-specific cache partitions as an aggregation of caching units (molecules). The parameters of each molecule namely size, associativity and line size are chosen so that the power consumed by it and access time are optimal for the given technology. 2. Application-Specific resizing of cache partitions with variable and adaptive associativity per cache line, way size and variable line size. 3. A replacement policy that is transparent to the partition in terms of size, heterogeneity in associativity and line size. Through simulation studies we establish the superiority of molecular cache (caches built as aggregations of molecules) that offers a 29% power advantage over that of an equivalently performing traditional cache.	"This section explains in brief some of the related work that has been carried out in the context of QoS in caches and Cache Partitioning.
Iyer <NO> in a recent publication presented a framework for enabling QoS in shared caches. The paper makes a key observation that the current design of caches is more suited towards single application memory access and is not well suited for newer computing paradigms such as CMPs, Simultaneous Multi-Threading (SMT) and Specialized Cores. The paper then proposes a framework for Cache QoS management that includes three aspects namely Priority Classification, Priority Assignment and Priority Enforcement. The Priority Enforcement is of particular interest to us. For priority enforcement three different schemes are proposed. These include static and dynamic partitioning of caches, selective cache allocation and heterogeneous cache regions. 
We only elaborate the technique for heterogeneous cache regions in this paper. In this technique, caches are composed of multiple caching units with different organizational structures and policies. The problem is that of assigning the right application to the appropriate cache structure, keeping in mind the priority of the application and its memory access pattern. The results from the paper indicates that heterogeneous cache regions help reduce dedicated cache size.
Cache Partitioning has been greatly researched, even before the arrival of CMPs. Of the several flavours of cache partitioning, multi-application dynamic partitioning techniques are of particular interest to us. Single application partitioning for low power in the context of direct mapped caches is reported in <NO>. In this section we discuss examples of both the hardware controlled and the software controlled variants. Several solutions have been proposed that address the issue of cache pollution through the use of NUCA architectures. Techniques such as Victim Replication <NO>, NuRAPID <NO>, CMP-NuRAPID<NO> address several different aspects of these. However none of these use application QoS parameters as the guiding principle. They attempt to reduce the miss rate experienced by a processor.
Suh et al <NO> propose the use of two different techniques for implementing partitioned caches namely Modified LRU and Column Caching. In Modified LRU scheme, the replacement decision depends on the number of cache blocks already allocated to a particular process. If the process has not exceeded its predefined space threshold, a global replacement is performed, else a local replacement is performed. The column caching approach restricts some processes to place data in some ’columns’ (i.e. ways) of a multi-way associative cache. Their results indicate that for a cache of size 1-2MB the hit rate improves by 40% as compared to standard LRU for a time quantum of 200,000 memory references and weighting factor of 0.5 (which is used to give preference to recent measurements as opposed to older ones). Compared with the standard LRU, relative IPC improvements of 13.29% for 2 processes on a 2MB cache and 14.21% for 4 processes on 2 MB cache are recorded. Kim et al <NO> propose a variant of the Modified LRU technique where fairness is taken as a measure of cache allocation. As indicated in <NO>, cache pollution violates operating system expectation that all scheduled entities get an equal chance to utilize the processor, hence fairness of allocation is required. We take this argument forward, since fairness weighted by application’s priorities is far more important. Yet another technique that takes fairness into consideration was proposed by Yeh et al <NO>. The approach used in this technique is very similar to our approach. The authors propose a phase-based approach for cache partition resizing.
Kim et al <NO> proposed a partitioning technique in which different banks of a multi-banked cache are allocated to
different processes. First a search of the ’home’ bank is performed, failing which the entire cache is searched in a set-associative manner. Called Process Ownership-based Cache Architecture (POCA), this cache contains four components viz. Current Set Number, Set Assignment Table, Data Enable Controller and the Victim Process Table. The authors <NO> used ATUM traces to evaluate the cache. Simulation performed on cache of size from 8KB-1MB with 4 banks indicate that the hit rate of the cache is higher than a direct mapped cache of equal size and is either as good or better than a 4 way associative cache of equal size. The cache access time of POCA is 10% better than an equivalent associative cache.
Is there a need for something better than the state of the art? Suh et al’s <NO> proposed cache partitioning solution does not look into the dimension of heterogeneous cache regions that can potentially improve the efficiency of the cache usage. A major drawback of their cache architecture is the reliance on multi-way associative caches. Multiway associative caches tend to consume a lot of power and have longer access time. Hence associativity cannot be increased beyond a certain value, since the power consumption increases exponentially with increase in associativity (refer <NO>). Kim et al’s <NO> work is a software based resizing solution and relies on operating system based information for determining the victim sets. Such a scheme would not be practically realizable due to the non-standard ways of obtaining the required information from the large variety of operating systems available today.
The work on heterogeneous cache regions by Iyer <NO> is based on the use of multiple caches each with different cache parameters and the optimal assignment of applications to appropriate caches with suitable cache parameters. This technique would only enable some applications to run efficiently. We extend this work by marrying the ideas of dynamic partitioning and heterogeneous cache regions to create more effective dynamic partitions that are dynamically customized to the application’s needs. We create caching structures that allow application dependent dynamic creation and reconfiguration of heterogeneous cache regions with different associativities and line sizes. The capacity of these regions too can be modified. The cache regions are created from homogeneous building blocks that are direct mapped. These cache regions also support nonuniform line associativity."	https://doi.org/10.1109/MICRO.2006.38	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Varadarajan, et al. [NO] proposes a caching structure that addresses the following : 1. definition of application-specific cache partitions as an aggregation of caching units (molecules). 
Data Cache locking for higher program predictability	['In the context of data caches, <NO> tries to balance the performance and predictability tradeoff introduced due to locking, by applying it only on parts of the program that are difficult to analyze statically.', '<NO> combine compile-time cache analysis and data cache locking in order to estimate a safe and tight worst-case memory performance.', 'Instruction cache locking has been primarily employed in hard real time systems for better timing predictability <NO>.', 'The work of <NO> is complementary to this paper since it presents a D-cache locking algorithm for WCET minimization.']	Xavier Vera, Björn Lisper, and Jingling Xue. 2003. Data Cache locking for higher program predictability. Proc. of ACM SIGMETRICS. ACM, New York, NY, 272–282.	Caches have become increasingly important with the widening gap between main memory and processor speeds. However, they are a source of unpredictability due to their characteristics, resulting in programs behaving in a different way than expected. Cache locking mechanisms adapt caches to the needs of real-time systems. Locking the cache is a solution that trades performance for predictability: at a cost of generally lower performance, the time of accessing the memory becomes predictable. This paper combines compile-time cache analysis with data cache locking to estimate the worst-case memory performance (WCMP) in a safe, tight and fast way. In order to get predictable cache behavior, we first lock the cache for those parts of the code where the static analysis fails. To minimize the performance degradation, our method loads the cache, if necessary, with data likely to be accessed. Experimental results show that this scheme is fully predictable, without compromising the performance of the transformed program. When compared to an algorithm that assumes compulsory misses when the state of the cache is unknown, our approach eliminates all overestimation for the set of benchmarks, giving an exact WCMP of the transformed program without any significant decrease in performance.	"Real-time systems rely on the assumption that tasks’ worstcase execution times (WCETs) are known. In order to get an accurate WCET, a tight worst-case memory performance (WCMP) is needed. However, cache behavior is very hard to predict, which leads to an overestimation of the WCMP, and thus for the WCET as well. For this reason, many safety-critical systems such as antilock brake systems do not use caches: it is very hard to prove that the system is reliable under all circumstances. For instance, the ARM966E-S processor does not have a cache in order to have predictable memory timings.
When using caches in hard real-time systems, there is an unacceptable possibility that a high cache miss penalty combined with a high miss ratio might cause a missed deadline, jeopardizing the safety of the controlled system. A system with disabled caches will however waste a lot of resources; not only will the CPU be underutilized but the power consumption will be higher. Memory accesses that fall into the cache are faster and consume less power than accesses to larger or off-chip memories.
Frameworks of WCET prediction are used to ensure that deadlines of tasks can be met. While the computation of WCET in the presence of instruction caches has progressed in such a way that makes it possible to obtain an accurate estimate of the WCET <NO>, there has not been much progress with the presence of data caches. The main problem when dealing with data caches is that each load/store instruction may access multiple memory locations (such as those that implement array or pointer accesses).
Cache locking allows some or all of the contents of the cache to be locked in place. Disabling the normal replacement mechanism, provided that the cache contents are known, makes the time required for a memory access predictable. This ability to lock cache contents is available on several commercial processors (PowerPC 604e <NO>, 405 and 440 families <NO>, Intel-960, some Intel x86, Motorola MPC7400 and others). Each processor implements cache locking in several ways, allowing in all cases static locking (the cache is loaded and locked at system start) and dynamic locking (the state of the cache is allowed to change during the system execution).
Whereas loading and locking the cache offers predictability, it does not guarantee good response time of tasks (thus, we are trading performance for predictability). On the other hand, static cache analysis allows us to predict the WCMP and does not affect the performance. However, static analyses only apply to codes free of data-dependent constructs.
We introduce a method that combines static cache analysis and cache locking in order to achieve both predictability and good performance. Furthermore, it allows computing a WCMP estimate of tasks in a fast and tight way. Our approach first transforms the original program issuing lock/unlock instructions to ensure a tight analysis of the WCMP at static time. In order to keep a high performance, load instructions are added when necessary. Later, the actual computation of the WCMP estimate is performed. We present results for a collection of programs drawn from several related papers in the real-time area <NO>. This collection includes kernels operating on both arrays and scalars, such as SQRT or FIBONACCI. We have also used FFT to show the feasibility of our approach for typical DSP codes. For the sake of concreteness, we present results for a directmapped and a set-associative cache with different cache line sizes. We have chosen the memory hierarchies of two modern processors widely used in the real-time area: microSPARCIIep <NO> and PowerPC 604e <NO>."	https://doi.org/10.1145/781027.781062	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Vera, et al. [NO] combines compile-time cache analysis with data cache locking to estimate the worst-case memory performance (wcmp) in a safe, tight and fast way. 
Making shared caches more predictable on multicore platforms	['While LLC offers high potential for average performance improvement, it challenges worst-case execution time (WCET) estimation, which has made LLC to be studied in the last years by the real-time community <NO>.', 'Software cache partitioning <NO> and hardware cache partitioning <NO> have been used so far to control inter-task interaction in the LLC.', 'Software cache partitioning is done through memory coloring <NO>.']	B.C. Ward, J.L. Herman, C.J. Kenna, and J.H. Anderson. 2013. Making shared caches more predictable on multicore platforms. Proc. of the 25th ECRTS. 157–167.	In safety-critical cyber-physical systems, the usage of multicore platforms has been hampered by problems due to interactions across cores through shared hardware. The inability to precisely characterize such interactions can lead to worst-case execution time pessimism that is so great, the extra processing capacity of additional cores is entirely negated. In this paper, several techniques are proposed and analyzed for dealing with such interactions in the context of shared caches. These techniques are applied in a mixedcriticality scheduling framework motivated by the needs of next-generation unmanned air vehicles.	"In this section, we provide background on scheduling, synchronization, and page coloring that is relevant to our work.
Task model. We consider real-time workloads that can be defined using the implicit-deadline periodic task model and
1Enabling even a quad-core machine to be used in an avionics setting would be a significant innovation.
we assume familiarity with this model. We specifically consider a task system τ = {T1, . . . , Tn}, which is to be scheduled on m processors,2 where task Ti’s period and worstcase execution time (WCET) are denoted pi and ei, respectively. We denote the jobs released by Ti as Ji,1, Ji,2, . . .. (We sometimes omit the job index and let Ji denote an arbitrary job of Ti.) We denote Ti’s utilization by ui = ei/pi. Algorithms for scheduling such a task system may follow a partitioned approach (tasks are statically assigned to processors), a global scheduling approach (any task may execute on any processor), or some hybrid of the two.
MC2. We consider periodic task systems scheduled under the MC2 mixed-criticality framework <NO>. Under mixed-criticality schedulability analysis <NO>, different methods for determining task execution times are assumed to be applied at different criticality levels, with greater pessimism at higher levels. For example, provably correct upper bounds on execution times from timing analysis tools might be assumed at the highest criticality level, while observed worst-case times from profiling might be sufficient at lower levels. When checking real-time correctness, L variants of a system with L criticality levels must be analyzed: in the level-l variant, level-l execution times are assumed for all tasks.
In MC2, four criticality levels exist, denoted A (highest) through D (lowest), as shown in Fig. 1. Higher criticality tasks are statically prioritized over lower criticality ones. Level-A tasks are partitioned and scheduled on each processor using a table-driven cyclic executive. Level-B tasks are also partitioned but are scheduled using a rate-monotonic (RM) scheduler on each processor.3 Level-A and -B tasks are required to be simply periodic (all tasks commence execution at time 0 and periods are harmonic). Level-C tasks are scheduled via a global earliest-deadline-first (G-EDF) scheduler. Level-D tasks are scheduled with no real-time guarantees on a best-effort basis (so we do not consider them further). A task’s execution time at its own criticality level is treated as an operating-system (OS) enforced execution budget: if a job of a task Ti has an execution time exceeding Ti’s budget, then more than one budget allocation will be required to service it. Level-A and -B tasks are HRT, while level-C tasks are SRT (under the “bounded deadline tardiness” definition of SRT <NO>). Most interesting cacherelated issues are exposed by focusing only on levels B and
2We use the terms “processor,” “core,” and “CPU” interchangeably. 3An EDF scheduler can be optionally used at level B.
C (one HRT level and one SRT level). Thus, due to space constraints, we hereafter focus on systems in which only levels B and C are present. We note that systems with two criticality levels have been the predominate focus of prior work on mixed-criticality scheduling (see, e.g., <NO>). Page coloring. The ARM platform used in our experiments has four cores that share an L2 cache. In this paper, we consider page coloring with respect to this cache. The L2 cache on this platform is a 1 MB 8-way set associative cache: it stores contents of physical memory in 32 B units called “lines,” each line of physical memory maps to a particular cache “set,” each such set can store 8 lines (equivalently, there are eight “ways” per set), and in total there are 212 sets. The physical memory of this platform is subdivided into 4 KB pages. To envision the coloring process, consider each page in sequence. For the first page in memory, assign the color “0” to it, and assign the same color to the cache sets to which its contents map. Then, since each page consists of 4KB/(32B/line) = 128 lines, sets 1 − 128 are assigned color 0. Repeat this process, assigning color 1 (mapping to sets 129− 256) to the second page in memory, color 2 (sets 257− 384) to the third page, and so on. Then, after the 32nd page, all 212 sets will have been used and color assignments will “wrap,” i.e., the 33rd page will map to the same cache sets as the first, so we reuse color 0 for it. Continuing this process, each page will be assigned to one of 32 colors. Moreover, two pages that are assigned different colors will map to different cache sets and thus cannot conflict with each other in the cache.
In Sec. 3, we consider techniques that exploit page coloring to eliminate or control cache conflicts. In discussing these techniques, we limit attention to non-shared task data pages, as only these pages are managed in the initial prototype system described in Sec. 4. We define the working set size (WSS) of a task to be the size (in bytes) of the set of data pages it may access in one job, i.e., the size of its perjob working set (WS). We assume that each task’s WSS is at most the size of the shared cache. Multiprocessor real-time locking. Some of the cache management schemes we consider utilize multiprocessor real-time locking protocols. In the protocols we consider, tasks wait by suspending execution. Locking protocols must ensure that priority inversion blocking (pi-blocking) can be analytically bounded. Pi-blocking is the duration of time a job is blocked while a lower-priority job is running. Per-task bounds on pi-blocking are required when analyzing schedulability. We let bi denote the pi-blocking bound for task Ti.
On a multiprocessor system, the actual definition of pi-blocking depends on how schedulability analysis is done <NO>. For some schedulers, suspensions are notoriously difficult to analyze, so suspension-oblivious (s-oblivious) analysis is applied: jobs may suspend, but each ei must be analytically inflated by bi prior to applying a schedulability test to account for lock-related delays. We utilize s-oblivious analysis in this paper. Some of the nuances of such analysis can best be explained by comparing it to suspension-aware (s-aware) analysis, which explicitly ac-
counts for bi and is available for some schedulers. Since suspended jobs are counted as demand under soblivious analysis, the mere presence of m higher-priority jobs rules out a priority inversion, whereas only ready higher-priority jobs can nullify a priority inversion under s-aware analysis.4 Accordingly, under s-oblivious (resp., saware) schedulability analysis, a job Ji incurs s-oblivious (resp., s-aware) pi-blocking at time t if Ji is pending but not scheduled and fewer than m higher-priority jobs are pending (resp., ready). This is illustrated in Fig. 2. Prior research has shown that s-aware and s-oblivious analysis are comparable in terms of schedulability achievable in practice <NO>. Cache-related locking problem. We now describe the basic synchronization problem that arises when using locking protocols for cache management (protocol-specific details are discussed in Sec. 3). When using such protocols, each color is viewed as a shared resource that has a number of “replicas” as given by the number of cache ways, as illustrated in Fig. 3. Before a job commences execution, it must first lock a replica of each color that it requires (as given by the pages it will access). If the job accesses r pages with the same color, then it must lock r replicas of that color. The needed synchronization protocol must enable a set of shared resources to be managed, where each resource has multiple replicas, and jobs may need to lock several replicas simultaneously. In actuality, such a protocol is utilized by the OS when making scheduling decisions, i.e., the jobs themselves do not acquire and release color-related locks. This means that the OS must know the pages a job will access prior to making a scheduling decision."	https://doi.org/10.1109/ECRTS.2013.26	2	['workload', 'priority', 'platform']	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Ward, et al. [NO], several techniques are proposed and analyzed for dealing with such interactions in the context of shared caches. 
Implementing time-predictable load and store operations	['The issues are tricky, but they have already been investigated during our earlier work on the scratchpad memory management unit (SMMU) <NO>.', 'This table is based on data published in <NO>.']	J. Whitham, and N. Audsley. 2009. Implementing time-predictable load and store operations. Proc. EMSOFT. 265–274.	Scratchpads have been widely proposed as an alternative to caches for embedded systems. Advantages of scratchpads include reduced energy consumption in comparison to a cache and access latencies that are independent of the preceding memory access pattern. The latter property makes memory accesses time-predictable, which is useful for hard real-time tasks as the worst-case execution time (WCET) must be safely estimated in order to check that the system will meet timing requirements. However, data must be explicitly moved between scratchpad and external memory as a task executes in order to make best use of the limited scratchpad space. When dynamic data is moved, issues such as pointer aliasing and pointer invalidation become problematic. Previous work has proposed solutions that are not suitable for hard real-time tasks because memory accesses are not time-predictable. This paper proposes the scratchpad memory management unit (SMMU) as an enhancement to scratchpad technology. The SMMU implements an alternative solution to the pointer aliasing and pointer invalidation problems which (1) does not require whole-program pointer analysis and (2) makes every memory access operation time-predictable. This allows WCET analysis to be applied to hard-real time tasks which use a scratchpad and dynamic data, but results are also applicable in the wider context of minimizing energy consumption or average execution time. Experiments using C software show that the combination of an SMMU and scratchpad compares favorably with the best and worst case performance of a conventional data cache.	"Figure 1 shows the contents of the ycc rgb convert function from libjpeg <NO>. This code forms part of the process of decoding a JPEG file into RGB data for display on a screen. It has been chosen as an example because previous techniques would force a time-predictable implementation to use external memory, with a high latency for each access.
Firstly, none of the data used by this function would be suitable for data scratchpad allocation using techniques described by Suhendra et al. <NO> or Deverge and Puaut <NO>
because each variable is accessed using a pointer from dynamically allocated memory. The surrounding library code would change significantly to accommodate static allocation.
Secondly, a time-predictable implementation could not use a data cache. The pointer values are unknown and some of the effective addresses are dependent on input data (e.g. Crrtab<NO>, since cr := inptr2<NO>). Current WCET analysis techniques for data caches would force the majority of memory accesses in the function to use external memory <NO>.
Future WCET analysis techniques for data caches might be able to support code of this kind. However, the results of tight WCET analysis would still be disappointing, as Table 2 illustrates. In Table 2, the approximate “best” and “worst” numbers of cache misses (and related data) are shown for Figure 1 and two cache sizes. These were computed using a genetic algorithm which searched the space of possible offsets for each array used in Figure 12. The algorithm used the measured execution time as a fitness value and attempted to minimize (or maximize) it, finding an estimate for the best (or worst) case. The huge difference between the “best” and “worst” cases observed here is entirely due to conflict misses <NO>. A conflict miss occurs in any data cache whenever two or more items of data are competing for a single cache line. The combination of a data cache and full support for pointers forces WCET analysis to account for all possible conflict miss scenarios (or use some other safe upper bound). Although the figures in Table 2 are specific to this example, a similar disparity between the best and worst case will be found whenever more than n unknown addresses are being accessed within a loop given an n-way associative cache.
The remainder of this paper shows how the SMMU is able to support code such as Figure 1 and approach the “best” case of Table 2 for any input data and any pointer values."	https://doi.org/10.1145/1629335.1629371	1	['data', 'size', 'partitioning']	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Whitham, et al. [NO] proposes the scratchpad memory management unit (smmu) as an enhancement to scratchpad technology. 
Explicit reservation of local memory in a predictable, preemptive multitasking real-time system	[]	Jack Whitham, and Neil C. Audsley. 2012. Explicit reservation of local memory in a predictable, preemptive multitasking real-time system. Proc. of the 2012 IEEE 18th Real Time and Embedded Technology and Applications Symposium. IEEE, 3–12.	This paper proposes Carousel, a mechanism to manage local memory space, i.e. cache or scratchpad memory (SPM), such that inter-task interference is completely eliminated. The cost of saving and restoring the local memory state across context switches is explicitly handled by the preempting task, rather than being imposed implicitly on preempted tasks. Unlike earlier attempts to eliminate inter-task interference, Carousel allows each task to use as much local memory space as it requires, permitting the approach to scale to large numbers of tasks. Carousel is experimentally evaluated using a simulator. We demonstrate that preemption has no effect on task execution times, and that the Carousel technique compares well to the conventional approach to handling interference, where worst-case interference costs are simply added to the worst-case execution times (WCETs) of lower-priority tasks.	"Inter-task interference may occur whenever tasks share a stateful resource such as a cache <NO> (e.g. Figure 1).
The problem of intra-task interference due to cache state is now quite well-known and has been studied thoroughly. Cache WCET analyses model the state of a cache at each point within a task in order to estimate the worst-case miss count, and hence the maximum execution time <NO>, <NO>, <NO>. Each cache state is dependent on earlier cache states. Earlier task activity may result in subsequent activity producing either a hit or a miss. This dependence is a form of interference, but intra-task interference, since it occurs between one part of a task and another.
Inter-task interference has also been examined <NO>, <NO>. This occurs between two or more tasks in a multitasking system. When two or more tasks share a cache, activity in one task can disturb data used by the other, producing hits or misses at unpredictable times. Inter-task analysis is pessimistic because the exact set of evicted and/or useful cache blocks of tasks cannot usually be computed offline.
Earlier work has prevented inter-task interference entirely by static partitioning: reserving local RAM space for each task <NO>, <NO>. Each task is only permitted to update its own partition (Figure 3). The remainder is locked <NO>. While the size of the partitions can vary between tasks, and non realtime tasks can share a single partition, the assignment is static. Tasks cannot use more than their fixed share of local RAM, not even temporarily. This is a problem, because tasks may have a suboptimal local RAM allocation <NO>. This situation becomes a near-certainty as the number of tasks increases.
Consequently, some researchers have suggested allowing inter-task interference, but bounding its impact. A number of approaches are evaluated in <NO>. They are suitable for timing-compositional systems where interference does not
cause any timing anomalies <NO>. They incorporate the cost of reloading evicted cache blocks into the worst-case response time equation <NO>.
A third approach involves explicitly saving and restoring the state of local RAM. This idea has been successfully applied within tasks for SPM <NO>, <NO>. It avoids intra-task interference by ensuring that the state of local RAM is known at each point within the task. But it is not trivial to expand the idea to multitasking. The large size of local RAM (several kilobytes or more) prevents saving and restoring the entire state on each context switch."	https://doi.org/10.1109/RTAS.2012.19	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Whitham, et al. [NO] proposes carousel, a mechanism to manage local memory space, i.e. 
WCET analysis for multi-core processors with shared L2 instruction caches	['To our best knowledge, the only known work on WCET analysis for multicores with shared cache is <NO>, which is only applicable to a special scenario and very simple hardware architecture (we will discuss its limitation in Section 2).', 'Yan and Zhang <NO> is the only known work to studied the WCET analysis problem for multicore systems with shared L2 cache.', 'Unlike <NO>, which accounts for all possible conflicts caused by tasks running on the other cores when estimating the WCET of a task, our approach drastically reduces the amount of inter-core interferences.', 'The method described in <NO> estimates the WCET in presence of shared caches on multi-cores by analysing inter-thread cache conflicts.', 'More generally, the method described in <NO> is expected to lack scalability with respect to the task size and number of tasks, because every conflict with every other task is considered.', 'Our proposed method, like <NO> identifies all inter-task interferences due to cache sharing for the sake of safety.', 'Our method is more general than <NO> in the sense that it supports multiple levels of shared caches, set-associative caches and an arbitrary number of realtime tasks and cores competing for the shared caches.', 'Note that the cache analysis method presented in this paragraph can be seen as a safe extension of the method presented in <NO> to set-associative caches, multiple levels of shared caches and an arbitrary number of tasks/cores competing for the shared caches.', 'This demonstrates the pessimism of methods such as the base method presented in Section 3 and the approach described in <NO>, that consider all interferences between cores without any mechanism to decrease inter-core interference.']	Jun Yan, and Wei Zhang. 2008. WCET analysis for multi-core processors with shared L2 instruction caches. Proc. of the 14th IEEE RTAS. 80–89.	Multi-core chips have been increasingly adopted by microprocessor industry. For real-time systems to safely harness the potential of multi-core computing, designers must be able to accurately obtain the worstcase execution time (WCET) of applications running on multi-core platforms, which is very challenging due to the possible runtime inter-core interferences in using shared resources such as the shared L2 caches. As the first step toward time-predictable multi-core computing, this paper presents a novel approach to bounding the worst-case performance for threads running on multi-core processors with shared L2 instruction caches. The idea of our approach is to compute the worst-case instruction access interferences between different threads based on the program control flow information of each thread, which can be statically analyzed. Our experiments indicate that the proposed approach can reasonably estimate the worstcase shared L2 instruction cache misses by considering inter-thread instruction conflicts. Also, the WCET of applications running on multi-core processors estimated by our approach is much better than the estimation by simply assuming all L2 instruction accesses are misses.	In a multi-core processor, each core typically has private L1 instruction and data caches. The L2 (and/or L3) caches can be either private or shared. While private L2 caches are more time-predictable in the sense that there are no inter-core L2 cache conflicts, each core can only exploit limited cache space. Due to the great impact of the L2 cache hit rate on the performance of multi-core processors <NO>, private L2 caches may have worse performance than shared L2 caches with the same total size, because each core with shared L2 cache can make use of the aggregate L2 cache space more efficiently. Moreover, shared L2 cache architecture makes it easier for multiple cooperative threads to share instructions, data and the precious memory bandwidth to maximize performance. Therefore, in this paper, we focus on studying WCET analysis of multi-core processors with shared L2 caches (by contrast, the WCET analysis for multicore chips with private L2 caches is a less challenging problem).	https://doi.org/10.1109/RTAS.2008.6	0	['shared', 'wcet', 'policy']	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Yan, et al. [NO] presents a novel approach to bounding the worst-case performance for threads running on multi-core processors with shared l2 instruction caches. 
Evaluation measures for ordinal regression	[]	Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Evaluation measures for ordinal regression. Ninth International Conference on Intelligent Systems Design and Applications, 2009. ISDA’09. IEEE, 283–287.	Ordinal regression (OR – also known as ordinal classification) has received increasing attention in recent times, due to its importance in IR applications such as learning to rank and product review rating. However, research has not paid attention to the fact that typical applications of OR often involve datasets that are highly imbalanced. An imbalanced dataset has the consequence that, when testing a system with an evaluation measure conceived for balanced datasets, a trivial system assigning all items to a single class (typically, the majority class) may even outperform genuinely engineered systems. Moreover, if this evaluation measure is used for parameter optimization, a parameter choice may result that makes the system behave very much like a trivial system. In order to avoid this, evaluation measures that can handle imbalance must be used. We propose a simple way to turn standard measures for OR into ones robust to imbalance. We also show that, once used on balanced datasets, the two versions of each measure coincide, and therefore argue that our measures should become the standard choice for OR. 	"Are the standard evaluation measures for OR robust to imbalance? The most commonly used such measures are
1) Mean Absolute Error (here denoted MAEµ, and also called ranking loss – see e.g., <NO>), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>. MAEµ is defined as the average deviation of the predicted class from the true class, i.e.,
MAEµ(Φ̂, T e) = 1 |Te| ∑ xi∈Te |Φ̂(xi)− Φ(xi)| (1)
where Te denotes the test set and the n classes in Y are assumed to be real numbers, so that |Φ̂(xi)−Φ(xi)| exactly quantifies the distance between the true and the predicted rank (the meaning of the µ superscript will be clarified later). 2) Mean Squared Error (MSEµ – also called Squared Error Loss), as used e.g., in <NO>, defined as
MSEµ(Φ̂, T e) = 1 |Te| ∑ xi∈Te (Φ̂(xi)− Φ(xi))2 (2)
A variant is Root Mean Square Error, as used e.g., in <NO>, which corresponds to the square root of MSEµ. 3) Mean Zero-One Error (more frequently known as Error Rate), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, and simply defined as the fraction of incorrect predictions, i.e.,
MZOEµ(Φ̂, T e) = |{xi ∈ Te : Φ̂(xi) 6= Φ(xi}|
|Te| (3)
Unlike MSEµ and MAEµ, MZOEµ has the disadvantage that all errors are treated alike, and thus insufficiently penalizes algorithms that incur into blatant errors. MSEµ penalizes blatant mistakes (e.g., misplacing an item into a rank faraway from the correct one) more than MAEµ, due
4Here we assume that the positives are the minority and the negatives are the majority, which is usually the case in binary classification.
to the presence of squaring; as such, it has been argued (see e.g., <NO>) that MSEµ is more adequate for measuring systems that classify product reviews, since different reviewers might attribute identical reviews to different but neighbouring classes.
It is quite evident that none of these measures is robust to imbalance, since they are all based on a sum of the classification errors across documents. Since the majorityclass classifier incurs in zero error for all the documents whose true class is the majority class, and since in an imbalanced dataset these documents are many, this trivial policy tends to be fairly “error-free”.
To make this problem even worse, it is easy to show that for all these error measures the “trivial class” Φ̃k need not be the majority class; in other words, there may exist trivial classifiers that are even more “error-free” than the majority-class classifier. For instance, in the TripAdvisor15763 dataset mentioned above, assuming that the class distribution in the test set is the same as that in the training set, by assigning all test documents 4 stars we obtain lower MAEµ than by assigning all of them 5 stars, which is the majority class. This is because 4 stars is only marginally less frequent than 5 stars, but in misclassifying all of the documents belonging to the lower classes (1 stars to 3 stars) as 4 stars we make a smaller mistake than in misclassifying them as 5 stars.
Little research has been performed in order to identify evaluation measures that overcome the shortcomings of measures (1)-(3). Gaudette and Japkovicz <NO> acknowledge that these and other measures are somehow problematic but do not concretely propose alternatives. Waegeman et al. <NO> instead propose an evaluation method based on ROC analysis. The problem with their method is that, like all methods based on ROC analysis, it is more apt to evaluate the ability of a classifier at correctly ranking the objects (i.e., at placing 5 stars reviews higher than 4 stars reviews) than to evaluate the ability of the classifier to classify an object into its true (or into a nearby) class. In other words, the ROC measure of <NO> does not reward the ability of a learning device to correctly identify the thresholds τj that separate a class yj from its successor class yj+1, for all j = 1, . . . , (n− 1)."	https://doi.org/10.1109/ISDA.2009.230	1	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']	Baccianella, et al. [NO] proposes a simple way to turn standard measures for or into ones robust to imbalance. 
An experimental design to evaluate class imbalance treatment methods	[]	Guilherme Batista, Danilo Silva, and Ronaldo Prati. 2012. An experimental design to evaluate class imbalance treatment methods. 2012 11th International Conference on Machine Learning and Applications (ICMLA), Vol. 2. IEEE, 95–101.	In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition applications; therefore, these research communities have responded to such interest with literally dozens of methods and techniques. Surprisingly, there are still many fundamental open-ended questions such as “Are all learning paradigms equally affected by class imbalance?”, “What is the expected performance loss for different imbalance degrees?” and “How much of the performance losses can be recovered by the treatment methods?”. In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods. This experimental setup uses real data sets with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We employ such experimental design in a large-scale experimental evaluation with twenty-two data sets and seven learning algorithms from different paradigms. Our results indicate that the expected performance loss, as a percentage of the performance obtained with the balanced distribution, is quite modest (below 5%) for the most balanced distributions up to 10% of minority examples. However, the loss tends to increase quickly for higher degrees of class imbalance, reaching 20% for 1% of minority class examples. Support Vector Machine is the classifier paradigm that is less affected by class imbalance, being almost insensitive to all but the most imbalanced distributions. Finally, we show that the sampling algorithms only partially recover the performance losses. On average, typically about 30% or less of the performance that was lost due to class imbalance was recovered by random oversampling and SMOTE. 	"Due to lack of space we assume the reader is familiar with the class imbalance problem and methods. In the case this assumption does not hold, the literature has several comprehensible surveys, we recommend <NO>. For the same reason, we are not able to include here all numerical results obtained in our experiments. Therefore, we decided to create a paper website <NO> that has detailed results, including tables and data; however, we note this paper is totally selfcontained.
Our experimental design is inspired by the design used in <NO>. The central idea is to generate several training set
distributions with increasing degrees of class imbalance while maintaining the training sets with a fixed number of examples. Our training set distributions range from the balanced distribution (denoted as 50/501) to the imbalanced distributions of 1/99 and 99/1. In order to generate these class distributions, we restricted the training sets to have a total number of instances equals to the number of instances of the minority class. We avoid using extremely imbalanced data sets; in particular, the combination of a small data set with large class imbalance would result in a training set with too few examples. We return to this discussion in Section V, where we comment possible limitations of this work.
The test sets have the naturally occurring class distributions. We reserved 25% of the original data as test set using a stratified sample, and did not touch this portion of the data until the final evaluation; the remaining 75% was used as training set, with class distributions manipulated as previously described. In order to reduce variance and increase statistical significance, we repeated the whole process 100 times using different train and test sample partitions.
For this specific study we assembled a database with twenty data sets. Since we want to promote reproducibility, most of them are public domain benchmark data sets available in repositories such as UCI Machine Learning Repository <NO> or used in projects like Statlog <NO>. A few data sets are not public domain, but we decided to include them since they are frequently used in studies involving class imbalance. These data sets include tumour identification in mammography images <NO>, <NO>. We also included data sets obtained in past research, and we make them publicly available for the first time in the paper website.
We use the area under the ROC curve (AUC) <NO> as the main measure to assess our results. Since we chose AUC and also because we want to control the degree of class imbalance, we converted all non-binary class data sets into binary class, associating a positive label to one of the classes (usually the minority one) and aggregating all remaining classes into a negative class. Table I presents a summarised description of the data sets included in our study. The table lists the data sets full names, as well as a short identifier used in this paper, the number of instances and attributes, labels associated with the positive and negative classes and attribute class distribution. The data sets are listed in increasing order of class imbalance.
Two data sets resulted in two entries each in Table I, because different classes were used as positive class. For the Letter dataset, Letter-a is the variation in which the positive class is the original letter “a” class; and Letter-vowel has the positive class composed by all classes that represent vowels. For the Splice-junction Gene Sequences dataset, one entry has the intron-exon (“ie”) boundaries as positive class
1We use the notation X/Y , with X + Y = 100 to denote that for a set of 100 instances, X belongs to the positive class and Y belongs to the negative class.
and the other has the exon-intron (“ei”) boundaries. The final number of data sets is twenty-two, considering the four entries generated from these two data sets.
We use this experimental design to answer the questions raised in abstract and introduction. We start discussing the expected performance loss for different learning paradigms in the next section."	https://doi.org/10.1109/ICMLA.2012.162	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Batista, et al. [NO] proposes a simple experimental design to assess the performance of class imbalance treatment methods. 
A study of the behavior of several methods for balancing machine learning training data	['Furthermore, there exist several comparisons between different external techniques in different frameworks <NO>, <NO>, <NO>.', '5 <NO> as base classifier for our experiments since it has been widely used in imbalanced domains <NO>, <NO>–<NO>; besides, most of the proposals we are studying were tested with C4.', '2) Data level (or external) approaches rebalance the class distribution by resampling the data space <NO>, <NO>, <NO>,', 'empirically proved that the application of a preprocessing step in order to balance the class distribution is usually a positive solution <NO>, <NO>.', 'Previous works have shown the positive synergy of this combination leading to significant improvements <NO>, <NO>.', 'The Wilcoxon test shows, in concordance with previous studies <NO>, <NO>, that making use of SMOTE as a preprocessing technique significantly outperforms C4.', 'So the traditional classification algorithms cannot achieve ideal effects because positive class is the more valuable class <NO> <NO>.', 'Drummond <NO> proposed that the performance of classifiers which are built based on under-sampling technology is superior to the performance of classifiers which are built based on over-sampling technology, Chris Seiffert <NO> put forward a similar view from the model training complexity and training time, GEBatista <NO> thought that over-sampling technique was better than under-sampling techniques when there are overlaps in the data-set.', 'Some sampling methods first use clustering to partition the data set and then apply undersampling and/or oversampling on different partitions’ data <NO>, <NO>, <NO>.', 'A cluster-based oversampling method was proposed in <NO>, which randomly oversampled both the minority class and majority class samples in such a way that all clusters became the same size.', 'There is a significant body of research comparing the various sampling methods <NO>.', 'The main drawback associated with undersampling is the loss of information that comes with deleting examples from the training data <NO>.', 'However, the training datasets are not always balance, and imbalanced class distribution problem in real applications hinders the application of the traditional classifiers <NO>.', 'Some other methods combine different sampling strategies to achieve further improvement <NO>.', 'Many researchers, suach as Batista, G <NO>, Estabrooks <NO> and Japkowicz <NO>, proposes various approach about two resampling strategies.', 'Some studies have shown that the degradation of classification performance attributed to imbalanced data is not necessarily the result of relative imbalances but rather due to the lack of representative examples (absolute imbalances) <NO>, <NO>, <NO>, <NO>, <NO>.', 'Other popular approaches include the DataBoost-IM (imbalanced learning) <NO>, AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (JOUS-Boost approach) <NO>, and the integration of SMOTE with Tomek links and edited nearest neighbor <NO>.', 'Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>.', 'However, they do not imply that classifiers cannot learn from imbalanced data sets; on the contrary, studies have also shown that classifiers induced from certain imbalanced data sets are comparable to classifiers induced from the same data set balanced by sampling techniques <NO>, <NO>.', 'Some representative work in this area includes the OSS method <NO>, the condensed nearest neighbor rule and Tomek Links ðCNNþTomek LinksÞ integration method <NO>, the neighborhood cleaning rule (NCL) <NO> based on the edited nearest neighbor (ENN) rule—which removes examples that differ from two of its three nearest neighbors, and the integrations of SMOTE with ENN ðSMOTEþENNÞ and SMOTE with Tomek links ðSMOTEþTomekÞ <NO>.', 'Some representative work in this area includes the OSS method <NO>, the condensed nearest neighbor rule and Tomek Links ðCNNþTomek LinksÞ integration method <NO>, the neighborhood cleaning rule (NCL) <NO> based on the edited nearest neighbor (ENN) rule—which removes examples that differ from two of its three nearest neighbors, and the integrations of SMOTE with ENN ðSMOTEþENNÞ and SMOTE with Tomek links ðSMOTEþTomekÞ <NO>.']	Gustavo E.A.P.A. Batista, Ronaldo C. Prati, and Maria Carolina Monard. 2004. A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 20–29.	There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.	"Learning from imbalanced data sets is often reported as being a difficult task. In order to better understand this problem, imagine the situation illustrated in Figure 1. In Fig. 1(a) there is a large imbalance between the majority class (-) and the minority class (+), and the data set presents some degree of class overlapping. A much more comfortable situation for learning is represented in Fig. 1(b), where the classes are balanced with well-defined clusters.
In a situation similar to the one illustrated in Fig. 1(a), spare cases from the minority class may confuse a classifier like k-Nearest Neighbor (k-NN). For instance, 1-NN may incorrectly classify many cases from the minority class because the nearest neighbors of these cases are examples belonging to the majority class. In a situation where the imbalance is very high, the probability of the nearest neighbor of a minority class case is a case of the majority class is likely to be high, and the minority class error rate will tend to have high values, which is unacceptable.
Figure 1: Many negative cases against some spare positive cases (a) balanced data set with well-defined clusters (b).
Decision trees also experience a similar problem. In the presence of class overlapping, decision trees may need to create many tests to distinguish the minority class cases from majority class cases. Pruning the decision tree might not necessarily alleviate the problem. This is due to the fact that pruning removes some branches considered too specialized, labelling new leaf nodes with the dominant class on this node. Thus, there is a high probability that the majority class will also be the dominant class of those leaf nodes."	https://doi.org/10.1145/1007730.1007735	3	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']	E.A.P.A. et al. [NO] performs a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen uci data sets. 
FSVM-CIL: Fuzzy support vector machines for class imbalance learning	[]	Rukshan Batuwita, and Vasile Palade. 2010. FSVM-CIL: Fuzzy support vector machines for class imbalance learning. IEEE Trans. Fuzzy Syst. 18, 3 (2010), 558–571.	Support vector machines (SVMs) is a popular machine learning technique, which works effectively with balanced datasets. However, when it comes to imbalanced datasets, SVMs produce suboptimal classification models. On the other hand, the SVM algorithm is sensitive to outliers and noise present in the datasets. Therefore, although the existing class imbalance learning (CIL) methods can make SVMs less sensitive to class imbalance, they can still suffer from the problem of outliers and noise. Fuzzy SVMs (FSVMs) is a variant of the SVM algorithm, which has been proposed to handle the problem of outliers and noise. In FSVMs, training examples are assigned different fuzzy-membership values based on their importance, and these membership values are incorporated into the SVM learning algorithm to make it less sensitive to outliers and noise. However, like the normal SVM algorithm, FSVMs can also suffer from the problem of class imbalance. In this paper, we present a method to improve FSVMs for CIL (called FSVM-CIL), which can be used to handle the class imbalance problem in the presence of outliers and noise. We thoroughly evaluated the proposed FSVM-CIL method on ten real-world imbalanced datasets and compared its performance with five existing CIL methods, which are available for normal SVM training. Based on the overall results, we can conclude that the proposed FSVMCIL method is a very effective method for CIL, especially in the presence of outliers and noise in datasets.	"In this section, we briefly review the learning algorithm of SVMs, which has been initially proposed in <NO>, <NO>. Consider we have a binary classification problem, which is represented by a dataset {(x1 , y1), (x2 , y2), . . . , (xl,yl)}, where xi ∈ n represents an n-dimensional data point, and yi ∈ {−1, 1} represents the class of that data point, for i = 1, . . . , l. The goal of the SVM learning algorithm is to find a separating hyperplane that separates these data points into two classes. In order to find a better separation of classes, the data are first transformed into a higher dimensional feature space by a mapping function Φ. Then, a possible separating hyperplane, which resides in the higher dimensional feature space, can be represented by
w · Φ(x) + b = 0. (1) If the dataset is completely linearly separable, the separating hyperplane with the maximum margin can be found by solving the following maximal-margin optimization problem:
Min (
1 2 w · w
)
s.t. yi(w · Φ(xi) + b) ≥ 1 i = 1, . . . , l. (2)
However, in most real-world problems, the datasets are not completely linearly separable, although they are mapped into a higher dimensional feature space. Therefore, the constrains in the aforementioned optimization problem in (2) are relaxed by introducing a slack variable εi ≥ 0, and then, the soft-margin optimization problem is formulated as follows:
Min (
1 2 w · w + C l∑ i=1 εi
)
s.t. yi(w · Φ(xi) + b) ≥ 1 − εi εi ≥ 0, i = 1, . . . , l. (3)
The slack variables εi > 0 hold for misclassified examples, and therefore, ∑l i=1 εi can be thought of as a measure of the amount of misclassifications. This new objective function in (3) has two goals. One is to maximize the margin, and the other one is to minimize the number of misclassifications. The parameter C controls the tradeoff between these two goals, and it can also be treated as the misclassification cost of a training
example. This quadratic-optimization problem can be solved by constructing a Lagrangian representation and transforming it into the following dual problem:
Max W (α) = l∑
i=1
αi − 1 2 l∑ i=1 l∑ j=1 αiαjyiyjΦ(xi) · Φ(xj )
s.t. l∑
i=1
yiαi = 0, 0 ≤ αi ≤ C, i = 1, . . . , l (4)
where αis are Lagrange multipliers, which should satisfy the following Karush–Kuhn–Tucker (KKT) conditions:
αi(yi(w · Φ(xi) + b) − 1 + εi) = 0, i = 1, . . . , l (5) (C − αi)ξi = 0, i = 1, . . . , l. (6)
An important property of SVMs is that it is not necessary to know the mapping function Φ(x) explicitly. By applying a kernel function, such that K(xi, xj ) = Φ(xi) · Φ(xj ), we would be able to transform the dual-optimization problem in (4) into
MaxW (α) = l∑
i=1
αi − 1 2 l∑ i=1 l∑ j=1 αiαjyiyjK(xi, xj )
s.t. l∑
i=1
yiαi = 0, 0 ≤ αi ≤ C, i = 1, . . . , l. (7)
By solving (7) and finding the optimal values for αi , w can be recovered as follows:
w = l∑
i=1
αiyiΦ(xi) (8)
and b can be determined from the KKT conditions in (5). The data points having nonzero αi values are called support vectors. Finally, the SVM decision function is given by
f(x) = sign(w · Φ(x) + b) = sign ( l∑
i=1
αiyiK(xi, x) + b ) .
(9)"	https://doi.org/10.1109/TFUZZ.2010.2042721	2	['fsvms', 'svms', 'svm']	['outliers_noise', 'input_points', 'cil_methods', 'svm_algorithm', 'algorithm_sensitive']	['algorithm_sensitive_outliers', 'fuzzy_svms_fsvms', 'learns_decision_surface', 'presence_outliers_noise', 'problem_outliers_noise']	Batuwita, et al. [NO] presents a method to improve fsvms for cil (called fsvm-cil), which can be used to handle the class imbalance problem in the presence of outliers and noise. 
Wrapper-based computation and evaluation of sampling methods for imbalanced datasets	[]	Nitesh V. Chawla, Lawrence O. Hall, and Ajay Joshi. 2005. Wrapper-based computation and evaluation of sampling methods for imbalanced datasets. Proceedings of the 1st International Workshop on UtilityBased Data Mining. ACM, New York, NY, 24–33.	Learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. When the imbalance is large, classification accuracy on the smaller class(es) tends to be lower. In particular, when a class is of great interest but occurs relatively rarely such as cases of fraud, instances of disease, and regions of interest in largescale simulations, it is important to accurately identify it. It then becomes more costly to misclassify the interesting class. In this paper, we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (SMOTE) to improve minority class accuracy. The f-value serves as the evaluation function. Experimental results show the wrapper approach is effective in optimization of the composite f-value, and reduces the average cost per test example for the datasets considered. We report both average cost per test example and the cost curves in the paper. The true positive rate of the minority class increases significantly without causing a significant change in the f-value. We also obtain the lowest cost per test example, compared to any result we are aware of for the KDD Cup-99 intrusion detection data set.	"Researchers in the machine learning community have dealt with the problem of class imbalance by using various approaches like over-sampling the minority classes, undersampling the majority classes, assigning different costs for different misclassification errors, learning by recognition as opposed to discrimination, etc <NO>. There is a significant body of research comparing the various sampling methods <NO>. Sampling strategies have almost become the de facto standard for countering the imbalance in datasets <NO>. With all this there is still no answer on how to do the sampling required for obtaining good classifier accuracies on minority classes.
There are a number of different approaches that can be applied to build classifiers on imbalanced data sets. In this work, we examined under sampling and over-sampling by creating synthetic examples of minority classes. Under-
sampling the majority class can reduce the bias of the learned classifier towards it and thus improve the accuracy on the minority classes.
Some studies <NO> have been done which combined under-sampling of majority classes with over sampling by replication of minority classes. While Japkowicz <NO> found this approach very effective, Ling and Li <NO> were not able to get significant improvement in their performance measures. Japkowicz experimented with only one-dimensional artificial data of varying complexity whereas Ling and Li used real data from a Direct Marketing problem. This might have been the reason for the discrepancy between their results. On the whole, from the body of literature, it was found that under-sampling of majority classes was better than over-sampling with replication of minority classes <NO> and that the combination of the two did not significantly improve the performance over under sampling alone.
Chawla et al. <NO> introduced a new over-sampling approach for two class problems that over-sampled the minority class by creating synthetic examples rather than replicating examples. They pointed out the limitation of oversampling with replication in terms of the decision regions in feature space for decision trees. They showed that as the minority class was over sampled by increasing amounts, for decision trees, the result was to identify similar but more specific regions in the feature space. A preferable approach is to build generalized regions around minority class examples.
The synthetic minority over-sampling technique (SMOTE) was introduced to provide synthetic minority class examples which were not identical but came from the same region in feature space. The over-sampling was done by selecting each minority class example and creating a synthetic example along the line segment joining the selected example and any/all of the k minority class nearest neighbors. In the calculations of the nearest neighbors for the minority class examples a Euclidean distance for continuous features and the value Distance Metric (with the Euclidean assumption) for nominal features was used. For examples with continuous features, the synthetic examples are generated by taking the difference between the feature vectors of selected examples under consideration and their nearest neighbors. The difference between the feature vectors is multiplied by a random number between 0 and 1 and then added to the feature vector of the example under consideration to get a new synthetic example. For nominal valued features, a majority vote for the feature value is taken between the example under consideration and its k nearest neighbors. This approach effectively selects a random point along the line segment between the two feature vectors. This strategy forces the decision regions of the minority class learned by the classifier to become more general and effectively provides better generalization performance on unseen data.
However, an investigation into how to choose the number of examples to be added was not done. In addition, the amount of under-sampling also needs to be determined. Given the various costs of making errors, it is important to identify potentially optimal values for both SMOTE and under-sampling. This is equivalent to discovering the operating point in the ROC space giving the best trade-off between True Positives and False Positives. In this paper, we develop an approach to automatically set the parameters. We discuss a wrapper framework using cross-validation that
performs a step-wise and greedy search for the parameters. Note that while the computational aspects of the automated approach induces certain costs, we do not incorporate that into our framework. We optimize based on the different types of errors made. However, we do try to restrict our search space. We show that this approach works on three highly skewed datasets. We also utilized a cost-matrix to indicate the costs per test example based on the different kinds of errors."	https://doi.org/10.1145/1089827.1089830	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	V. et al. [NO] implements a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (smote) to improve minority class accuracy. 
Ramoboost: Ranked minority oversampling in boosting	['Some of the most popular approaches to deal with imbalanced learning problems are based on the synthetic oversampling methods <NO>, <NO>, <NO>, <NO>.', 'Thus, boosting and oversampling together provide a good option for efficiently learning imbalanced data <NO>, <NO>, <NO>, <NO>.', 'Synthetic oversampling methods have been shown to be very successful in dealing with imbalance data <NO>, <NO>, <NO>, <NO>.', 'Oversampling methods ADASYN <NO> and RAMOBoost <NO> try to avoid the aforementioned problem by adaptively assigning weights to the minority class samples.', 'A large weight helps in generating many synthetic samples from the corresponding minority class sample <NO> or enhances the chance for the minority class sample as a participant in the synthetic sample generation process <NO>.', 'To assign the weight, both <NO> and <NO> use a parameter , defining the number of the majority class samples among the k-nearest neighbors of the minority class sample.', ', <NO>, <NO>, <NO>) employ the k-nearest neighbor (also called k-NN)-based approach.', ', <NO>, <NO>, <NO>) do not explicitly identify the hard-to-learn samples.', 'SMOTE <NO>, ADASYN <NO>, and RAMO <NO>.', 'The RAMO method is found from <NO> by excluding its boosting part.', 'M2 is chosen for ensemble because of its better performance for imbalance problems <NO>, <NO>, <NO>.', 'TABLE 2 Accuracy, Precision, and Recall Performance Values of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using Single Neural Network Classifier, and AdaBoost.', 'TABLE 3 F-Measure, G-Mean, and AUC Performance Values of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using Single Neural Network Classifier, and AdaBoost.', 'TABLE 4 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using k-Nearest Neighbor, and Decision Tree', 'Averaged ROC curves of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE for single neural network classifier ((a)-(d)) and for AdaBoost.', 'Tables 2, 3, and 4 summarize the results of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on four', 'TABLE 6 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Real-World Data Sets Using Single Neural Network Classifier, and AdaBoost.', 'Averaged ROC curves of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE for Neural Network classifier ((a)-(d)) and for ensemble of Neural Network classifiers ((e)-(h)): (a) Glass data set, (b) Libra data set, (c) Vehicle data set, (d) Satimage data set, (e) Breast Cancer Original data set, (f) Breast Tissue data set, (g) Pima data set, (h) Yeast data set.', 'M2 Ensemble of Neural Network: Significance Test of Averaged AUC of MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO> TABLE 7 Simulation 1 on Real-World Data Sets Using Single Neural Network: Significance Test of Averaged AUC between MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO>', 'M2 Ensemble of Neural Network: Significance Test of Averaged AUC of MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO> TABLE 7 Simulation 1 on Real-World Data Sets Using Single Neural Network: Significance Test of Averaged AUC between MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO>', 'TABLE 9 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Real-World Data Sets Using k-NN, and Decision Tree as Base Classifiers']	Sheng Chen, Haibo He, and Edwardo A. Garcia. 2010. Ramoboost: Ranked minority oversampling in boosting. IEEE Trans. Neural Networks 21, 10 (2010), 1624–1642.	In recent years, learning from imbalanced data has attracted growing attention from both academia and industry due to the explosive growth of applications that use and produce imbalanced data. However, because of the complex characteristics of imbalanced data, many real-world solutions struggle to provide robust efficiency in learning-based applications. In an effort to address this problem, this paper presents Ranked Minority Oversampling in Boosting (RAMOBoost), which is a RAMO technique based on the idea of adaptive synthetic data generation in an ensemble learning system. Briefly, RAMOBoost adaptively ranks minority class instances at each learning iteration according to a sampling probability distribution that is based on the underlying data distribution, and can adaptively shift the decision boundary toward difficult-to-learn minority and majority class instances by using a hypothesis assessment procedure. Simulation analysis on 19 real-world datasets assessed over various metrics—including overall accuracy, precision, recall, F-measure, G-mean, and receiver operation characteristic analysis—is used to illustrate the effectiveness of this method.	A comprehensive review of the development of research in learning from imbalanced data, including the nature of the problem, the state-of-the-art approaches, the assessment metrics, and the major opportunities and challenges, has been presented in <NO>. Interested readers can refer to that article for details. In this section, we provide a focused review of four major categories of research activity in imbalanced learning.	https://doi.org/10.1109/TNN.2010.2066988	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Chen, et al. [NO] presents ranked minority oversampling in boosting (ramoboost), which is a ramo technique based on the idea of adaptive synthetic data generation in an ensemble learning system. 
Fast: A roc-based feature selection metric for small samples and imbalanced data classification problems	[]	Xue-wen Chen, and Michael Wasikowski. 2008. Fast: A roc-based feature selection metric for small samples and imbalanced data classification problems. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, 124–132.	The class imbalance problem is encountered in a large number of practical applications of machine learning and data mining, for example, information retrieval and filtering, and the detection of credit card fraud. It has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifier’s suboptimal performance. This is even more true when the imbalanced data are also high dimensional. In such cases, feature selection methods are critical to achieve optimal performance. In this paper, we propose a new feature selection method, Feature Assessment by Sliding Thresholds (FAST), which is based on the area under a ROC curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. FAST is compared to two commonly-used feature selection methods, correlation coefficient and RELevance In Estimating Features (RELIEF), for imbalanced data classification. The experimental results obtained on text mining, mass spectrometry, and microarray data sets showed that the proposed method outperformed both RELIEF and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred, the classification performance of the proposed method was significantly improved compared to correlation and RELIEF-based methods.	In this section, we briefly review two commonly-used feature selection methods, CC and RELIEF.	https://doi.org/10.1145/1401890.1401910	1	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']	Chen, et al. [NO] proposes a new feature selection method, feature assessment by sliding thresholds (fast), which is based on the area under a roc curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. 
The relationship between Precision-Recall and ROC curves	['Similar to AUC-ROC, an area under precision/recall curve (AUC-PR) can be used to indicate the detection ability of a classifier between precision and recall as a function of varying a decision threshold <NO>', 'For example, researchers have recently argued that precision-recall curves are preferable when dealing with highly skewed datasets <NO>.']	Jesse Davis, and Mark Goadrich. 2006. The relationship between Precision-Recall and ROC curves. ICML’06: Proc. of the 23rd Int. Conf. on Machine Learning (ACM ICPS). ACM, New York, NY, 233– 240.	Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm’s performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.	"In a binary decision problem, a classifier labels examples as either positive or negative. The decision made by the classifier can be represented in a structure known as a confusion matrix or contingency table. The confusion matrix has four categories: True positives (TP) are examples correctly labeled as positives. False positives (FP) refer to negative examples incorrectly labeled as positive. True negatives (TN) correspond to negatives correctly labeled as negative. Finally, false negatives (FN) refer to positive examples incorrectly labeled as negative.
A confusion matrix is shown in Figure 2(a). The confusion matrix can be used to construct a point in either ROC space or PR space. Given the confusion matrix, we are able to define the metrics used in each space as in Figure 2(b). In ROC space, one plots the False Positive Rate (FPR) on the x-axis and the True Positive Rate (TPR) on the y-axis. The FPR measures the fraction of negative examples that are misclassified as positive. The TPR measures the fraction of positive examples that are correctly labeled. In PR space, one plots Recall on the x-axis and Precision on the y-axis. Recall is the same as TPR, whereas Precision measures that fraction of examples classified as positive that are truly positive. Figure 2(b) gives the definitions for each metric. We will treat the metrics as functions that act on the underlying confusion matrix which defines a point in either ROC space or PR space. Thus, given a confusion matrix A, RECALL(A) returns the Recall associated with A."	https://doi.org/10.1145/1143844.1143874	3	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']	Davis, et al. [NO] shows that a deep connection exists between roc space and pr space, such that a curve dominates in roc space if and only if it dominates in pr space. 
A multistrategy approach for digital text categorization from imbalanced documents	[]	Marı́a Dolores Del Castillo, and José Ignacio Serrano 2004. A multistrategy approach for digital text categorization from imbalanced documents. ACM SIGKDD Explor. Newslett	The goal of the research described here is to develop a multistrategy classifier system that can be used for document categorization. The system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample. The learners work in a parallel manner, where each learner carries out its own feature selection based on evolutionary techniques and then obtains a classification model. In classifying documents, the system combines the predictions of the learners by applying evolutionary techniques as well. The system relies on a modular, flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain.	"HYCLA operates in two stages, learning and integration. In the learning stage, learners apply an evolutionary technique to obtain their own feature set, and then they are trained to obtain their classification model. In the integration stage, individual learned models are evaluated on a test set, and the predictions made are combined in order to achieve the best classification of test documents. The subsections below describe the modules and procedures of this system.
The underlying architecture of HYCLA can be instantiated to approach a different text mining task by upgrading its modules."	https://doi.org/10.1145/1007730.1007740	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	In classifying documents, the system combines the predictions of the learners by applying evolutionary techniques as well [NO].
Explicitly representing expected cost: An alternative to ROC representation	['Drummond and Holte <NO> introduced the cost space representation that allows for comparing different classifiers in terms of the expected cost.', 'We also implemented cost-curves over the range of PCF (+) established by varying C(+|−) and C(−|+) <NO>.', 'In order to provide a more comprehensive evaluation metric to address these issues, cost curves were proposed in <NO>, <NO>, <NO>.', 'such as ROC curve <NO> or cost curve <NO> can be used to', 'useful nontrivial classifiers can be identified <NO>.']	Chris Drummond, and Robert C. Holte. 2000. Explicitly representing expected cost: An alternative to ROC representation. Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, 198–207.	This paper proposes an alternative to ROC representation, in which the expected cost of a classi er is represented explicitly . This expected cost representation maintains many of the adv an tagesof R OCrepresen tation, but is easier to understand. It allows the experimenter to immediately see the range of costs and class frequencies where a particular classi er is the best and quantitatively how much better it is than other classi ers. This paper demonstrates there is a point/line duality between the tw o represen tations.A point in ROC space representing a classi er becomes a line segment spanning the full range of costs and class frequencies. This duality produces equivalen t operations in the tw o spaces, allowing most techniques used in ROC analysis to be readily reproduced in the cost space.	In this section we brie y review ROC analysis and how it is used in evaluating or comparing a classi er's performance. We then introduce our alternative dual representation, which maintains these advantages but by making explicit the expected cost is much easier to understand. In both representations, the analysis is restricted to two class problems which are referred to as the positive and negative class.	https://doi.org/10.1145/347090.347126	3	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']	Drummond, et al. [NO] proposes an alternative to roc representation, in which the expected cost of a classi er is represented explicitly. 
Learning on the border: Active learning in imbalanced data classification	['For example, an SVM-based active learning approach for imbalanced datasets was proposed in <NO> and <NO>.', 'Recently, however, various issues on active learning from imbalanced data sets have been discussed in literature <NO>, <NO>, <NO>, <NO>.', '8 illustrates the motivation for the selection procedure for imbalanced data sets <NO>.', '<NO> and <NO> proposed an efficient SVM-based active learning method which queries a small pool of data at each iterative step of active learning instead of querying the entire data set.', '<NO> and <NO> also point out that the search process for the most informative instances can be computationally expensive because, for each instance of unseen data, the algorithm needs to recalculate the distance between each instance and the current hyperplane.', 'To solve this problem, they proposed a method to effectively select such informative instances from a random set of training populations to reduce the computational cost for large-scale imbalanced data sets <NO>, <NO>.', 'Data imbalance ratio within and outside the margin <NO>.']	Şeyda Ertekin, Jian Huang, Leon Bottou, and Lee Giles. 2007. Learning on the border: Active learning in imbalanced data classification. Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management. ACM, New York, NY, 127–136.	This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly less number of observations of the target concept. Various real-world classification tasks, such as medical diagnosis, text categorization and fraud detection suffer from this phenomenon. The standard machine learning algorithms yield better prediction performance with balanced datasets. In this paper, we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes. We also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire dataset. The proposed method yields an efficient querying system and allows active learning to be applied to very large datasets. Our experimental results show that with an early stopping criteria, active learning achieves a fast solution with competitive prediction performance in imbalanced data classification.	"Recent research on class imbalance problem has focused on several major groups of techniques. One is to assign distinct costs to the classification errors <NO>. In this method, the misclassification penalty for the positive class is assigned a higher value than that of the negative class. This method requires tuning to come up with good penalty parameters for the misclassified examples. The second is to resample the original training dataset, either by over-sampling the minority class and/or under-sampling the majority class until the classes are approximately equally represented <NO>. Both resampling methods introduce additional computational costs of data preprocessing and oversampling can be overwhelming in the case of very large scale training data. Undersampling has been proposed as a good means of increasing the sensitivity of a classifier. However this method may discard potentially useful data that could be important for the learning process therefore significant decrease in the prediction performance may be observed. Discarding the redundant examples in undersampling has been discussed in <NO> but since it is an adaptive method for ensemble learning and does not involve an external preprocessing step it can not be applied to other types of algorithms. Oversampling has been proposed to create synthetic positive instances from the existing positive samples to increase the representation of the class. Nevertheless, oversampling may suffer from overfitting and due to the increase in the number of
samples, the training time of the learning process gets longer. If a complex oversampling method is used, it also suffers from high computational costs during preprocessing data. In addition to those, oversampling methods demand more memory space for the storage of newly created instances and the data structures based on the learning algorithm (i.e., extended kernel matrix in kernel classification algorithms). Deciding on the oversampling and undersampling rate is also another issue of those methods. Another technique suggested for class imbalance problem is to use a recognition-based, instead of discrimination-based inductive learning <NO>. These methods attempt to measure the amount of similarity between a query object and the target class, where classification is accomplished by imposing a threshold on the similarity measure. The major drawback of those methods is the need for tuning the similarity threshold of which the success of the method mostly relies on. On the other hand, discrimination-based learning algorithms have been proved to give better prediction performance in most domains.
In <NO> the behavior of Support Vector Machines (SVM) with imbalanced data is investigated. They applied <NO>’s SMOTE algorithm to oversample the data and trained SVM with different error costs. SMOTE is an oversampling approach in which the minority class is oversampled by creating synthetic examples rather than with replacement. The k nearest positive neighbors of all positive instances are identified and synthetic positive examples are created and placed randomly along the line segments joining the k minority class nearest neighbors. Preprocessing the data with SMOTE may lead to improved prediction performance at the classifiers, however it also brings more computational cost to the system for preprocessing and yet the increased number of training data makes the SVM training very costly since the training time at SVMs scales quadratically with the number of training instances. In order to cope with today’s tremendously growing dataset sizes, we believe that there is a need for more computationally efficient and scalable algorithms. We show that such a solution can be achieved by using active learning strategy."	https://doi.org/10.1145/1321440.1321461	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Ertekin, et al. [NO] is concerns with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. 
Active learning for class imbalance problem	['For example, an SVM-based active learning approach for imbalanced datasets was proposed in <NO> and <NO>.', 'This algorithm locates the “most informative” sample by evaluating a small fixed number of randomly selected examples instead of the entire dataset <NO>.', 'Recently, however, various issues on active learning from imbalanced data sets have been discussed in literature <NO>, <NO>, <NO>, <NO>.', 'SVM-based active learning aims to select the most informative instances from the unseen training data in order to retrain the kernel-based model <NO>, i.', '<NO> and <NO> proposed an efficient SVM-based active learning method which queries a small pool of data at each iterative step of active learning instead of querying the entire data set.', '<NO> and <NO> also point out that the search process for the most informative instances can be computationally expensive because, for each instance of unseen data, the algorithm needs to recalculate the distance between each instance and the current hyperplane.', 'To solve this problem, they proposed a method to effectively select such informative instances from a random set of training populations to reduce the computational cost for large-scale imbalanced data sets <NO>, <NO>.']	Şeyda Ertekin, Jian Huang, and C. Lee Giles. 2007. Active learning for class imbalance problem. Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New York, NY, 823–824.	The class imbalance problem has been known to hinder the learning performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem.	"The basic SVM based active learning selects the closest instance to the current hyperplane from the unseen training data and adds it to the training set to retrain the model. In classical active learning <NO>, the search for the most informative (closest) instance is done through the entire unseen dataset. Each iteration of active learning involves the recomputation of the distances of each instance to the new hyperplane. Thus, for large datasets, searching the entire training set is very time-consuming and computationally expensive.
We propose a selection method which will not necessitate a full search through the entire dataset but locates an approximate most informative sample by examining a small constant number of randomly chosen samples. The method picks L (L # training instances) random training samples in each iteration and selects the best (closest to the hyperplane) among them. Suppose, instead of picking the closest instance among all the training samples XN = (x1, x2, · · · , xN) at each iteration, we first pick a random subset XL, L N and select the closest sample xi from XL based on the condition that xi is among the top p% closest instances in XN with probability (1− η). Any numerical modification to these constraints can be met by varying the size of L, and is independent of N . To demonstrate, the probability that at least one of the L instances is among the closest p% is 1 − (1 − p%)L. Due to the requirement of (1 − η) probability, we have
1 − (1 − p%)L = 1 − η (1) which follows the solution of L in terms of η and p
L = log η / log(1 − p%) (2) For example, the active learner will pick one instance, with 95% probability, that is among the top 5% closest instances to the hyperplane, by randomly sampling only log(.05)/ log(.95) = 59 instances regardless of the training set size. This approach scales well since the size of the subset L is independent of the training set size N , requires significantly less training time and does not have an adverse effect on the classification performance of the learner. In our experiments, we set L = 59.
Early Stopping: In SVM learning the classification boundary (hyperplane) is only determined by support vectors. This means that there is no point of adding new instances to the model after the
number of support vectors saturates. A practical implementation of this idea is to count the number of support vectors during the active learning training process. If the number of the support vectors stabilizes, it implies that all possible support vectors have been selected by the active learning method and the rest of the training instances are redundant. Therefore, we choose our stopping point where the number of support vectors saturates."	https://doi.org/10.1145/1277741.1277927	1	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']	Ertekin, et al. [NO] demonstrates that active learning is capable of solving the problem. 
Theoretical analysis of a performance measure for imbalanced data	[]	Vicente Garcı́a, Ramón Alberto Mollineda, and José Salvador Sánchez 2010. Theoretical analysis of a performance measure for imbalanced data. In 2010 20th International Conference on Pattern Recognition (ICPR)	This paper analyzes a generalization of a new metric to evaluate the classification performance in imbalanced domains, combining some estimate of the overall accuracy with a plain index about how dominant the class with the highest individual accuracy is. A theoretical analysis shows the merits of this metric when compared to other well-known measures.	"Traditionally, classification accuracy (Acc) and/or error rates have been the standard metrics used to estimate the performance of learning systems. For a twoclass problem, they can be easily derived from a 2 × 2 confusion matrix as that given in Table 1.
However, empirical and theoretical evidences show that these measures are biased with respect to data imbalance and proportions of correct and incorrect classifications. These shortcomings have motivated a search for new metrics based on simple indexes, such as the true positive rate (TPrate) and the true negative rate (TNrate). The TPrate (TNrate) is the percentage of positive (negative) examples correctly classified.
One of the most widely-used evaluation methods in the context of class imbalance is the ROC curve, which is a tool for visualizing and selecting classifiers based on their trade-offs between benefits (true positives) and costs (false positives). A quantitative representation of a ROC curve is the area under it (AUC) <NO>. For just one run of a classifier, the AUC can be computed as <NO> AUC = (TPrate+ TNrate)/2.
1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.156
62117
Kubat et al. <NO> use the geometric mean of accuracies measured separately on each class, with the aim of maximizing the accuracies of both classes while keeping them balanced, Gmean = √ TPrate · TNrate.
Both AUC and Gmean minimize the negative influence of skewed distributions of classes, but they do not show up the contribution of each class to the overall performance, nor which is the prevalent class. This means that different combinations of TPrate and TNrate may produce the same result for those metrics.
Recently, Ranawana and Palade <NO> introduced the optimized precision, which can be computed as,
OP = Acc− |TNrate− TPrate| TNrate+ TPrate
(1)
This represents the difference between the global accuracy and a second term that computes how balanced both class accuracies are. High OP values require high global accuracy and well-balanced class accuracies. However, OP can be strongly affected by the biased influence of the global accuracy."	https://doi.org/10.1109/ICPR.2010.156	1	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']	Garcı́a, et al. [NO] analyzes a generalization of a new metric to evaluate the classification performance in imbalanced domains, combining some estimate of the overall accuracy with a plain index about how dominant the class with the highest individual accuracy is. 
Active learning from positive and unlabeled data	[]	Alireza Ghasemi, Hamid R. Rabiee, Mohsen Fadaee, Mohammad T. Manzuri, and Mohammad H. Rohban. 2011. Active learning from positive and unlabeled data. 2011 IEEE 11th International Conference on Data Mining Workshops (ICDMW). IEEE, 244–250.	During recent years, active learning has evolved into a popular paradigm for utilizing user’s feedback to improve accuracy of learning algorithms. Active learning works by selecting the most informative sample among unlabeled data and querying the label of that point from user. Many different methods such as uncertainty sampling and minimum risk sampling have been utilized to select the most informative sample in active learning. Although many active learning algorithms have been proposed so far, most of them work with binary or multi-class classification problems and therefore can not be applied to problems in which only samples from one class as well as a set of unlabeled data are available. Such problems arise in many real-world situations and are known as the problem of learning from positive and unlabeled data. In this paper we propose an active learning algorithm that can work when only samples of one class as well as a set of unlabeled data are available. Our method works by separately estimating probability density of positive and unlabeled points and then computing expected value of informativeness to get rid of a hyper-parameter and have a better measure of informativeness. Experiments and empirical analysis show promising results compared to other similar methods. 	"Many active learning algorithms have been proposed in the literature so far. <NO> is a comprehensive survey of recent works in this field. Among the earliest and most popular active learning paradigms is the uncertainty sampling approach which is based on selecting the least confident sample for querying. The definition of confidence depends on the base classifier in use. For example, <NO> proposes an active learning approach for SVM which selects for querying the sample which is closest to the separating hyperplane. selecting the sample with minimum margin <NO> and the data sample with maximum entropy <NO> are other approaches which have been applied to active learning problems.
For classifiers that are unable to define a similarity measure over their predictions, committee-based active learning methods have been proposed. these methods form an ensemble or committee of diverse classifiers and measure uncertainty by the amount of disagreement between committee members’ votes for a data sample <NO>.
In problems where samples of only one class are available, traditional uncertainty assessment methods can not work since they require information about at least two of the classes or their separating hyperplane . Therefore, specific active learning methods are required for one-class problems. One of the earlier works is <NO> which uses active learning for outlier detection. This methods works in two stages: First, a number of unlabeled samples are selected as negative samples by means of statical methods. Then a traditional committee based active learning algorithm is used to perform active learning on the rest of samples. The main advantage of this approach is that it’s flexible and can utilize a wide range of traditional active learning algorithms. However, <NO> approaches the problem of one-class learning by a traditional binary classification method. This causes degradation in accuracy of the resulting classifier since the two classes have very different characteristics and should not be treated equally. moreover, because of using two learning algorithms, the runtime complexity of this approach is much higher than other similar methods.
Another method for active learning from positive and unlabeled data has been proposed by <NO>. This paper suggests that the best choice for active learning is selecting the most relevant data sample. the justification behind this claim comes from the nature of relevance feedback in image retrieval applications. In other words, the most informative data will be chosen by the following rule:
x∗ = argmaxx∈U f(x), (1)
in which f(.) is the scoring function of one-class learning which is used to rank data samples by their likelihood to the target (positive) class. The main advantages of this method lie in its simplicity and speed. However, since this method does not consider uncertainty in choosing samples, the selected data point may lack informativeness.
A more recent approach has been proposed in <NO>, which tries to apply active learning to the well-known SVDD method. <NO> considers likelihood as well as local density of data point to assess their uncertainty. First, the algorithm constructs a neighborhood graph over all data samples. Then, the most informative sample is selected using the following rule:
x∗ =
argminxi∈U σ ||d(xi,C)−R|| c + 1−σ 2k Σxj∈L∪U (yj + 1)aij
(2)
In (2), parameters c and σ are used to manipulate the significance of any of two factors in the final decision measure, d(xi, C) is the distance between xi and center of sphere formed by the SVDD approach. R is radius of that sphere. y is 0 for unlabeled data, +1 for positive and −1 for negative samples. a is the adjacency matrix of the data neighborhood graph. aij = 1 if there is an edge between xi and xj , and 0 otherwise.
The main advantage of <NO> is that it considers both selection based on uncertainty of data, and exploring unknown regions of the feature space. This fact can be easily inferred from the two terms of equation 2. However, this methods is biased toward exploring regions containing negative data in the feature space. This causes algorithm to be biased toward selecting data which are more likely negative samples. Due to the nature of one-class learning, positive data are much more valuable than negative data samples and therefore selecting negative samples may not be much helpful in improving classification accuracy. Moreover, constructing the neighborhood graph is a time consuming task and makes the algorithm infeasible for real-time applications."	https://doi.org/10.1109/ICDMW.2011.20	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Ghasemi, et al. [NO] proposes an active learning algorithm that can work when only samples of one class as well as a set of unlabeled data are available. 
Learning from imbalanced data	['itself <NO>, <NO>, the issue is that usually a series of difficulties related to this problem turn up.', 'Most of these works fall under four different categories: sampling-based methods, cost-based methods, kernel-based methods, and active learning-based methods <NO>.', 'Details of work performed on the other categories can be found in <NO>.', 'The main problem associates with the accuracy measure is its dependence on the distribution of positive class and negative class samples in the data set, thus not suitable for imbalanced learning problems <NO>.', 'They are precision, recall, geometric-mean (G-mean), and F-measure <NO>.', 'For several base classifiers, studies have shown that a balanced training dataset provides high prediction accuracy and good generalization capability compared to an imbalanced dataset <NO> <NO>.', 'However, the drawbacks of over-sampling approaches are enlarging the size of the original training dataset, and leading to overfitting<NO><NO>.', 'Traditionally, metrics which are used by the most standard algorithm are accuracy and error rate <NO><NO><NO><NO>.', 'Indeed, there are at least three surveys of methods for improving classification performance on imbalanced datasets <NO>, <NO>, <NO>.', 'In the case this assumption does not hold, the literature has several comprehensible surveys, we recommend <NO>.', 'LEARNING from imbalanced data (imbalanced learning) <NO>, <NO> has become a critical and significant research issue in many of today’s data-intensive applications, such as financial engineering, anomaly detection, biomedical data analysis, and many others.', 'The imbalance learning problem generally manifests itself in two forms: relative imbalances and absolute imbalances <NO>, <NO>.', 'Some studies have shown that the degradation of classification performance attributed to imbalanced data is not necessarily the result of relative imbalances but rather due to the lack of representative examples (absolute imbalances) <NO>, <NO>, <NO>, <NO>, <NO>.', 'In particular, for a given dataset that contains several sub-concepts, the distribution of minority examples over the minority class concepts may yield clusters with insufficient representative examples to form a classification rule <NO>.', 'A comprehensive review of the development of research in learning from imbalanced data, including the nature of the problem, the state-of-the-art approaches, the assessment metrics, and the major opportunities and challenges, has been presented in <NO>.', 'These techniques have shown great success when applied to imbalanced learning problems <NO>.', 'Kernel-based methods have recently become very popular across various fields including imbalanced learning <NO>.', 'Recently, active learning methods have found increased use in imbalanced learning applications <NO>.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'The detailed discussions on these metrics and their applications for imbalanced learning can be found in <NO>.', 'The interested reader can refer to <NO> for further details on H -measure and <NO> for a critical review of the assessment metrics for imbalanced learning.', 'Hence, several specific approaches have been proposed <NO>.', 'Although the SVM algorithm works effectively with balanced datasets, when it comes to imbalanced datasets, it could often produce suboptimal results <NO>–<NO>, i.', 'It has been well-studied that the SVM algorithm can be sensitive to class imbalance <NO>–<NO>, i.', 'Generally, these methods can be divided into two categories: external methods and internal methods <NO>, <NO>, <NO>.', 'A comprehensive review of different CIL methods can be found in <NO>.', 'until a particular class ratio is met <NO>.', 'Therefore, we used the geometric mean of sensitivity (SE = proportion of the positives correctly recognized) and specificity (SP = proportion of the negatives correctly recognized), given by Gm = √ SE × SP, for the classifier performance evaluation in this study, as commonly used in class imbalanced-learning research <NO>, <NO>, <NO>.']	Haibo He, and Edwardo A. Garcia. 2009. Learning from imbalanced data. IEEE Knowl. Data Eng. 21, 9 (2009), 1263–1284.	With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.	"Technically speaking, any data set that exhibits an unequal distribution between its classes can be considered imbalanced. However, the common understanding in the community is that imbalanced data correspond to data sets exhibiting significant, and in some cases extreme, imbalances. Specifically, this form of imbalance is referred to as a between-class imbalance; not uncommon are betweenclass imbalances on the order of 100:1, 1,000:1, and 10,000:1, where in each case, one class severely outrepresents another <NO>, <NO>, <NO>. Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. In this paper, we only briefly touch upon the multiclass imbalanced learning problem, focusing instead on the two-class imbalanced learning problem for space considerations.
In order to highlight the implications of the imbalanced learning problem in the real world, we present an example from biomedical applications. Consider the “Mammography Data Set,” a collection of images acquired from a series of mammography exams performed on a set of distinct patients, which has been widely used in the analysis of algorithms addressing the imbalanced learning problem <NO>, <NO>, <NO>. Analyzing the images in a binary sense, the natural classes (labels) that arise are “Positive” or “Negative” for an image representative of a “cancerous” or “healthy” patient, respectively. From experience, one would expect the number of noncancerous patients to exceed greatly the number of cancerous patients; indeed, this data
set contains 10,923 “Negative” (majority class) samples and 260 “Positive” (minority class) samples. Preferably, we require a classifier that provides a balanced degree of predictive accuracy (ideally 100 percent) for both the minority and majority classes on the data set. In reality, we find that classifiers tend to provide a severely imbalanced degree of accuracy, with the majority class having close to 100 percent accuracy and the minority class having accuracies of 0-10 percent, for instance <NO>, <NO>. Suppose a classifier achieves 10 percent accuracy on the minority class of the mammography data set. Analytically, this would suggest that 234 minority samples are misclassified as majority samples. The consequence of this is equivalent to 234 cancerous patients classified (diagnosed) as noncancerous. In the medical industry, the ramifications of such a consequence can be overwhelmingly costly, more so than classifying a noncancerous patient as cancerous <NO>. Therefore, it is evident that for this domain, we require a classifier that will provide high accuracy for the minority class without severely jeopardizing the accuracy of the majority class. Furthermore, this also suggests that the conventional evaluation practice of using singular assessment criteria, such as the overall accuracy or error rate, does not provide adequate information in the case of imbalanced learning. Therefore, more informative assessment metrics, such as the receiver operating characteristics curves, precision-recall curves, and cost curves, are necessary for conclusive evaluations of performance in the presence of imbalanced data. These topics will be discussed in detail in Section 4 of this paper. In addition to biomedical applications, further speculation will yield similar consequences for domains such as fraud detection, network intrusion, and oil-spill detection, to name a few <NO>, <NO>, <NO>, <NO>, <NO>.
Imbalances of this form are commonly referred to as intrinsic, i.e., the imbalance is a direct result of the nature of the dataspace. However, imbalanced data are not solely restricted to the intrinsic variety. Variable factors such as time and storage also give rise to data sets that are imbalanced. Imbalances of this type are considered extrinsic, i.e., the imbalance is not directly related to the nature of the dataspace. Extrinsic imbalances are equally as interesting as their intrinsic counterparts since it may very well occur that the dataspace from which an extrinsic imbalanced data set is attained may not be imbalanced at all. For instance, suppose a data set is procured from a continuous data stream of balanced data over a specific interval of time, and if during this interval, the transmission has sporadic interruptions where data are not transmitted, then it is possible that the acquired data set can be imbalanced in which case the data set would be an extrinsic imbalanced data set attained from a balanced dataspace.
In addition to intrinsic and extrinsic imbalance, it is important to understand the difference between relative imbalance and imbalance due to rare instances (or “absolute rarity”) <NO>, <NO>. Consider a mammography data set with 100,000 examples and a 100:1 between-class imbalance. We would expect this data set to contain 1,000 minority class examples; clearly, the majority class dominates the minority class. Suppose we then double the sample space by testing more patients, and suppose further that the distribution
does not change, i.e., the minority class now contains 2,000 examples. Clearly, the minority class is still outnumbered; however, with 2,000 examples, the minority class is not necessarily rare in its own right but rather relative to the majority class. This example is representative of a relative imbalance. Relative imbalances arise frequently in real-world applications and are often the focus of many knowledge discovery and data engineering research efforts. Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>. These results are particularly suggestive because they show that the degree of imbalance is not the only factor that hinders learning. As it turns out, data set complexity is the primary determining factor of classification deterioration, which, in turn, is amplified by the addition of a relative imbalance.
Data complexity is a broad term that comprises issues such as overlapping, lack of representative data, small disjuncts, and others. In a simple example, consider the depicted distributions in Fig. 2. In this figure, the stars and circles represent the minority and majority classes, respectively. By inspection, we see that both distributions in Figs. 2a and 2b exhibit relative imbalances. However, notice how Fig. 2a has no overlapping examples between its classes and has only one concept pertaining to each class, whereas Fig. 2b has both multiple concepts and severe overlapping. Also of interest is subconcept C in the distribution of Fig. 2b. This concept might go unlearned by some inducers due to its lack of representative data; this issue embodies imbalances due to rare instances, which we proceed to explore.
Imbalance due to rare instances is representative of domains where minority class examples are very limited, i.e., where the target concept is rare. In this situation, the lack of representative data will make learning difficult regardless of the between-class imbalance <NO>. Furthermore, the minority concept may additionally contain a subconcept with limited instances, amounting to diverging degrees of classification difficulty <NO>, <NO>. This, in fact, is the result of another form of imbalance, a within-class imbalance, which concerns itself with the distribution of representative data for subconcepts within a class <NO>, <NO>, <NO>. These ideas are again highlighted in our simplified example in Fig. 2. In Fig. 2b, cluster B represents the dominant minority class concept and cluster C represents a subconcept of the minority class. Cluster D represents two subconcepts of the majority class and cluster A (anything
not enclosed) represents the dominant majority class concept. For both classes, the number of examples in the dominant clusters significantly outnumber the examples in their respective subconcept clusters, so that this dataspace exhibits both within-class and between-class imbalances. Moreover, if we completely remove the examples in cluster B, the dataspace would then have a homogeneous minority class concept that is easily identified (cluster C), but can go unlearned due to its severe underrepresentation.
The existence of within-class imbalances is closely intertwined with the problem of small disjuncts, which has been shown to greatly depreciate classification performance <NO>, <NO>, <NO>, <NO>. Briefly, the problem of small disjuncts can be understood as follows: A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept <NO>, <NO>, <NO>. In the case of homogeneous concepts, the classifier will generally create large disjuncts, i.e., rules that cover a large portion (cluster) of examples pertaining to the main concept. However, in the case of heterogeneous concepts, small disjuncts, i.e., rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts <NO>, <NO>, <NO>. Moreover, since classifiers attempt to learn both majority and minority concepts, the problem of small disjuncts is not only restricted to the minority concept. On the contrary, small disjuncts of the majority class can arise from noisy misclassified minority class examples or underrepresented subconcepts. However, because of the vast representation of majority class data, this occurrence is infrequent. A more common scenario is that noise may influence disjuncts in the minority class. In this case, the validity of the clusters corresponding to the small disjuncts becomes an important issue, i.e., whether these examples represent an actual subconcept or are merely attributed to noise. For example, in Fig. 2b, suppose a classifier generates disjuncts for each of the two noisy minority samples in cluster A, then these would be illegitimate disjuncts attributed to noise compared to cluster C, for example, which is a legitimate cluster formed from a severely underrepresented subconcept.
The last issue we would like to discuss is the combination of imbalanced data and the small sample size problem <NO>, <NO>. In many of today’s data analysis and knowledge discovery applications, it is often unavoidable to have data with high dimensionality and small sample size; some specific examples include face recognition and gene expression data analysis, among others. Traditionally, the small sample size problem has been studied extensively in the pattern recognition community <NO>. Dimensionality reduction methods have been widely adopted to handle this issue, e.g., principal component analysis (PCA) and various extension methods <NO>. However, when the representative data sets’ concepts exhibit imbalances of the forms described earlier, the combination of imbalanced data and small sample size presents a new challenge to the community <NO>. In this situation, there are two critical issues that arise simultaneously <NO>. First, since the sample size is small, all of the issues related to absolute rarity and within-class imbalances are applicable. Second and more importantly, learning algorithms often fail to
Fig. 2. (a) A data set with a between-class imbalance. (b) A highcomplexity data set with both between-class and within-class imbalances, multiple concepts, overlapping, noise, and lack of representative data.
generalize inductive rules over the sample space when presented with this form of imbalance. In this case, the combination of small sample size and high dimensionality hinders learning because of difficultly involved in forming conjunctions over the high degree of features with limited samples. If the sample space is sufficiently large enough, a set of general (albeit complex) inductive rules can be defined for the dataspace. However, when samples are limited, the rules formed can become too specific, leading to overfitting. In regards to learning from such data sets, this is a relatively new research topic that requires much needed attention in the community. As a result, we will touch upon this topic again later in our discussions."	https://doi.org/10.1145/1007730.1007733	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	He, et al. [NO] provides a comprehensive review of the development of research in learning from imbalanced data. 
An improved SMOTE imbalanced data classification method based on support degree	[]	Kewen Li, Wenrong Zhang, Qinghua Lu, and Xianghua Fang. 2014. An improved SMOTE imbalanced data classification method based on support degree. 2014 International Conference on Identification, Information and Knowledge in the Internet of Things (IIKI). IEEE, 34–38.	Imbalanced data-set Classification has become a hotspot problem in Data Mining. The essential assumption of the traditional classification algorithms is that the distribution of the classes is balanced, therefore the algorithms used in Imbalanced data-set Classification cannot achieve an ideal effect. In view of imbalance date-set classification, we propose an oversampling method based on support degree in order to guide people to select minority class samples and generate new minority class samples. In the light of support degree, it is now possible to identify minority class boundary samples, then produce a number of new samples between the boundary samples and their neighbors, finally add the synthetic samples to the original data-set to participate in training and testing. Experimental results show that the method has an obvious advantage in dealing with imbalanced data-set. 	"In data level, people mainly use sampling techniques to deal with imbalanced data. The basic idea of sampling is that we change the distribution of training samples to overcome the imbalance of data-set. Data sampling techniques include three types: under-sampling, over-sampling, mixed sampling. Under-sampling removes some majority class samples in order to achieve balanced data-set; oversampling increases the number of minority class samples to change the distribution of data-set; mix sampling uses both over-sampling and undersampling techniques to deal with data-set.
1) Under-sampling technique Random under-sampling <NO> is the most simple and common method in under-sampling technique, it changes the distribution of data-set by removing some negative class samples randomly, but this method also exists shortcomings, such as deleting samples artificially may lose the samples with important information and reduce the performance of classifiers.
NCR(Neighborhood Cleaning Rule, NCR) proposed by J.Laurikkala <NO> is an under-sampling method. It uses the nearest neighbor thought to remove negative class samples. Its basic idea is as follows: select a sample iX from data-set randomly, then find its three nearest neighbors and their categories, compare iX with the three neighbors: if iX is a negative class sample, at least two of the three samples are positive class samples, then remove iX from the data-set; if
iX is a positive class sample, at least two of the three samples are negative class samples, then remove the three neighbors from the data-set. So you can use this method to under-sample negative class samples.
978-1-4799-8003-1/14 $31.00 © 2014 IEEE DOI 10.1109/IIKI.2014.14
34
2) Over-sampling technique Random over-sampling <NO> is the most simple and common method in oversampling technique. It increases the number of positive class by copying positive class samples randomly. This method really changes the distribution of dataset, but it has some shortcomings: copying too many positive class samples may cause classifier over-fitting , the time required for building classifiers becomes longer.
SMOTE algorithm <NO> is a classic oversampling algorithm. The basic idea of SMOTE is that new positive class samples are synthesized through linear interpolation between two near positive class samples, then add them to the original data-set. The two classes could be balanced by increasing new minority class samples. The specific approach is: for a positive class sample iX , calculate its distance from other samples of positive class, then select a sample jX from the k-nearest neighbor samples of positive class randomly, finally generate new samples as the following manner:
(0,1) ( )new i j iX X rand X X
According to Eq. 1, newX is added to participate in the training and testing. The method can prevent the occurrence of over-fitting effectively because it is not just copying positive class samples, but it cannot provide a scalar control of the number of new samples, and it cannot select positive class samples and synthesize new samples with guidance, so the quality of the new samples is not very good.
3) Mix sampling technique Both over-sampling and under-sampling are able to reduce the imbalance of data-set, but they have some drawbacks inevitably. C.Drummond <NO> proposed that the performance of classifiers which are built based on under-sampling technology is superior to the performance of classifiers which are built based on over-sampling technology, Chris Seiffert <NO> put forward a similar view from the model training complexity and training time, GEBatista <NO> thought that over-sampling technique was better than under-sampling techniques when there are overlaps in the data-set. There is not a uniform conclusion about which is better method. Therefore, combination of the two techniques is a common approach to imbalanced data classification."	https://doi.org/10.1109/IIKI.2014.14	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Li, et al. [NO] proposes an oversampling method based on support degree in order to guide people to select minority class samples and generate new minority class samples. 
A hybrid re-sampling method for SVM learning from imbalanced data sets	[]	Peng Li, Pei-Li Qiao, and Yuan-Chao Liu. 2008. A hybrid re-sampling method for SVM learning from imbalanced data sets. Fifth International Conference on Fuzzy Systems and Knowledge Discovery, 2008. FSKD’08. Vol. 2. IEEE, 65–69.	Support Vector Machine (SVM) has been widely studied and shown success in many application fields. However, the performance of SVM drops significantly when it is applied to the problem of learning from imbalanced data sets in which negative instances greatly outnumber the positive instances. This paper analyzes the intrinsic factors behind this failure and proposes a suitable re-sampling method. We re-sample the imbalance data by using variable SOM clustering so as to overcome the flaws of the traditional re-sampling methods, such as serious randomness, subjective interference and information loss. Then we prune the training set by means of K-NN rule to solve the problem of data confusion, which improves the generalization ability of SVM. Experiment results show that our method obviously improves the performance of the SVM on imbalanced data sets.	"Imbalanced datasets have two inner factors, namely, imbalance ratio (IR) and lack of information (LI). Imbalance ratio is the value of Number of Majority/ Number of Minority and LI is the lack of information for the monority class. For a data set consisting of 100:10 majority : minority examples the imbalance factor IR is the same as in a data set of 1000:100, but the intuition implicate us there are several defference in them. In the first case the minority class is poorly respresented and suffers more from the LI factor than in the second case. Both the above inherent factors are present in every IDS learning problem, in combination with other external factors, such as overlap, complexity, size of the data and high dimension etc.
We theoretically analyze Influencing factors by means of linear separable imbalanced datasets. In figure 1(a), SVM learning from an imbanlanced data set, the result show that the learning hyperplane has almost the same orientation as the ideal hyperplane, but the distance of the learning hyperplane is far away from the ideal hyperplane. Furthermore, learning hyperplane is too close to the positive support vectors. In figure 1(b), the learning hyperplane will lean toward the negative instances and mis-identify the positive instances to negative ones in the process of testing. The phenomemon is data-whelming, when the training data gets more imbalanced, the ratio between the positive and negative support vectors also becomes more skewed. They dicide the learning hyperplane far off the ideal hyperplane, the neighborhood of a testing instances close to the boundary is more likely to be dominated by negative support vectors and hence the decision function is more likely to classify a boundary point negative and led to the majority whelm the minority.
If we randomly under-sampe the majority instances of imbalance training data, until their numbers are equal to the minority instances in gross. In figure 2(a), we can
see that the learning hyperplane is close to ideal hyperplane but the orientation of the learning hyperplane is no longer accurate. In figure 2(b), the learning hyperplane will lead to severely wrong classification result in the process of testing. This phenomemon is information loss. The reason is that mass negative instances are cut down randomly lead to many valuable information is lossing, the remainder negative instances can no longer give good cues about the orientation of the hyperplane and there is a greater degree of freedom for the orientation to vary.
From above analysis, we get the guiding principle to our re-sampling method that is to find a strategy to filter large number of majority instances, which are far away from the target boundary, without losing too many minority instances. This allows us to concentrate on distinguishing the more difficult boundary instances and reduce the imbablance ratio which makes the learning task more tractable."	https://doi.org/10.1109/FSKD.2008.407	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Li, et al. [NO] analyzes the intrinsic factors behind this failure and proposes a suitable re-sampling method. 
Fuzzy support vector machines	['Fuzzy support vector machines (FSVMs) is a variant of the SVM learning algorithm, which was originally proposed in <NO> in order to handle the problem of outliers and noise.', 'Lin and Wang <NO> applies a fuzzymembership value for each training example, which is based on the importance of that example of its class, and reformulates the SVM learning algorithm such that different input points can make different contributions when finding the separating hyperplane.', 'initially proposed in <NO>, as a solution for the problem of outliers and noise.', 'As mentioned earlier, the FSVM method proposed in <NO> assigns different fuzzy-membership values mi (or weights) for different examples to reflect their importance for their own class, where more important examples are assigned higher membership values, while the less-important ones, such as outliers and noise, are assigned lower membership values.', 'In order to solve the FSVM optimization problem, (13) is transformed into the following dual Lagrangian <NO>:', '1) f(xi) is Based on the Distance from the Own Class Center: In this method, f(xi) is defined with respect to dcen i , which is the distance between xi and its own class center, as previously proposed for FSVMs in <NO>.']	Chun-Fu Lin, and Sheng-De Wang. 2002. Fuzzy support vector machines. IEEE Trans. Neur. Network. 13, 2 (2002), 464–471.	A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different constributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).	In this section we briefly review the basis of the theory of SVM in classification problems <NO>–<NO>.	https://doi.org/10.1109/TFUZZ.2010.2042721	2	['fsvms', 'svms', 'svm']	['outliers_noise', 'input_points', 'cil_methods', 'svm_algorithm', 'algorithm_sensitive']	['algorithm_sensitive_outliers', 'fuzzy_svms_fsvms', 'learns_decision_surface', 'presence_outliers_noise', 'problem_outliers_noise']	Lin, et al. [NO] applies a fuzzy membership to each input point and reformulate the svms such that different input points can make different constributions to the learning of decision surface. 
Exploratory undersampling for class-imbalance learning	['Literally hundreds of research papers have reported that imbalanced data lead to performance losses, and some sort of treatment (such as sampling <NO>, cost-sensitive learning <NO>, ensembles <NO>, among others) are able to improve classification.']	Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. 2009. Exploratory undersampling for class-imbalance learning. IEEE Trans. Syst. Man Cybernet. B 39, 2 (2009), 539–550.	Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. EasyEnsemble samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. BalanceCascade trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the ROC Curve, F-measure, and G-mean values than many existing classimbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods.	"As mentioned in the previous section, many existing classimbalance learning methods manipulate the following four components: training set size, class prior, cost matrix, and placement of decision boundary. Here, we pay special attention to two classes of methods that are most widely used: sampling
1083-4419/$25.00 © 2008 IEEE
and cost-sensitive learning. For other methods, we refer the readers to <NO> for a more complete and detailed review.
Sampling is a class of methods that alters the size of training sets. Undersampling and oversampling change the training sets by sampling a smaller majority training set and repeating instances in the minority training set, respectively <NO>. The level of imbalance is reduced in both methods, with the hope that a more balanced training set can give better results. Both sampling methods are easy to implement and have been shown to be helpful in imbalanced problems <NO>, <NO>. Undersampling requires shorter training time, at the cost of ignoring potentially useful data. Oversampling increases the training set size and thus requires longer training time. Furthermore, it tends to lead to overfitting since it repeats minority class examples <NO>, <NO>. Aside from the basic undersampling and oversampling methods, there are also methods that sample in more complex ways. SMOTE <NO> added new synthetic minority class examples by randomly interpolating pairs of closest neighbors in the minority class. The one-sided selection procedures <NO> tried to find a representative subset of majority class examples by only removing “borderline” and “noisy” majority examples. Some other methods combine different sampling strategies to achieve further improvement <NO>. In addition, researchers have studied the effect of varying the level of imbalance and how to find the best ratio when a C4.5 tree classifier was used <NO>.
Cost-sensitive learning <NO>, <NO> is another important class of class-imbalance learning methods. Although many learning algorithms have been adapted to accommodate class-imbalance and cost-sensitive problems, variants of AdaBoost appear to be the most popular ones. Many cost-sensitive boosting algorithms have been proposed <NO>. A common strategy of these variants was to intentionally increase the weights of examples with higher misclassification cost in the boosting process. In <NO>, the initial weights of high cost examples were increased. It was reported that, however, the weight differences between examples in different classes disappear quickly when the boosting process proceeds <NO>. Thus, many algorithms raised high cost examples’ weights in every iteration of the boosting process, for example, AsymBoost <NO>, AdaCost <NO>, CSB <NO>, DataBoost <NO>, and AdaUBoost <NO>, just to name a few. Another way to adapt a boosting algorithm to cost-sensitive problems is to change the weights of the weak classifiers in forming the final ensemble classifier, such as BMPM <NO> and LAC <NO>. Unlike the heuristic methods mentioned earlier, Asymmetric Boosting <NO> directly minimized a cost-sensitive loss function in the statistical interpretation of boosting.
SMOTEBoost <NO> is designed for class-imbalance learning, which is very similar to AsymBoost. Both methods alter the distribution for the minority class and majority class in separate ways. The only difference is how these distributions are altered. AsymBoost directly updates instance weights for the majority class and minority class differently in each iteration, while SMOTEBoost alters distribution by first updating instance weights for majority class and minority class equally and then using SMOTE to get new minority class instances.
Chan and Stolfo <NO> introduced an approach to explore majority class examples. They split the majority class into several nonoverlapping subsets, with each subset having ap-
proximately the same number of examples as the minority class. One classifier was trained from each of these subsets and the minority class. The final classifier ensembled these classifiers using stacking <NO>. However, when a data set is highly imbalanced, this approach requires a much longer training time than undersampling. Moreover, since the minority class examples are used by every classifier, stacking these classifiers will have a high probability of suffering from overfitting when the number of minority class examples is limited.
III. EasyEnsemble AND BalanceCascade
As was shown by Drummond and Holte <NO>, undersampling is an efficient strategy to deal with class imbalance. However, the drawback of undersampling is that it throws away many potentially useful data. In this section, we propose two strategies to explore the majority class examples ignored by undersampling: EasyEnsemble and BalanceCascade."	https://doi.org/10.1109/TSMCB.2008.2007853	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Liu, et al. [NO] proposes two algorithms to overcome this deficiency. 
Disturbing neighbors ensembles of trees for imbalanced data	[]	Juan J. Rodrı́guez, José-Francisco Dı́ez-Pastor, Jesús Maudes, and César Garcı́a-Osorio 2012. Disturbing neighbors ensembles of trees for imbalanced data. In 2012 11th International Conference on Machine Learning and Applications (ICMLA),	Disturbing Neighbors (DN ) is a method for generating classifier ensembles. Moreover, it can be combined with any other ensemble method, generally improving the results. This paper considers the application of these ensembles to imbalanced data: classification problems where the class proportions are significantly different. DN ensembles are compared and combined with Bagging, using three tree methods as base classifiers: conventional decision trees (C4.5), Hellinger distance decision trees (HDDT) —a method designed for imbalance data— and model trees (M5P) —trees with linear models at the leaves—. The methods are compared using two collections of imbalanced datasets, with 20 and 66 datasets, respectively. The best results are obtained combining Bagging and DN , using conventional decision trees. 	"As many other ensemble methods (e.g., Bagging, Random Subspaces) the Disturbing Neighbors (DN ) method is based on training each base classifier with a random transformation of the dataset.
In this method, the transformation adds new attributes. These attributes are obtained from a small sample of randomly selected training examples. These examples are called the Disturbing Neighbors.
The additional attributes are based on which one of the Disturbing Neighbors is the nearest. The first added attribute is nominal, the possible values are the class labels. For a binary class data set, the attribute will be binary. For a given example, the value of this attribute is the class label of the nearest Disturbing Neighbor.
For each Disturbing Neighbor an additional boolean attribute is included. The value of the attribute is true for a given example if the corresponding Disturbing Neighbor is the nearest one.
As an additional source of diversity (a necessary ingredient of successful ensembles) for each base classifier the distances are calculated in a random subspace. This random subspace is only used for determining the nearest Disturbing Neighbor, the transformed dataset contains all the original attributes.
It is possible to obtain ensembles using only the DN method, but usually is possible to obtain better results combining DN with other ensemble methods. The combination generally is better than the other ensemble method and the DN ensemble <NO>.
In Figure 1 the tree at the right was obtained with the same method and from the same dataset (glass4) as the other trees but with the additional attributes from the Disturbing Neighbors. One of this attributes, Nearest 8, was selected as the root of the tree. This attribute indicates if the nearest DN is the eighth.
From the considered sources of diversity, in <NO> it was determined that for non particularly imbalanced data sets the most important source was the use of the boolean features, the impact of the nominal feature and the calculation of the distances in a random subspace were less important."	https://doi.org/10.1109/ICMLA.2012.181	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	J. et al. [NO] considers the application of these ensembles to imbalanced data : classification problems where the class proportions are significantly different. 
RUSBoost: A hybrid approach to alleviating class imbalance	['On the other hand, with regard to ensemble learning methods, a large number of different approaches have been proposed in the literature, including but not limited to SMOTEBoost <NO>, RUSBoost <NO>, IIVotes <NO>, EasyEnsemble <NO>, or SMOTEBagging <NO>.', 'have arisen as a possible solution to the class imbalance problem attracting great interest among researchers <NO>, <NO>, <NO>, <NO>.', 'Inside this family, we include SMOTEBoost <NO>, MSMOTEBoost <NO>, RUSBoost <NO>, and DataBoost-IM <NO> algorithms.', ', Bagging, Boosting) has given good results <NO>, <NO>, <NO>.']	Chris Seiffert, Taghi M. Khoshgoftaar, Jason Van Hulse, and Amri Napolitano. 2010. RUSBoost: A hybrid approach to alleviating class imbalance. IEEE Trans.Syst. Man Cybernet. A 40, 1 (2010), 185–197.	Class imbalance is a problem that is common to many application domains. When examples of one class in a training data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classification models. Several techniques have been used to alleviate the problem of class imbalance, including data sampling and boosting. In this paper, we present a new hybrid sampling/boosting algorithm, called RUSBoost, for learning from skewed training data. This algorithm provides a simpler and faster alternative to SMOTEBoost, which is another algorithm that combines boosting and data sampling. This paper evaluates the performances of RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using 15 data sets from various application domains, four base learners, and four evaluation metrics. RUSBoost and SMOTEBoost both outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a simpler and faster technique. Given these experimental results, we highly recommend RUSBoost as an attractive alternative for improving the classification performance of learners built using imbalanced data.	"Much research has been performed with respect to the class imbalance problem. Weiss <NO> provides a survey of the class imbalance problem and techniques for reducing the negative impact imbalance that has on classification performance. The study identifies many methods for alleviating the problem of class imbalance, including data sampling and boosting, which are the two techniques investigated in this paper. Japkowicz <NO> presents another study addressing the issue of class imbalance, including an investigation of the types of imbalance that most negatively impact classification performance, and a small case study comparing several techniques for alleviating the problem.
Data sampling has received much attention in research related to class imbalance. Data sampling attempts to overcome imbalanced class distributions by adding examples to (oversampling) or removing examples from (undersampling) the data set. The simplest form of undersampling is RUS. RUS randomly removes examples from the majority class until a desired class distribution is found. While there is no universally accepted optimal class distribution, a balanced (50 : 50) distribution is often considered to be near optimal <NO>. However, when
examples from the minority class are very rare, a ratio closer to 35 : 65 (minority:majority) may result in better classification performance <NO>.
In addition to random data sampling techniques, several more “intelligent” algorithms for resampling data have been proposed. Barandela et al. <NO> and Han et al. <NO> examine the performance of some of these “intelligent” data sampling techniques, such as SMOTE, borderline SMOTE, and Wilson’s editing. Van Hulse et al. <NO> examine the performance of seven different data sampling techniques (both “intelligent” and random) using a large number of different learning algorithms and experimental data sets, finding both RUS and SMOTE to be very effective data sampling techniques.
Another technique for dealing with class imbalance is boosting. While boosting is not specifically designed to handle the class imbalance problem, it has been shown to be very effective in this regard <NO>. The most commonly used boosting algorithm is AdaBoost <NO>, which has been shown to improve the performance of any weak classifier, provided that the classifier results in better performance than random guessing. Several variations have been proposed to make AdaBoost cost sensitive <NO>–<NO> or to improve its performance on imbalanced data <NO>–<NO>. One of the most promising of these techniques is SMOTEBoost <NO>. SMOTEBoost combines an intelligent oversampling technique (SMOTE) with AdaBoost, resulting in a highly effective hybrid approach to learning from imbalanced data. Our proposed technique, which is RUSBoost, is based on the SMOTEBoost algorithm but provides a faster and simpler alternative for learning from imbalanced data with performance that is usually as good (and often better) than that of SMOTEBoost.
Closely related to the issue of class imbalance is costsensitive learning. Weiss et al. <NO> compare the performances of oversampling, undersampling, and cost-sensitive learning when dealing with data that have both an imbalanced class distribution and unequal error costs. Sun et al. <NO> present an in-depth examination of cost-sensitive boosting. Chawla et al. <NO> perform a detailed evaluation of a wrapper-based sampling approach to minimize misclassification cost. A detailed evaluation of RUSBoost as a cost-sensitive learning technique, including a comparison to existing methodologies, is left as a future work."	https://doi.org/10.1109/TSMCA.2009.2029559	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Seiffert, et al. [NO] presents a new hybrid sampling/boosting algorithm, called rusboost, for learning from skewed training data. 
An improved AdaBoost algorithm for unbalanced classification data	[]	Jie Song, Xiaoling Lu, and Xizhi Wu. 2009. An improved AdaBoost algorithm for unbalanced classification data. Sixth International Conference on Fuzzy Systems and Knowledge Discovery, 2009. FSKD’09. Vol. 1. IEEE, 109–113.	AdaBoost algorithm is proved to be a very efficient classification method for the balanced dataset with all classes having similar proportions. However, in real application, it is quite common to have unbalanced dataset with a certain class of interest having very small size. It will be problematic since the algorithm might predict all the cases into majority classes without loss of overall accuracy. This paper proposes an improved AdaBoost algorithm called BABoost (Balanced AdaBoost), which gives higher weights to the misclassified examples from the minority class. Empirical results show that the new method decreases the prediction error of minority class significantly with increasing the prediction error of majority class a little bit. It can also produce higher values of margin which indicates a better classification method	"In this paper, we propose a new algorithm called BABoost based on dividing the overall misclassification error into several parts. We want to utilize BABoost to improve the accuracy over the minority class without much sacrificing the accuracy over the majority class.
Adaboost algorithm gives equal weight to each misclassified example. But the misclassification error of each class is not same. Generally, the misclassification error of the minority class will larger than the majority’s. So Adaboost algorithm will lead to higher bias and smaller margin when encountering skew distribution. Our goal is to reduce the bias of Adaboost algorithm and increase the margin between each two classes. The proposed BABoost algorithm (Figure 1) in each round of boosting assigns more weights to the misclassified examples, especially those in the minority class. More generally, we focus on multiclass (J-class) problems in which in this paper. Following Schapire and Singer’s <NO>, <NO> approach to multiclass problems, we change the multiclass problem into two-class problems. That is, if equals , we set it equal to a 1 by vector with the element as +1 and -1 for others; if equals , then it equals a 1 by J-1 vector with all elements as -1. D denotes the resampling probabilities for each case and H is the matrix for the ensemble predictor.
The differences between BABoost and Adaboost are the calculation of , , and the update of the H. indicates the prediction error for the predictor when predicting the cases into the class. is the weight for the
predictor and class. For example, instead of the overall error in the AdaBoost, it will be two within group prediction errors: and for binary problems.
Except that and will lead to different weights to majority class and minority class, also give a strong impact to reweight the distribution of samples. Different class corresponds to its own . The value of should be positive and larger for majority class, smaller for minority class in order to emphasize more on minority class: The larger the is, the smaller the is. So the corresponding misclassified objects of the class get lower weights."	https://doi.org/10.1109/FSKD.2009.608	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Song, et al. [NO] proposes an improved adaboost algorithm called baboost (balanced adaboost), which gives higher weights to the misclassified examples from the minority class. 
Boosting for learning multiple classes with imbalanced class distribution	['Algorithmic methods include adjusting the costs associated with misclassification so as to improve performance <NO>, shifting the bias of a classifier to favor the rare class <NO>, creating Adaboost-like boosting schemes <NO>, and learning from one class <NO>.', 'Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', '<NO> extend the G-mean definition (see (17)) to the geometric means of recall values']	Yanmin Sun, Mohamed S. Kamel, and Yang Wang. 2006. Boosting for learning multiple classes with imbalanced class distribution. Sixth International Conference on Data Mining, 2006. ICDM’06. IEEE, 592–602.	Classification of data with imbalanced class distribution has posed a significant drawback of the performance attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. This learning difficulty attracts a lot of research interests. Most efforts concentrate on bi-class problems. However, bi-class is not the only scenario where the class imbalance problem prevails. Reported solutions for bi-class applications are not applicable to multi-class problems. In this paper, we develop a cost-sensitive boosting algorithm to improve the classification performance of imbalanced data involving multiple classes. One barrier of applying the cost-sensitive boosting algorithm to the imbalanced data is that the cost matrix is often unavailable for a problem domain. To solve this problem, we apply Genetic Algorithm to search the optimum cost setup of each class. Empirical tests show that the proposed cost-sensitive boosting algorithm improves the classification performances of imbalanced data sets significantly.	"The original AdaBoost algorithm reported in <NO> takes as input a training set {(x1, y1), · · ·, (xm, ym)} where each xi is an n-tuple of attribute values belonging to a certain domain or instance space X , and yi is a label in a label set Y = {−1, +1} in the context of bi-class applications. The Pseudocode for AdaBoost is given in Figure 1.
It has been shown in <NO> that the training error of the final classifier is bounded as
1 m |{i : H(xi) 6= yi}| ≤ ∏ t Zt (3)
Let
f(x) = T∑ t=1 αtht(x)
By unraveling the update rule of Equation 1, we have that
Dt+1(i) = exp(−
∑ t αtht(xi)yi)
m ∏
t Zt
= exp(−yif(xi))
m ∏
t Zt
(4)
By the definition of the final hypothesis of Equation 2, if H(xi) 6= yi, the yif(xi) ≤ 0 implying that exp(−yif(xi)) ≥ 1. Thus,
<NO> ≤ exp(−yif(xi)). (5)
where for any predicate π,
<NO> = { 1 if π holds 0 otherwise
(6)
Combining Equation 4 and 5 gives the error upper bound of Equation 3 since
1
m ∑ i <NO> ≤ 1 m ∑ i exp(−yif(xi)) (7)
= ∑
i
( ∏
t
Zt)D t+1(i) = ∏ t Zt (8)
To minimize the error upper-bound, on each boosting round, the learning objective is to minimize
Zt = ∑
i
Dt(i)exp(−αtyiht(xi)) (9)
= ∑
i
Dt(i)( 1 + yiht(xi) 2 e−α + 1− yiht(xi) 2 eα) (10)
Then, by minimizing Zt on each round, αt is induced as
αt = 1
2 log(
∑ i,yi=ht(xi) Dt(i)
∑ i,yi 6=ht(xi) Dt(i) ) (11)
The sample weight updating goal of AdaBoost is to decrease the weight of training samples which are correctly classified and increase the weights of the opposite part. Therefore, αt should be a positive value demanding that the training error should be less than randomly guessing (0.5) based on the current data distribution. That is
∑ i,yi=ht(xi) Dt(i) > ∑ i,yi 6=ht(xi) Dt(i) (12)"	https://doi.org/10.1109/ICDM.2006.29	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Sun, et al. [NO] develops a cost-sensitive boosting algorithm to improve the classification performance of imbalanced data involving multiple classes. 
Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval	[', Asymmetric Bagging <NO> and QuasiBagging <NO>.']	Dacheng Tao, Xiaoou Tang, Xuelong Li, and Xindong Wu. 2006. Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval. IEEE Trans. Pattern Anal. Mach. Intell. 28, 7 (2006), 1088–1099.	Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM’s optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance.	"FEEDBACK IN CONTENT-BASED IMAGE RETRIEVAL
SVM <NO>, <NO> is a very effective binary classification
algorithm. Consider a linearly separable binary classifica-
tion problem (as shown in Fig. 1):
fðxi; yiÞgNi¼1 and yi ¼ þ1; 1f g; ð1Þ
where xi is an n-dimension vector and yi is the label of the class that the vector belongs to. SVM separates the two
classes of points by a hyperplane,
wTxþ b ¼ 0; ð2Þ
where w is an input vector, x is an adaptive weight vector,
and b is a bias. SVM finds the parameters w and b for the
optimal hyperplane to maximize the geometric margin 2= k w k , subject to
yi w Txi þ b
þ1: ð3Þ
The solution can be found through a Wolfe dual problem
with the Lagrangian multiplied by i:
Qð Þ ¼ Xm i¼1 i Xm i;j¼1 i jyiyjðxi xjÞ=2; ð4Þ
subject to i 0 and Pm
i¼1 iyi ¼ 0. In the dual format, data points only appear in the inner
product. To get a potentially better representation of the
data, the data points are mapped into a Hilbert Inner
Product space through a replacement:
xi xj ! ðxiÞ ðxjÞ ¼ Kðxi;xjÞ; ð5Þ
where Kð:Þ is a kernel function. We then get the kernel version of the Wolfe dual problem:
Qð Þ ¼ Xm i¼1 i Xm i;j¼1 i jyiyjKðxi;xjÞ=2: ð6Þ
Thus, for a given kernel function, the SVM classifier is
given by
F xð Þ ¼ sgn f xð Þð Þ; ð7Þ
where f xð Þ ¼ Pl
i¼1 iyiK xi;xð Þ þ b is the output hyperplane decision function of the SVM.
In general, when f xð Þj j for a given pattern is high, the corresponding prediction confidence will be high. Meanwhile, a low f xð Þj j of a given pattern means that the pattern is close to the decision boundary and its corresponding
prediction confidence will be low. Consequently, the output of SVM, f xð Þ, has been used to measure the dissimilarity <NO>, <NO> between a given pattern and the query image, in
traditional SVM-based CBIR RF."	https://doi.org/10.1109/TPAMI.2006.134	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Tao, et al. [NO] develops a mechanism to overcome these problems. 
Regression error characteristic surfaces	[]	Luı́s Torgo 2005. Regression error characteristic surfaces. In KDD’05: Proc. of the 11th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining. ACM Press,	This paper presents a generalization of Regression Error Characteristic (REC) curves. REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems. REC curves provide useful information for analyzing the performance of models, particularly when compared to error statistics like for instance the Mean Squared Error. In this paper we present Regression Error Characteristic (REC) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable, i.e. the joint cumulative distribution function of the errors and the target variable. This provides a more detailed analysis of the performance of models when compared to REC curves. This extra detail is particularly relevant in applications with non-uniform error costs, where it is important to study the performance of models for specific ranges of the target variable. In this paper we present the notion of REC surfaces, describe how to use them to compare the performance of models, and illustrate their use with an important practical class of applications: the prediction of rare extreme values.	"Bi and Bennet <NO> have presented REC curves. These curves play a role similar to ROC curves (e.g. <NO>) in classification tasks, but for regression problems. They provide a graphical description of the cumulative distribution function of the error of a model, i.e. D( ) = P (ε ≤ ). The authors describe a simple algorithm for plotting these curves based on estimating the probabilities using the observed frequencies of the errors.
REC curves provide a better description of a model predictive performance when compared to prediction error statistics because they illustrate its performance across the range of possible errors. It is thus possible to extract more information by comparing the REC curves of two alternative models than with the two respective error statistics. Moreover, the interpretation of REC curves is quite appealing to non-experts and it is possible to obtain the same quantitative information given by prediction error statistics by calculating the Area Over the Curve (AOC), which Bi and Bennet <NO> have proved to be a biased estimate of the expected error of a model.
Figure 1 shows an example of the REC curves of three models. This example shows a model (model A) clearly
dominating the others over all range of possible errors. On the contrary models B and C have performances that are harder to compare. For smaller errors model C dominates model B, but as we move towards larger errors we see model B overcoming model C. The decision on which of these two is preferable may be domain dependent, provided their area over the curve (i.e. expected error) is similar.
In spite of the above mentioned advantages there are some specific domain requirements that are difficult to check using REC curves. These have to do with domains where the cost of errors is non-uniform, i.e. where the importance of an error with an amplitude of say 1.2, can be different depending on the true target variable value. For this type of applications, it may be important to inspect the distribution of the errors across the distribution of the target variable. In effect, it is possible to have two different models with exactly the same REC curves but still one being preferable to the other just because smaller errors occur for target values that are more relevant (e.g. have higher cost) for the application being studied. Distinguishing these two models and checking that one behaves more favorably than the other is not possible with the information provided by REC curves. This is the objective of our work."	https://doi.org/10.1145/1081870.1081959	3	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']	Torgo et al. [NO] presents a generalization of regression error characteristic (rec) curves. 
Experimental perspectives on learning from imbalanced data	['RUS has been shown to perform very well despite its simplicity <NO>.', ', sensitivity/specificity via ROC analysis, the g-mean) <NO>.', 'For classification, this simple approach works well <NO> and is theoretically motivated <NO>.', 'Data sampling techniques include random undersampling <NO>, onesided selection <NO>, Wilson’s editing <NO>, random oversampling,']	Jason Van Hulse, Taghi M. Khoshgoftaar, and Amri Napolitano. 2007. Experimental perspectives on learning from imbalanced data. Proceedings of the 24th International Conference on Machine Learning. ACM, 935–942.	We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the performance of learners built from imbalanced data? Is the effectiveness of sampling related to the type of learner? Do the results change if the objective is to optimize different performance metrics? We address these and other issues in this work, showing that sampling in many cases will improve classifier performance.	The 35 datasets utilized in our empirical study are listed in Table 1. The percentage of minority examples varies from 1.33% (highly imbalanced) to almost 35% (only slightly imbalanced). The datasets also come from a wide variety of application domains, and 19 are from the UCI repository (Blake & Merz, 1998). The Mammography dataset was generously provided by Dr. Nitesh Chawla (Chawla et al., 2002). Fifteen datasets (some of which are proprietary) are from the domain of software engineering measurements. We have also considered datasets with diversity in the number of attributes, and datasets with both continuous and categorical attributes. The smallest dataset had 214 total examples (Glass-3), while the two largest datasets each contain 20,000 observations. Note that all datasets have, or were transformed to have, a binary class. We only consider binary classification problems in this work.	https://doi.org/10.1145/1273496.1273614	3	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']	Van et al. [NO] presents a comprehensive suite of experimentation on the subject of learning from imbalanced data. 
Class probability estimates are unreliable for imbalanced data (and how to fix them)	[]	Byron C. Wallace, and Issa J. Dahabreh. 2012. Class probability estimates are unreliable for imbalanced data (and how to fix them). 2012 IEEE 12th International Conference on Data Mining (ICDM). IEEE, 695–704.	Obtaining good probability estimates is imperative for many applications. The increased uncertainty and typically asymmetric costs surrounding rare events increases this need. Experts (and classification systems) often rely on probabilities to inform decisions. However, we demonstrate that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. To our knowledge, this problem has not previously been explored. Motivated by our exposition of this issue, we propose a simple, effective and theoretically motivated method to mitigate the bias of probability estimates for imbalanced data that bags estimators calibrated over balanced bootstrap samples. This approach drastically improves performance on the minority instances without greatly affecting overall calibration. We show that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates. 	"The standard method for estimating probabilities in the supervised learning framework is to regress measurements correlated with (predicted) class labels output by a trained
1We note that Cieslak and Chawla have investigated the specific case of Probability Estimation Trees (PETs) for imbalanced data <NO>, and that Foster and Stine have considered the related task of variable selection for prediction under imbalance <NO>.
classifier against the true target labels <NO>, <NO>. This process is called calibration. By convention these measurements are denoted by fi, where i indexes instances. This calibration squashes the (arbitrarily scaled) fi’s into the <NO> range permissible for probabilities. When the sigmoid form is used (Equation 1), this method is referred to as Platt scaling <NO>. It is perhaps the most popular way of obtaining probability estimates from classification models.
P (yi|fi) = 1 1 + exp{−β0 − β1fi} (1)
The fi’s may be any scalar that is predictive of class membership. We focus on two specific post-training calibration strategies; Platt calibration with SVMs and with boosted decision trees. We selected these methods because they have been shown to out-perform other supervised learning algorithms with respect to class probability estimation <NO>, <NO>.
In the case of SVMs, the fi is taken as the signed distance from the hyperplane w, i.e., fi = wTxi. This was the method originally proposed by Platt <NO>, and is now widely used <NO>. Niculescu-Mizil and Caruana, meanwhile, have proposed attaining probabilities via calibrated boosted decision trees <NO>. More precisely, recall that in boosting one induces a sequence of learners h0, h1, ... , hk over different distributions of the training set. These are in turn associated with a set of weights α0, α1, ... , αk reflecting the their estimated performance. A prediction is then taken as a function over these, i.e., as sign( ∑ j αjhj(x)). The natural value for fi is then the sum of the weighted class predictions over the ensemble, i.e., ∑ j αjhj(x).
Figure 2 displays the overall and stratified residual errors of probability estimates (obtained via Platt’s method) for the instances comprising a particular imbalanced dataset.2 Specifically, each sub-plot shows histograms of the absolute differences between the true (observed) labels and corresponding probability estimates, i.e., |yi−P̂{yi|xi}|. Density to the left therefore suggests good calibration, as this implies probability estimates largely agree with the observed labels. For example, if yi = 1 and P̂{yi|xi} = .99, the difference would be .01. Were the estimate .01, on the other hand, the difference would be .99.
The left-hand side of Figure 2 shows this histogram for all instances, corresponding to overall calibration. Over 80% of instances are in the left-most bin, implying that the estimator is well-calibrated, i.e., its estimates do not much diverge from the observed labels. But this ostensibly good calibration belies the unreliability of the probability estimates for the instances comprising the rare class. One can see this by looking at the middle plot, which is the same figure but includes only minority instances. In this case, the estimates diverge strikingly from the observed
2The proton beam dataset in Table I.
labels; indeed the model assigned a probability (of belonging to the minority class) of less than 20% to most of the minority instances. In other words, the probability estimates for instances comprising the minority class are completely unreliable (we demonstrate this on 16 datasets in Section V)."	https://doi.org/10.1109/ICDM.2012.115	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	C. et al. [NO] demonstrates that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. 
Combating the small sample class imbalance problem using feature selection	[]	Mike Wasikowski, and Xue-wen Chen. 2010. Combating the small sample class imbalance problem using feature selection. IEEE Trans. Knowl. Data Eng. 22, 10 (2010), 1388–1400.	The class imbalance problem is encountered in real-world applications of machine learning and results in a classifier’s suboptimal performance. Researchers have rigorously studied the resampling, algorithms, and feature selection approaches to this problem. No systematic studies have been conducted to understand how well these methods combat the class imbalance problem and which of these methods best manage the different challenges posed by imbalanced data sets. In particular, feature selection has rarely been studied outside of text classification problems. Additionally, no studies have looked at the additional problem of learning from small samples. This paper presents a first systematic comparison of the three types of methods developed for imbalanced data classification problems and of seven feature selection metrics evaluated on small sample data sets from different applications. We evaluated the performance of these metrics using area under the receiver operating characteristic (AUC) and area under the precisionrecall curve (PRC). We compared each metric on the average performance across all problems and on the likelihood of a metric yielding the best performance on a specific problem. We examined the performance of these metrics inside each problem domain. Finally, we evaluated the efficacy of these metrics to see which perform best across algorithms. Our results showed that signal-tonoise correlation coefficient (S2N) and Feature Assessment by Sliding Thresholds (FAST) are great candidates for feature selection in most applications, especially when selecting very small numbers of features.	The two Learning from Imbalanced Data Sets workshops thoroughly explored the approaches to combating the class imbalance problem: sampling, new algorithms, and feature selection. The first was held at the AAAI conference in 2000 <NO>, and the second was held at the ICML conference in 2003 <NO>. Also, Weiss reviewed these approaches in SIGKDD Explorations <NO>.	https://doi.org/10.1109/TKDE.2009.187	1	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']	Wasikowski, et al. [NO] presents a first systematic comparison of the three types of methods developed for imbalanced data classification problems and of seven feature selection metrics evaluated on small sample data sets from different applications. 
An active under-sampling approach for imbalanced data classification	[]	Zeping Yang, and Daqi Gao. 2012. An active under-sampling approach for imbalanced data classification. 2012 Fifth International Symposium on Computational Intelligence and Design (ISCID). Vol. 2. IEEE, 270–273.	An active under-sampling approach is proposed for handling the imbalanced problem in this paper. Traditional classifiers usually assume that training examples are evenly distributed among different classes, so they are often biased to the majority class and tend to ignore the minority class. In this case, it is important to select the suitable training dataset for learning from imbalanced data. The samples of the majority class which are far away from the decision boundary should be got rid of the training dataset automatically in our algorithm, and this process doesn’t change the density distribution of the whole training dataset. As a result, the ratio of majority class is decreased significantly, and the final balance training dataset is more suitable for the traditional classification algorithms. Compared with other under-sampling methods, our approach can effectively improve the classification accuracy of minority classes while maintaining the overall classification performance by the experimental results. 	"V. CONCLUSION Learning from imbalanced dataset is a challenging problem, because traditional classifiers are designed on the assumption that training examples are evenly distributed among different classes. Sampling methods can change data distribution by adding or deleting samples from the original training dataset. An active under-sampling approach has been presented on imbalanced data in this paper. Instead of getting rid of majority class samples randomly, our algorithm
actively selected the samples of majority class near the decision boundary, and, at the same time, maintained the original density distribution. The experimental results show that the proposed algorithm can achieve better performance compared to other methods on imbalanced datasets. For the future, the proposed method and suitable data cleaning technique were combined to handle highly imbalanced and overlapping datasets."	https://doi.org/10.1109/ISCID.2012.219	3	['sampling', 'related', 'usually']	['sampling_method', 'classi_er', 'performance_models', 'rec_curves', 'roc_space']	['cumulative_distribution_function', 'analyzing_performance_models', 'costs_class_frequencies', 'distribution_function_errors', 'error_characteristic_rec']	an active under-sampling approach is proposed for handling the imbalanced problem in Yang, et al. [NO].
An unsupervised learning approach to resolving the data imbalanced issue in supervised learning problems in functional genomics	[]	Kihoon Yoon, and Stephen Kwek. 2005. An unsupervised learning approach to resolving the data imbalanced issue in supervised learning problems in functional genomics. Fifth International Conference on Hybrid Intelligent Systems, 2005. HIS’05. IEEE, 6–pp.	Learning from imbalanced data occurs very frequently in functional genomic applications. One positive example to thousands of negative instances is common in scientific applications. Unfortunately, traditional machine learning treats the extremely small instances as noise. The standard approach for this difficulty is balancing training data by resampling them. However, this results in high false positive predictions. Hence, we propose preprocessing majority instances by partitioning them into clusters. This greatly reduces the ambiguity between minority instances and instances in each cluster. For moderately high imbalance ratio and low in-class complexity, our technique gives better prediction accuracy than undersampling method. For extreme imbalance ratio like splice site prediction problem, we demonstrate that this technique serves as a good filter with almost perfect recall that reduces the amount of imbalance so that traditional classification techniques can be deployed and yield significant improvements over previous predictor. We also show that the technique works for subcellular localization and post-translational modification site prediction problems.	"Most supervised learning algorithms tend to focus on obtaining high accuracy on the observed labeled training data. To further aggravate this difficulty, almost all algorithms tend to follow the Occam’s razor principle (or related minimum description length MDL principle) where there is a preference toward simple
hypothesis <NO>. Short decision trees and neural networks with small weights are preferred. The underlying assumption here is that events (instances) that occur infrequently are considered as noise. This further discriminates against the minority class so as to achieve high overall prediction accuracy. For highly imbalance data, the classifiers constructed using these algorithms would simply predict negative all the time and achieve almost 100% accuracy! This is nonsensical for applications in functional genomic (and computer security) where the aims are to detect minority instances within a certain reasonable tolerance of false positive mistakes.
Various approaches <NO> have been proposed to tackle the challenge posed by the imbalance ratio problem. These approaches fall into two different categories, namely weighting or resampling based methods. Weighting methods either assign heavier weights to the minority training instances or penalties for misclassifications of minority instances <NO>. The other way is to preprocess training data to minimize discrepancy between the classes. Oversampling <NO> the minority class and undersampling <NO> the majority class are the data level approaches. Ling and Li <NO> combining oversampling and undersampling methods but did not achieve significant improvement in the ""lift index"" metric that they used. Both methods effectively change the training distribution to one that no longer resemble the original (highly imbalance) distribution, resulting in overfitting. Other important related works similar to resampling approaches are to focus on solving small disjuncts problem within each class. Japkowicz <NO> discussed about the cause for lower performance in standard classifiers is actually small disjuncts of within-class. These works agree with what we observed from our experiments."	https://doi.org/10.1109/ICHIS.2005.23	0	['minority', 'learner', 'prediction']	['minority_class', 'imbalanced_data', 'active_learning', 'boosting_algorithm', 'class_distributions']	['imbalanced_data_sets', 'cost_sensitive_boosting', 'cost_test_example', 'data_set_classification', 'experimental_results_method']	Yoon, et al. [NO] proposes preprocessing majority instances by partitioning them into clusters. 
Feature selection for text categorization on imbalanced data	['Zheng, Wu, and Srihari empirically tested different ratios of features indicating membership in a class versus features indicating lack of membership in a class <NO>.', 'Conversely, if features are chosen based on their absolute value, Zheng, Wu, and Srihari argue that we may not select a ratio of positive to negative features that gives the best results based on the imbalance in the data <NO>.', 'Zheng <NO> used the Naive Bayes classifier and logistic regression methods, and Forman <NO> used the linear SVM and noted its superiority over decision trees, Naive Bayes, and logistic regression.']	Zhaohui Zheng, Xiaoyun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced data. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 80–89.	A number of feature selection metrics have been explored in text categorization, among which information gain (IG), chi-square (CHI), correlation coefficient (CC) and odds ratios (OR) are considered most effective. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and nonmembership (e.g. negative features) by ignoring the signs of features. The former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data. In this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework. Using multinomial näıve Bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.	"In this section, we present six feature selection metrics (four known measures and two proposed variants), which are functions of the following four dependency tuples:
1. (t, ci): presence of t and membership in ci.
2. (t, ci): presence of t and non-membership in ci.
3. (t, ci): absence of t and membership in ci.
4. (t, ci): absence of t and non-membership in ci.
where: t and ci represent a term and a category respectively. The frequencies of the four tuples in the collection are denoted by A, B, C and D respectively. The first and last tuples represent the positive dependency between t and ci, while the other two represent the negative dependency.
Information gain (IG) Information gain <NO> measures the number of bits of information obtained for category prediction by knowing the presence or absence of a term in a document. The information gain of term t and category ci is defined to be:
IG(t, ci) = ∑
c∈{ci,ci}
∑
t′∈{t,t}
P (t′, c) · log P (t ′, c)
P (t′) · P (c)
Information gain is also known as Expected Mutual Information. The Expected Likelihood Estimation (ELE) smoothing technique was used in this paper to handle singularities when estimating those probabilities.
Chi-square (CHI) Chi-square measures the lack of independence between a term t and a category ci and can be compared to the chi-square distribution with one degree of freedom to judge extremeness <NO>. It is defined as:
χ2(t, ci) = N <NO>2
P (t)P (t)P (ci)P (ci)
where: N is the total number of documents.
Correlation coefficient (CC) Correlation coefficient of a word t with a category ci was defined by Ng et al. as <NO>
CC(t, ci) =
√ N <NO>
√
P (t)P (t)P (ci)P (ci)
It is a variant of the CHI metric, where CC2 = χ2. CC can be viewed as a “one-sided” chi-square metric.
Odds ratio (OR) Odds ratio measures the odds of the word occurring in the positive class normalized by that of the negative class. The basic idea is that the distribution of features on the relevant documents is different from the distribution of features on the nonrelevant documents. It has been used by Mladenić for selecting terms in text categorization <NO>. It is defined as follows:
OR(t, ci) = log P (t|ci)<NO> <NO>P (t|ci)
Similar to IG, ELE smoothing was used when estimating those conditional probabilities.
According to the definitions, OR considers the first two dependency tuples, and IG, CHI, and CC consider all the four tuples. CC and OR are one-sided metrics, whose positive and negative values correspond to the positive and negative features respectively. On the other hand, IG and CHI are two-sided, whose values are non-negative. We can easily obtain that the sign for a one-sided metric, e.g. CC or OR, is sign(AD −BC).
A one-sided metric could be converted to its two-sided counterpart by ignoring the sign, while a two-sided metric could be converted to its one-sided counterpart by recovering the sign, e.g. CHI vs. CC.
We propose the two-sided counterpart of OR, namely ORsquare, and the one-sided counterpart of IG, namely signed IG as follows.
OR-square (ORS) and Signed IG (SIG)
ORS(t, ci) = OR 2(t, ci),
SIG(t, ci) = sign(AD −BC) · IG(t, ci)
The overall feature selection procedure is to score each potential feature according to a particular feature selection metric, and then take the best features. Feature selection using one-sided metrics like SIG, CC, and OR pick out the terms most indicative of membership only. The basic idea behind this is the features coming from non-relevant documents are useless. They will never consider negative features unless all the positive features have already been selected. Feature selection using two-sided metrics like IG, CHI, and ORS, however, do not differentiate between the positive and negative features. They implicitly combine the two."	https://doi.org/10.1145/1007730.1007741	1	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']	Zheng, et al. [NO] investigates the usefulness of explicit control of that combination within a proposed feature selection framework. 
Training cost-sensitive neural networks with methods addressing the class imbalance problem	['Other examples of cost-sensitive learning include the MetaCost framework <NO>, cost-sensitive neural network <NO>, costsensitive support vector machines (SVMs) <NO>, and others.', 'Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'domains <NO>, <NO>, <NO>, cost-sensitive learning is superior to', 'However, we note that a more significant performance increase can be achieved by applying this estimate to ensemble methods by using cross-validation techniques on a given set; a similar approach is considered in <NO>, however using a slightly different estimate.', 'cost-sensitive neural networks <NO>, <NO>, the ensemble', 'For instance, in cost-sensitive learning, it is natural to use misclassification costs for performance evaluation for multiclass imbalanced problems <NO>, <NO>, <NO>.', 'Algorithmic techniques have been developed for the different classification algorithms, such as neural networks <NO>, decision trees <NO>, fuzzy systems <NO>, <NO> etc.']	Zhi-Hua Zhou, and Xu-Ying Liu. 2006. Training cost-sensitive neural networks with methods addressing the class imbalance problem. IEEE Trans. Knowl. Data Eng. 18, 1 (2006), 63–77.	This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and softensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that costsensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training costsensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.	"Suppose there areC classes and the ith class hasNi number of training examples. LetCost½i; c ði; c 2 f1::CgÞdenote the cost of misclassifying an example of the ith class to the cth class (Cost½i; i ¼ 0) and Cost½i ði 2 f1::CgÞ denote the cost of the ith class. Moreover, suppose the classes are ordered such that, for the ith class and the jth class, if i < j, then ðCost½i < Cost½j Þor ðCost½i ¼ Cost½j and Ni NjÞ.Cost½i is usually derived from Cost½i; c . There are many possible rules for the derivation, among which a popular one is Cost½i ¼PC
c¼1 Cost½i; c <NO>, <NO>."	https://doi.org/10.1109/TKDE.2006.17	1	['feature', 'selection', 'metric']	['feature_selection', 'correlation_coefficient', 'evaluation_measure', 'negative_features', 'selection_methods']	['feature_selection_methods', 'the_class_imbalance', 'applications_machine_learning', 'assessment_sliding_thresholds', 'classifier’s_suboptimal_performance']	Zhou, et al. [NO] studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. 
Improving malware classification: Bridging the staticdynamic gap	[]	Blake Anderson, Curtis Storlie, and Terran Lane. 2012. Improving malware classification: Bridging the static/dynamic gap. Proceedings of 5th ACM Workshop on Security and Artificial Intelligence (AISec).	Malware classification systems have typically used some machine learning algorithm in conjunction with either static or dynamic features collected from the binary. Recently, more advanced malware has introduced mechanisms to avoid detection in these views by using obfuscation techniques to avoid static detection and execution-stalling techniques to avoid dynamic detection. In this paper we construct a classification framework that is able to incorporate both static and dynamic views into a unified framework in the hopes that, while a malicious executable can disguise itself in some views, disguising itself in every view while maintaining malicious intent will prove to be substantially more difficult. Our method uses kernels to place a similarity metric on each distinct view and then employs multiple kernel learning to find a weighted combination of the data sources which yields the best classification accuracy in a support vector machine classifier. Our approach opens up new avenues of malware research which will allow the research community to elegantly look at multiple facets of malware simultaneously, and which can easily be extended to integrate any new data sources that may become popular in the future.	I.5.2 <NO>: Classifier design and evaluation; K.6.5 <NO>: Invasive software (e.g., viruses, worms, Trojan horses	https://doi.org/10.1145/2381896.2381900	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Anderson, et al. [NO] constructs a classification framework that is able to incorporate both static and dynamic views into a unified framework in the hopes that, while a malicious executable can disguise itself in some views, disguising itself in every view while maintaining malicious intent will prove to be substantially more difficult. 
A survey on heuristic malware detection techniques	[]	Zahra Bazrafshan, Hashem Hashemi, Seyed Mehdi Hazrati Fard, and Ali Hamzeh. 2013. A survey on heuristic malware detection techniques. Proceedings of the 5th Conference on Information and Knowledge Technology (IKT).	Malware is a malicious code which is developed to harm a computer or network. The number of malwares is growing so fast and this amount of growth makes the computer security researchers invent new methods to protect computers and networks. There are three main methods used to malware detection: Signature based, Behavioral based and Heuristic ones. Signature based malware detection is the most common method used by commercial antiviruses but it can be used in the cases which are completely known and documented. Behavioral malware detection was introduced to cover deficiencies of signature based method. However, because of some shortcomings, the heuristic methods have been introduced. In this paper, we discuss the state of the art heuristic malware detection methods and briefly overview various features used in these methods such as API Calls, OpCodes, N-Grams etc. and discuss their advantages and disadvantages. Keywords-Malware Detection, Computer Security, API Call, NGram, OpCode, Control Flow Graph.	"Nowadays pattern matching is the most common method in malware detection, and signature based detection is the most popular method in this area <NO>. Signature is a unique feature for each file, something like a fingerprint of an executable. Signature based methods use the patterns extracted from various malwares to identify them and are more efficient and faster than any other methods. These signatures are often extracted with special sensitivity for being unique, so those detection methods that use this signature have small error rate. Where this small error rate is the main reason that most common commercial antiviruses use this technique <NO>.
These methods are unable to detect unknown malware variants and also requires high amount of manpower, time, and money to extract unique signatures. These are the main disadvantages of these methods. Also, inability to confront against the malwares that mutate their codes in each infection such as polymorphic and metamorphic one is another disadvantage. To tackle these challenges, research societies propose completely new malware detection family."	http://scholar.google.com/scholar?hl=en&q=Zahra+Bazrafshan%2C+Hashem+Hashemi%2C+Seyed+Mehdi+Hazrati+Fard%2C+and+Ali+Hamzeh.+2013.+A+survey+on+heuristic+malware+detection+techniques.+In+Proceedings+of+the+5th+Conference+on+Information+and+Knowledge+Technology+%28IKT%29.+10.1109%2FIKT.2013.6620049	1	['heuristic', 'virus', 'use']	['n_grams', 'heuristic_methods', 'account_dependence', 'approach_conducts', 'classifier_taking']	['detect_malicious_executables', 'account_dependence_relationships', 'anti_virus_systems', 'approach_conducts_exhaustive', 'classifier_taking_account']	Bazrafshan, et al. [NO] discusses the state of the art heuristic malware detection methods and briefly overview various features used in these methods such as api calls, opcodes, n-grams etc. 
Web tap: Detecting covert web traffic	[]	Kevin Borders, and Atul Prakash. 2004. Web tap: Detecting covert web traffic. Proceedings of the 11th ACM Conference on Computer and Communications Security.	As network security is a growing concern, system administrators lock down their networks by closing inbound ports and only allowing outbound communication over selected protocols such as HTTP. Hackers, in turn, are forced to find ways to communicate with compromised workstations by tunneling through web requests. While several tools attempt to analyze inbound traffic for denial-of-service and other attacks on web servers, Web Tap’s focus is on detecting attempts to send significant amounts of information out via HTTP tunnels to rogue Web servers from within an otherwise firewalled network. A related goal of Web Tap is to help detect spyware programs, which often send out personal data to servers using HTTP transactions and may open up security holes in the network. Based on the analysis of HTTP traffic over a training period, we designed filters to help detect anomalies in outbound HTTP traffic using metrics such as request regularity, bandwidth usage, interrequest delay time, and transaction size. Subsequently, Web Tap was evaluated on several available HTTP covert tunneling programs as well as a test backdoor program, which creates a remote shell from outside the network to a protected machine using only outbound HTTP transactions. Web Tap’s filters detected all the tunneling programs tested after modest use. Web Tap also analyzed the activity of approximately thirty faculty and students who agreed to use it as a proxy server over a 40 day period. It successfully detected a significant number of spyware and aware programs. This paper presents the design of Web Tap, results from its evaluation, as well as potential limits to Web Tap’s capabilities.	"Network security has been an increasing concern for network administrators and executives alike. Consequently, Firewalls and proxy servers have become prevalent among high-security networks (and even private homes). Many networks require all traffic to the internet to go through an HTTP proxy server or mail server, allowing no direct access to the internal network. This makes the job of a hacker much more difficult than before, where direct access to network machines was available.
When a hacker attacks a network with no direct access to the internet, the first step is getting a user to access a malicious file or web site. This can be done effectively by e-mailing a Trojan horse program or a link to a page which exploits the browser <NO>. Once the machine is compromised, the next step is to establish a path of communication. Traditionally, this would be done by installing a backdoor program such as BackOrifice <NO>. The problem with using such programs on firewalled networks is that they listen for an incoming connection on a specific port. All incoming traffic, however, is blocked. This means that the only way to communicate with a compromised machine is to have it make a callback (outbound connection). Often, the only two ways out of the network are through a mail server or through a proxy server. Since e-mail is often more closely logged and filtered, the hacker may find outbound HTTP transactions to be the best avenue for communication with a compromised workstation.
Spyware is also a huge problem for both system administrators and users alike <NO>. Besides annoying users by popping up advertisements, spyware can leak information about a user’s behavior or even send data on the machine to outside servers. Spyware programs can also degrade system performance and take valuable time and effort to remove. In addition to these lesser threats, Security holes have been found in Gator and eZula (two popular spyware programs) that would allow a hacker to execute arbitrary code on a target machine <NO>.
Web Tap is a network-level anomaly detection system that takes advantage of legitimate web request patterns to detect covert communication, backdoors, and spyware activity that is tunneled through outbound HTTP connections. We note that, unlike the previous work on securing web servers (e.g., <NO>), Web Tap’s focus is on analyzing outbound HTTP traffic from protected network machines to outside web servers, rather than guarding web servers against hostile attacks. The goal is to make it more difficult for hackers or malicious users to run Trojan and HTTP tunnel programs within an organization that leak information to the outside. Web Tap is designed for deployment at an organization’s HTTP proxy server (either passively or actively) to help detect anomalies in outbound traffic.
To evaluate Web Tap, we used it to look at web traffic from 30 clients over a 40-day period as well as traffic from known Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CCS’04, October 25-29, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-961-6/04/0010…$5.00.
HTTP tunneling programs. We were successful at detecting different types of spyware and adware as well as many data mining and advertisement servers. During the 40 days of observation, Web Tap generated alerts for adware clients such as Weatherbug, Kazaa, Lycos search bar, Google search bar, and Gator. It also was able to find programs which may be unwanted in a workplace environment such as iTunes, BitTorrent, and AIM Express. In addition to non-browser clients, Web Tap detected data mining and advertisement sites such as coremetrics.com, ru4.com abetterinternet.com, and doubleclick.net. Two of the three known HTTP tunneling programs tested, Wsh <NO> and Firepass <NO>, immediately caused Web Tap to raise an alert. The third HTTP tunnel, Hopster <NO>, was detected an hour and twenty minutes after it began running. A custom HTTP tunnel that we designed, which does a better job of mimicking legitimate browser requests, was detected within seven hours. When the backdoor was actively used to transfer files, it was detected almost immediately.
The rest of the paper is presented as follows. Section 2 discusses related work. Section 3 gives a threat model. Section 4 presents the design of filtering methods, based on measurements during the one week training phase. Section 5 provides an evaluation of Web Tap for an extended period after the evaluation phase. Section 6 talks about vulnerabilities in Web Tap filters. Section 7 outlines future work and Section 8 concludes."	https://doi.org/10.1145/1030083.1030100	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Borders, et al. [NO] designs filters to help detect anomalies in outbound http traffic using metrics such as request regularity, bandwidth usage, interrequest delay time, and transaction size. 
Polyglot: Automatic extraction of protocol message format using dynamic binary analysis	[', Polyglot <NO> and Panorama <NO>), Siren <NO> and oth-']	Juan Caballero, Heng Yin, Zhenkai Liang, and Dawn Song. 2007. Polyglot: Automatic extraction of protocol message format using dynamic binary analysis. Proceedings of the 14th ACM Conference on Computer and Communications Security (CCS).	Protocol reverse engineering, the process of extracting the application-level protocol used by an implementation, without access to the protocol specification, is important for many network security applications. Recent work [17] has proposed protocol reverse engineering by using clustering on network traces. That kind of approach is limited by the lack of semantic information on network traces. In this paper we propose a new approach using program binaries. Our approach, shadowing, uses dynamic analysis and is based on a unique intuition—the way that an implementation of the protocol processes the received application data reveals a wealth of information about the protocol message format. We have implemented our approach in a system called Polyglot and evaluated it extensively using real-world implementations of five different protocols: DNS, HTTP, IRC, Samba and ICQ. We compare our results with the manually crafted message format, included in Wireshark, one of the state-ofthe-art protocol analyzers. The differences we find are small and usually due to different implementations handling fields in different ways. Finding such differences between implementations is an added benefit, as they are important for problems such as fingerprint generation, fuzzing, and error detection.	Security	https://doi.org/10.1145/1315245.1315286	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Caballero, et al. [NO] proposes a new approach using program binaries. 
Mining specifications of malicious behavior	['So far, several data mining and machine-learning approaches have been used in malware detection <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'tion <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'In order to overcome the disadvantages of the widely-used signature-based malware detection method, data mining and machine learning approaches are proposed for malware detection <NO>.', 'Recent advances in malware analysis <NO> show', '<NO> propose a technique by comparing the execution behavior of a known malware against the execution behaviors of a set of benign programs.', 'Principal dynamic techniques include virtual machine inspection <NO>, function call monitoring <NO>, <NO>, dynamic binary instrumentation <NO> and information flow tracking <NO>.', 'introspection <NO>, information flow tracking <NO>, <NO>, instruction trace monitoring <NO>–<NO>.', 'Capturing information flow in dependence graphs: Existing techniques for constructing dependence graphs from programs provide only data-flow (and sometimes controlflow) dependencies between operations <NO>, <NO>, <NO>.']	Mihai Christodorescu, Somesh Jha, and Christopher Kruegel. 2007. Mining specifications of malicious behavior. Proceedings of ESEC/FSE.	Malware detectors require a specification of malicious behavior. Typically, these specifications are manually constructed by investigating known malware. We present an automatic technique to overcome this laborious manual process. Our technique derives such a specification by comparing the execution behavior of a known malware against the execution behaviors of a set of benign programs. In other words, we mine the malicious behavior present in a known malware that is not present in a set of benign programs. The output of our algorithm can be used by malware detectors to detect malware variants. Since our algorithm provides a succinct description of malicious behavior present in a malware, it can also be used by security analysts for understanding the malware. We have implemented a prototype based on our algorithm and tested it on several malware programs. Experimental results obtained from our prototype indicate that our algorithm is effective in extracting malicious behaviors that can be used to detect malware variants.	"Malicious software (malware) is code that achieves the harmful intent of an attacker. Typical examples include viruses, worms, trojans, and spyware. Although the history of malware reaches back more than two decades, the advent of large-scale computer worm epidemics and waves of email viruses have elevated the problem to a major security threat. Recently, this threat has also acquired an economic dimension as attackers benefit financially from compromised machines (e.g., by selling hosts as email relays to spammers).
Historically, detection tools such as virus scanners have performed poorly, particularly when facing previously unknown malware programs or novel variants of existing ones. The fundamental cause is the disconnect between the malware specification used for detection and the actual malware behavior (which is the attacker’s goal). Certain malicious behavior desired by an attacker (e.g., virus self replication through mass-mailing) can be realized in many different ways. However, current detectors focus only on the specific characteristics of individual malware instances, e.g., on the presence of particular instruction sequences. Therefore, they fail to detect different manifestations of the same malicious behavior. Attackers are quick to exploit this weakness by using program obfuscation techniques such as polymorphism and metamorphism <NO>. Recent research results have highlighted how shortcomings in both network-based and host-based detection techniques can be effectively exploited by attackers to evade detection <NO>.
Advanced detection techniques such as semantics-aware malware detection <NO> and malicious-code model checking <NO> counter the obfuscation techniques of attackers by using higher-level specifications of malicious behavior. Instead of focusing on individual characteristics of particular malware instances, these detectors specify general behavior exhibited by an entire family of malicious code. Examples of such specifications include self-unpacking and selfpropagation via email. The power of these approaches resides in the use of high-level specifications that abstract details specific to a malware instance. Thus, obfuscation transformations, which preserve the behavior of the malware but may change its form, are no longer effective techniques for evading detection.
Unfortunately, the high-level specifications of malicious behavior used by these advanced malware detectors are currently manually developed. Creating specifications manually is a time-consuming task that requires expert knowledge, which reduces the appeal and deployment of these new detection techniques. To address this limitation, this paper
introduces a technique to automatically derive specifications of malicious behavior from a given malware sample. Such a specification can then be used by a malware detector, allowing for the creation of an end-to-end tool-chain to update malware detectors when a new malware appears. Since our technique provides a succinct description of the malicious intent of a malware, it can also be used by security analysts for malware understanding.
We cast malicious-specification mining as the problem of finding differences between a malware sample and a set of benign programs. This approach supports the requirement that the specification of malicious behavior must capture aspects that are specific to the malware and absent from any benign programs. The software-engineering research community has put a lot of effort in analyzing commonalities and differences between programs and many techniques for clone detection <NO> and program differencing <NO> have been proposed. The differencing techniques are closest to our goal of mining specifications of malicious behavior, but they fall short because they generally require access to source code and because they produce differences between low-level elements of the program (e.g., individual statements) or between structural elements (e.g., type hierarchy, procedures). Since we do not have the malware source code and since the malware writer controls the structure of the program, mining malicious specifications requires a new approach.
Our mining technique takes into account an adversarial setting in which the malware writer tries to make his software hard to analyze and detect. We define a new graph representation of program behavior and a mining algorithm that constructs a malicious specification. The representation explicitly captures the system calls made by the program and summarizes all other program code, because system calls are the primary interaction with the operating system. Our algorithm infers the system-call graphs from execution traces, then derives a specification by computing the minimal differences between the system-call graphs of malicious and benign programs.
This paper makes the following contributions:
• A language for specifying malicious behavior in terms of dependences between system calls (Section 3).
• An algorithm, called MiniMal1 that mines specifications of malicious behavior from dependence graphs (Section 4).
• An experimental evaluation that shows that specifications extracted MiniMal are qualitatively equivalent to those manually developed by Symantec’s expert virus analysts and can be used with a malware detector to identify subsequent malware variants (Section 5)."	https://doi.org/10.1145/1287624.1287628	2	['behavior', 'malware', 'algorithm']	['anti_malware', 'malware_categorization', 'random_knn', 'file_contents', 'discriminative_specifications']	['anti_malware_industry', 'automatic_malware_categorization', 'access_processing_behavior', 'anti_malware_software', 'calculating_mutual_information']	Christodorescu, et al. [NO] presents an automatic technique to overcome this laborious manual process. 
On deriving unknown vulnerabilities from zero-day polymorphic and metamorphic worm exploits	['Although signature detection techniques are widely used, they are not effective against zero-day attacks (new malicious code), polymorphic attacks (different encryptions of the same binary), or metamorphic attacks (different code for the same functionality) <NO>.']	Jedidiah R. Crandall, Zhendong Su, S. Felix Wu, and Frederic T. Chong. 2005. On deriving unknown vulnerabilities from zero-day polymorphic and metamorphic worm exploits. Proceedings of the 12th ACM Conference on Computer and Communications Security (CCS).	Vulnerabilities that allow worms to hijack the control flow of each host that they spread to are typically discovered months before the worm outbreak, but are also typically discovered by third party researchers. A determined attacker could discover vulnerabilities as easily and create zero-day worms for vulnerabilities unknown to network defenses. It is important for an analysis tool to be able to generalize from a new exploit observed and derive protection for the vulnerability. Many researchers have observed that certain predicates of the exploit vector must be present for the exploit to work and that therefore these predicates place a limit on the amount of polymorphism and metamorphism available to the attacker. We formalize this idea and subject it to quantitative analysis with a symbolic execution tool called DACODA. Using DACODA we provide an empirical analysis of 14 exploits (seven of them actual worms or attacks from the Internet, caught by Minos with no prior knowledge of the vulnerabilities and no false positives observed over a period of six months) for four operating systems. Evaluation of our results in the light of these two models leads us to conclude that 1) single contiguous byte string signatures are not effective for content filtering, and tokenbased byte string signatures composed of smaller substrings are only semantically rich enough to be effective for content filtering if the vulnerability lies in a part of a protocol that is not commonly used, and that 2) practical exploit analysis must account for multiple processes, multithreading, and kernel processing of network data necessitating a focus on primitives instead of vulnerabilities. 	Zero-day worms that exploit unknown vulnerabilities are a very real threat. Typically vulnerabilities are discovered by “white hat” hackers using fuzz testing <NO>, reverse engineering, or source code analysis and then the software vendors are notified. The same techniques for discovering these vulnerabilities could be as easily employed by “black hat” hackers, especially now that computer criminals are increasingly seeking profit rather than mischief. None of the 14 exploits analyzed in this paper are for vulnerabilities discovered by the vendors of the software being attacked. A vulnerability gives the attacker an important primitive (a primitive is an ability the attacker has, such as the ability to write an arbitrary value to an arbitrary location in a process’ address space), and then the attacker can build different exploits using this primitive. The host contains information about the vulnerability and primitive that cannot be determined from network traffic alone. It is impossible to generalize how the attack might morph in the future without this information. In order to respond effectively during an incipient worm outbreak, an automated analysis tool must be able to generalize one instance of an exploit and derive protection for the exploited vulnerability, since a worm can build multiple exploits for the same vulnerability from primitives.	https://doi.org/10.1145/1102120.1102152	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	R. et al. [NO] formalizes this idea and subject it to quantitative analysis with a symbolic execution tool called dacoda. 
Adversarial classification	[]	Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. 2004. Adversarial classification. Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 99–108.	Essentially all data mining algorithms assume that the datagenerating process is independent of the data miner’s activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary’s optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary’s evolving manipulations.	"Many major applications of KDD share a characteristic that has so far received little attention from the research community: the presence of an adversary actively manipulating the data to defeat the data miner. In these domains, deployment of a KDD system causes the data to change so as to make the system ineffective. For example, in the domain of email spam detection, standard classifiers like naive Bayes were initially quite successful (e.g., <NO>). Unfortunately, spammers soon learned to fool them by inserting “non-spam” words into emails, breaking up “spam” ones with spurious punctuation, etc. Once spam filters were modified to detect these tricks, spammers started using new ones <NO>. Effectively, spammers and data miners are engaged in a never-ending game where data miners continually come up with new ways to detect spam, and spammers continually come up with new ways to avoid detection.
Similar arms races are found in many other domains: computer intrusion detection, where new attacks circumvent the defenses put in place against old ones <NO>; fraud detection, where perpetrators learn to avoid the actions that previously gave them away <NO>; counter-terrorism, where terrorists disguise their identity and activities in ever-shifting ways <NO>; aerial surveillance, where targets are camouflaged with increasing sophistication <NO>; comparison shopping, where merchants continually change their Web sites to avoid wrapping by shopbots <NO>; file sharing, where media companies try to detect and frustrate illegal copying, and users find ways to circumvent the obstacles <NO>; Web search, where webmasters manipulate pages and links to inflate their rankings, and search engines reengineer their ranking functions to deflate them back again <NO>; etc.
In many of these domains, researchers have noted the presence of adaptive adversaries and the need to take them into account (e.g., <NO>), but to our knowledge no systematic approach for this has so far been developed. The result is that the performance of deployed KDD systems in adversarial domains can degrade rapidly over time, and much human effort and cost is incurred in repeatedly bringing the systems back up to the desired performance level. This paper proposes a first step towards automating this process. While complete automation will never be possible, we believe our approach and its future extensions have the potential to significantly improve the speed and cost-effectiveness of keeping KDD systems up to date with their adversaries.
Notice that adversarial problems cannot simply be solved by learners that account for concept drift (e.g., <NO>): while these learners allow the data-generating process to change
over time, they do not allow this change to be a function of the classifier itself.
We first formalize the problem as a game between a costsensitive classifier and a cost-sensitive adversary (Section 2). Focusing on the naive Bayes classifier (Section 3), we describe the optimal strategy for the adversary against a standard (adversary-unaware) classifier (Section 4), and the optimal strategy for a classifier playing against this strategy (Section 5). We provide efficient algorithms for computing or approximating these strategies. Experiments in a spam detection domain illustrate the sometimes very large utility gains that an adversary-aware classifier can yield, and its ability to co-evolve with the adversary (Section 6). We conclude with a discussion of future research directions (Section 7)."	https://doi.org/10.1145/2381896.2381900	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Dalvi, et al. [NO] develops a formal framework and algorithms for this problem. 
Semantics-based online malware detection: Towards efficient real-time protection against malware	[]	Sanjeev Das, Yang Liu, Wei Zhang, and Mahintham Chandramohan. 2016. Semantics-based online malware detection: Towards efficient real-time protection against malware. IEEE Transactions on Information Forensics and Security 11, 2 (2016), 289–302.	Recently, malware has increasingly become a critical threat to embedded systems, while the conventional software solutions, such as antivirus and patches, have not been so successful in defending the ever-evolving and advanced malicious programs. In this paper, we propose a hardwareenhanced architecture, GuardOL, to perform online malware detection. GuardOL is a combined approach using processor and field-programmable gate array (FPGA). Our approach aims to capture the malicious behavior (i.e., highlevel semantics) of malware. To this end, we first propose the frequency-centric model for feature construction using system call patterns of known malware and benign samples. We then develop a machine learning approach (using multilayer perceptron) in FPGA to train classifier using these features. At runtime, the trained classifier is used to classify the unknown samples as malware or benign, with early prediction. The experimental results show that our solution can achieve high classification accuracy, fast detection, low power consumption, and flexibility for easy functionality upgrade to adapt to new malware samples. One of the main advantages of our design is the support of early prediction—detecting 46% of malware within first 30% of their execution, while 97% of the samples at 100% of their execution, with <3% false positives.	Since the early days of computing, malware has evolved from the simple exploits into the complex ones in the forms of — virus, worm, trojan, adware, spyware, backdoor, flooder, botnet, rootkit and bootkits <NO>. Over the years, the motivation of malware authors has changed from exploits-for-fun to money-making business. With the arms race between security professionals and malware writers, the present day malware has highly evolved, which uses sophisticated techniques to exploit the vulnerabilities. Malware uses several channels to penetrate in the system, e.g., phising emails, usb, memory card, corrupt downloaded files, click on links, repackaged into normal applications, browser utilities, abusing application stores or exploiting old vulnerabilities <NO>. Our work is based on the behavior of the malware in the host system, i.e., after they have penetrated in the system.	http://scholar.google.com/scholar?hl=en&q=Sanjeev+Das%2C+Yang+Liu%2C+Wei+Zhang%2C+and+Mahintham+Chandramohan.+2016.+Semantics-based+online+malware+detection%3A+Towards+efficient+real-time+protection+against+malware.+IEEE+Transactions+on+Information+Forensics+and+Security+11%2C+2+%282016%29%2C+289%2D%2D302.+10.1109%2FTIFS.2015.2491300	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Das, et al. [NO] proposes a hardwareenhanced architecture, guardol, to perform online malware detection. 
Ether: Malware analysis via hardware virtualization extensions	['We choose the Intel Pin program <NO> because it allowed us to collect both instructions and system calls simultaneously, but it does not make an effort to be a transparent tracing tool like the Ether framework <NO>.', 'introspection <NO>, information flow tracking <NO>, <NO>, instruction trace monitoring <NO>–<NO>.']	Artem Dinaburg, Paul Royal, Monirul Sharif, and Wenke Lee. 2008. Ether: Malware analysis via hardware virtualization extensions. Proceedings of the 15th ACM Conference on Computer and Communications Security (CCS).	Malware has become the centerpiece of most security threats on the Internet. Malware analysis is an essential technology that extracts the runtime behavior of malware, and supplies signatures to detection systems and provides evidence for recovery and cleanup. The focal point in the malware analysis battle is how to detect versus how to hide a malware analyzer from malware during runtime. State-of-the-art analyzers reside in or emulate part of the guest operating system and its underlying hardware, making them easy to detect and evade. In this paper, we propose a transparent and external approach to malware analysis, which is motivated by the intuition that for a malware analyzer to be transparent, it must not induce any side-effects that are unconditionally detectable by malware. Our analyzer, Ether, is based on a novel application of hardware virtualization extensions such as Intel VT, and resides completely outside of the target OS environment. Thus, there are no in-guest software components vulnerable to detection, and there are no shortcomings that arise from incomplete or inaccurate system emulation. Our experiments are based on our study of obfuscation techniques used to create 25,000 recent malware samples. The results show that Ether remains transparent and defeats the obfuscation tools that evade existing approaches.	"Malware–the increasingly common vehicle by which criminal organizations facilitate online crime–has become an artifact whose use intersects multiple major security threats (e.g., botnets) faced by information security practitioners. Given the financially motivated nature of these threats, methods of recovery now mandate more than just remediation: knowing what occurred after an asset became compromised is as valuable as knowing it was compromised. Concisely, independent of simple detection, there exists a pronounced need to understand the intentions or runtime behavior of modern malware.
Recent advances in malware analysis <NO> show promise in understanding modern malware, but before these and other approaches can be used to determine what a malware instance does or might do, the runtime behavior of that instance and/or an unobstructed view of its code must be obtained. However, malware authors are incentivized to complicate attempts at understanding the internal workings of their creations. Therefore, modern malware contain a myriad of anti-debugging, anti-instrumentation, and anti-VM techniques to stymie attempts at runtime observation <NO>. Similarly, techniques that use a malware instance’s static code model are challenged by runtime-generated code, which often requires execution to discover.
In the obfuscation/deobfuscation game played between attackers and defenders, numerous anti-evasion techniques have been applied in the creation of robust in-guest API call tracers and automated deobfuscation tools <NO>. More recent frameworks <NO> and their discrete components <NO> attempt to offer or mimic a level of transparency analogous to that of a non-instrumented OS running on physical hardware. However, given that nearly all of these approaches reside in or emulate part of the guest OS or its underlying hardware, little effort is required by a knowledgeable adversary to detect their existence and evade <NO>.
In this paper we present a transparent, external approach to malware analysis. Our approach is motivated by the intuition that for a malware analyzer to be transparent, it must not induce any side-effects that are unconditionally detectable by its observation target. In formalizing this intuition, we model the structural properties and execution semantics of modern programs to derive the requirements for transparent malware analysis. An analyzer that satisfies these transparency requirements can obtain an execution trace of a program identical to that if it were run in an environment with no analyzer present. Approaches unable to fulfill these requirements are vulnerable to one or more de-
tection attacks–categorical, formal abstractions of detection techniques employed by modern malware.
Creating a transparent malware analyzer required us to diverge from existing approaches that employ in-guest components, API virtualization or partial or full system emulation, because none of these implementations satisfy all the transparency requirements. Based on novel application of hardware virtualization extensions such as Intel VT <NO>, our analyzer–called Ether–resides completely outside of the target OS environment– there are no in-guest software components vulnerable to detection or attack. Additionally, in contrast to other external approaches, the hardware-assisted nature of our approach implicitly avoids many shortcomings that arise from incomplete or inaccurate system emulation.
To demonstrate the efficacy of our approach we tested Ether with other academic and commercial approaches. Our testing included the analysis of specific in-the-wild malware instances that attempt to detect instrumentation and/or a virtual environment. In addition, we also surveyed over 25,000 recent malware samples to identify the distribution of obfuscation tools used in their creation; this knowledge was then used to create a synthetic sample set that represents the majority of the original corpus. The results of testing (presented in Section 5) show that Ether is able to remain transparent and defeat a large percentage of the obfuscation tools that evade existing approaches.
Our work represents the following contributions:
• A formal framework for describing program execution and analyzing the requirements for transparent malware analysis.
• Implementation of Ether, an external, transparent malware analyzer that operates using hardware virtualization extensions to offer both fine- (single instruction) and coarse- (system call) granularity tracing. To motivate the use of our approach by the information security community, the GPL’ed source code for Ether is available for download at http://ether.gtisc.gatech.edu.
• Broad-scale evaluation of current approaches using a proxy set of samples representing the majority of a recent, large malware corpus. Copies of discrete samples referenced in this paper and the 25,000 malware sample corpus used for our survey are available to any academic or industry professional at an accredited organization.
The remainder of this paper is organized as follows. Section 2 describes related work. Section 3 presents our model for programs and their execution, formal requirements for transparency, and abstract representations of failures in transparency that lead to detection attacks. Section 4 describes Ether’s design and implementation, including an in-depth explanation of how hardware virtualization extensions are leveraged. Section 5 details the experiment selection process and how experimentation was performed, and provides an analysis of the results. Finally, Section 6 briefly describes future work and provides some concluding remarks."	https://doi.org/10.1145/1455770.1455779	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Dinaburg, et al. [NO] proposes a transparent and external approach to malware analysis, which is motivated by the intuition that for a malware analyzer to be transparent, it must not induce any side-effects that are unconditionally detectable by malware. 
A survey on automated dynamic malware-analysis techniques and tools	['Since the early days of computing, malware has evolved from the simple exploits into the complex ones in the forms of — virus, worm, trojan, adware, spyware, backdoor, flooder, botnet, rootkit and bootkits <NO>.', 'Traditionally, the two major approaches for malware detection can be roughly split based on the approach that is used to analyze the malware, either static and dynamic analysis (see review <NO>).', 'Machine learning has been applied to malware detection at least since <NO>, with numerous approaches since (see reviews <NO>).']	Manuel Egele, Theodoor Scholte, Engin Kirda, and Christopher Kruegel. 2012. A survey on automated dynamic malware-analysis techniques and tools. ACM Computing Surveys (CSUR) 44, 2 (2012), 6.	Anti-virus vendors are confronted with a multitude of potentially malicious samples today. Receiving thousands of new samples every day is not uncommon. The signatures that detect confirmed malicious threats are mainly still created manually, so it is important to discriminate between samples that pose a new unknown threat and those that are mere variants of known malware. This survey article provides an overview of techniques based on dynamic analysis that are used to analyze potentially malicious samples. It also covers analysis programs that employ these techniques to assist human analysts in assessing, in a timely and appropriate manner, whether a given sample deserves closer manual inspection due to its unknown malicious behavior.	"The Internet has become an essential part of daily life as more and more people use services that are offered on the Internet. The Internet has evolved from a basic communication network to an interconnected set of information sources enabling, new forms of (social) interactions and marketplaces for the sale of products and services among other things. Online banking or advertising are examples of the commercial aspects of the Internet. Just as in the physical world, there are people on the Internet with malevolent intents, who strive to enrich themselves by taking advantage of legitimate users whenever money is involved. Malware (i.e., software of malicious intent) helps these people accomplish their goals.
To protect legitimate users from these threats, security vendors offer tools that aim to identify malicious software components. Typically, these tools apply some sort of signature matching process to identify known threats. This technique requires the
This work has been supported by the European Commission through project FP7-ICT-216026-WOMBAT, by FIT-IT through the SECoverer project, and by Secure Business Austria. Corresponding author’s address: M. Egele, Automation Systems Group (E183-1), Vienna University of Technology, Treitlstr. 1, 1040 Vienna, Austria; email: manuel@seclab.tuwien.ac.at. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c© 2012 ACM 0360-0300/2012/02-ART6 $10.00 DOI 10.1145/2089125.2089126 http://doi.acm.org/10.1145/2089125.2089126
ACM Computing Surveys, Vol. 44, No. 2, Article 6, Publication date: February 2012.
vendor to provide a database of signatures which are then compared against potential threats. Once the security vendor obtains a sample of a new potential threat to study, the first step is for a human analyst to determine whether this (so far unknown) sample poses a threat to users by analyzing the sample. If the sample poses a threat, the analyst attempts to find a pattern that allows her to identify this sample (i.e., the signature). This pattern should be generic enough to also match with variants of the same threat, but not falsely match on legitimate content. The analysis of malware and the successive construction of signatures by human analysts is time-consuming and error-prone. It is also trivial for malware authors to automatically generate a multitude of different malicious samples derived from a single malware instance. It is not extraordinary for an anti-virus vendor to receive thousands of unknown samples per day. Symantec <NO> (averaging 4,300 per day) as well as McAfee <NO> (averaging 12,300 per day) report receiving over 1.6M new samples in 2008. This substantial quantity requires an automated approach to quickly differentiating between samples that deserve closer (manual) analysis and those that are a variation of an already known threats. This automatic analysis can be performed in two ways. Dynamic analysis refers to techniques that execute a sample and verify the actions this sample performs in practice, while static analysis performs its task without actually executing the sample.
This article focuses on the techniques that can be applied to analyze potential threats and discriminate samples that are mere variations of already known threats. In addition, it presents the currently available tools and their underlying approaches to performing automated dynamic analysis on potentially malicious software."	https://doi.org/10.1145/2089125.2089126	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Egele, et al. [NO] article provides an overview of techniques based on dynamic analysis that are used to analyze potentially malicious samples. 
Synthesizing near-optimal malware specifications from suspicious behaviors	[]	Matt Fredrikson, Somesh Jha, Mihai Christodorescu, Reiner Sailer, and Xifeng Yan. 2010. Synthesizing near-optimal malware specifications from suspicious behaviors. Proceedings of IEEE Symposium on Security and Privacy.	Fueled by an emerging underground economy, malware authors are exploiting vulnerabilities at an alarming rate. To make matters worse, obfuscation tools are commonly available, and much of the malware is open source, leading to a huge number of variants. Behavior-based detection techniques are a promising solution to this growing problem. However, these detectors require precise specifications of malicious behavior that do not result in an excessive number of false alarms. In this paper, we present an automatic technique for extracting optimally discriminative specifications, which uniquely identify a class of programs. Such a discriminative specification can be used by a behavior-based malware detector. Our technique, based on graph mining and concept analysis, scales to large classes of programs due to probabilistic sampling of the specification space. Our implementation, called HOLMES, can synthesize discriminative specifications that accurately distinguish between programs, sustaining an 86% detection rate on new, unknown malware, with 0 false positives, in contrast with 55% for commercial signature-based antivirus (AV) and 62-64% for behavior-based AV (commercial or research).	"The behavior of a program can be thought of as its effect on the state and environment of the system on which it executes. Most malware relies on system calls to deliver a malicious payload, so reasoning about behavior in terms of the system calls made by a program allows us to succinctly and precisely capture the intent of the malware author, while ignoring many implementation-specific artifacts. Using this representation, we wish to derive a behavioral specification that is descriptive of a given set of programs (the positive set of programs) but does not describe any program in a second set (the negative set of programs). In the malware detection case, the positive set consists of malicious programs and the negative set consists of benign programs, and the goal is to construct a specification that is characteristic of the malicious programs but not of the benign programs.
For an arbitrarily chosen positive set of programs one is unlikely to find a single behavior common to all of them; if there is one such common behavior, it is likely also present in the programs from the negative set. Thus, we need to partition the positive set into subsets of similar programs, such that programs in the same subset share many behaviors. This leads us to the high-level workflow of our technique, which is presented in 1 and proceeds as follows:
I. The positive set of programs is divided into disjoint subsets of behaviorally similar programs. This can be performed manually, or using existing malware clustering techniques <NO>, <NO>.
II. Using existing techniques for dependence-graph construction <NO>, a graph is constructed for each malware and benign application to represent its behavior.
III. The significant behaviors specific to each positive subset are mined. (III)
A significant behavior is a sequence of operations that distinguishes the programs in a positive subset from
all of the programs in the negative subset. We use structural leap mining <NO> to identify multiple distinct graphs that are present in the dependence graphs of the positive subset and absent from the dependence graphs of the negative set.
IV. The significant behaviors mined from each positive subset are combined to obtain a discriminative specification for the whole positive set. (IV)
Two significant behaviors can be combined either by merging them into one significant behavior that is more general, or by taking the union of the two behaviors. We use concept analysis to identify the behaviors that can be combined with little or no increase in the coverage rate of the negative set, while maintaining the coverage rate of the positive set. As there are exponentially many ways of combining behaviors, we use probabilistic sampling to approximate the optimal solution in an efficient manner."	https://doi.org/10.1109/SP.2010.11	2	['behavior', 'malware', 'algorithm']	['anti_malware', 'malware_categorization', 'random_knn', 'file_contents', 'discriminative_specifications']	['anti_malware_industry', 'automatic_malware_categorization', 'access_processing_behavior', 'anti_malware_software', 'calculating_mutual_information']	Fredrikson, et al. [NO] presents an automatic technique for extracting optimally discriminative specifications, which uniquely identify a class of programs. 
The WEKA data mining software: An update	['The tests and experiments were conducted using Weka <NO> 3.', 'For classification, we use the Weka machine learning open-source package <NO>.', 'WEKA tool <NO> for the offline evaluation of machine learning classifier.', '5 decision tree (J48), naive bayes (NB), logistic regression (LR), support vector machine (SVM) using LibSVM, sequential minimal optimization (SMO), RIPPER rule learner (JRIP) and multilayer perceptron (MLP) <NO>.']	Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. ACM SIGKDD Explorations Newsletter (2009).	More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on SourceForge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.	"The WEKA project aims to provide a comprehensive collection of machine learning algorithms and data preprocessing tools to researchers and practitioners alike. It allows users to quickly try out and compare different machine learning methods on new data sets. Its modular, extensible architecture allows sophisticated data mining processes to be built up from the wide collection of base learning algorithms and tools provided. Extending the toolkit is easy thanks to a simple API, plugin mechanisms and facilities that automate the integration of new learning algorithms with WEKA’s graphical user interfaces.
The workbench includes algorithms for regression, classification, clustering, association rule mining and attribute selection. Preliminary exploration of data is well catered for by data visualization facilities and many preprocessing tools. These, when combined with statistical evaluation of learning schemes and visualization of the results of learning, supports process models of data mining such as CRISP-DM <NO>."	https://doi.org/10.1145/1656274.1656278	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Hall, et al. [NO] provides an introduction to the weka workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (weka 3.4) released in 2003. 
A feature selection and evaluation scheme for computer virus detection	['<NO> used intra-family and inter-family support for n-grams for feature selection.', 'N-grams Static Hybrid <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO> Misuse <NO>']	Olivier Henchiri, and Nathalie Japkowicz. 2006. A feature selection and evaluation scheme for computer virus detection. Proceedings of the 6th International Conference on Data Mining.	Anti-virus systems traditionally use signatures to detect malicious executables, but signatures are overfitted features that are of little use in machine learning. Other more heuristic methods seek to utilize more general features, with some degree of success. In this paper, we present a data mining approach that conducts an exhaustive feature search on a set of computer viruses and strives to obviate over-fitting. We also evaluate the predictive power of a classifier by taking into account dependence relationships that exist between viruses, and we show that our classifier yields high detection rates and can be expected to perform as well in real-world conditions.	"Current research applying data mining to virus detection strives to automate the search for features used in classification. This process has been tackled from two different angles: extracting optimal signatures from a dataset of viruses, and discovering more general features for use in a complex classification scheme.
Extracting virus signatures is not a new problem. Kephart et al. <NO> developed a popular extraction method for virus signatures, by infecting a large number of files with a given virus and then harvesting for constant regions of 12 to 36 bytes. Then, from the considerable number of signatures collected, the ones with lowest predicted false positive rates were selected. While this method make it possible to extract signatures quickly and without the help of an expert, the authors concede that the algorithm fails for viruses that are moderately polymorphic.
Some detection methods utilize a variety of features, such as Win32 dll file calls, ASCII strings and byte sequences contained in the binary files. In an early heuristic approach <NO>, features such as duplicated UNIX system calls and files targeted by the program for writing purposes were used to detect malicious executables. In a machine learning method developed by Matthew Schultz et al. <NO>, ASCII strings and bytes sequences yielded good results. However, despite the byte sequences having a fixed length of 16, the feature space was very large, such that their dataset had to be split into partitions and different classifiers trained separately on each of them. Research in non signature-based heuristics has shown that sequences as short as 4 bytes can be used to detect unseen virus instances successfully <NO>. However, as was found in <NO>, the list of candidate features extracted from a small dataset can contain tens of thousands of sequences.
Finally, many viruses are considered to belong to common virus families, based on the similarities in structure, code or method of infection that they share <NO>. This classification is crucial to properly evaluating the effectiveness of a virus detection system. The first occurrence of a new kind of virus is typically the most devastating, as virus scanners are often incapable of detecting it. Then a host of variants typically emerge soon after the initial outbreak, albeit with less damaging consequences. Our method uses a priori knowledge of virus families, and evaluates the ability of our classifier to detect instances of a family without having been trained on any other instance from that same family."	https://doi.org/10.1109/ICDM.2006.4	1	['heuristic', 'virus', 'use']	['n_grams', 'heuristic_methods', 'account_dependence', 'approach_conducts', 'classifier_taking']	['detect_malicious_executables', 'account_dependence_relationships', 'anti_virus_systems', 'approach_conducts_exhaustive', 'classifier_taking_account']	Henchiri, et al. [NO] presents a data mining approach that conducts an exhaustive feature search on a set of computer viruses and strives to obviate over-fitting. 
Statistical pattern recognition: A review	['<NO>, <NO> and Bishop <NO>, feature selection', 'selection can be found in <NO>, <NO>, and <NO>.', 'Some more survey papers can also be found in <NO>, <NO>, <NO>, <NO>, and <NO>.', 'second-order relationships in the covariance matrix, Other linear transforms, like independent component analysis (ICA) and projection pursuit, which use higher order statistical information, are more suited for non-Gaussian distributions <NO>, <NO>.', 'Identifying the most representative features is critical to improve the performance and efficiency of the classifiers <NO>.', ', feature selection (or variable selection, among many other names) <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, is critical to minimize the classification error.', ', (see <NO>, <NO>, <NO> for a detailed comparison.', 'In other words, “them best features are not the bestm features” <NO>, <NO>, <NO>, <NO>.', ', <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>) and select features with the minimal redundancy (Min-Redundancy).', 'The data set HDR <NO>, <NO>, <NO>, <NO> contains 649 features for 2,000 handwritten digits.', 'This seems underconstrained, especially when rules of thumb suggest that one needs ten times more examples than features <NO>.']	Anil K. Jain, Robert P.W. Duin, and Jianchang Mao. 2000. Statistical pattern recognition: A review. IEEE Trans. Pattern Anal. Mach. Intell. 22, 1 (2000), 4–37.	ÐThe primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field. Index TermsÐStatistical pattern recognition, classification, clustering, feature extraction, feature selection, error estimation, classifier combination, neural networks.	"BY the time they are five years old, most children canrecognize digits and letters. Small characters, large characters, handwritten, machine printed, or rotatedÐall are easily recognized by the young. The characters may be written on a cluttered background, on crumpled paper or may even be partially occluded. We take this ability for granted until we face the task of teaching a machine how to do the same. Pattern recognition is the study of how machines can observe the environment, learn to distinguish patterns of interest from their background, and make sound and reasonable decisions about the categories of the patterns. In spite of almost 50 years of research, design of a general purpose machine pattern recognizer remains an elusive goal.
The best pattern recognizers in most instances are
humans, yet we do not understand how humans recognize
patterns. Ross <NO> emphasizes the work of Nobel Laureate
Herbert Simon whose central finding was that pattern
recognition is critical in most human decision making tasks:
ªThe more relevant patterns at your disposal, the better
your decisions will be. This is hopeful news to proponents
of artificial intelligence, since computers can surely be
taught to recognize patterns. Indeed, successful computer
programs that help banks score credit applicants, help
doctors diagnose disease and help pilots land airplanes
depend in some way on pattern recognition... We need to pay much more explicit attention to teaching pattern recognition.º Our goal here is to introduce pattern recognition as the best possible way of utilizing available sensors, processors, and domain knowledge to make decisions automatically."	https://doi.org/10.1109/34.824819	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Index TermsÐStatistical pattern recognition, classification, clustering, feature extraction, feature selection, error estimation, classifier combination, neural networks [NO].
Renovo: A hidden code extractor for packed executables	['Automated unpackers–common applications for fine-grained analysis–include PolyUnpack <NO> and Renovo <NO>.']	Min Gyung Kang, Pongsin Poosankam, and Heng Yin. 2007. Renovo: A hidden code extractor for packed executables. Proceedings of the 5th ACM Workshop on Recurring Malcode (WORM).	As reverse engineering becomes a prevalent technique to analyze malware, malware writers leverage various anti-reverse engineering techniques to hide their code. One technique commonly used is code packing as packed executables hinder code analysis. While this problem has been previously researched, the existing solutions are either unable to handle novel samples, or vulnerable to various evasion techniques. In this paper, we propose a fully dynamic approach that captures an intrinsic nature of hidden code execution that the original code should be present in memory and executed at some point at run-time. Thus, this approach monitors program execution and memory writes at run-time, determines if the code under execution is newly generated, and then extracts the hidden code of the executable. To demonstrate its effectiveness, we implement a system, Renovo, and evaluate it with a large number of real-world malware samples. The experiments show that Renovo is accurate compared to previous work, yet practical in terms of performance.	"Reverse engineering is one of the main techniques used for malware analysis. To make the analysis more difficult, malware writers usually have their programs heavy-armored with various anti-reverse engineering techniques. Such techniques include binary and source code obfuscation <NO>, control-flow obfuscation <NO>, instruction virtualization <NO>, and binary code packing <NO>. This paper focuses on identifying and extracting the hidden code generated using binary code packing, one of the most common anti-reverse engineering methods. Code packing transforms a program into a packed program by compressing or encrypting the original code and data into packed data and associating it with a restoration routine. A restoration routine is a piece of code for recovering the original code and data as well as setting an execution context to the original code when the packed program is executed. This technique is available as commercial products <NO> and open-source tools. According to the anti-virus (AV) program test results of AVTest GmbH <NO>, the detection rates of 8 major AV programs varied from 10% to 80% when known malware binaries have been packed.
Various tools have been developed to identify and extract the hidden code in packed executables. Commonly known tools such as PEiD <NO> employ a simple pattern matching approach. These tools check an executable with a signature database to determine what kind of packing tool is used to create the executable. Then, using a priori knowledge about the packing tool, it is possible to extract the hidden binary from the executable <NO>. Although this approach is usually fast and accurate for known packing tools, it is unable to detect novel and modified packing techniques. For example, a variant of the Bagle worm employed its own compression engine which is not known to the public <NO>. In fact, by modifying the open source anti-reverse engineering tools like YodaProtector <NO>, it is easy for malware writers to implement new anti-reverse engineering algorithms and tricks.
Dynamic analysis is a promising solution to the problem of hidden code extraction because it does not depend on signatures. Regardless of what packing technique might be applied to the original program, the original code or its equivalent must eventually be present in memory and get executed at some point at run-time. By taking advantage of this intrinsic nature of packed executables, one could potentially extract the hidden binary code or its equivalent as a raw memory dump. However, it is not clear which regions in the memory contain the hidden binary and when is the right time to dump such regions, i.e., when the execution context
jumps to the hidden original code. In addition to the hidden code, other information such as the original entry point (OEP) is also crucial for further analyses of the malware. The original entry point is the first hidden instruction being executed when the program control flow is transferred from the restoration routine to the hidden code. Several approaches, such as Universal PE Unpacker <NO> and PolyUnpack <NO>, have shown that extracting packed binaries and finding the OEP using dynamic analysis is feasible. These approaches either rely on some heuristics or require disassembling the packed program. However, heuristics about packed code may not be reliable in all cases and can be easily evaded. In addition, correctly disassembling a binary program itself is challenging and error-prone, as demonstrated in <NO>. To overcome the disassembly challenge required for packed code extraction, a tool like PolyUnpack needs to perform a series of static and dynamic analysis which leads to performance overhead.
In this paper, we present a fully dynamic approach for extracting the original hidden code and additional information useful for further analysis of the extracted malware binary. We capture an intrinsic nature of packed programs that is independent of the packing techniques applied on the programs. That is, the original code will be dynamically generated and then executed.
The contributions of this paper are as follows:
Propose a fully dynamic approach for extracting the original hidden code of packed executables: A considerable effort has been made to come up with practical solutions for identifying compressed executables and restoring their original hidden code and data. Previous work relies on either heuristics of known packing tools or the accuracy of the disassembler. However, as we see in the Bagle case, malware writers can apply modified binary compression techniques to evade heuristic-based tools <NO>. In addition, disassembling binary executables as being done in <NO> and <NO> is an arduous task. In this paper, we present a binary extraction technique which is fully dynamic and thus does not depend on the program disassembly or the known signatures of packing techniques. We also show that our proposed technique can extract the original hidden code and data, and find the entry point of the original program that enables efficient code analysis.
Provide additional information for the next-step analysis: In addition to extracting the hidden code, our proposed method can provide additional information on the packed binaries:
• Identify the exact regions of memory where the hidden code and data reside: by tracking the newly-written memory areas of the program, we can distinguish newlygenerated code and data at run-time from the packed binary, and thus obtain the exact regions of them.
• Extract information on multiple hidden layers : even in the case that the original program is hidden through multiple rounds of compression and encryption, we can keep track of intermediate code and data for each round. This provides valuable information on what kind of packing methods are in use and what kind of data is generated at each round.
Implement and evaluate Renovo, an automated framework for extracting hidden code: Applying our pro-
posed technique, we build a framework for automatically examining executable binaries and extracting their original hidden code. Since this is a fully automated process, it could be used by anti-virus programs and on-line malware binary analysis services <NO>. We also present the evaluation results of Renovo, demonstrating that it is both highly effective and efficient compared to previous approaches."	https://doi.org/10.1145/1314389.1314399	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Gyung et al. [NO] proposes a fully dynamic approach that captures an intrinsic nature of hidden code execution that the original code should be present in memory and executed at some point at run-time. 
Learning to detect malicious executables in the wild	['A similar approach was used by <NO>, where they trained Instance-Based Learner, TFIDF, Naive-Bayes, support vector machines, decision tree, boosted Naive-Bayes, boosted SVMs and boosted decision tree on n-grams.', 'N-grams Static Hybrid <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO> Misuse <NO>', 'Hence, malware detection is one of the internet security topics that are of great interest <NO>.', 'Recently, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'In the first step, various features such as Application Programming Interface (API) calls <NO> and program strings <NO> are extracted to capture the characteristics of the file samples.', 'In the second step, intelligent classification techniques such as decision trees <NO>, Naive Bayes, and associative classifiers <NO> are used to automatically classify the file samples into different classes based on computational analysis of the feature representations.', 'Classification: For classification, over the last couple of years, many data mining and machine learning approaches have been adopted for malware detection <NO>.', 'Naive Bayes method, Support Vector Machine(SVM), decision tree and associative classification methods are applied to detect new malicious executables in previous studies <NO>.', 'So far, several data mining and machine-learning approaches have been used in malware detection <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'the previous studies <NO>, <NO>, <NO> and the results show that our IMDS applying associative classification method outperforms other classification approaches in both detection rate', 'tion <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Naive Bayes method, SVM, and decision tree classifiers are used to detect new malicious executables in previous studies <NO>, <NO>, <NO>.', 'The collected data in our work is significantly larger than those used in previous studies on data mining for malware detection <NO>.', '<NO> gathered 1971 benign executables and 1651 malicious executables in Windows PE format, and examined the performance of different classifiers such as Naive Bayes, support vector machine (SVM) and Decision Tree using 10-fold cross validation and plotting ROC curves <NO>.', 'Over the last few years, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'Both classifiers have been successfully used in malware detection <NO> and have distinct properties.', 'In order to overcome the disadvantages of the widely-used signature-based malware detection method, data mining and machine learning approaches are proposed for malware detection <NO>.', 'Naive Bayes, Support Vector Machine(SVM) and Decision Tree classifiers are used to detect new malicious executables based on small data collection in the previous studies <NO>.', '2) Both associative classification and SVM have been successfully applied in malware detection <NO>.', 'The problem of detecting malware using data mining <NO> involves classifying each executable as either benign or malicious.']	Jeremy Z. Kolter, and Marcus A. Maloof. 2004. Learning to detect malicious executables in the wild. Proceedings of the International Conference on Knowledge Discovery and Data Mining (SIGKDD).	In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.	"Malicious code is “any code added, changed, or removed from a software system to intentionally cause harm or subvert the system’s intended function” <NO>. Such software has been used to compromise computer systems, to destroy their information, and to render them useless. It has also been used to gather information, such as passwords and credit card numbers, and to distribute information, such as pornography, all without the knowledge of the system’s users. As more novice users obtain sophisticated computers
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD’04, August 22–25, 2004, Seattle, Washington, USA. Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
with high-speed connections to the Internet, the potential for further abuse is great.
Malicious executables generally fall into three categories based on their transport mechanism: viruses, worms, and Trojan horses. Viruses inject malicious code into existing programs, which become “infected” and, in turn, propagate the virus to other programs when executed. Viruses come in two forms, either as an infected executable or as a virus loader, a small program that only inserts viral code. Worms, in contrast, are self-contained programs that spread over a network, usually by exploiting vulnerabilities in the software running on the networked computers. Finally, Trojan horses masquerade as benign programs, but perform malicious functions. Malicious executables do not always fit neatly into these categories and can exhibit combinations of behaviors.
Excellent technology exists for detecting known malicious executables. Software for virus detection has been quite successful, and programs such as McAfee Virus Scan and Norton AntiVirus are ubiquitous. Indeed, Dell recommends Norton AntiVirus for all of its new systems. Although these products use the word virus in their names, they also detect worms and Trojan horses.
These programs search executable code for known patterns, and this method is problematic. One shortcoming is that we must obtain a copy of a malicious program before extracting the pattern necessary for its detection. Obtaining copies of new or unknown malicious programs usually entails them infecting or attacking a computer system.
To complicate matters, writing malicious programs has become easier: There are virus kits freely available on the Internet. Individuals who write viruses have become more sophisticated, often using mechanisms to change or obfuscate their code to produce so-called polymorphic viruses <NO>. Indeed, researchers have recently discovered that simple obfuscation techniques foil commercial programs for virus detection <NO>. These challenges have prompted some researchers to investigate learning methods for detecting new or unknown viruses, and more generally, malicious code.
Our efforts to address this problem have resulted in a fielded application, built using techniques from machine learning <NO> and data mining <NO>. The Malicious Executable Classification System (mecs) currently detects unknown malicious executables “in the wild”, that is, without removing any obfuscation. To date, we have gathered 1971 system and non-system executables, which we will refer to as “benign” executables, and 1651 malicious executables with a variety of transport mechanisms and payloads (e.g., key-
loggers and backdoors). Although all were for the Windows operating system, it is important to note that our approach is not restricted to this operating system.
We extracted byte sequences from the executables, converted these into n-grams, and constructed several classifiers: ibk, tfidf, naive Bayes, support vector machines (svms), decision trees, boosted naive Bayes, boosted svms, and boosted decision trees. In this domain, there is an issue of unequal but unknown costs of misclassification error, so we evaluated the methods using receiver operating characteristic (roc) analysis <NO>, using area under the roc curve as the performance metric. Ultimately, boosted decision trees outperformed all other methods with an area under the curve of 0.996.
We delivered mecs to the mitre Corporation, the sponsors of this project, as a research prototype. Users interact with mecs through a command line. They can add new executables to the collection, update learned models, display roc curves, and produce a single classifier at a specific operating point on a selected roc curve.
With this paper, we make three main contributions. We show how established methods for text classification apply to executables. We present empirical results from an extensive study of inductive methods for detecting malicious executables in the wild. We report on a fielded application developed using machine learning and data mining.
In the three sections that follow, we describe related work, our data collection, and the methods we applied. Then, in Section 6, we present empirical results, and in Section 7, we discuss these results and other approaches."	https://doi.org/10.1145/1014052.1014105	1	['heuristic', 'virus', 'use']	['n_grams', 'heuristic_methods', 'account_dependence', 'approach_conducts', 'classifier_taking']	['detect_malicious_executables', 'account_dependence_relationships', 'anti_virus_systems', 'approach_conducts_exhaustive', 'classifier_taking_account']	Z. et al. [NO] describes the development of a fielded application for detecting malicious executables in the wild. 
Input feature selection by mutual information based on parzen window	[', feature selection (or variable selection, among many other names) <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, is critical to minimize the classification error.', ', Parzen windows)toapproximateIðx; yÞ,assuggestedbyearlierwork in medical image registration <NO> and feature selection <NO>.', 'In earlier work, Kwak and Choi <NO> used the density estimation approach to calculate the mutual information between an individual feature xi and the target class c.']	Nojun Kwak, and Chong-Ho Choi. 2002. Input feature selection by mutual information based on parzen window. IEEE Trans. Pattern Anal. Mach. Intell. 24, 12 (2002), 1667–1671.	Mutual information is a good indicator of relevance between variables, and have been used as a measure in several feature selection algorithms. However, calculating the mutual information is difficult, and the performance of a feature selection algorithm depends on the accuracy of the mutual information. In this paper, we propose a new method of calculating mutual information between input and class variables based on the Parzen window, and we apply this to a feature selection algorithm for classification problems.	"MUTUAL information is considered as a good indicator of relevance between two random variables <NO>. Recently, efforts to adopt mutual information in feature selection problems resulted in a series of researches <NO>, <NO>, <NO>. Because the computation of mutual information between continuous variables is a very difficult job requiring probability density functions (pdf) and involving integration of those functions, mutual information feature selector (MIFS) <NO>, and its variants <NO>, <NO> used histograms in approximating the pdfs to avoid these complexities. Thus, the performance can be degraded as a result of large errors in estimating the mutual information. In addition, MIFS methods have another limitation in that these methods do not provide a direct measure to judge whether to add additional features or not. More direct calculation of mutual information is attempted using the quadratic mutual information in the feature transformation field <NO>, <NO>, <NO>, but the relationship between Shannon’s mutual information and the quadratic mutual information is not clear so far.
In this paper, a new feature selection method with the mutual information maximization scheme is proposed for classification problems. In calculating the mutual information between the input features and the output class, instead of dividing the input space into several partitions, we use the Parzen window method to estimate the input distribution. With this method, more accurate mutual information is calculated giving better performance than other methods.
In the following section, the basics of information theory and the Parzen window method are briefly presented. In Section 3, we propose a new feature selection method and in Section 4, the proposed algorithms are applied to several classification problems to show their effectiveness. And finally, conclusions follow in Section 5."	https://doi.org/10.1109/TPAMI.2002.1114861	2	['behavior', 'malware', 'algorithm']	['anti_malware', 'malware_categorization', 'random_knn', 'file_contents', 'discriminative_specifications']	['anti_malware_industry', 'automatic_malware_categorization', 'access_processing_behavior', 'anti_malware_software', 'calculating_mutual_information']	Kwak, et al. [NO] proposes a new method of calculating mutual information between input and class variables based on the parzen window, and Kwak, et al. [NO] apply this to a feature selection algorithm for classification problems. 
Random KNN	[]	Shengqiao Li, E. James Harner, and Donald A. Adjeroh. 2014. Random KNN. Proceedings of the 2014 IEEE International Conference on Data Mining Workshops. 629–636.	We present Random KNN, a novel generalization of traditional nearest-neighbor modeling. Random KNN consists of an ensemble of base k-nearest neighbor classifiers, each constructed from a random subset of the input variables. We study the properties of the proposed Random KNN. Using various datasets, we perform an empirical analysis of Random KNN performance and compare it with recently proposed methods for high-dimensional datasets. It is shown that Random KNN provides significant advantages in both the computational requirement and classification performance.	"The proposed Random KNN has a connection with classifier fusion and ensemble algorithms, such as those studied by Kitler et al. <NO>, Freud and Schapire <NO>, <NO>, Ji and Ma <NO>, Breiman <NO>, or Sewell <NO>. However, the method is most closely related to, and motivated by, the technique of Random Forests (RF) developed by Breiman <NO>.
One problem with the RF algorithm is that the prediction relies on all the input variables. Variable selection methods have been developed for RF <NO>, <NO>. Various authors have studied Random Forests from different perspectives. For instance, Lin and Jeon studied the connections between Random Forests and adaptive nearest neighbor methods, which use adaptive local distance metrics <NO>. Even with these extensions, which stabilize the hierarchical tree structures, RF may need to keep a relative large number of variables to generate many dissimilar trees.
Random KNN uses KNN as base classifiers that are simple to implement and are stable <NO>, compared with decision trees. Each KNN classifier classifies a test point by its majority, or weighted majority class, of its k neighbors. The final classification in each case is determined by majority voting of r KNN classifications. This can be viewed as a sort of voting by Majority of a Majority.
More formally, let F = {f1, f2, . . . , fp} be the p input features, and X be the n original input data vectors of length p, i.e., an n × p matrix. For a given integer m < p, denote F
(m) = {fj1 , fj2 , . . . , fjm |fjl ∈ F, 1 ≤ l ≤ m} a random subset drawn from F with equiprobability. Similarly, let X(m) be the data vectors in the subspace defined by F(m), i.e., an n × m matrix. Then a KNN(m) classifier is constructed by applying the basic KNN algorithm to the random collection of features in X(m). A collection of r such base classifiers is then combined to build the final Random KNN classifier."	https://doi.org/10.1023/A:1010933404324	2	['behavior', 'malware', 'algorithm']	['anti_malware', 'malware_categorization', 'random_knn', 'file_contents', 'discriminative_specifications']	['anti_malware_industry', 'automatic_malware_categorization', 'access_processing_behavior', 'anti_malware_software', 'calculating_mutual_information']	Li, et al. [NO] present random knn, a novel generalization of traditional nearest-neighbor modeling. 
Cloud-based malware detection for evolving data streams	[]	Mohammad M. Masud, Tahseen Al-Khateeb, Kevin W. Hamlen, Jing Gao, Latifur Khan, Jiawei Han, and Bhavani M. Thuraisingham. 2011. Cloud-based malware detection for evolving data streams. ACM Trans. Management Inf. Syst. 2, 3 (2011), 16.	Data stream classification for intrusion detection poses at least three major challenges. First, these data streams are typically infinite-length, making traditional multipass learning algorithms inapplicable. Second, they exhibit significant concept-drift as attackers react and adapt to defenses. Third, for data streams that do not have any fixed feature set, such as text streams, an additional feature extraction and selection task must be performed. If the number of candidate features is too large, then traditional feature extraction techniques fail. In order to address the first two challenges, this article proposes a multipartition, multichunk ensemble classifier in which a collection of v classifiers is trained from r consecutive data chunks using v-fold partitioning of the data, yielding an ensemble of such classifiers. This multipartition, multichunk ensemble technique significantly reduces classification error compared to existing single-partition, single-chunk ensemble approaches, wherein a single data chunk is used to train each classifier. To address the third challenge, a feature extraction and selection technique is proposed for data streams that do not have any fixed feature set. The technique’s scalability is demonstrated through an implementation for the Hadoop MapReduce cloud computing architecture. Both theoretical and empirical evidence demonstrate its effectiveness over other state-of-the-art stream classification techniques on synthetic data, real botnet traffic, and malicious executables.	"Malware is a potent vehicle for many successful cyber attacks every year, including data and identity theft, system and data corruption, and denial of service; it therefore constitutes a significant security threat to many individuals and organizations. The average direct malware cost damages worldwide per year from 1999 to 2006 have been estimated at $14 billion USD <NO>. This includes labor costs for analyzing, repairing, and disinfecting systems, productivity losses, revenue losses due to system loss or degraded performance, and other costs directly incurred as the result of the attack. However, the direct cost does not include the prevention cost, such as antivirus software, hardware, and IT security staff salary, etc. Aside from these monetary losses, individuals and organizations also suffer identity theft, data theft, and other intangible losses due to successful attacks.
Malware includes viruses, worms, Trojan horses, time and logic bombs, botnets, and spyware. A number of techniques have been devised by researchers to counter these attacks; however, the more successful the researchers become in detecting and preventing the attacks, the more sophisticated malicious code appears in the wild. Thus, the arms race between malware authors and malware defenders continues to escalate. One popular technique applied by the antivirus community to detect malicious code is signature detection. This technique matches untrusted executables against a unique telltale string or byte pattern known as a signature, which is used as an identifier for a particular malicious code. Although signature detection techniques are widely used, they are not effective against zero-day attacks (new malicious code), polymorphic attacks (different encryptions of the same binary), or metamorphic attacks (different code for the same functionality) <NO>. There has therefore been a growing need for fast, automated, and efficient detection techniques that are robust to these attacks. This article describes a data mining technique that is dedicated to automated generation of signatures to defend against these kinds of attacks."	https://doi.org/10.1145/2019618.2019622	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	To address the third challenge, a feature extraction and selection technique is proposed for data streams that do not have any fixed feature set [NO].
Limits of static analysis for malware detection	[', encryption) that remove, or limit, access to the code <NO>.', 'These transformations have long been the downfall of static analysis frameworks for traditional analyses <NO> and mobile malware analysis [Rastogi et al.', 'In our work, we also incorporate learning with dynamic trace data, which has been shown to be very important for classifying classes of malware which are packed or obfuscated in other ways <NO>.', 'perform quite well, it has been shown that these methods can be evaded by using advanced obfuscation transformations <NO>.', 'Traditional static analysis techniques have been shown to be insufficient given the rise of newer malware obfuscation techniques <NO>.', 'In principle, dynamic detection can provide direct observation of malware action, is less vulnerable to obfuscation <NO>, and makes it harder to recycle existing malware.']	Andreas Moser, Christopher Kruegel, and Engin Kirda. 2007. Limits of static analysis for malware detection. Proceedings of the 23rd Annual Computer Security Applications Conference (ACSAC).	Malicious code is an increasingly important problem that threatens the security of computer systems. The traditional line of defense against malware is composed of malware detectors such as virus and spyware scanners. Unfortunately, both researchers and malware authors have demonstrated that these scanners, which use pattern matching to identify malware, can be easily evaded by simple code transformations. To address this shortcoming, more powerful malware detectors have been proposed. These tools rely on semantic signatures and employ static analysis techniques such as model checking and theorem proving to perform detection. While it has been shown that these systems are highly effective in identifying current malware, it is less clear how successful they would be against adversaries that take into account the novel detection mechanisms. The goal of this paper is to explore the limits of static analysis for the detection of malicious code. To this end, we present a binary obfuscation scheme that relies on the idea of opaque constants, which are primitives that allow us to load a constant into a register such that an analysis tool cannot determine its value. Based on opaque constants, we build obfuscation transformations that obscure program control flow, disguise access to local and global variables, and interrupt tracking of values held in processor registers. Using our proposed obfuscation approach, we were able to show that advanced semantics-based malware detectors can be evaded. Moreover, our opaque constant primitive can be applied in a way such that is provably hard to analyze for any static code analyzer. This demonstrates that static analysis techniques alone might no longer be sufficient to identify malware.	"Malicious code (or malware) is defined as software that fulfills the harmful intent of an attacker. The damage caused by malware has dramatically increased in the past few
years <NO>. One reason is the rising popularity of the Internet and the resulting increase in the number of available vulnerable machines because of security-unaware users. Another reason is the elevated sophistication of the malicious code itself.
Current systems to detect malicious code (most prominently, virus scanners) are largely based on syntactic signatures. That is, these systems are equipped with a database of regular expressions that specify byte or instruction sequences that are considered malicious. A program is declared malware when one of the signatures is identified in the program’s code.
Recent work <NO> has demonstrated that techniques such as polymorphism and metamorphism are successful in evading commercial virus scanners. The reason is that syntactic signatures are ignorant of the semantics of instructions. To address this problem, a novel class of semantics-aware malware detectors was proposed. These detectors <NO> operate with abstract models, or templates, that describe the behavior of malicious code. Because the syntactic properties of code are (largely) ignored, these techniques are (mostly) resilient against the evasion attempts discussed above. The premise of semantics-aware malware detectors is that semantic properties are more difficult to morph in an automated fashion than syntactic properties. While this is most likely true, the extent to which this is more difficult is less obvious. On one hand, semantics-aware detection faces the challenge that the problem of deciding whether a certain piece of code exhibits a certain behavior is undecidable in the general case. On the other hand, it is also not trivial for an attacker to automatically generate semantically equivalent code.
The question that we address in this paper is the following: How difficult is it for an attacker to evade semanticsbased malware detectors that use powerful static analysis to identify malicious code? We try to answer this question by introducing a binary code obfuscation technique that makes it difficult for an advanced, semantics-based malware detector to properly determine the effect of a piece of code. For this obfuscation process, we use a primitive known as
1063-9527/07 $25.00 © 2007 IEEE DOI 10.1109/ACSAC.2007.21
421
opaque constant, which denotes a code sequence to load a constant into a processor register whose value cannot be determined statically. Based on opaque constants, we build a number of obfuscation transformations that are difficult to analyze statically.
Given our obfuscation scheme, the next question that needs to be addressed is how these transformations should be applied to a program. The easiest way, and the approach chosen by most previous obfuscation approaches <NO>, is to work on the program’s source code. Applying obfuscation at the source code level is the normal choice when the distributor of a binary controls the source (e.g., to protect intellectual property). For malware that is spreading in the wild, source code is typically not available. Also, malware authors are often reluctant to revealing their source code to make analysis more difficult. Thus, to guard against objections that our presented threats are unrealistic, we present a solution that operates directly on binaries.
The core contributions of our paper are as follows:
• We present a binary obfuscation scheme based on the idea of opaque constants. This scheme allows us to demonstrate that static analysis of advanced malware detectors can be thwarted by scrambling control flow and hiding data locations and usage.
• We introduce a binary rewriting tool that allows us to obfuscate Windows and Linux binary programs for which no source code or debug information is available.
• We present experimental results that demonstrate that semantics-aware malware detectors can be evaded successfully. In addition, we show that our binary transformations are robust, allowing us to run real-world obfuscated binaries under both Linux and Windows.
The code obfuscation scheme introduced in this paper provides a strong indication that static analysis alone might not be sufficient to detect malicious code. In particular, we introduce an obfuscation scheme that is provably hard to analyze statically. Because of the many ways in which code can be obfuscated and the fundamental limits in what can be decided statically, we firmly believe that dynamic analysis is a necessary complement to static detection techniques. The reason is that dynamic techniques can monitor the instructions that are actually executed by a program and thus, are immune to many code obfuscating transformations.
2 Code Obfuscation
In this section, we present the concepts of the transformations that we apply to make the code of a binary difficult to analyze statically. As with most obfuscation approaches,
the basic idea behind our transformations is that either some instructions of the original code are replaced by program fragments that are semantically equivalent but more difficult to analyze, or that additional instructions are added to the program that do not change its behavior.
2.1 Opaque Constants
Simple Opaque Constant Calculation Figure 1 shows one approach to create a code sequence that makes use of random input and different intermediate variable values on different branches. In this code sequence, the value unknown is a random value loaded during runtime. To prepare the opaque constant calculation, the bits of the constant that we aim to create have to be randomly partitioned into two groups. The values of the arrays zero and one are crafted such that after the for loop, all bits of the first group have the correct, final value, while those of the second group depend on the random input (and thus, are unknown). Then, using the appropriate values for set ones and set zeros, all bits of the second group are forced to
their correct values (while those of the first group are left unchanged). The result is that all bits of constant hold the desired value at the end of the execution of the code.
An important question is how the arrays zero and one can be prepared such that all bits of the first group are guaranteed to hold their correct value. This can be accomplished by ensuring that, for each i, all bits that belong to the first group have the same value for the two array elements zero<NO> and one<NO>. Thus, independent of whether zero<NO> or one<NO> is used in the xor operation with constant, the values of all bits in the first group are known after each loop iteration. Of course, the bits that belong to the second group can be randomly chosen for all elements zero<NO> and one<NO>. Thus, the value of constant itself is different after each loop iteration. Because a static analyzer cannot determine the exact path that will be chosen during execution, the number of possible constant values doubles after each loop iteration. In such a case, the static analyzer would likely have to resort to approximation, in which case the exact knowledge of the constant is lost.
This problem could be addressed for example by introducing a more complex encoding for the constant. If we use for instance the relationship between two bits to represent one bit of actual information, we avoid the problem that single bits have the same value on every path. In this case, off-the-shelf static analyzers can no longer track the precise value of any variable.
Of course, given the knowledge of our scheme, the defender has always the option to adapt the analysis such that the used encoding is taken into account. Similar to before, it would be possible to keep the exact values for those variables that encode the same value after each loop iteration. However, this would require special treatment of the particular encoding scheme in use. Our experimental re-
sults demonstrate that the simple opaque constant calculation is already sufficient to thwart current malware detectors. However, we also explored the design space of opaque constants to identify primitives for which stronger guarantees with regard to robustness against static analysis can be provided. In the following paragraphs, we discuss a primitive that relies on the NP-hardness of the 3-satisfiability problem.
NP-Hard Opaque Constant Calculation The idea of the following opaque constant is that we encode the instance of an NP-hard problem into a code sequence that calculates our desired constant. That is, we create an opaque constant such that the generation of an algorithm to precisely determine the result of the code sequence would be equivalent to finding an algorithm to solve an NP-hard problem. For our primitive, we have chosen the 3-satisfiability problem (typically abbreviated as 3SAT) as a problem that is known to be hard to solve. The 3SAT problem is a decision problem where a formula in Boolean logic is given in the following form: ∧n
i=1(Vi1 ∨ Vi2 ∨ Vi3)
where Vij ∈ {v1, ..., vm} and v1, ..., vm are Boolean variables whose value can be either true or false. The task is now to determine if there exists an assignment for the variables vk such that the given formula is satisfied (i.e., the formula evaluates to true). 3SAT has been proven to be NPcomplete in <NO>.
Consider the code sequence in Figure 2. In this primitive, we define m boolean variables v1 . . . vm, which correspond directly to the variables in the given 3SAT formula. By v1 . . . vm, we denote their negations. The pointers V11 to Vn3 refer to the variables used in the various clauses of the formula. In other words, the pointers V11 to Vn3 encode a 3SAT problem based on the variables v1 . . . vm. The loop simply evaluates the encoded 3SAT formula on the input. If the assignment of variables v1 . . . vm does not satisfy the formula, there will always be at least one clause i that evaluates to false. When the check in the loop is evaluated for that specific clause, the result will always be true (as the check is performed against the negate of the clause). Therefore, the opaque constant will be set to 0. On the other hand, if the assignment satisfies the encoded formula, the check performed in the loop will never be true. Therefore, the value of the opaque constant is not overwritten and remains 1.
In the opaque constant presented in Figure 2, the 3SAT problem (that is, the pointers V11 to Vn3) is prepared by the obfuscator. However, the actual assignment of boolean values to the variables v1 . . . vm is randomly performed during runtime. Therefore, the analyzer cannot immediately evaluate the formula. The trick of our opaque constant is that the
3SAT problem is prepared such that the formula is not satisfiable. Thus, independent of the actual input, the constant will always evaluate to 0. Of course, when a constant value of 1 should be generated, we can simply invert the result of the satisfiability test. Note that it is possible to efficiently generate 3SAT instances that are not satisfiable with a high probability <NO>. A static analyzer that aims to exactly determine the possible values of our opaque constant has to solve the instance of the 3SAT problem. Thus, 3SAT is reducible in polynomial time to the problem of exact static analysis of the value of the given opaque constant.
Note that the method presented above only generates one bit of opaque information but can be easily extended to create arbitrarily long constants.
Basic Block Chaining One practical drawback of the 3SAT primitive presented above is that its output has to be the same for all executions, regardless of the actual input. As a result, one can conceive an analysis technique that evaluates the opaque constant function for a few concrete inputs. When all output values are equal, one can assume that this output is the opaque value encoded. To counter this analysis, we introduce a method that we denote basic block chaining.
With basic block chaining, the input for the 3SAT problems is not always selected randomly during runtime. Moreover, we do not always generate unsatisfiable 3SAT instances, but occasionally insert also satisfiable instances. In addition, we ensure that the input that solves a satisfiable formula is provided during runtime. To this end, the input variables v1 . . . vm to the various 3SAT formulas are realized as global variables. At the end of every basic block, these global variables are set in one of the three following ways: (1) to static random values, (2) to random values generated at runtime, or (3), to values specially crafted such that they satisfy a solvable formula used to calculate the opaque constant in the next basic block in the control flow graph.
To analyze a program that is obfuscated with basic block chaining, the analyzer cannot rely on the fact that the encoded formula is always unsatisfiable. Also, when randomly executing a few sample inputs, it is unlikely that the analyzer chooses values that solve a satisfiable formula. The only way to dissect an opaque constant would be to first identify the basic block(s) that precede a certain formula and then determine whether the input values stored in this block satisfy the 3SAT problem. However, finding these blocks is not trivial, as the control flow of the program is obfuscated to make this task difficult (see the following Section 2.2 for more details). Thus, the analysis would have to start at the program entry point and either execute the program dynamically or resort to an approach similar to whole program simulation in which different branches are followed from the start, resolving opaque constants as the
analysis progresses. Obviously, our obfuscation techniques fail against such methods, and indeed, this is consistent with an important point that we intend to make in this paper: dynamic analysis techniques are a promising and powerful approach to deal with obfuscated binaries."	http://scholar.google.com/scholar?hl=en&q=Andreas+Moser%2C+Christopher+Kruegel%2C+and+Engin+Kirda.+2007.+Limits+of+static+analysis+for+malware+detection.+In+Proceedings+of+the+23rd+Annual+Computer+Security+Applications+Conference+%28ACSAC%29.+10.1109%2FACSAC.2007.21	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Moser, et al. [NO] is to explore the limits of static analysis for the detection of malicious code. 
Valgrind: A framework for heavyweight dynamic binary instrumentation	[]	Nicholas Nethercote, and Julian Seward. 2007. Valgrind: A framework for heavyweight dynamic binary instrumentation. Proceedings of ACM SIGPLAN 2007 Conference on Programming Language Design and Implementation.	Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values—a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.	"Many programmers use program analysis tools, such as error checkers and profilers, to improve the quality of their software. Dynamic binary analysis (DBA) tools are one such class of tools; they analyse programs at run-time at the level of machine code.
DBA tools are often implemented using dynamic binary instrumentation (DBI), whereby the analysis code is added to the original code of the client program at run-time. This is convenient for users,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
PLDI’07 June 11–13, 2007, San Diego, California, USA. Copyright c© 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00
as no preparation (such as recompiling or relinking) is needed. Also, it gives 100% instrumentation coverage of user-mode code, without requiring source code. Several generic DBI frameworks exist, such as Pin <NO>, DynamoRIO <NO>, and Valgrind <NO>. They provide a base system that can instrument and run code, plus an environment for writing tools that plug into the base system.
The performance of DBI frameworks has been studied closely <NO>. Less attention has been paid to their instrumentation capabilities, and the tools built with them. This is a shame, as it is the tools that make DBI frameworks useful, and complex tools are more interesting than simple tools. As a result, we believe the potential of DBI has not been fully exploited."	https://doi.org/10.1145/1250734.1250746	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Nethercote, et al. [NO] believes the potential of dbi has not been fully exploited. 
Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy	['First, we rank each API using Max-Relevance algorithm <NO>, Industrial and Government Track Short Paper', 'API Calls: As not all of the API calls are contributing to malware detection, we rank each API call using MaxRelevance algorithm <NO> and select a set of API calls with the highest relevance to the target class, i.', 'We then also apply Max-Relevance algorithm <NO> to select a set of the most representative strings for later classification.', 'As not all of the features contributing to malware detection, we rank each API call and interpretable string using Max-Relevance algorithm <NO> and select top k API calls and interpretable strings as the features for later classification.']	Hanchuan Peng, Fuhui Long, and Chris Ding. 2005. Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence 27, 8 (2005), 1226–1238.	Feature selection is an important problem for pattern classification systems.We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.	"IN many pattern recognition applications, identifying themost characterizing features (or attributes) of the observed data, i.e., feature selection (or variable selection, among many other names) <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, is critical to minimize the classification error. Given the input data D tabled as N samples and M features X ¼ fxi; i ¼ 1; . . . ;Mg, and the target classification variable c, the feature selection problem is to find from the M-dimensional observation space, RM , a subspace of m features, Rm, that “optimally” characterizes c.
Given a condition defining the “optimal characterization,” a search algorithm is needed to find the best subspace. Because the total number of subspaces is 2M , and the number of subspaces with dimensions no larger than m is mi¼1 M i , it is hard to search the feature subspace exhaustively. Alternatively, many sequential-search-based approximation schemes have been proposed, including best individual features, sequential forward search, sequential forward floating search, etc., (see <NO>, <NO>, <NO> for a detailed comparison.).
The optimal characterization condition often means the minimal classification error. Inanunsupervisedsituationwhere theclassifiersarenotspecified,minimalerrorusuallyrequires themaximal statistical dependency of the target class c on the data distribution in the subspace Rm (and vice versa). This scheme ismaximal dependency (Max-Dependency).
One of the most popular approaches to realize MaxDependency is maximal relevance (Max-Relevance) feature selection: selecting the features with the highest relevance to the target class c. Relevance is usually characterized in terms of correlation ormutual information, ofwhich the latter is one of the widely used measures to define dependency of variables. In this paper, we focus on the discussion of mutual-information-based feature selection.
Given two random variables x and y, their mutual information is defined in terms of their probabilistic density functions pðxÞ, pðyÞ, and pðx; yÞ:
Iðx; yÞ ¼ ZZ
pðx; yÞ log pðx; yÞ pðxÞpðyÞ dxdy: ð1Þ
In Max-Relevance, the selected features xi are required, individually, to have the largest mutual information Iðxi; cÞ with the target class c, reflecting the largest dependency on the target class. In terms of sequential search, the m best individual features, i.e., the top m features in the descent ordering of Iðxi; cÞ, are often selected as the m features.
In feature selection, it has been recognized that the combinations of individually good features do not necessarily lead to good classification performance. In other words, “them best features are not the bestm features” <NO>, <NO>, <NO>, <NO>. Some researchers have studied indirect or direct means to reduce the redundancy among features1 (e.g., <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>) and select features with the minimal redundancy (Min-Redundancy). For example, in the sequential forward floating search <NO>, the joint dependency of features on the target class ismaximized; as a by-product, the redundancy among featuresmight be reduced. In <NO>, Jaeger et al. presentedaprefilteringmethod togroupvariables, thus, redundant variables within each group can be removed. In
. H. Peng and F. Long are with the Lawrence Berkeley National Laboratory, University of California at Berkeley, 1 Cyclotron Road, MS. 84-171, Berkeley, CA 94720. E-mail: {hpeng, flong}@lbl.gov. . C. Ding is with the Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720. E-mail: CHQDing@lbl.gov.
Manuscript received 6 Aug. 2003; revised 1 May 2004; accepted 3 Dec. 2004; published online 13 June 2005. Recommended for acceptance by A. Del Bimbo. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMI-0215-0803. 1. Minimal redundancy has also been studied in feature extraction, which aims to find good features in a transformed domain. For instance, it has been well addressed in various techniques such as principal component analysis and independent component analysis <NO>, neural network feature extractors (e.g., <NO>), etc.
0162-8828/05/$20.00 2005 IEEE Published by the IEEE Computer Society
<NO>, we proposed a heuristic minimal-redundancy-maximalrelevance (mRMR) framework to minimize redundancy, and used a series of intuitive measures of relevance and redundancy to select promising features for both continuous and discrete data sets.
Our work in this paper focuses on three issues that have not been touched in earlier work. First, although both MaxRelevance and Min-Redundancy have been intuitively used for feature selection, no theoretical analysis is given on why they can benefit selecting optimal features for classification. Thus, the first goal of this paper is to present a theoretical analysis showing that mRMR is equivalent to Max-Dependency for first-order feature selection, but is more efficient.
Second, we investigate how to combine mRMR with other feature selection methods (such as wrappers <NO>, <NO>) into a two-stage selection algorithm. By doing this, we show that the space of candidate features selected by mRMR is more characterizing. This property of mRMR facilitates the integration of other feature selection schemes to find a compact subset of superior features at very low cost.
Third, through comprehensive experiments we compare mRMR, Max-Relevance, Max-Dependency, and the twostage feature selection algorithm, using three different classifiers and four data sets. The results show that mRMR and our two-stage algorithm are very effective in a wide range of feature selection applications.
This paper is organized as follows: Section 2 presents the theoretical analysis of the relationships of Max-Dependency, Max-Relevance, andMin-Redundancy. Section 3presents the two-stage feature selection algorithm, including schemes to integrate wrappers to select a squeezed subset of features. Section 4 discusses implementation issues of density estimation for mutual information and several different classifiers. Section 5 gives experimental results on four data sets, including handwritten characters, arrhythmia, NCI cancer cell lines, and lymphoma tissues. Sections 6 and 7 are discussions and conclusions, respectively."	https://doi.org/10.1109/TPAMI.2005.159	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Peng, et al. [NO] studies how to select good features according to the maximal statistical dependency criterion based on mutual information. 
PolyUnpack: Automating the hidden-code extraction of unpack-executing malware	['Automated unpackers–common applications for fine-grained analysis–include PolyUnpack <NO> and Renovo <NO>.', 'In <NO>, the authors first disassemble the code, and then run the code looking for sequences of instructions in the dynamic trace that are not found in the disassembled data.', 'Several approaches, such as Universal PE Unpacker <NO> and PolyUnpack <NO>, have shown that extracting packed binaries and finding the OEP using dynamic analysis is feasible.', 'In addition, disassembling binary executables as being done in <NO> and <NO> is an arduous task.', 'However, as shown in <NO> and in our results, some malware can evade this heuristicbased approach.', 'PolyUnpack <NO> is a general approach for extracting the original hidden code without any heuristic assumptions.', 'The details of how these mechanisms work are not present in <NO>, but some malware in the wild are still shown to be able to evade these commercial virus scanners <NO>.', 'Unlike previous approaches <NO> which employ disassembly techniques, our approach depends solely on the origin of the instructions being executed.', 'Note that determining whether a program has hidden code or not is an undecidable problem <NO>.', 'Since determining whether an executable contains hidden code or not is an undecidable problem as shown in <NO>, we employ a time-out mechanism.', 'To verify that Renovo generates accurate results, we have tested Renovo and two other extraction techniques, Universal PE Unpacker <NO> and PolyUnpack <NO>, against the synthetic sample programs generated by using 14 different pack-', 'PolyUnpack: We obtained the analysis results of PolyUnpack <NO> by submitting samples to the Malfease website <NO> of which PolyUnpack operates as its sub-module.']	Paul Royal, Mitch Halpin, David Dagon, Robert Edmonds, and Wenke Lee. 2006. PolyUnpack: Automating the hidden-code extraction of unpack-executing malware. Proceedings of the 22nd Annual Computer Security Applications Conference.	Modern malware often hide the malicious portion of their program code by making it appear as data at compiletime and transforming it back into executable code at runtime. This obfuscation technique poses obstacles to researchers who want to understand the malicious behavior of new or unknown malware and to practitioners who want to create models of detection and methods of recovery. In this paper we propose a technique for automating the process of extracting the hidden-code bodies of this class of malware. Our approach is based on the observation that sequences of packed or hidden code in a malware instance can be made self-identifying when its runtime execution is checked against its static code model. In deriving our technique, we formally define the unpack-executing behavior that such malware exhibits and devise an algorithm for identifying and extracting its hidden-code. We also provide details of the implementation and evaluation of our extraction technique; the results from our experiments on several thousand malware binaries show our approach can be used to significantly reduce the time required to analyze such malware, and to improve the performance of malware detection tools.	"In <NO>, Christodorescu et. al. further the detection of malware to include models based on semantic behavior. Called templates, these models leverage the power of context-free grammars (CFGs) to automatically identify classes (rather than instances) of malware. In such a framework, a template could be created for detecting malware instances that contain unpack-execute behavior based on the semantic class of their unpacking mechanism; indeed, one of the templates presented serves to capture the decryption loop of a polymorphic worm. In the context of our focus, if a malware instance is matched to a template describing a particular unpacking mechanism, the code segment matched could be used to unpack and extract the instance’s hidden-code. Unfortunately, however, even the power of CFGs do not provide a comprehensive mechanism for extracting the hidden-code of all unpack-executing malware. All the malware writer needs to do to successfully evade
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06) 0-7695-2716-7/06 $20.00 © 2006
detection is find a different semantic mechanism of unpacking for which a template has not yet been written, such as pulling malicious code from a network.
Among pattern-matching based extraction approaches, the program Portable Executable (PE) Identifier (PEiD) <NO> stands out as a widely used tool for detecting binaries that exhibit unpack-execute behavior. PEiD uses a signature database to determine if a binary contains packed-code. If a signature match is found, knowledge of the identified packing mechanism can be used to unpack and extract the hidden-code contained in the binary. The key limitations of PEiD are identical to that of a pattern-matching anti-virus tool: its signature database must be updated for it to detect new unpack-executing binary instances and can fail to detect even minor variations of an otherwise known packing method in the same semantic class.
The closest industry work to ours is the Universal PE Unpacker plugin <NO>, available for IDA Pro 4.9. The plugin uses the behavioral heuristic that a program will return to its original entry point once it starts unpacking. Although a good approach for handling compression-based packing techniques, there exists a straightforward possibility of evasion. That is, any program that begins execution in its entry point area, transforms a small portion of data in one of its data sections, then directs execution to the data section it transformed would evade the plugin’s detection heuristic. Using the results of testing our approach as described in Section 6, we discovered several hundred malware instances sampled in the wild that evade the IDA Pro plugin’s heuristic. Our approach does not make the same assumption, and represents a more general solution to the problem of unpacking. As an additional confirmation, we corresponded with Ilfak Guilfanov, the author of IDA Pro; he agrees that our technique provides a new approach not addressed by the Universal PE Unpacker plugin <NO>.
Finally, anti-virus companies have made reference to the notion of malware-instance-independent code extraction, occasionally referring to its implementation as a generic decryption engine <NO>. Just like the authors in <NO>, we found it impossible to obtain further details about the workings of commercial AV tools. In 2004, however, Christodorescu and Jha demonstrated in <NO> that encapsulation techniques used in modern polymorphic viruses prevented commercial anti-virus products from detecting otherwise functionally identical variants of known malware. Given the closed nature of commercial AV products, investigation of the efforts employed by these companies is difficult, and the degree of success in their implementations remains an open question."	https://doi.org/10.1109/ACSAC.2006.38	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Royal, et al. [NO] proposes a technique for automating the process of extracting the hidden-code bodies of this class of malware. 
Deep neural network based malware detection using two dimensional binary program features	[]	Joshua Saxe, and Konstantin Berlin. 2015. Deep neural network based malware detection using two dimensional binary program features. Proceedings of the 10th International Conference on Malicious and Unwanted Software (MALWARE).	In this paper we introduce a deep neural network based malware detection system that Invincea has developed, which achieves a usable detection rate at an extremely low false positive rate and scales to real world training example volumes on commodity hardware. We show that our system achieves a 95% detection rate at 0.1% false positive rate (FPR), based on more than 400,000 software binaries sourced directly from our customers and internal malware databases. In addition, we describe a non-parametric method for adjusting the classifier’s scores to better represent expected precision in the deployment environment. Our results demonstrate that it is now feasible to quickly train and deploy a low resource, highly accurate machine learning classification model, with false positive rates that approach traditional labor intensive expert rule based malware detection, while also detecting previously unseen malware missed by these traditional approaches. Since machine learning models tend to improve with larger datasizes, we foresee deep neural network classification models gaining in importance as part of a layered network defense strategy in coming years.	"Malware continues to facilitate cyber attacks, as attackers use malware as a key tool their campaigns. One problem in computer security is therefore to detect malware, so that it can be stopped before it can achieve its objectives, or at least so that it can be expunged once it has been discovered.
Various detection approaches have been proposed, including rule or signature based approaches, which require analysts to hand craft rules that reason over relevant data to make detections, and machine learning approaches, which automatically reason about malicious and benign data to fit detection model parameters. A middle path between these
∗Authors contributed equally to the work.
approach is the automatic generation of signatures. To date, the computer security industry has favored manual and automatically created rules and signatures over machine learning and statistical methods, because of the low false positive rates achievable by rule and signature-based methods.
In recent years, however, a confluence of three developments have increased the possibility for success in machinelearning based approaches, holding the promise that these methods might achieve high detection rates at low false positive rates without the burden of human signature generation required by manual methods.
The first of these trends is the rise of commercial threat intelligence feeds that provide large volumes of new malware, meaning that for the first time, timely, labeled malware data are available to the security community. The second trend is that computing power has become cheaper, meaning that researchers can more rapidly iterate on malware detection machine learning models and fit larger and more complex models to data. Third, machine learning as a discipline has evolved, meaning that researchers have more tools to craft detection models that achieve breakthrough performance in terms of both accuracy and scalability.
In this paper we introduce an approach that takes advantage of all three of these trends: a deployable deep neural network based malware detector using static features that gives what we believe to be the best reported accuracy results of any previously published detection engine that uses exclusively static features.
The structure of the rest of this paper is as follows. In Section 2 we describe our approach, giving a description of our feature extraction methods, our deep neural network, and our Bayesian calibration model. In Section 3 we provide multiple validations of our approach. Section 4 treats related work, surveying relevant malware detection research and comparing our results to other proposed methods. Finally, Section 5 concludes the paper, reiterating our findings and discussing plans for future work."	https://doi.org/10.1109/MALWARE.2015.7413680	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Saxe, et al. [NO] introduces a deep neural network based malware detection system that invincea has developed, which achieves a usable detection rate at an extremely low false positive rate and scales to real world training example volumes on commodity hardware. 
A survey of data mining techniques for malware detection using file features	[]	Muazzam Siddiqui, Morgan C. Wang, and Joohan Lee. 2008. A survey of data mining techniques for malware detection using file features. Proceedings of ACM-SE.	This paper presents a survey of data mining techniques for malware detection using file features. The techniques are categorized based upon a three tier hierarchy that includes file features, analysis type and detection type. File features are the features extracted from binary programs, analysis type is either static or dynamic, and the detection type is borrowed from intrusion detection as either misuse or anomaly detection. It provides the reader with the major advancement in the malware research using data mining on file features and categorizes the surveyed work based upon the above stated hierarchy. This served as the major contribution of this paper.	K.6.5 <NO>: Security and Protection— Invasive software; H.2.8 <NO>: Database Applications—Data mining ; I.2.6 <NO>: Learning—Concept learning, Connectionism and neural nets	https://doi.org/10.1145/1593105.1593239	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Siddiqui, et al. [NO] presents a survey of data mining techniques for malware detection using file features. 
Static analyzer of vicious executables (SAVE)	['Hence, malware detection is one of the internet security topics that are of great interest <NO>.', 'niques, chief among them being automated obfuscation <NO>.', 'Nowdays malware samples increasingly employ techniques such as polymorphism <NO>, metamorphism <NO>, packing, instruction virtualization, and emulation to bypass signatures and defeat attempts to analyze their inner mechanisms <NO>.', 'Recently, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'Classification: For classification, over the last couple of years, many data mining and machine learning approaches have been adopted for malware detection <NO>.', 'However, this classic signature-based method always fails to detect variants of known malware or previously unknown malware, because the malware writers always adopt techniques like obfuscation to bypass these signatures <NO>.', 'So far, several data mining and machine-learning approaches have been used in malware detection <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'tion <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Besides the traditional signature-based malware detection methods, there is some work to improve the signature-based detection <NO> and also a few attempts to apply data mining and machine learning techniques to detect new malicious executables.', '<NO> developed a signature based malware detection system called SAVE (Static Analyzer of Vicious Executables) which emphasized on detecting polymorphic malware.', 'For each virus, we apply the obfuscation techniques described in <NO> to create a set of polymorphic versions.', 'First, we compare the efficiency of our system with different scanners including the scanner named “SAVE” <NO> described in related work and some widely-used anti-virus software.', 'The problem lies in the signature extraction and generation process, and in fact these signatures can be easily bypassed <NO>.']	Andrew H. Sung, Jianyun Xu, Patrick Chavez, and Srinivas Mukkamala. 2004. Static analyzer of vicious executables (SAVE). Proceedings of the 20th Annual Computer Security Applications Conference.	Software security assurance and malware (trojans, worms, and viruses, etc.) detection are important topics of information security. Software obfuscation, a general technique that is useful for protecting software from reverse engineering, can also be used by hackers to circumvent the malware detection tools. Current static malware detection techniques have serious limitations, and sandbox testing also fails to provide a complete solution due to time constraints. In this paper, we present a robust signature-based malware detection technique, with emphasis on detecting obfuscated (or polymorphic) malware and mutated (or metamorphic) malware. The hypothesis is that all versions of the same malware share a common core signature that is a combination of several features of the code. After a particular malware has been first identified, it can be analyzed to extract the signature, which provides a basis for detecting variants and mutants of the same malware in the future. Encouraging experimental results on a large set of recent malware are presented.	"Due to the increasing prevalence of malware (trojans, worms, and virues, etc.) of all sorts and the significant economic loss they incur to individuals and organizations, one of the current computer security topics of great interest is malware detection.
A classification of malware based on payload, enabling vulnerability, and propagation mechanism gives three generations <NO>."	https://doi.org/10.1109/CSAC.2004.37	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	H. et al. [NO] presents a robust signature-based malware detection technique, with emphasis on detecting obfuscated (or polymorphic) malware and mutated (or metamorphic) malware. 
The evolution of android malware and android analysis techniques	[]	Kimberly Tam, Ali Feizollah, Nor Badrul Anuar, Rosli Salleh, and Lorenzo Cavallaro. 2017. The evolution of android malware and android analysis techniques. ACM Computing Surveys (CSUR) 49, 4 (2017), 76.	With the integration of mobile devices into daily life, smartphones are privy to increasing amounts of sensitive information. Sophisticated mobile malware, particularly Android malware, acquire or utilize such data without user consent. It is therefore essential to devise effective techniques to analyze and detect these threats. This article presents a comprehensive survey on leading Android malware analysis and detection techniques, and their effectiveness against evolving malware. This article categorizes systems by methodology and date to evaluate progression and weaknesses. This article also discusses evaluations of industry solutions, malware statistics, and malware evasion techniques and concludes by supporting future research paths.	"Smartphones, tablets, and other mobile platforms have quickly become ubiquitous due to their highly personal and powerful attributes. As the current dominating personal computing device, with mobile shipments surpassing PCs in 2010 <NO>, smartphones have spurred an increase of sophisticated mobile malware. Over six million mobile malware samples have been accumulated by McAfee as of Q4 2014, up 14% over Q3, and roughly 98% of them target primarily Android devices <NO>. Given Android’s all-pervasive nature and the threats against this particular mobile platform, there is a pressing need for effective analysis techniques to support the development of reliable detection and classification tools. In an attempt to evaluate the progress of research within this specific area of work, this article provides the following contributions.
(1) This work first presents background information on mobile devices and their characteristics. This leads to a detailed description of the Android operating system, as well as notable Android malware and general mobile malware traits (see Section 2). Unlike previous mobile malware surveys, this article primarily focuses on the malware traits that hinder accurate studies and presents them in conjunction with a comprehensive snapshot of today’s Android research techniques. (2) This work presents a comprehensive study on an extensive and diverse set of Android malware analysis frameworks, including methods (e.g., static, dynamic, hybrid), year, and outcome. Similar studies are then reviewed to identify evolving state-of-the-art techniques in an attempt to identify their strengths, weaknesses, performance, and uses. For example, this article discusses how robust some techniques are to major changes within Android, such as replacing the Dalvik runtime. Studies were primarily selected from well-established and top-ranked research venues. However, this work does include, wherever appropriate, a number of additional studies in an attempt to demonstrate the entire breadth of this research area (see Sections 3 and 4). (3) Section 5 addresses several Android malware tactics used to obstruct or evade analysis. This article classifies and describes transformation attacks and examines several advanced malware evasion techniques, such as encryption, native exploits, and Virtual machine (VM)-awareness. With that knowledge, this article performs a comparison of malware strengths to common analysis weaknesses, creating a more comprehensive view than surveys focused on individual aspects. We then confirm trends in evasive malware, found in similar studies, with our own experiments. (4) This work further supports several directions of future research and highlights issues that may not be apparent when looking at individual studies, including malware trends and plausible research paths. While some have recently been receiving more attention, others have yet to be explored sufficiently. Section 6 gives an overview of the state-of-the-art and future research discussion.
Unlike previous works, this article is not a general study on mobile attack vectors or defense <NO> but instead focuses on Android-related analysis techniques systematically and in detail. As can be seen in Table I, this differs from a number of previous works. In similar surveys (e.g, on Android malware families, evolution, characteristics), although analysis techniques are often mentioned, the information is scattered throughout the article to support other material. Furthermore, when combined, those pieces often formed an incomplete picture of all available methods. This study fills that gap by presenting a method-focused view. Furthermore, unlike similar surveys, for example, Vidas et al. <NO>, this article primarily concentrates on the malware aspects that hinder or deter analysis, detection, and classification, allowing us to explore the symbiotic relationship between malware and defense. These findings on how the newest malware and analysis techniques influence each other sets this survey apart from those focused on purely on malware threats or Android defense. However, while it is not the main focus, this article does discuss aspects of malware like market infections.
By narrowing the scope, this article provides in-depth studies on both sides of the arms race with respect to Android malware. A more general study on Android ecosystem weaknesses, for example, the level of app developer skills, and protection schemes can be found in Sufatrio et al. <NO>. This is unlike the focused details on analysis-related techniques, and anti-analysis methods, for Android malware in this article. The last section containing discussions and future research possibilities also differs from the most recent, and most relevant, articles. This may be useful to a wide range of readers."	https://doi.org/10.1145/3017427	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	This article presents a comprehensive survey on leading Android malware analysis and detection techniques, and their effectiveness against evolving malware [NO].
Guilt by association: Large scale malware detection by mining file-relation graphs	[]	Acar Tamersoy, Kevin Roundy, and Duen Horng Chau. 2014. Guilt by association: Large scale malware detection by mining file-relation graphs. Proccedings of ACM International Conference on Knowledge Discovery and Data Mining (ACM SIGKDD).	The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present Aesop, a scalable algorithm that identifies malicious executable files by applying Aesop’s moral that“a man is known by the company he keeps.” We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. Aesop leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. Aesop attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.	"Protection against novel malware attacks, also known as 0-day malware, is becoming increasingly important as the cost of these attacks increases. For individuals, the dollars and cents cost is rising due to the increasing prevalence of financial fraud and the increasing viciousness of malware, such as the CryptoLocker ransomware program that encrypts personal data files and holds them for a ransom of 300 dollars <NO>. Emotional and professional costs can be much higher, as when attacks result in the loss of privacy. The situation is arguably worse for governments and businesses, which find themselves under siege by well-funded attackers that routinely create devastating financial losses, and perhaps even more impactful losses of intellectual property and operational secrets <NO>.
Computer security providers recognize the need to respond with better protection against novel threats. The goal of these 0-day threat protections is to limit the malware’s window of effectiveness, so that malicious files are detected as soon as possible after their first appearance. Another critical measure of success is a vanishingly small false positive rate, as labeling a benign file as malicious can have devastating consequences, particularly if it is a popular file or one that is essential to the stability of the system, as in the case of operating system and driver files.
We present Aesop (Figure 2), a novel approach to detecting malicious executable files by applying the well-known aphorism that “a man is known by the company he keeps,” and in our case, a file’s goodness may be judged by the other files that often appear with it on users’ machines. More precisely, we infer unlabeled files’ reputation (or goodness) by analyzing their relations with labeled peers.
Aesop is not the first attempt to detect malware by establishing file reputation scores. A representative work in this space is Polonium <NO>, which leverages the insight that some computer users have poor internet hygiene in that they attract many more malicious files than users that follow se-
curity best practices. Polonium constructs a bipartite graph between files and machines, in which a file-machine edge represents the existence of a particular file on a particular machine. This approach proved to be successful, Symantec has deployed Polonium; it has detected millions of malicious files. However, Polonium misses many malicious files as it can only observe malware’s file-to-file relationships indirectly through the lens of low-hygiene machines. By contrast, Aesop directly captures file-to-file affinity and can therefore identify malicious files that co-occur with one another, even when they do not appear on heavily infected machines. As we shall demonstrate, Aesop is able to detect many malicious files over a week before they are labeled by Symantec’s existing Polonium-based technology, with a 0.0001 false positive rate (see Figure 1).
Like Polonium, in this work we leverage Symantec’s Norton Community Watch data, the most important elements of which are unique file and machine identifiers. File identifiers are SHA-256 or MD5 cryptographic hash values that are computed over the file’s raw bytes. Symantec’s proxy for a true machine identifier is based on the serial number of Norton security products, which is an adequate but imperfect fit because product re-installation on a single machine may result in a serial number change, and a single serial number can be carried from one machine to another. The scale of this dataset is impressive, comprising 119 million machines and 10.1 billion files.
This paper makes the following contributions:
• We formulate the malware detection problem as a largescale graph mining and inference problem, where our goal is to identify an unknown file’s relations with other files so that we can establish guilt or innocence by its association with files that are known to be benign or malicious. • We present theAesop algorithm that leverages locality-
sensitive hashing to efficiently compute file similarity values to construct a file-relation graph for inferring file goodness based on belief propagation.
• Aesop achieved early detection of 99% of benign files and 79% of malicious files that remained unlabeled by Symantec for over a week before they were eventually labeled, with exceptionally low error rates (see Figure 1).
The remainder of this paper proceeds as follows. We begin by describing our dataset and the notation we will use throughout this paper. We then proceed to a description of Aesop and its various components, followed by the experiments we conducted to demonstrate its effectiveness. Finally, we discuss our plans to deploy Aesop in support of Symantec’s malware detection capabilities, and end by presenting our conclusions."	https://doi.org/10.1145/2623330.2623342	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Tamersoy, et al. [NO] present aesop, a scalable algorithm that identifies malicious executable files by applying aesop ’ s moral that “ a man is known by the company he keeps. ” Tamersoy, et al. [NO] use a large dataset voluntarily contributed by the members of norton community watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. 
Stealth breakpoints	['Though current debuggers to some extent, support self-modifying code and code obfuscations, they are not tailored specifically towards malware analysis and fall prey to several anti-debugging tricks employed by them <NO>.', 'An overlay point under Cobra is defined by employing SPiKE <NO> (a stealth coarse-grained malware analysis framework) and/or VAMPiRE <NO> (a stealth breakpoint framework).', 'The analysis tool also initializes support frameworks used by Cobra such as VAMPiRE <NO> and/or SPiKE <NO> at this stage and establishes the first overlay point using their API (in our example in Figure 6, SPiKE is used to setup an overlay point on KiSwitchContext, a Windows internal kernel function).', 'It makes use of Cobra (apart from other frameworks such as SPiKE <NO> and VAMPiRE <NO>) for real-time malware analysis in both a coarse- and fine-grained fashion.', 'In the obfuscation/deobfuscation game played between attackers and defenders, numerous anti-evasion techniques have been applied in the creation of robust in-guest API call tracers and automated deobfuscation tools <NO>.', 'include VAMPiRE <NO>, BitBlaze <NO> and Cobra <NO>.']	Amit Vasudevan, and Ramesh Yerraballi. 2005. Stealth breakpoints. Proceedings of the 21st Annual Computer Security Applications Conference.	Microscopic analysis of malicious code (malware) requires the aid of a variety of powerful tools. Chief among them is a debugger that enables runtime binary analysis at an instruction level. One of the important services provided by a debugger is the ability to stop execution of code at an arbitrary point during runtime, using breakpoints. Software breakpoints support an unlimited number of breakpoint locations by changing the code being debugged so that it can be interrupted during runtime. Most, if not all, malware are very sensitive to code modification with self-modifying and/or self-checking (SM-SC) capabilities, rendering the use of software breakpoints limited in their scope. Hardware breakpoints supported by the underlying processor, on the other hand, use a subset of the processor register set and exception mechanisms to provide breakpoints that do not entail code modification. This makes hardware breakpoints the most powerful breakpoint mechanism for malware analysis. However, current processors provide a very limited number of hardware breakpoints (typically 2–4 locations). Thus, a serious restriction is imposed on the debugger to set a desired number of breakpoints without resorting to the limited alternative of software breakpoints. Also, with the ever evolving nature of malware, there are techniques being employed that prevent the use of hardware breakpoints. This calls for a new breakpoint mechanism that retains the features of hardware breakpoints while providing an unlimited number of breakpoints, which cannot be detected or countered. In this paper, we present the concept of stealth breakpoints and discuss the design and implementation of VAMPiRE 1, a realization of this concept. VAMPiRE cannot be detected or countered and provides unlimited number of breakpoints to be set on code, data, and I/O with the same precision as that of hardware breakpoints. It does so by employing a subtle combination of simple stealth techniques using virtual memory and hardware single-stepping mechanisms that are available on all processors, old and new. This technique makes VAMPiRE portable to any architecture, providing powerful breakpoint ability similar to hardware breakpoints for microscopic malware analysis. 1 VAMPiRE is a beast (in folklore) that attacks in a stealth fashion.	"Microscopic malware analysis — a fine-grained analysis process that provides insight into malware structure and inner functioning — helps in gleaning important information regarding a malware to facilitate the development of an antidote. Fine-grained analysis requires the aid of various powerful tools, chief among them being a debugger that enables runtime binary analysis at an instruction level. One of the important services provided by a debugger is the ability to stop execution of code being debugged at an arbitrary point during runtime. This is achieved using breakpoints, which can be of two types: Hardware and Software. Hardware breakpoints, as the name suggests, are provided by the underlying processor and support precise breakpoints on code, data and I/O. They are deployed by programming specific processor registers to specify the breakpoint locations and type. Software breakpoints on the other hand are implemented by changing the code being debugged to trigger certain exceptions upon execution (usually a breakpoint exception).
Software breakpoints support unlimited number of breakpoint locations but suffer from the fact that they modify the target code at runtime. This is clearly unsuitable in the context of malware since most if not all malware possess SMSC capabilities and are very sensitive to changes made to their code. For example, viruses such as W32.HIV <NO>, W9x.CIH <NO>, W32.MyDoom <NO> etc. use polymorphic/metamorphic code envelopes and employ a host of integrity checks to detect any changes made to their internal code fragments, to prevent their analysis. Hardware breakpoints on the other hand do not involve any form of code modification and, hence, are the most powerful tool in the repertoire of any debugger tailored for malware. Current processors , however, provide a very limited number of hardware breakpoints (typically 2–4 locations). Thus, a serious restriction is imposed on a debugger to set desired number of breakpoints without resorting to the limited alternative of software breakpoints. Also, with the ever evolving nature of malware, there are techniques being employed that prevent the use of hardware breakpoints to analyze them. For example, the W32.HIV virus uses the processor debug registers and the breakpoint exception for its internal computations, thereby effectively thwarting hardware breakpoints. This situation calls for a new breakpoint mechanism that retains the features of hardware breakpoints while providing unlimited
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 1063-9527/05 $20.00 © 2005 IEEE
number of breakpoints that cannot be detected or countered. This paper discusses the concept of stealth breakpoints and presents VAMPiRE, a realization of this concept that offers the best of both worlds in the sense of unlimited number of precise breakpoints on code, data and I/O which cannot be detected or countered. This is achieved by employing simple stealth techniques that involve virtual memory, singlestepping and task state segments 2 (for processors supporting legacy I/O) — features found in most new and old processor architectures.
While various ideas using virtual memory for breakpoint purposes have been explored in many debuggers <NO>, most if not all, allow only data read and/or write breakpoints. Also none of them are specifically tailored for malware analysis and their breakpoint implementation can be easily detected and defeated. To the best of our knowledge, VAMPiRE is the first to combine virtual memory, single-stepping, task state segments (TSS) and stealth techniques to provide a stealth and portable breakpoint framework highly conducive for malware analysis. By stealth we mean that the breakpoints inserted using VAMPiRE is completely invisible to the code being debugged. VAMPiRE currently runs under the Windows (9x, NT, 2K and XP) and Linux operating systems (OS) on IA-32 (and compatible) processors and is portable on any platform (OS and processor architecture) that supports virtual memory and single-stepping. The framework performance is well within the limits to suit interactive debugging and having a simple and easy-to-use API allows it to be incorporated into existing debuggers with ease.
This paper is organized as follows: In Section 2 we consider related work on breakpoints and compare them with VAMPiRE. In Section 3 we discuss the design and implementation of VAMPiRE. In Section 4 we demonstrate the use of VAMPiRE and present some performance numbers for the framework. We conclude the paper in Section 5 summarizing our contributions with suggestions for future work."	https://doi.org/10.1109/CSAC.2005.52	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Vasudevan, et al. [NO] presents the concept of stealth breakpoints and discuss the design and implementation of vampire 1, a realization of this concept. 
Graph-based malware distributors detection	[]	Andrei Venzhega, Polina Zhinalieva, and Nikolay Suboch. 2013. Graph-based malware distributors detection. Proceedings of the 22nd International Conference on World Wide Web Companion (WWW).	Search engines are currently facing a problem of websites that distribute malware. In this paper we present a novel efficient algorithm that learns to detect such kind of spam. We have used a bipartite graph with two types of nodes, each representing a layer in the graph: web-sites and file hostings (FH), connected with edges representing the fact that a file can be downloaded from the hosting via a link on the web-site. The performance of this spam detection method was verified using two set of ground truth labels: manual assessments of antivirus analysts and automatically generated assessments obtained from antivirus companies. We demonstrate that the proposed method is able to detect new types of malware even before the best known antivirus solutions are able to detect them.	"Due to Internet propagation malware has been rapidly spreading and infecting computers around the world at an unprecedented rate <NO> and malware detection became one of the top internet security topics <NO>. Security software developers reported that the release rate of malicious code and other unwanted programs may be exceeding that of legitimate software applications <NO>.
Search engines (SE) have become one of the principal boosters of malware distribution. Users are looking for software with SE, but sometimes instead of sites of software developers or legal distributors, they get fake websites or malware distributors (MD).
Recently, SEs began to realize their unintentional contribution in malware distribution. To protect users from malware search results they made agreements on cooperation between SE and antivirus companies. Web services enable the identification of malware with a huge partners data about viruses collected, e.g. virustotal.com1. But even a huge malware database does not guarantee detection of new ones. Most of anti-malware software products, such as Kaspersky, Symantec, MacAfee typically use the signaturebased method to recognize threats2. But malware writers successfully invent counter-measures against proposed malware analysis techniques. Today’s malware samples are created at a rate of thousands per day. According to Symantec’s annual report <NO>: 5,5 billion malware attacks were blocked in 2011, 81% more than in 2010. More than 403 million new types of malicious software were detected in 2011, 41% more than in 2011. Symantec reports huge amount of blocked malware, but they estimate that new malware techniques are able to generate an almost unique version of their malware for each potential victim. This suggests traditional signature-based malware detection solutions will likely be outpaced by the number of innovative threats being created by malware authors. A new radically different approach to the problem is currently needed.
SE companies are the first who face a threat from newmalwares. That is why early detection of new malware and in particular their distributors is the principle objective of ensuring safe and high-quality web search. Some websites even if they are not MDs, but closely related to the distributors, for example, linked with hyperlinks, can also be dangerous. We can even suspect them of intentional cooperation with distributors of viral software. Therefore to find suspicious websites, we propose an approach that consists in spreading information about MD via connections between neighbours, which is similar to the idea of homophily. We used a bipartite graph with two types of nodes: website and FH. An edge represents the fact that a file hosted on FH and can be downloaded from the website."	https://doi.org/10.1145/2487788.2488136	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Venzhega, et al. [NO] presents a novel efficient algorithm that learns to detect such kind of spam. 
Comparison of stability for different families of filter-based and wrapper-based feature selection	[]	Randall Wald, Taghi M. Khoshgoftaar, and Amri Napolitano. 2013. Comparison of stability for different families of filter-based and wrapper-based feature selection. ICMLA.	Due to the prevalence of high dimensionality (having a large number of independent attributes), feature selection techniques (which reduce the feature subset to a more manageable size) have become quite popular. These reduced feature subsets can help improve the performance of classification models and can also inform researchers about which features are most relevant for the problem at hand. For this latter problem, it is often most important that the features chosen are consistent even in the face of changes (perturbations) to the dataset. While previous studies have considered the problem of finding so-called “stable” feature selection techniques, none has examined stability across all three major categories of feature selection technique: filterbased feature rankers (which use statistical measures to assign scores to each feature), filter-based subset evaluators (which also employ statistical approaches, but consider whole feature subsets at a time), and wrapper-based subset evaluation (which also considers whole subsets, but which builds classification models to evaluate these subsets). In the present study, we use two datasets from the domain of Twitter profile mining to compare the stability of five filter-based rankers, two filter-based subset evaluators, and five wrapper-based subset evaluators. We find that the rankers are most stable, followed by the filter-based subset evaluators, with the wrappers being the least stable. We also show that the relative performance among the techniques within each group is consistent across dataset and perturbation level. However, the relative stability of the two datasets does vary between the groups, showing that the effects are more complex than simply “one group is always more stable than another group.” Keywords-Stability, filter-based feature selection, wrapperbased feature selection	"Feature selection is an extremely important problem across a wide range of application domains: any time a dataset has too many features, or practitioners would like to know which features are most important, feature selection can provide a reduced list with just those that matter most. A broad survey of this field is presented by Guyon and Elisseeff <NO>, who divide feature selection techniques into two broad categories: filters and wrappers. Filters are defined as any technique which uses statistical methods to find the best features from the dataset; these can score each feature individually and then rank features based on their scores, or calculate a metric from a whole feature subset to describe the goodness of that subset. Wrappers, on the other hand, incorporate learners to determine which feature subsets are actually best for building models. Embedded techniques are noted as a special case of wrapper selection, where instead of using the learner to select features and then using it again
to build a model, the feature selection is embedded directly in the model-building process.
Feature selection has been heavily studied for many years <NO>, but although feature ranking techniques have been widely examined, the increased complexity of filterbased subset evaluation and wrapper-based subset selection techniques has resulted in these being less well-studied <NO>. Even fewer works directly compare three different categories of feature selection (filter-based ranking, filter-based subset evaluation, and wrapper-based subset selection), and those that do are relatively unsophisticated: for example, Molina et al. <NO> use rankers which later research has shown to be ineffective <NO>, and the chosen subset search techniques are also fairly simple. In contrast, Gheyas and Smith <NO> propose a hybrid technique incorporating both filter and wrapperbased ideas, and compare this with other filter-wrapper hybrids, but do not consider either filters or wrappers alone. No previous works consider the previous decade of advancement in the area of feature selection to determine how feature ranking, filter-based subset evaluation, and wrapper-based subset selection compare with one another when using the best-known examples of each family.
While feature subset selection has received relatively little attention, evaluating the stability of these techniques has received even less focus. He and Yu <NO> reviewed causes of instability and stability metrics, including metrics which may be applied towards feature subset selection. Somol and Novovičová <NO> also evaluated stability metrics, using both simulated and real data to observe how different metrics can give different results as well as how three wrapper-based techniques (using a Bayesian classifier, 3-Nearest Neighbor, and Support Vector Machines) compare to one another. Yu et al. <NO> proposed both a new filter-based feature subset evaluation technique and a new stability metric for evaluating such techniques, both based on the idea of feature groups (selecting a collection of feature subsets, where the features within each subset are expected to be redundant with one another, rather than simply selecting individual features). Lustgarten et al. <NO> propose a new stability metric for feature subset evaluation (based on Kuncheva’s consistency index), and compare this metric with the Jaccard index using three wrapper-based subset selection techniques (Logistic Regression, Naı̈ve Bayes, and SVM). Dunne et al. <NO> consider wrappers using a 3-nearest neighbor learner and three choices of search technique, evaluating stability by resampling the original dataset and finding the Hamming distance between the various feature subset masks. Haury et al. <NO> evaluate a number of different feature selection techniques (primarily rankers, but including one wrapper-based subset evaluation technique using least squares regression) and consider stability in terms of how many features are in common between two subsets generated from independent subsamples of the original data. Overall, no work has considered both filter-based subset evaluation and wrapper-
based subset selection at the same time, or has examined a wide range of both learners and performance metrics within the context of wrapper-based feature selection.
The large number of users on Twitter makes it a major target for agencies seeking to create buzz for a product or service. In particular, automatically-generated advertising messages have become a significant problem for Twitter users <NO>. This has led to research focused on detecting spam on Twitter, using techniques including traditional classifiers <NO>, considering the network relationships between the sender and receiver <NO>, and evaluation of the URLs being promoted by the spammers <NO>.
In 2011, the Web Ecology Project began their Socialbot Challenge <NO> to promote the study of Twitter social bots. Three teams created bot clusters in order to elicit real users to follow and reply to their bots, accruing points based on how many users interacted with the “lead” bot on each team. Over the course of the two-week challenge, the top team was able to accumulate approximately 8 followers per day and 14 replies per day, with the latter metric far outweighing the other two competitors. An unaffiliated set of researchers, Wagner et al. <NO>, realized that the data from this challenge could help understand users who choose to follow bots. Three categories of features were extracted from each user to predict their susceptibility to bots: 70 linguistic features (which used the Linguistics Inquiry and Word Count (LIWC) <NO> package to extract word-use dimensions from users’ tweets), nine network-based features using three different forms of graph generation (a followerbased directed graph, an undirected retweet-based graph, and a raw interaction graph), and 13 behavioral features based on the scope and range of tweet contents. Using this dataset and six classification learners, Wagner et al. were able to achieve an overall accuracy of 0.71, and a closer analysis of the features shed light onto the psychological traits leading to bot susceptibility."	https://doi.org/10.1109/ICMLA.2013.162	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Wald, et al. [NO] uses two datasets from the domain of twitter profile mining to compare the stability of five filter-based rankers, two filter-based subset evaluators, and five wrapper-based subset evaluators. 
Toward automated dynamic malware analysis using cwsandbox	['In the obfuscation/deobfuscation game played between attackers and defenders, numerous anti-evasion techniques have been applied in the creation of robust in-guest API call tracers and automated deobfuscation tools <NO>.', 'Approaches that employ these ideas to obtain scalability include malware analysis services such as Norman Sandbox <NO>, CWSandbox <NO> and Anubis <NO>.']	Carsten Willems, Thorsten Holz, and Felix Freiling. 2007. Toward automated dynamic malware analysis using cwsandbox. IEEE Security and Privacy.	Malware is notoriously difficult to combat because it appears and spreads so quickly. Most security products such as virus scanners look for signatures—characteristic byte sequences—to identify malicious code. Malware, however, has adapted to that approach. Polyor metamorphic worms avoid detection by changing their appearance, for example, whereas flash worms stealthily perform reconnaissance without infecting vulnerable machines, waiting to pursue strategic spreading plans that can infect thousands of machines within seconds. In the face of such automated threats, security researchers can’t combat malicious software using manual methods of disassembly or reverse engineering. Therefore, analysis tools must analyze malware automatically, effectively, and correctly. Automating this process means that the analysis tool should create detailed reports of malware samples quickly and without user intervention. Analysts could then use the machinereadable reports to initiate automated responses—automatically updating an intrusion detection system’s signatures, for example, and protecting networks from new malware on the fly. An effective analysis tool must log the malware’s relevant behaviors—the tool shouldn’t overlook any of the executed functionality because analysts will use the information to realistically assess the threat. Finally, the tool should correctly analyze the malware—the sample should initiate every logged action to avoid false positives. In this article, we describe the design and implementation of CWSandbox, a malware analysis tool that fulfills our three design criteria of automation, effectiveness, and correctness for the Win32 family of operating systems. We show how to use API hooking and dynamic linked library (DLL) injection techniques to implement the necessary rootkit functionality to avoid detection by the malware. We acknowledge that these techniques aren’t new; however, we’ve assembled the techniques in a unique combination that provides a fully functional, elegantly simple, and arguably powerful automated malware analysis tool.	"Combining dynamic malware analysis, API hooking, and DLL injection within the CWSandbox lets analysts trace and monitor all relevant system calls and generates an automated, machine-readable report that describes
• the files the malware sample created or modified; • the changes the malware sample performed on the
Windows registry; • which DLLs the malware loaded before execution; • which virtual memory areas it accessed; • the processes that it created; • the network connections it opened and the informa-
tion it sent; and • other information, such as the malware’s access to pro-
tected storage areas, installed services, or kernel drivers.
CWSandbox’s reporting features aren’t perfect—that is, it reports only the malware’s visible behavior and not how it’s programmed, and using the CWSandbox might cause some harm to other machines connected to the network. Yet, the information derived from the CWSandbox for even the shortest of time periods is sur-
Toward Automated Dynamic Malware Analysis Using CWSandbox
32 PUBLISHED BY THE IEEE COMPUTER SOCIETY ■ 1540-7993/07/$25.00 © 2007 IEEE ■ IEEE SECURITY & PRIVACY
The authors present CWSandbox, which executes malware
samples in a simulated environment, monitors all system
calls, and automatically generates a detailed report to
simplify and automate the malware analyst’s task.
CARSTEN WILLEMS, THORSTEN HOLZ , AND FELIX FREILING University of Mannheim, Germany
prisingly rich; in most cases, it’s more than sufficient to assess the danger originating from malware.
In the following paragraphs, we introduce the individual building blocks and techniques behind CWSandbox."	https://doi.org/10.1109/MSP.2007.45	0	['malware', 'code', 'network']	['hardware_breakpoints', 'hide_code', 'based_subset', 'dbi_frameworks', 'filter_based']	['based_subset_evaluators', 'extracts_hidden_code', 'feature_extraction_selection', 'feature_selection_techniques', 'filter_based_subset']	Willems, et al. [NO] describes the design and implementation of cwsandbox, a malware analysis tool that fulfills our three design criteria of automation, effectiveness, and correctness for the win32 family of operating systems. 
Survey of clustering algorithms	['partitioning clustering are two common type of clustering methods and each of them has its own traits <NO>.']	Rui Xu, and Donald Wunsch. 2005. Survey of clustering algorithms. IEEE Transactions on Neural Networks 16, 3 (2005), 645–678.	Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.	"Different starting points and criteria usually lead to different taxonomies of clustering algorithms <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. A rough but widely agreed frame is to classify clustering techniques as hierarchical clustering and partitional clustering, based on the properties of clusters generated <NO>, <NO>. Hierarchical clustering groups data objects with a sequence of partitions, either from singleton clusters to a cluster including all individuals or vice versa, while partitional clustering directly divides data objects into some prespecified number of clusters without the hierarchical structure. We follow this frame in surveying the clustering algorithms in the literature. Beginning with the discussion on proximity measure, which is the basis for most clustering algorithms, we focus on hierarchical clustering and classical partitional clustering algorithms in Section II-B–D. Starting from part E, we introduce and analyze clustering algorithms based on a wide variety of theories and techniques, including graph theory, combinatorial search techniques, fuzzy set theory, neural networks, and kernels techniques. Compared with graph theory and fuzzy set
theory, which had already been widely used in cluster analysis before the 1980s, the other techniques have been finding their applications in clustering just in the recent decades. In spite of the short history, much progress has been achieved. Note that these techniques can be used for both hierarchical and partitional clustering. Considering the more frequent requirement of tackling sequential data sets, large-scale, and high-dimensional data sets in many current applications, we review clustering algorithms for them in the following three parts. We focus particular attention on clustering algorithms applied in bioinformatics. We offer more detailed discussion on how to identify appropriate number of clusters, which is particularly important in cluster validity, in the last part of the section."	https://doi.org/10.1109/TNN.2005.845141	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Xu, et al. [NO] survey clusters algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. 
Automatic malware categorization using cluster ensemble	[]	Yanfang Ye, Tao Li, Yong Chen, and Qingshan Jiang. 2010. Automatic malware categorization using cluster ensemble. Proccedings of ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD).	Malware categorization is an important problem in malware analysis and has attracted a lot of attention of computer security researchers and anti-malware industry recently. Today’s malware samples are created at a rate of millions per day with the development of malware writing techniques. There is thus an urgent need of effective methods for automatic malware categorization. Over the last few years, many clustering techniques have been employed for automatic malware categorization. However, such techniques have isolated successes with limited effectiveness and efficiency, and few have been applied in real anti-malware industry. In this paper, resting on the analysis of instruction frequency and function-based instruction sequences, we develop an Automatic Malware Categorization System (AMCS) for automatically grouping malware samples into families that share some common characteristics using a cluster ensemble by aggregating the clustering solutions generated by different base clustering algorithms. We propose a principled cluster ensemble framework for combining individual clustering solutions based on the consensus partition. The domain knowledge in the form of sample-level constraints can be naturally incorporated in the ensemble framework. In addition, to account for the characteristics of feature representations, we propose a hybrid hierarchical clustering algorithm which combines the merits of hierarchical clustering and k-medoids algorithms and a weighted subspace K-medoids algorithm to generate base clusterings. The categorization results of our AMCS system can be used to generate signatures for malware families that are useful for malware detection. The case studies on large and real daily malware collection from Kingsoft Anti-Virus Lab demonstrate the effectiveness and efficiency of our AMCS system.	"Due to its damage to computer security, malware (such as virus, worms, Trojan Horses, spyware, backdoors, and rootkits) has caught the attention of computer security researchers for decades. Currently, the most significant line of defense against malware is AntiVirus (AV) software products which mainly use signature-based method to recognize threats. Given a collection of malware samples, these AV venders first categorize the samples into families so that samples in the same family share some common traits, and generate the common string(s) to detect variants of a family of malware samples.
For many years, malware categorization have been primarily done by human analysts, where memorization, looking up description libraries, and searching sample collections are typically required. The manual process is time-consuming and labor-intensive. Today’s malware samples are created at a rate of millions per day with the development of malware writing techniques. For example, the number of new malware samples collected by the Anti-virus Lab of Kingsoft is usually larger than 10, 000 per day. There is thus an urgent need of effective methods for automatic malware categorization.
Over the last few years, many research efforts have been conducted on developing automatic malware categorization systems <NO>. In these systems, the detection process is generally divided into two steps: feature extraction and categorization. In the first step, various features such as Application Programming Interface (API) calls and instruction sequences are extracted to capture the characteristics of the file samples. These features can be extracted via static analysis and/or dynamic analysis. In the second step, intelligent techniques are used to automatically categorize the file samples into different classes based on computational analysis of the feature representations. These intelligent malware detection systems are varied in their use of feature representations and categorization methods. They have isolated successes in clustering and/or classifying particular sets of malware samples, but they have limitations on the effectiveness and efficiency and few have
been applied in real anti-malware industry. For example, clustering techniques can be naturally used to automatically discover malware relationships <NO>. However, clustering is an inherently difficult problem due to the lack of supervision information. Different clustering algorithms and even multiple trials of the same algorithm may produce different results due to random initializations and stochastic learning methods <NO>. In this paper, resting on the analysis of instruction frequency and function-based instruction sequences of the Windows Portable Executable (PE) files, we develop AMCS for automatically grouping malware samples into families that share some common characteristics using a cluster ensemble by aggregating the clustering solutions generated by different base clustering algorithms.
To overcome the instability of clustering results and improve clustering performance, our AMCS system use a cluster ensemble to aggregate the clustering solutions generated by different algorithms. We develop new base clustering algorithms to account for the different characteristics of feature representations and propose a novel cluster ensemble framework for combining individual clustering solutions. We show that the domain knowledge in the form of sample-level constraints can be naturally incorporated in the ensemble framework. To the best of our knowledge, this is the first work of applying such cluster ensemble methods for malware categorization. In short, our AMCS system has the following major traits:
• Well-Chosen Feature Representations: Instruction frequency and function-based instruction sequences are used as malware feature representations. These instruction-level features well represent variants of malware families and can be efficiently extracted. In addition, these features can be naturally used to generate signatures for malware detection.
• Carefully-Designed Base Clusterings: The choice of base clustering algorithms is largely dependent on the underlying feature distributions. To deal with the irregular and skewed distributions of instruction frequency features, we propose a hybrid hierarchical clustering algorithm which combines the the merits of hierarchical clustering and k-medoids algorithms. To identify the hidden structures in the subspace of function-based instruction sequences, we use a weighted subspace K-medoids algorithm to generate base clusterings.
• A Principled Cluster Ensemble Scheme: Our AMCS system uses a cluster ensemble scheme to combine the clustering solutions of different algorithms. Our cluster ensemble scheme is a principled approach based on the consensus partition and is able to utilize the domain knowledge in the form of sample-level constraints.
• Human-in-the-Loop: In many cases, the domain knowledge and expertise of virus analysts can greatly help improve the categorization results. Our AMCS system offers a mechanism to incorporate the domain knowledge in the form of sample-level constraints (such as, some file samples are variants of a single malware; or some file samples belong to different malware types).
• Natural Application for Signature Generation: The categorization results generated by our AMCS system can be naturally used to generate signatures for each malware family. These signatures are very useful for malware detection.
All these traits make our AMCS system a practical solution for automatic malware categorization. The case studies on large and real daily malware collection from Kingsoft Anti-Virus Lab demonstrate the effectiveness and efficiency of our AMCS system. As a result, our AMCS has already been incorporated into Kingsoft’s Anti-Virus software products. The rest of this paper is organized as follows. Section 2 presents the overview of our AMCS system and Section 3 discusses the related work. Section 4 describes the feature extraction and representation; Section 5 introduces the base clustering methods we proposed to account for different characteristics of feature representations; Section 6 presents the cluster ensemble framework used in our AMCS system. In Section 7, using the daily data collection obtained from Kingsoft Anti-virus Lab, we systematically evaluate the effects and efficiency of our AMCS system in comparison with other proposed classification/clustering methods, as well as some of the popular Anti-Virus software such as Kaspersky and NOD32. Section 8 presents the details of system development and operation. Finally, Section 9 concludes our discussion."	https://doi.org/10.1145/1835804.1835820	2	['behavior', 'malware', 'algorithm']	['anti_malware', 'malware_categorization', 'random_knn', 'file_contents', 'discriminative_specifications']	['anti_malware_industry', 'automatic_malware_categorization', 'access_processing_behavior', 'anti_malware_software', 'calculating_mutual_information']	Ye, et al. [NO] develops an automatic malware categorization system (amcs) for automatically grouping malware samples into families that share some common characteristics using a cluster ensemble by aggregating the clustering solutions generated by different base clustering algorithms. 
Combining file content and file relations for cloud based malware detection	['<NO> presents a malware detection approach that combines file-to-file relationship data with features extracted at the individual file level.', 'There has been a rising interest in performing virus classification in the cloud <NO> due to the constraints of users’ resources.', 'Due to Internet propagation malware has been rapidly spreading and infecting computers around the world at an unprecedented rate <NO> and malware detection became one of the top internet security topics <NO>.', 'Valkyrie Technology by Comodo <NO> is based on a semiparametric classifier model to combine file content and file relations for malware detection.']	Yanfang Ye, Tao Li, Shenghuo Zhu, Weiwei Zhuang, Egemen Tas, Umesh Gupta, and Melih Abdulhayoglu. 2011. Combining file content and file relations for cloud based malware detection. Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD).	Due to their damages to Internet security, malware (such as virus, worms, trojans, spyware, backdoors, and rootkits) detection has caught the attention not only of anti-malware industry but also of researchers for decades. Resting on the analysis of file contents extracted from the file samples, like Application Programming Interface (API) calls, instruction sequences, and binary strings, data mining methods such as Naive Bayes and Support Vector Machines have been used for malware detection. However, besides file contents, relations among file samples, such as a “Downloader” is always associated with many Trojans, can provide invaluable information about the properties of file samples. In this paper, we study how file relations can be used to improve malware detection results and develop a file verdict system (named “Valkyrie”) building on a semi-parametric classifier model to combine file content and file relations together for malware detection. To the best of our knowledge, this is the first work of using both file content and file relations for malware detection. A comprehensive experimental study on a large collection of PE files obtained from the clients of anti-malware products of Comodo Security Solutions Incorporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our Valkyrie system outperform other popular anti-malware software tools such as Kaspersky AntiVirus and McAfee VirusScan, as well as other alternative data mining based detection systems. Our system has already been incorporated into the scanning tool of Comodo’s Anti-Malware software.	"Malware is software designed to infiltrate or damage a computer system without the owner’s informed consent (e.g., virus, worms, trojans, spyware, backdoors, and rootkits) <NO>. Numerous attacks made by the malware pose a major security threat to Internet users <NO>. Hence, malware detection is one of the internet security topics that are of great interest <NO>. Currently, the most significant line of defense against malware is antimalware software products, such as Kaspersky, MacAfee and Comodo’s Anti-Malware software. Typically, these widely used malware detection software tools use the signature-based method to recognize threats. Signature is a short string of bytes, which is unique for each known malware so that its future examples can be correctly classified with a small error rate.
However, driven by the economic benefits, malware writers quickly invent counter-measures against proposed malware analysis tech-
niques, chief among them being automated obfuscation <NO>. Because of automated obfuscation, today’s malware samples are created at a rate of thousands per day. Figure 1 shows the increasing trend of malware samples in P.R.China from Year 2003 to Year 2010 (this data is provided by Comodo China Anti-Malware Lab). It can be observed that the number of malware samples has increased sharply since 2008. In fact the number of malware samples in 2008 alone is much larger than the total sum of previous five years.
Nowdays malware samples increasingly employ techniques such as polymorphism <NO>, metamorphism <NO>, packing, instruction virtualization, and emulation to bypass signatures and defeat attempts to analyze their inner mechanisms <NO>. In order to remain effective, many Anti-Malware venders have turned their classic signaturebased method to cloud (server) based detection. The work flow of the cloud based detection method adopted by Comodo Security Solutions Incorporation is shown in Figure 2.
The work flow of this cloud based malware detection scheme can be described as follows:
1. On the client side, users may receive new files from emails, media or IM(Instant Message) tools when they are using the Internet.
2. Anti-malware products will first use the signature set on the clients for scanning. If these new files are not detected by existing signatures, then they will be marked as “unknown”.
3. In order to detect malware from the unknown file collection, file features (like file content as well as the file relations) are extracted and sent to Comodo Cloud Security Center.
4. Based on these collected features, the classifier(s) on the cloud server will predict and generate the verdicts for the unknown file samples, either benign or malicious.
5. Then the cloud server will send the verdict results to the clients and notify the clients immediately.
6. According to the response from the cloud server, the scanning process can detect new malware samples and remove the threats.
7. Due to the fast response from the cloud server, the client users can have most up-to-date security solutions.
To sum-up, using the cloud-based architecture, malware detection is now conducted in a client-server manner: authenticating valid software programs from a whitelist and blocking invalid software programs from a blacklist using the signature-based method at the client (user) side, and predicting any unknown software (i.e., the gray list) at the cloud (server) side and quickly generating the verdict results to the clients within seconds. The gray list, containing unknown software programs which could be either benign or malicious, was usually authenticated or rejected manually by malware analysts before. With the development of the malware writing techniques, the number of file samples in the gray list that need to be analyzed by malware analysts on a daily basis is constantly increasing. For example, the gray list collected by the Anti-Malware Lab of Comodo Security Solutions Incorporation usually contains about 500,000 file samples per day. Therefore, there is an urgent need for anti-malware industry to develop intelligent methods for efficient and effective malware detection at the cloud (server) side.
Recently, many research efforts have been conducted on developing intelligent malware detection systems <NO>. In these systems, the detection process is generally divided into two steps: feature extraction and classification. In the first step, various features such as Application Programming Interface (API) calls <NO> and program strings <NO> are extracted to capture the characteristics of the file samples. In the second step, intelligent classification techniques such as decision trees <NO>, Naive Bayes, and associative classifiers <NO> are used to automatically classify the file samples into different classes based on computational analysis of the feature representations. These intelligent malware detection systems are varied in their use of feature representations and classification methods. For example, IMDS <NO> performs association classification on Windows API calls extracted from executable files, while Naive Bayes methods on the extracted strings and byte sequences are applied in <NO>.
These intelligent techniques have isolated successes in classifying particular sets of malware samples, but they have limitations that leave a large room for improvement. In particular, none of these techniques have taken the relationships among file samples into consideration for malware detection. Simply treating file programs as independent samples allows many off-the-shelf classification tools to be directly adapted for malware classification. However, the relationships among file samples may imply the interdependence among them and thus the usual i.i.d (independent and identical distributed) assumption may not hold for malware sam-
ples. As a result, ignoring the relations among file samples is a significant limitation of current malware classification methods. For malware detection, the relations among file samples provide invaluable information about their properties. Here we use some examples for illustration. Based on the collected file lists from clients, we construct a co-occurrence graph to describe the relations among file samples. Generally, two files are related if they are shared by many clients (or equivalently, file lists). As shown in Figure 3, we can observe that the file “yy(1).exe” is associated with many trojans which are marked as purple color. Actually, this “yy(1).exe” file is a kind of Trojan-Downloader malware. Trojan-Downloader refers to any malicious software that downloads and installs multiple unwanted applications of adware and malware from remote servers. Malware samples of this type are spread from malicious websites or by emails as attachments or links, and are installed secretly without the user’s consent. Therefore, from the relations shown in Figure 3, we can infer that if an unknown file always co-occurs with many kinds of trojans in users’ computers, then most likely, it is a malicious Trojan-Downloader file.
Another example showing the relations among benign files is illustrated in Figure 4. From Figure 4, we can observe that an unknown file “everest.exe” can be possibly recognized as benign since it is always associated with known benign files marked in green color. Actually, this “everest.exe” is a benign system diagnostic application which always co-occurs with its related Dynamic Link Library files, such as, “everest_start.dll”, “everest_mondiag.dll”, “everest_rcs.dll” and so on.
Sometime it is not easy to determine whether a file is malicious or not solely based on file content information itself. According to the experience and knowledge of our anti-malware experts, file relations among samples can be a novel and practical feature representation for malware detection. Some malware samples may have stronger connections with benign files than malicious ones. In such cases, those file samples might be infected files. Actually, these unexpected relations can be filtered and removed, because the infected samples can be detected independently using the infected file detector which is developed by our anti-malware experts. To improve the performance of file sample classification for malware detection, in this paper, we utilize both file content and file relation information. However, relation information and file content have different properties. Relation information provides a graph
structure in the data and induces pairwise similarity between objects while the file content provides inherent characteristic information about the file samples. Although both the relation information and file content can be used independently to classify file samples, classification algorithms that make use of them simultaneously should be able to achieve a better performance.
The problem of combining content information and relation information (i.e., link information) have been widely studied for web document categorization in data mining and information retrieval community <NO>. The approaches for combining content and link information generally fall into two categories: (1) feature integration which treats the relation information as additional features and enlarges the feature representation <NO>; and (2) Kernel Integration which integrates the data at the similarity computation or the Kernel level <NO>. However, both types of approaches have limitations: feature integration may degrade the quality of information as file relations and file content typically have different properties, while kernel integration fails to explore the correlation and the inherent consistency between the content information and the relation information <NO>. In this paper, we propose a semi-parametric classification model for combining file content and file relations. The semi-parametric model consists of two components: a parametric component reflecting file content information and a non-parametric component reflecting file relation information. The model seamlessly integrates these two components and formulates the classification problem using the graph regularization framework. Our model can be viewed as an extension of recently developed joint-embedding approaches which aims to seek a common low-dimensional embedding via joint factorization of both the content and relation information <NO>. However, different from the joint-embedding approaches, our model does not explicitly infer the embedding and is directly optimized for classification. We develop a file verdict system (named ""Valkyrie"") using the proposed model to integrate file content and file relations for malware detection. To the best of our knowledge, this is the first work of using both file content and file relations for malware detection. In short, our developed Valkyrie system has the following major traits:
• Novel Usage of File Relation: Different from previous studies for malware detection, we not only make use of file content, but also use the file relations for malware detection.
• A Principled Model for Combining File Content and File Relations: We propose a semi-parametric classification model
to seamlessly combine file content and file relation, and formulate the classification problem using the graph regularization framework.
• A Practical Developed System for Real Industry Application: Based on 37,930 clients, we obtain 30,950 malware samples, 225,830 benign files and 434,870 unknown files from Comodo Cloud Security Center. We build a practical system for malware detection and provide a comprehensive experimental study.
All these traits make our Valkyrie system a practical solution for automatic malware detection. The case studies on large and real daily malware collection from Comodo Cloud Security Center demonstrate the effectiveness and efficiency of our Valkyrie system. As a result, our Valkyrie system has already been incorporated into the scanning tool of Comodo’s Anti-Malware software. The rest of this paper is organized as follows. Section 2 presents the overview of our Valkyrie system. Section 3 describes the feature extraction and representation; Section 4 introduces the proposed semi-parametric model combining file content and file relations together for malware detection; In Section 5, using the daily data collection obtained from Comodo Cloud Security Center, we systematically evaluate the effectiveness and efficiency of our Valkyrie system in comparison with other proposed classification methods, as well as some of the popular Anti-Malware software such as Kaspersky and NOD32. Section 6 presents the details of system development and operation. Section 7 discusses the related work. Finally, Section 8 concludes the paper."	https://doi.org/10.5555/1025118.1025582	2	['behavior', 'malware', 'algorithm']	['anti_malware', 'malware_categorization', 'random_knn', 'file_contents', 'discriminative_specifications']	['anti_malware_industry', 'automatic_malware_categorization', 'access_processing_behavior', 'anti_malware_software', 'calculating_mutual_information']	Ye, et al. [NO] studies how file relations can be used to improve malware detection results and develop a file verdict system (named “ valkyrie ”) building on a semi-parametric classifier model to combine file content and file relations together for malware detection. 
IMDS: Intelligent malware detection system	"['<NO> developed Intelligent Malware Detection System (IMDS) that used Objective-Oriented Association (OOA) mining based classification.', 'API/System Calls Static Hybrid <NO>, <NO> Dynamic Anomaly <NO>', '<NO> proposed Intelligent Malware Detection System (IMDS) using Object Oriented Association (OOA) mining based classification.', 'API <NO> Detects polymorphic and unknown malware.', 'Hence, malware detection is one of the internet security topics that are of great interest <NO>.', 'Recently, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'In the first step, various features such as Application Programming Interface (API) calls <NO> and program strings <NO> are extracted to capture the characteristics of the file samples.', 'In the second step, intelligent classification techniques such as decision trees <NO>, Naive Bayes, and associative classifiers <NO> are used to automatically classify the file samples into different classes based on computational analysis of the feature representations.', 'For example, IMDS <NO> performs association classification on Windows API calls extracted from executable files, while Naive Bayes methods on the extracted strings and byte sequences are applied in <NO>.', 'PE is designed as a common file format for all flavor of Windows operating system, and PE malware are in the majority of the malware rising in recent years <NO>.', 'We extract the Application Programming Interface (API) calls from the Import Tables <NO> of collected malicious and benign PE files, convert them to a group of 32-bit global IDs (for example, the API ""MAPI32.', 'Compared with dynamic feature extraction methods, static feature extraction methods are easier and less expensive <NO>.', 'Classification: For classification, over the last couple of years, many data mining and machine learning approaches have been adopted for malware detection <NO>.', 'Naive Bayes method, Support Vector Machine(SVM), decision tree and associative classification methods are applied to detect new malicious executables in previous studies <NO>.', 'So far, several data mining and machine-learning approaches have been used in malware detection <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'In our previous paper <NO>, <NO>, since', 'tion <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Due to the fact that frequent itemsets (sets of API calls) discovered by association mining can well represent the underlying semantics (profiles) of malware and benign file datasets, associative classification has been successfully used in the IMDS system developed in <NO>', 'It can be effectively used in malware detection <NO>, <NO>, since frequent itemsets are typically of statistical significance and classifiers based on frequent pattern analysis are', 'Based on the system architecture of our previous malware detection system IMDS <NO>, we extract the API calls as the features of the file samples and store them in the signature database.', 'For rule generation, we use the OOA_Fast_FP-Growth algorithm proposed in <NO> to derive the complete set of the rules with certain support and confidence thresholds, since it is much faster than Apriori for mining frequent itemsets.', 'In our paper, based on the complete set of the rules generated by the malware detection rule generator, IMDS applied the technique of CBACB <NO> to build a classifier as the malware detection module to predict new file samples <NO>.', 'By using the OOA_Fast_FP-Growth algorithm <NO>, <NO>, we generate 31', 'Over the last few years, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'For example, IMDS <NO> performs association classification on Windows API calls extracted from executable files while Naive Bayes methods on the extracted strings and byte sequences are applied in <NO>.', 'Both classifiers have been successfully used in malware detection <NO> and have distinct properties.', 'Recently, associative classification <NO>, with its ability to utilize relationships among attributes, has been also applied in <NO>.', '2) Both associative classification and SVM have been successfully applied in malware detection <NO>.', 'Various classification approaches including association classifiers, support vector machines, and Naive Bayes have been applied in malware detection <NO>.']"	Yanfang Ye, Dingding Wang, Tao Li, and Dongyi Ye. 2007. IMDS: Intelligent malware detection system. Proccedings of ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD).	The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based antivirus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using ObjectiveOriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA Fast FPGrowth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of KingSoft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system outperform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.	"Besides the traditional signature-based malware detection methods, there is some work to improve the signature-based detection <NO> and also a few attempts to apply data mining and machine learning techniques to detect new malicious executables.
Sung et al. <NO> developed a signature based malware detection system called SAVE (Static Analyzer of Vicious Executables) which emphasized on detecting polymorphic malware. The basic idea of this approach is to extract the signatures from the original malware with the hypothesis that all versions of the same malware
share a common core signature. Schultz et al. <NO> applied Naive Bayes method to detect previously unknown malicious code. Decision Tree was studied in <NO>. Kolter et al. <NO> gathered 1971 benign executables and 1651 malicious executables in Windows PE format, and examined the performance of different classifiers such as Naive Bayes, support vector machine (SVM) and Decision Tree using 10-fold cross validation and plotting ROC curves <NO>. Their results also showed that the ROC curve of the Decision Tree method dominated all others.
Different from earlier studies, our work is based on a large collection of malicious executables collected at KingSoft Anti-Virus Laboratory. In addition, we apply OOA mining technique to extract the characterizing frequent patterns to achieve accurate malware detection since frequent patterns found by association mining carry the underlying semantics of the data."	https://doi.org/10.1145/1281192.1281308	3	['file', 'malware', 'spam']	['file_features', 'data_sets', 'spam_detection', 'this_article', 'able_detect']	['byte_string_signatures', 'effective_content_filtering', 'malware_detection_techniques', 'potentially_malicious_samples', 'bayes_support_vector']	Ye, et al. [NO] develops the intelligent malware detection system (imds) using objectiveoriented association (ooa) mining based classification. 
Panorama: Capturing system-wide information flow for malware detection and analysis	[', Polyglot <NO> and Panorama <NO>), Siren <NO> and oth-', 'BitBlaze has been shown to provide reliable data that has been used to produce very accurate malware classification results <NO>.', 'The first phase is implemented by the execution monitor <NO>.', 'Capturing information flow in dependence graphs: Existing techniques for constructing dependence graphs from programs provide only data-flow (and sometimes controlflow) dependencies between operations <NO>, <NO>, <NO>.', 'Previous work has shown that using data flows to describe malicious behavior is a powerful approach <NO>, <NO>, <NO> and that the system-call interface is the right abstraction for characterizing user-space malware <NO>,', 'HOLMES builds on existing work for creating the behavior graph and will benefit from more powerful tools that use, for example, dynamic taint tracing <NO>, <NO> and multipath analysis <NO>.', 'describe a behavioral specification of browser-based spyware based on taint-tracking <NO>, and Panorama uses whole-system taint analysis in a similar vein to detect more general classes of spyware <NO>.']	Heng Yin, Dawn Song, Manuel Egele, Christopher Kruegel, and Engin Kirda. 2007. Panorama: Capturing system-wide information flow for malware detection and analysis. Proceedings of the 14th ACM Conference on Computer and Communications Security (CCS).	Malicious programs spy on users’ behavior and compromise their privacy. Even software from reputable vendors, such as Google Desktop and Sony DRM media player, may perform undesirable actions. Unfortunately, existing techniques for detecting malware and analyzing unknown code samples are insufficient and have significant shortcomings. We observe that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users’ privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. We propose a system, Panorama, to detect and analyze malware by capturing this fundamental trait. In our extensive experiments, Panorama successfully detected all the malware samples and had very few false positives. Furthermore, by using Google Desktop as a case study, we show that our system can accurately capture its information access and processing behavior, and we can confirm that it does send back sensitive information to remote servers in certain settings. We believe that a system such as Panorama will offer indispensable assistance to code analysts and malware researchers by enabling them to quickly comprehend the behavior and inner-workings of an unknown sample.	D.4.6 <NO>: Security and Protection—Invasive software	https://doi.org/10.5555/1025118.1025582	2	['behavior', 'malware', 'algorithm']	['anti_malware', 'malware_categorization', 'random_knn', 'file_contents', 'discriminative_specifications']	['anti_malware_industry', 'automatic_malware_categorization', 'access_processing_behavior', 'anti_malware_software', 'calculating_mutual_information']	Yin, et al. [NO] observes that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users ’ privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. 
Robustness of signature verification systems to imitators with increasing skills	[]	F. Alonso-Fernandez, J. Fierrez, A. Gilperez, J. Galbally, and J. Ortega-Garcia 2009. Robustness of signature verification systems to imitators with increasing skills. In 10th Int. Conf. on Document Analysis and Recognition	In this paper, we study the impact of an incremental level of skill in the forgeries against signature verification systems. Experiments are carried out using both off-line systems, involving the discrimination of signatures written on a piece of paper, and on-line systems, in which dynamic information of the signing process (such as velocity and acceleration) is also available. We use for our experiments the BiosecurID database, which contains both on-line and offline versions of signatures, acquired in four sessions across a 4 month time span with incremental level of skill in the forgeries for different sessions. We compare several scenarios with different size and variability of the enrolment set, showing that the problem of skilled forgeries can be alleviated as we consider more signatures for enrolment.	Nowadays, due to the expansion of the networked society, an automatic correct assessment of identity is a crucial point. This has resulted in the establishment of a new research and technology area known as biometrics <NO>, which refers to automatic recognition of an individual based on behavioral and/or anatomical characteristics (e.g., fingerprints, face, iris, voice, signature, etc.). The handwritten signature is one of the most widely used individual authentication methods due to its acceptance in government, legal and commercial transactions <NO>. There are two main signature recognition approaches <NO>: offline and on-line. Off-line methods consider only the signature image, so only static information is available for the recognition task. On-line systems use pen tablets or digitizers which capture dynamic information such as velocity and acceleration of the signing process, providing a richer source of information and more reliability <NO>. Despite the evident advantages of biometric systems, they are not free from external attacks which can decrease their level of security. Thus, it is of utmost importance to analyze the vulnerabilities of biometric systems, in order to find their limitations and to develop useful countermeasures for foreseeable attacks <NO>. Like other biometric systems, signature verification systems are exposed to forgeries, which can be easily performed by direct observation and learning of the signature by the forger. Signature verification systems are usually evaluated by analyzing their ability to accept genuine signatures and to reject forgeries. In this paper, we evaluate the robustness of signature verification systems to forgeries created with an increasing level of skill. For this purpose, we use the BiosecurID database <NO>, which contains both on-line and off-line versions of signatures acquired in several sessions with an incremental level of skill in the forgeries. For the verification experiments, three machine experts exploiting information at different levels have been used (one on-line <NO> and two off-line <NO>). Several enrolment strategies with different size and variability of the enrolment set are studied. The rest of this paper is organized as follows. The problem of forgeries with different level of skill is briefly addressed in Section 2. The three machine experts used are described in Section 3. The experimental framework used, including the database and protocol, is described in Section 4. The results obtained are presented in Section 5, and conclusions are finally drawn in Section 6.	https://doi.org/10.1109/ICDAR.2009.261	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Alonso-Fernandez, et al. [NO] studies the impact of an incremental level of skill in the forgeries against signature verification systems. 
Online signature verification based on generative models	['Mixture Model (GMM) <NO> or discriminative ones such', 'a study of generative models, primarily the HMM based approaches has been presented in <NO>.', 'There are also a few recent works in which ten reference signatures are used for the experimentation, particularly in <NO>, <NO>, <NO>.']	E. Argones Rua, and J.L. Alba Castro 2012. Online signature verification based on generative models. IEEE Transactions on Systems, Man, and Cybernetics Part B (Cybernetics) 42,	The success of generative models for online signature verification has motivated many research works on this topic. These systems may use hidden Markov models (HMMs) in two different modes: user-specific HMM (US-HMM) and user-adapted universal background models (UBMs) (UA-UBMs). Verification scores can be obtained from likelihood ratios and a distance measure on the Viterbi decoded state sequences. This paper analyzes several factors that can modify the behavior of these systems and which have not been deeply studied yet. First, we study the influence of the feature set choice, paying special attention to the role of dynamic information order, suitability of feature sets on each kind of generative model-based system, and the importance of inclination angles and pressure. Moreover, this analysis is also extended to the influence of the HMM complexity in the performance of the different approaches. For this study, a set of experiments is performed on the publicly available MCYT-100 database using only skilled forgeries. These experiments provide interesting outcomes. First, the Viterbi path evidences a notable stability for most of the feature sets and systems. Second, in the case of US-HMM systems, likelihood evidence obtains better results when lowest order dynamics are included in the feature set, while likelihood ratio obtains better results in UA-UBM systems when lowest dynamics are not included in the feature set. Finally, US-HMM and UA-UBM systems can be used together for improved verification performance by fusing at the score level the Viterbi path information from the US-HMM system and the likelihood ratio evidence from the UA-UBM system. Additional comparisons to other state-of-the-art systems, from the ESRA 2011 signature evaluation contest, are also reported, reinforcing the high performance of the systems and the generality of the experimental results described in this paper.	"NOWADAYS, the society demands secure means for per-son authentication. Traditional authentication methods are based on the knowledge (password, Personal Identification Number numbers) or on the possession of a token (Identificator card, keys), which can be forgotten or stolen. This fact places a lot of attention in biometrics as an alternative method for person authentication and identification. Biometrics is defined in <NO> as the use of physiological or behavioral characteristics for person recognition, and hence, they are not affected by the disadvantages of the traditional authentication methods since they cannot be forgotten or stolen.
Biometrics can be coarsely categorized into behavioral and physiological biometrics. Physiological biometrics are based on measurable physiological traits, such as fingerprints or the iris pattern. Behavioral biometrics are based on measurements extracted from an activity performed by the user, in a conscious or unconscious way, and they are inherent to his/her own personality or learned behavior, although influenced also by the physical characteristics of the person. In this sense, behavioral biometrics have some interesting advantages, like user acceptance and cancelability, but they still lack the same level of uniqueness as physiological biometrics. Among all the biometric traits that can be categorized as pure behavioral, the signature, and the way that we sign, is the one that has the widest social acceptance for identity authentication.
On the basis of the signature acquisition method, signature recognition methods can be categorized into static (or offline) and dynamic (or online) methods. Offline signature verification uses the shape of the signature to authenticate the signer. Online signature verification uses dynamic characteristics of the signature (time-dependent signals) to authenticate the signer. Learning the dynamics of the real signer is a difficult task for an impostor, compared to replicate only the shape of a signature. Moreover, the use of devices with built-in pen input such as smartphones, Personal Digital Assistants, or tablet PCs has been spread in the last years. These facts have motivated great research efforts in the last decade on dynamic or online signature verification, as reviewed in <NO>."	https://doi.org/10.1109/TSMCB.2012.2188508	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Argones et al. [NO] analyzes several factors that can modify the behavior of these systems and which have not been deeply studied yet. 
Biometric template protection using universal background models: An application to online signature	['Recently, Argones et al <NO> have proposed a set of HMM model features from a universal background model.', 'Other papers <NO>, <NO>, <NO>, <NO>, and <NO> choose the first k samples, according to the original order in which the data was acquired, as the training set.', 'There are also a few recent works in which ten reference signatures are used for the experimentation, particularly in <NO>, <NO>, <NO>.']	E. Argones Rua, E. Maiorana, J.L. Alba Castro, and P. Campisi 2012. Biometric template protection using universal background models: An application to online signature. IEEE Transactions on Information Forensics and Security	Data security and privacy are crucial issues to be addressed for assuring a successful deployment of biometrics-based recognition systems in real life applications. In this paper, a template protection scheme exploiting the properties of universal background models, eigen-user spaces, and the fuzzy commitment cryptographic protocol is presented. A detailed discussion on the security and information leakage of the proposed template protection system is given. The effectiveness of the proposed approach is investigated with application to online signature recognition. The given experimental results, evaluated on the public MCYT signature database, show that the proposed system can guarantee competitive recognition accuracy while providing protection to the employed biometric data.	"THE USE of biometric data to identify people raises sev-eral security concerns not affecting other approaches employed for automatic people recognition <NO>. If a characteristic is somehow stolen or copied, its owner cannot use it anymore, making it possible for other people to impersonate him using the compromised feature. The use of biometrics also poses many privacy concerns <NO>: when an individual gives out his biometrics, either willingly or unwillingly, he can disclose sensitive information about his personality and health status <NO> which can be used to profile him. Moreover, data collected for some specific purposes might be used in the long run for different ones, a possibility usually referred to as function creep. Also, the uniqueness characterizing biometric data, and the fact that biometrics are permanently associated with their users, can be exploited to perform an unauthorized tracking of the activities of the subjects enrolled in different biometric databases. The use of biometrics can also raise cultural and religious concerns. Therefore, the need to protect privacy, both from a procedural point of view and from a technological point of view, arises. Currently deployed systems typically employ classical cryptographic techniques to securely store data. However, even if the stored data are kept secure when not used, decryption has to be performed during authentication, thus leaving the biometric information unprotected. It is rather desirable to permanently keep secret an acquired biometrics, and also to be able to revoke or to renew a template when compromised, as well as to obtain from the same biometrics different keys to access different locations, either physical or logical. Therefore a template protection scheme should satisfy the following properties: 1) Renewability: for each user, it should be possible to revoke a compromised template and reissue a new one based on the same biometric data (revocability). Moreover, each template generated from a biometrics should not match with the others previously generated from the same data (diversity). The renewability property is needed to ensure the user’s privacy.
2) Security: it must be impossible or computationally hard to obtain the original biometric template from the stored and secured one. This property is needed to prevent an adversary from creating fake biometric traits from stolen templates. In fact, although it was commonly believed that it is not possible to reconstruct an original biometric characteristic from the corresponding extracted template, some concrete counter examples, which contradict this assumption, have been provided in the recent literature, as in <NO>. 3) Performance: the recognition performance should not degrade significantly with the introduction of a template protection scheme, with respect to the performance of a nonprotected system.
The approaches followed to design biometric template protection schemes able to properly satisfy each of the aforementioned properties have been categorized into feature transformations and biometric cryptosystems <NO>. When implementing a feature transformation approach, a function dependent on some parameters, which can be used as a key, is applied to the input biometrics. Invertible functions are employed in salting schemes, whose security therefore relies on the protection of the defining transformation parameters <NO>. The loss or disclosure of the key, therefore, results in serious security issues <NO>. One-way functions are employed in noninvertible transform approaches <NO>, which produce templates from which it is computationally hard to retrieve the original data, even if the transforms’ defining parameters are known. However, it is in general difficult to quantitatively determine the actual difficulty of the invertibility. In <NO>, noninvertible functions are applied to face images to obtain transformed templates which, however, allows human inspection. In <NO>, Cartesian, polar, and functional noninvertible transformations are used to modify the fingerprint minutiae pattern. However, only a small fraction of the resulting data is in practice noninvertible <NO> and the scheme is vulnerable to a record multiplicity attack, where an adversary who acquires two or more distinct stored templates is able to recover the original information <NO>. In <NO>, noninvertible transforms designed for biometrics which can be expressed in terms of a set of sequences have also been described, and a detailed security and renewability analysis is provided. Biometric cryptosystems provide the means to integrate cryptographic protocols with biometric recognition. They can be further classified into key generation schemes, where binary keys are directly created from the acquired biometrics, and key binding schemes, which store information obtained by combining biometric data with randomly selected keys. The main issue characterizing key generation approaches regards the possibility of creating multiple keys from the same biometrics without using any external data, and the stability of the resulting cryptographic key <NO>. Moreover, due to the difficulties in managing the intraclass variability of biometric data, the recognition performance of such schemes are typically significantly lower than those of their unprotected counterparts <NO>. A key binding system can be twofold: it can be used to protect a biometric template by means of a binary key, thus securing a biometric recognition system, or to release a cryptographic key only when its owner presents a specific biometric trait. In both cases a secret key, independent of the considered biometrics, is combined during enrollment with a reference template to generate the so-called helper data, from which it should be impossible, or at least computationally hard, to retrieve information about the original biometric trait or the key. The helper data is then used in conjunction with a query biometrics during authentication to retrieve the secret. Error-correcting codes are commonly employed to manage the intraclass variability of the considered templates. Among the cryptographic protocols most employed in a key binding scenario, we can mention the fuzzy commitment <NO>, already applied to ear <NO>, fingerprint <NO>, face <NO>, iris <NO>, and online signatures <NO>. The fuzzy vault scheme <NO> is able to manage biometrics represented as unordered sets of data, and has been applied to signature <NO>, face <NO>, and iris <NO>, although it has been found to be vulnerable to different attacks <NO>. Also Quantization Index Modulation (QIM) has been proposed to bind biometric characteristics with binary keys <NO>, providing an increased flexibility in managing the templates’ intraclass variability. It is worth pointing out that both feature transformation approaches and biometric cryptosystems have their own pros and cons. Specifically, it is difficult to define noninvertible transformations which preserve the discriminability of the templates, and it is hard to properly provide renewability with key binding schemes relying on helper data. Hybrid approaches <NO> able to exploit the advantages of both biometric cryptosystems and feature transformation approaches would be needed, therefore, to cope with all the requirements of a template protection system. In this paper, we present a general hybrid approach which properly guarantees the necessary protection to biometric templates by exploiting the properties of Universal Background Models (UBMs). Specifically, UBMs are statistical descriptors employed to represent person-independent biometric observations. They have been extensively employed for biometric authentication purposes for speaker <NO>, for face <NO>, and for gait <NO> recognition, while some analyses have been made in <NO> for online signature recognition. UBM’s parameters are estimated by processing a large number of biometric acquisitions, taken from as many different subjects as possible. The biometrics of a specific user is then modeled by adapting the well-trained parameters in the UBM to the characteristics of the acquired user’s traits. The traditional approach for user verification in UBM-based systems relies on a likelihood ratio evaluation, which requires the use of the original user-adapted models. It is here demonstrated that UBMs can also be employed to provide a parametric feature representation which can be exploited in a key binding scheme, thus preserving the user’s privacy. The feature transformation part of the proposed protection scheme relies on the use of UBMs, also employed to provide renewability, while the cryptosystem part is given by a user-adaptive version of the fuzzy commitment cryptographic protocol <NO>, which is exploited to provide template noninvertibility and to manage the intraclass variability of the users’ biometrics. As a proof of concept, the presented protection scheme is applied to online signature biometrics to verify its effectiveness. The present paper is organized as follows. UBM-based biometric modeling and verification is presented in Section II. The proposed protected system is described in Section III, where a detailed discussion on feature and error correction code selection is carried out and security, privacy, and renewability issues are deeply analyzed. The application of the proposed approach to online signature biometrics is presented in Section IV. Eventually, the experimental results are presented in Section V, while some conclusions are drawn in Section VI."	https://doi.org/10.1109/TIFS.2011.2168213	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Argones et al. [NO], a template protection scheme exploiting the properties of universal background models, eigen-user spaces, and the fuzzy commitment cryptographic protocol is presented. 
The 4NSigComp2010 off-line signature verification competition: Scenario 2	['This procedure is conducted with two off-line signature databases, namely the GPDS-300 <NO> and MCYT-75 <NO> alongwith four published state-of-the-art-ASVs so as to avoid biased results and to obtain more consistent and general conclusions.']	M. Blumenstein, M.A. Ferrer, and J.F. Vargas 2010. The 4NSigComp2010 off-line signature verification competition: Scenario 2. In Int. Conf. on Frontiers in Handwriting Recognition,	The objective of this competition (4NSigComp2010) is to ascertain the performance of automatic off-line signature verifiers to evaluate recent technology developments in the areas of document analysis and machine learning. The current paper focuses on the second scenario, which aims at performance evaluation of off-line signature verification systems on a newly-created large dataset that comprises genuine, simulated signatures produced by unskilled imitators or random signatures (genuine signatures from other writers). Ten systems were evaluated, and some interesting results are presented in terms of accuracy and execution time. The top ranking system attained an overall error of 8.94%. This result interestingly correlates with the top ranking accuracy achieved in a previous signature verification competition at ICDAR 2009.	"Automatic verification of a signature, a behavioural biometric, can be performed using a tablet with a stylus or using the signature of a scanned image. The former is called on-line verification and the latter is called off-line verification. Off-line verification has several advantages over its on-line counterpart. Firstly, it has widely been accepted in society. Secondly, it is more convenient as it does not require any special instruments. Thirdly, with the large amount of bank cheques, credit card authorisation forms, or legal documents still being signed every day, off-line verification can be considered commercially important.
Scenario 2 of the 4NSigComp2010 competition at ICFHR 2010 has been formulated given the intent interest in off-line signature verification. The second scenario has been primarily concerned with the detection of skilled versus non-skilled simulated signatures and aims at evaluating the performance of signature verification systems in a security-less critical environment. The questioned signatures can either be genuine (written by the reference writer), or forged (simulated by other writers than the reference writer), or a random forgery (genuine signature of other writers).
The current paper reports the outcomes for Scenario 2 of the 4NSigComp2010 competition. The remainder of this paper is divided into five sections. Section 2 outlines the participants in this competition, Section 3 discusses the signature database employed, Section 4 elaborates on Systems Evaluation and Section 5 presents the results obtained. Finally, Section 6 provides some concluding remarks."	https://doi.org/10.1109/ICFHR.2010.117	2	['sample']	['signature_identification', 'chain_code', 'code_direction', 'modified_chain', 'line_signatures']	['chain_code_direction', 'modified_chain_code', 'segmented_signature_image', 'signature_identification_proposed', 'line_signature_verification']	This result interestingly correlates with the top ranking accuracy achieved in a previous signature verification competition at ICDAR 2009 [NO].
Generation of duplicated off-line signature images for verification systems	[]	M. Diaz, M. A Ferrer, G. S Eskander, and R. Sabourin 2017. Generation of duplicated off-line signature images for verification systems. IEEE Transactions on Pattern Analysis and Machine Intelligence 39,	Biometric researchers have historically seen signature duplication as a procedure relevant to improving the performance of automatic signature verifiers. Different approaches have been proposed to duplicate dynamic signatures based on the heuristic affine transformation, nonlinear distortion and the kinematic model of the motor system. The literature on static signature duplication is limited and as far as we know based on heuristic affine transforms and does not seem to consider the recent advances in human behavior modeling of neuroscience. This paper tries to fill this gap by proposing a cognitive inspired algorithm to duplicate off-line signatures. The algorithm is based on a set of nonlinear and linear transformations which simulate the human spatial cognitive map and motor system intra-personal variability during the signing process. The duplicator is evaluated by increasing artificially a training sequence and verifying that the performance of four state-of-the-art off-line signature classifiers using two publicly databases have been improved on average as if we had collected three more real signatures.	"FOR centuries, the handwritten signature has beenaccepted world-wide for the purpose of authentication. Classical applications include the legal validation of documents such as contracts, last wills or testaments, corporative tax statements, financial transfers and so on. It leads forensic scientists, graphologists, neurological practitioners and therapists among others to be interested in the creation and validity of handwritten signatures. This interest is manifested in many publications and surveys <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO> published in the literature during previous decades. However, modeling its intra-personal variability is still an open challengewhich has caught the attention of researchers on pattern recognition andmachine intelligence.
It is important to understand intra-personal variability of the signatures of a signer. Its modeling allows the widening of the distinction between genuine and non-genuine signatures. The generation of duplicated specimens with realistic appearance helps in gaining a better understanding of signature execution from several neuroscientific perspectives. This also supports coherent decision making in psychology and forensic science and assists in optimizing speed and performance for indexing purposes.
This paper is focused onmodeling intra-personal variability for duplicating static signatures. In this context, duplicating a signature means generating artificially a new specimen from a real or several real signatures. Among its advantages, signature duplication can improve the training of ASV systems, allows the carrying out of statistically meaningful evaluations, enlarges the number of signatures in databases, can match the baseline performances for real signatures, which are often difficult to obtain, and can improve the performances of existing automatic signature verifiers.
According to the literature, there are three methods of signature duplication: iÞ the creation of dynamic (on-line) signatures by using real on-line samples (On-2-On); iiÞ the creation of static (off-line) signatures by using dynamic specimens (On-2-Off) and iiiÞ the production of signature images (off-line) from static signatures (Off-2-Off). Note that on-line signature generation from off-line specimens (Off-2On) remains to the best of our knowledge an open issue.
Most of the recent advances in modeling intra-personal variability are focused on on-line signatures. For this category of signature capture we can find how, by applying random and affine deformations, it is possible to improve the performance of an HMM-based classifier <NO>. In <NO> another method is studied for increasing the training set on the basis of a clonal selection of the enrolled signatures without modifying either the diversity of the overall set or the sparse feature distribution. Also, in Diaz et al. <NO>, the kinematic theory of rapid movement is applied to enlarge the enrolled signature data set. Furthermore, the resultant set has been used for testing purposes in <NO>.
There are other proposals in the literature focused on the generation of signature images from on-line signatures <NO>, <NO>, <NO>, <NO>, <NO>. The common tendency is to apply different methods to dynamic signatures since these record the kinematic and the timing order in which the traces are registered. Once a new trajectory is obtained, the samples of the new specimen are interpolated in order to create new images. Then, an off-line automatic classifier is used to assess the performance improvement. Parallel to this approach, a method of generating enhanced synthetic signatures images has been formulated using a novel architecture to improve the performance of dynamic verifiers <NO>.
In our review of previous work, we have found little on duplication from off-line to off-line signatures. One example is in <NO> where an off-line signature dataset composed of 6 genuine specimens per user and 38 signers is enlarged by applying affine transformations to the original signatures. Since the database contained only genuine signatures, this study was focused only on recognition. The authors did not include the deliberate forgery test. Although the authors enlarged the training set, the paper scarcely addressed either how the duplicated signatures were constructed or gave reference to the cognitive signing procedure.
In order to locate in the current literature the work we report here, Table 1 summarizes schematically the state-ofthe-art in duplicated signature generation. This analysis reveals the need for more research in duplicating off-line signatures from off-line real signatures.
As well as covering geometrical and affine image deformations, the work reported in this paper is addressed at filling the gap between heuristic methods used for off-line signature duplication and methods for intra-personal variability modeling. The techniques we develop are based on human behavior, as examined by neuroscience. Specifically, a duplication procedure is proposed on the basis of modeling neuromotor equivalence theory <NO>, which divides human action into effector dependent and effector independent parts of the signing procedure. Note that we do not pretend to model human behavior as neuroscience does: we simply use ideas from neuroscience to generate signature duplicates. Our results are encouraging.
The realism of the intra-personal variability model is evaluated by increasing a training sequence with duplicates and ascertaining the improvement in performance of four different state-of-the-art generative classifiers. So as to consider as many aspects of the variability as possible, we have chosen verifiers which are based on different features and classifiers. Additionally, we have used two different public datasets. The improved performance after training with the enlarged set is discussed as well as the complementary information contained in data produced by the cognitive inspired duplication algorithm.
The reminder of this paper has the following organization. Section 2 surveys the cognitive ideas used to design our method which is the algorithm detailed in Section 3. Section 4 describes the method used to evaluate the duplicator, while Sections 5 and 6 present the results and conclusions respectively."	https://doi.org/10.1109/TPAMI.2016.2560810	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Diaz, et al. [NO] knows bases on heuristic affine transforms and does not seem to consider the recent advances in human behavior modeling of neuroscience. 
A new algorithm and system for the characterization of handwriting strokes with delta-lognormal parameters	[]	M. Djioua, and R. Plamondon 2009. A new algorithm and system for the characterization of handwriting strokes with delta-lognormal parameters. IEEE Transactions on Pattern Analysis and Machine Intelligence 31,	In this paper, we present a new analytical method for estimating the parameters of Delta-Lognormal functions and characterizing handwriting strokes. According to the Kinematic Theory of rapid human movements, these parameters contain information on both the motor commands and the timing properties of a neuromuscular system. The new algorithm, called XZERO, exploits relationships between the zero crossings of the first and second time derivatives of a lognormal function and its four basic parameters. The methodology is described and then evaluated under various testing conditions. The new tool allows a greater variety of stroke patterns to be processed automatically. Furthermore, for the first time, the extraction accuracy is quantified empirically, taking advantage of the exponential relationships that link the dispersion of the extraction errors with its signal-to-noise ratio. A new extraction system which combines this algorithm with two other previously published methods is also described and evaluated. This system provides researchers involved in various domains of pattern analysis and artificial intelligence with new tools for the basic study of single strokes as primitives for understanding rapid human movements.	"HANDWRITING strokes constitute a specific class of rapidhuman movements. They have been intensively used and studied in many fields of research. For example, various online handwriting pattern analysis and recognition methods represent a complex pattern, like a graph, a letter, a word, a signature, etc., as made up of simple handwriting strokes superimposed <NO>, <NO>, <NO>, <NO>. In motor control, these strokes are considered as primitives from which complex movements are assumed to be planned and executed <NO>, <NO>, <NO>, <NO>, <NO>. In the forensic sciences, a detailed study of individual stroke patterns, focusing on tiny variations, often constitutes the grounds for a decision about the authenticity of a signature or a document <NO>, <NO>. In education, various interactive teaching methods have been developed to reproduce letters and words from the concatenation of neat strokes <NO>, <NO>. Similarly, strokes are analyzed to study handwriting deficits in children <NO>, <NO>, as well as for training patients with writer’s cramp <NO>. In the neurosciences, strokes are investigated to characterize neurodegenerative processes like Parkinson’s disease, Alzheimer’s disease <NO>, <NO>, and multiple sclerosis <NO>, and in the diagnosis of dyspraxia <NO>, as well as to assess recovery in the rehabilitation of patients having suffered cerebrovascular accidents <NO> or to assess the severity of their loss of visuospatial capacity <NO>. Strokes are also investigated in studies dealing with drug treatments <NO>, <NO>, as well as nicotine <NO> or caffeine <NO> effects and alcohol intoxication <NO>. In anthropomorphic robotics, the superimposition of handwriting strokes is exploited to explore the biomechanical principles employed by humans to produce movements, and later to apply them in the control of a robot arm <NO>, <NO>. Recently, synthetic strokes have been proposed for cybersecurity applications <NO>.
Many of these examples emphasize the need to study the basic properties of single strokes for a realistic analysis and understanding of how the motor control system performs complex movements. The cornerstone of this whole methodology is then to understand the genesis of strokes, since this reflects some of the fundamental properties of a writer’s neuromuscular system, as well as some of the basic features of the motor control strategies employed to produce these simple movements. Among these basic features, the most remarkable is the invariance of the velocity profile of the end-effector of a subject performing a rapid stroke over a wide range of movement size and speed <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. Its asymmetric bell shape is considered a fundamental property reflecting the synergetic activity of a large number of neuromotor networks <NO>, <NO>, <NO>.
Many studies have been conducted from this perspective to model the various processes involved in the production of single strokes. These models can be classified in various categories. Neural network models <NO>, <NO>, <NO>, <NO> have been developed to study the emergence of invariants in arm movements, velocity invariance being considered as the product of an intrinsic property of the differential equation
. The authors are with the Laboratoire Scribens, Département de Génie Electrique, Ecole Polytechnique de Montréal, PO Box 6079, Station Centre-Ville, Montréal, QC H3C 3A7, Canada. E-mail: {moussa.djioua, rejean.plamondon}@polymtl.ca.
Manuscript received 17 Jan. 2008; revised 6 Aug. 2008; accepted 21 Oct. 2008; published online 30 Oct. 2008. Recommended for acceptance by J. Hu. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMI-2008-01-0036. Digital Object Identifier no. 10.1109/TPAMI.2008.264.
0162-8828/09/$25.00 2009 IEEE Published by the IEEE Computer Society
network describing the dynamics of the system. Other approaches, like dynamic models, focus on the mass-spring characteristics of muscles <NO>, <NO> or on the changes in equilibrium points <NO> to understand trajectory formation. Generalized motor program models <NO>, <NO> rely on different representations of the action plan <NO> or on various stochastic properties of strokes <NO>. Finally, principle-oriented models track the velocity invariance problem with various minimization criteria: minimum time <NO>, minimum acceleration <NO>, minimum jerk <NO>, minimum snap <NO>, and minimum torque changes <NO>. These models generally provide, directly or indirectly, an analytical expression describing the velocity profile. However, not all these models can be integrated into the design of practical applications like those mentioned above. For example, neural networks are preferable for simulating learning mechanisms, and analytical models for studying basic skills. Some models require too many parameters and are not of practical use, although they might be essential, from a neuroscience perspective, for developing theories of human motor behavior.
Among these various computational models and approaches, the Kinematic Theory <NO> has been found to be successful in reproducing the various invariants observed in rapid strokes <NO>. So far, few applications based on this theory, or inspired by it, have been developed <NO>, <NO>, <NO>, <NO>. This is mainly due to the lack of a robust and efficient software tool (a parameter extraction system) which can accurately fit any stroke velocity profile with a minimum reconstruction error.
In this paper, we propose a new analytical algorithm to improve the performances of the existing extraction system, and to enlarge the range of velocity profiles that can be processed automatically. Furthermore, this study introduces a methodology to quantify the accuracy of the extraction results under noise constraints by constructing a confidence interval for each extracted parameter value. Indeed, to apply such models particularly in forensics, educational, and biomedical applications dealing with real data, parameter estimation should be conducted accurately and produce confidence intervals, which can be used, for example, in the classification of genuine signatures and forgeries or in the detection of incapacities in normal and compromised students or in diseased individuals.
The remainder of the paper is organized as follows: In Section 2, a brief overview of the Kinematic Theory and its Delta-Lognormal model is presented, while the current status of the extraction problem is reported in Section 3. The new XZERO algorithm is described in detail in Section 4. The performance results obtained under ideal and noisy conditions are presented and discussed in Section 5. In Section 6, a complete extraction system is proposed and the methodology used to quantify its accuracy is set out. Typical examples involving real data are reported in Section 7 to highlight the power of the upgraded DeltaLognormal extraction system."	https://doi.org/10.1109/TPAMI.2008.264	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Djioua, et al. [NO] presents a new analytical method for estimating the parameters of delta-lognormal functions and characterizing handwriting strokes. 
A behavioral handwriting model for static and dynamic signature synthesis	[]	M. A Ferrer, M. Diaz, C. Carmona-Duarte, and A. Morales 2017. A behavioral handwriting model for static and dynamic signature synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence 39,	The synthetic generation of static handwritten signatures based on motor equivalence theory has been recently proposed for biometric applications. Motor equivalence divides the human handwriting action into an effector dependent cognitive level and an effector independent motor level. The first level has been suggested by others as an engram, generated through a spatial grid, and the second has been emulated with kinematic filters. Our paper proposes a development of this methodology in which we generate dynamic information and provide a unified comprehensive synthesizer for both static and dynamic signature synthesis. The dynamics are calculated by lognormal sampling of the 8-connected continuous signature trajectory, which includes, as a novelty, the pen-ups. The forgery generation imitates a signature by extracting the most perceptually relevant points of the given genuine signature and interpolating them. The capacity to synthesize both static and dynamic signatures using a unique model is evaluated according to its ability to adapt to the static and dynamic signature interand intra-personal variability. Our highly promising results suggest the possibility of using the synthesizer in different areas beyond the generation of unlimited databases for biometric training.	"A handwritten signature is the final response to a com-plex cognitive and neuromuscular process which is the result of the growing up and learning functions. This starts during childhood with lines and scribbles. Later, young children begin to use letters in their own style, although their motor control is usually not very accurate. Tracing helps both to improve their motor control and the learning of the shape and spatial relationships between objects, thus creating the spatial memory or cognitive map. Once this knowledge is acquired and themotor skills begin tomature, it is possible to select an ordered sequence of target points to perform fluent and effortless handwriting. At this stage, the person is ready to define and practice his or her own signature <NO>.
Although signature definition, which is the basis of interpersonal variability, is barely modified, its execution changes continuously due to long and short term factors. The long term drifting is mainly caused by aging whereas short term variability is due to either psychological reasons or external factors such as different stances, writing tools, signing surfaces, or other unaccountable reasons. Modeling these variabilities is a key factor in signature analysis and it is treated differently in several scientific fields.
For instance, automatic signature verification for personal identification <NO>, <NO> looks for features with both inter-personal discriminative ability and intra-personal stability. These features are extracted from either the static signature image or the signature dynamics <NO>, <NO>. Forensic analysis examines the authenticity of inked signatures by careful visual inspection of features such as the caliber, proportion, spacing, progression, pressure, gesture and the area occupied by the signature. This is to detect forgeries and illegitimate changes <NO>.
In the field of health, handwriting can also be applied to the identification of some learning problems for children or early stage degenerative cognitive or motor problems <NO>. In this connection, rapid hand movement velocity profiles made during the handwriting process have been studied in depth <NO>. Such models are currently being used in many applications such as identifying factors responsible for brain strokes <NO>, <NO> and diagnosing and preventing neurodegenerative diseases, such as Parkinson’s <NO> and Alzheimer’s <NO> among others. Also graphology uses handwriting to estimate, for instance, general personality traits, intelligence, social skills, emotions and social attitudes <NO>.
A common problem for researching in the above areas is the lack of handwritten signature samples. Their collection is costly. Besides, the design of a new database is usually focused on solving a well-defined problem which limits application to other areas. Additionally, legal issues regarding data protection hamper the sharing and distribution of biometric data <NO>.
The generation of synthetic handwritten signature data has emerged to alleviate these drawbacks and to better understand the cognitive and muscular processes involved in handwriting. Synthetic datasets can be used for preliminary algorithm assessment without the economic and time cost of developing real datasets. Additionally, these synthetic data can be shared without legal concerns."	https://doi.org/10.1109/TPAMI.2016.2582167	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	A et al. [NO] generates dynamic information and provide a unified comprehensive synthesizer for both static and dynamic signature synthesis. 
Robustness of offline signature verification based on gray level features	['processing is carried out to remove it <NO>.', 'features and Hidden Markov Models (HMM); System B <NO> employs the Boosting Feature Selection (BFS) approach and single grid-based features; System C <NO> is a Support Vector Matching (SVM) classifier with texture features; and System D <NO>, which is a third-party system, uses pose-orientated grid features and an SVM.']	M. A Ferrer, J.F. Vargas, A. Morales, and A. Ordóñez 2012. Robustness of offline signature verification based on gray level features. IEEE Transactions on Information Forensics and Security 7,	Several papers have recently appeared in the literature which propose pseudo-dynamic features for automatic static handwritten signature verification based on the use of gray level values from signature stroke pixels. Good results have been obtained using rotation invariant uniform local binary patterns LBP plus LBP and statistical measures from gray level co-occurrence matrices (GLCM) with MCYT and GPDS offline signature corpuses. In these studies the corpuses contain signatures written on a uniform white “nondistorting” background, however the gray level distribution of signature strokes changes when it is written on a complex background, such as a check or an invoice. The aim of this paper is to measure gray level features robustness when it is distorted by a complex background and also to propose more stable features. A set of different checks and invoices with varying background complexity is blended with the MCYT and GPDS signatures. The blending model is based on multiplication. The signature models are trained with genuine signatures on white background and tested with other genuine and forgeries mixed with different backgrounds. Results show that a basic version of local binary patterns (LBP) or local derivative and directional patterns are more robust than rotation invariant uniform LBP or GLCM features to the gray level distortion when using a support vector machine with histogram oriented kernels as a classifier.	"BIOMETRICS is playing an increasingly important role inpersonal identification and authentication systems. Several technologies that have been developed in this area are based on fingerprints, iris, face, voice, the handwritten signature, hand, etc. Handwritten signatures occupy a very special place in this wide set of biometric traits. The main reason is tradition: handwritten signatures have long been established as the most widespread means of personal verification. Signatures are generally accepted by governments and financial institutions as a legal means of verifying identity. Moreover, verification by signature analysis requires no invasive measurements and people are used to this event in their day to day activities A handwritten signature is the result of a complex process depending on the psychophysical state of the signer and the conditions under which the signing process occurs. Although complex theories have been proposed to model the psychophysical mechanisms underlying handwriting and the ink processes, signature verification is still an open challenge since a signature is usually judged to be genuine or a forgery on the basis of only a few reference specimens <NO>. Two methods of signature verification stand out. One is an offline method that uses an optical scanner to obtain handwriting data from a signature written on paper. The other, which is generally more successful, is an online method, which, with a special device, measures the sequential data, such as handwriting speed and pen pressure. Although less successful than the online method, offline systems do have a significant advantage because they do not require access to special processing systems when the signatures are produced <NO>. There are two main approaches for offline signature verification: static approaches and pseudo-dynamic approaches. The static one involves geometric measures of the signature while the pseudo-dynamic one tries to estimate dynamic information from the static image <NO>. Three different approaches in the reconstruction of dynamic information from static handwriting records can be used: mathematical methods, which estimate the temporal order of stroke production; methods inspired by motor control theory, which recover temporal features on the basis of stroke geometries such as curvature; and methods analyzing stroke thickness and stroke intensity variations. Among the techniques that analyze the stroke thickness or stroke intensity variations, we highlight those that focus on the gray level distribution in the signature stroke. Ammar et al. <NO> study the higher pressure points (HPPs) which indicate the regions where more effort has been made by the signer and are located as the darker zones of signature strokes. Mitra et al. in <NO> propose analyzing both higher and lower pressure points. Lv et al. <NO> divide the gray level values into 12 segments between the thresholds to segment the foreground and edge points. The percentage of pixels in each segment is added to the feature vector which also includes a stroke width distribution. Reference <NO> proposes a radial and angular grid for a local rate of higher pressure points numbers versus number of pixels. Franke in <NO> and <NO> identify several ink-trace characteristics affected by the interaction of biomechanical writing and physical ink-deposition
1556-6013/$31.00 © 2012 IEEE
processes. Some authors thus suggest that the selected characteristic can be measured using texture features such as local binary pattern (LBP) which has already been applied in face and hand biometrics <NO>, <NO>. Studies carried out after the initial LBP proposal have allowed the characterization carried out by this operator to improve, e.g., <NO> and <NO>. Specifically, <NO> proposes the use of rotational invariant uniform local binary patterns LBP plus LBP and statistical measures from gray level co-occurrence matrices (GLCM) with theMCYT database and the first 100 signers of GPDS960GraySignature database. The results presented in <NO> are summarized in Table I. An improved version of <NO> that uses the mentioned texture features worked out from the detail matrices of signature wavelet transform was published in <NO>. The major problem LBP has is its sensitivity when noise is present in the signature. Given that gradients are more stable than gray level features, the local derivative pattern (LDerivP) <NO> and local directional pattern (LDP) <NO> have been proposed. These patterns have been applied to face biometrics <NO>. The LDP has also been applied to black and white signatures in <NO> and to gray signatures in <NO>. The previously mentioned developments occurred with databases using signatures from uniform and white backgrounds. It is well known that writing a signature on complex backgrounds changes the gray level distribution of signature strokes. This change suggests that the robustness of pseudo-dynamic features based on gray level should be assessed. The aim of this paper is to evaluate the dependence of the gray level based features <NO> and propose strategies to improve their robustness to gray level distortion and segmentation errors due to complex backgrounds. All of the experimentation presented in this paper has been performed from a biometric point of view, which basically decides whether a signature is authentic or forged. We are aware that it is neither optimally informative nor practical for the forensic handwriting experts (FHEs) to carry out tasks such as detecting disguised signatures <NO> or identifying the author of a forgery <NO>. This paper proposes new methods that improve the automatic verification of signatures retrieved from checks and contributes to closing the gap between biometric state-of-the-art and FHEs requirements. The rest of the paper is organized as follows: Section II describes the signature and check databases, Section III presents the signature preprocessing and Section IV the signature features. Section V describes classifiers design while Section VI shows the experimental methodology, experiments and results. The paper closes with conclusions in Section VII."	https://doi.org/10.1109/TIFS.2012.2190281	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	A et al. [NO] is to measure gray level features robustness when it is distorted by a complex background and also to propose more stable features. 
Persian signature verification based on fractal dimension using testing hypothesis	[]	A. Foroozandeh, Y. Akbari, M.J. Jalili, and J. Sadri 2012. Persian signature verification based on fractal dimension using testing hypothesis. In Int. Conf. on Frontiers in Handwriting Recognition	A new approach for verifying off-line Persian signatures is presented, in this paper. In our method, feature extraction step is conducted based on estimated Fractal Dimension (FD) of signatures images, and making decision about acceptance/rejection of test signature is formulated as testing hypothesis which is used for the first time in order to verify offline Persian signatures. The proposed method has been tested on our new created database included 1000 genuine signatures and 200 skilled forgeries which have been collected from a population of 100 human subjects with different educational background. Obtained results confirm the effectiveness of the presented method. Keywords-Off-Line Signature Verification; Fractal Dimension; Testing Hypothesis; Database of Persian Signatures.	"Automatic signature verification (SV) is one of the biometric approaches in person authentication. Compared to other biometrics such as: iris, voice, and fingerprint, the cost of personal verification using his/her signatures is very low. Analysis of signature to authenticate the identity of an individual is conducted through discrimination between genuine signature from a forgery <NO>. Automatic SV has many applications including authenticating bank checks, contracts, and other security documents. SV can be conducted in two ways: on-line, and off-line. So many behavioural information such as: pen-point velocity, tremor information, and writing pressure which can be extracted in on-line SV, are not available in off-line SV and only some signatures signed on some sheets of papers are available in off-line SV. Mentioned restrictions lead to off-line SV be more difficult than on-line. Many attempts have been conducted in both off-line, and on-line SV in Latin language. Two comprehensive surveys of these conducted works have been done by Impedovo et al. <NO>, and Pal et al. <NO>.
During the last decade, some works have been conducted for verifying Persian signatures <NO>. Compared to Latin there are fewer number of works on Persian signatures, so more attention is needed in this field. It should be noted that in spite of Latin signatures which are reshaped of their handwritten names, Persian signatures are usually made of cursive sketches <NO>. Some samples of Persian and Latin signatures have been shown and compared in Figure 1.
Due to above mentioned issues, we decided to focus on off-line Persian SV and proposed a new approach for this target. Our presented approach uses two main concepts: Fractal Dimension (FD) as extracted feature from each signature image, and testing hypothesis used for discrimination between genuine signatures from skilled forgeries. Making decision about acceptance/rejection of signature in question using testing hypothesis yields to obtain more accurate results. Notably statistical tools have been used by some of researches in the process of SV specially in on-line SV systems such as <NO>. Also, there are some similar works in off-line SV such as <NO>. However, based on our searches, presented approach with us has not been proposed in Latin and Persian conducted works, till today. Our presented SV method uses FD values as global features and testing hypothesis as a statistical tool for final decision. The performance of the presented method has been evaluated using our new database. This database includes 1200 Persian signatures which have been signed by 105 writers.
The rest of this paper is organized as follows. Section 2 proposes some background information used in our method. Section 3 describes our presented approach. In Section 4, the experimental results are shown. Finally, our conclusions and future works are presented in Section 5."	https://doi.org/10.1109/ICFHR.2012.254	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	a new approach for verifying off-line persian signatures is presented, in Foroozandeh, et al. [NO].
Touchalytics: On the applicability of touchscreen input as a behavioral biometric for continuous authentication	[]	M. Frank, R. Biedert, E. Ma, I. Martinovic, and D. Song 2013. Touchalytics: On the applicability of touchscreen input as a behavioral biometric for continuous authentication. IEEE Transactions on Information Forensics and Security 8,	We investigate whether a classifier can continuously authenticate users based on the way they interact with the touchscreen of a smart phone. We propose a set of 30 behavioral touch features that can be extracted from raw touchscreen logs and demonstrate that different users populate distinct subspaces of this feature space. In a systematic experiment designed to test how this behavioral pattern exhibits consistency over time, we collected touch data from users interacting with a smart phone using basic navigation maneuvers, i.e., up–down and left–right scrolling. We propose a classification framework that learns the touch behavior of a user during an enrollment phase and is able to accept or reject the current user by monitoring interaction with the touch screen. The classifier achieves a median equal error rate of 0% for intrasession authentication, 2%–3% for intersession authentication, and below 4% when the authentication test was carried out one week after the enrollment phase. While our experimental findings disqualify this method as a standalone authentication mechanism for long-term authentication, it could be implemented as a means to extend screen-lock time or as a part of a multimodal biometric authentication system.	MOST methods for authenticating users on desktopcomputers or mobile devices define an entry point into the system. Typically, the user faces a password challenge and is granted access only if she inputs the correct password. While such entry-point based methods dominate the authentication schemes today, they have flaws from both usability and security perspectives. From a usability perspective, traditional authentication schemes are inconvenient because users must focus on the authentication step every time they begin interacting with their device. Such inconvenience is amplified under the usage pattern of mobile devices, since they are more frequently accessed, and each use is typically shorter. Authentication with a PIN or secret gesture is too cumbersome for short bursts of activity, such as briefly checking one’s e-mail or reading an SMS. Hence, users often choose simple and weak secrets, increase the screen lock time-outs of their devices, or completely disable unlock <NO>, <NO>. Recent studies have demonstrated how simple attacks such as smudge attacks <NO> can break entry-point authentication schemes. Furthermore, the device cannot detect intruders after the authentication step is performed successfully. Mobile devices are at a higher risk of loss or theft compared to desktop computers <NO>. Continuous or implicit authentication approaches would provide an additional line of defense, designed as a nonintrusive and passive security countermeasure. Such approaches monitor the user’s interaction with the device, and ideally, at every point in time (or at least with a high frequency) the system estimates if the legitimate user is using the device. Hence, a continuous authentication method can either complement entry-point based authentication methods by monitoring the user after a successful login or, if the method satisfies particular accuracy requirements, it could even substitute entry-point based authentication. Although there is a growing body of literature about keystroke dynamics or mouse dynamics for continuous authentication, there is surprisingly little work on continuous authentication for touchscreen devices. The growing popularity of mobile devices—in 2011, more smart phones were sold than desktop PCs and notebooks combined <NO>—increases the value of research on their security mechanisms. Specifically, to the best of our knowledge, there is no existing method for continuous authentication based on touch biometrics (i.e., without requiring a dedicated activity of the user). One reason might be the difficulty of extracting a set of sufficiently discriminative features from touch data, because atomic navigation behavior mostly consists of simple and short movements (see Fig. 1). In this paper, we lay foundational work for continuous authentication schemes that rely on touchscreen input as a data source.We investigate if it is possible to authenticate users while they perform basic navigation steps on a touchscreen device and without any dedicated and explicit security action that requires attention from the user. Our goal is to analyze how robustly such schemes operate and if they are sufficiently reliable to be used on commodity devices. Our contribution is a classification framework that serves as a proof-of-concept for touch-based behavioral biometric authentication. We propose a set of 30 behavioral features that can be extracted from the touch screen input of commodity mobile devices. We designed experiments that let users interact with touchscreens in different sessions and with different tasks, and demonstrate that our features are highly discriminative. Along the way, we discuss design decisions and usage scenarios for such a continuous authenticationmethod. Our study provides insights in the operational modes and scenarios that are permitted given the accuracy of the proposed method. All data collected for this paper is available online.	https://doi.org/10.1109/TIFS.2012.2225048	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Frank, et al. [NO] investigates whether a classifier can continuously authenticate users based on the way they interact with the touchscreen of a smart phone. 
Online signature verification with support vector machines based on LCSS kernel functions	['Other matching techniques include the Longest Common Subsequence (LCSS) <NO>, <NO> and the Edit Distance <NO>.']	C. Gruber, T. Gruber, S. Krinninger, and B. Sick 2010. Online signature verification with support vector machines based on LCSS kernel functions. IEEE Transactions on Systems, Man, and Cybernetics Part B: Cybernetics 40,	In this paper, a new technique for online signature verification or identification is proposed. The technique integrates a longest common subsequences (LCSS) detection algorithm which measures the similarity of signature time series into a kernel function for support vector machines (SVM). LCSS offers the possibility to consider the local variability of signals such as the time series of pen-tip coordinates on a graphic tablet, forces on a pen, or inclination angles of a pen measured during a signing process. Consequently, the similarity of two signature time series can be determined in a more reliable way than with other measures. A proprietary database with signatures of 153 test persons and the SVC 2004 benchmark database are used to show the properties of the new SVM-LCSS. We investigate its parameterization and compare it to SVM with other kernel functions such as dynamic time warping (DTW). Our experiments show that SVM with the LCSS kernel authenticate persons very reliably and with a performance which is significantly better than that of the best comparing technique, SVM with DTW kernel.	"AUTHENTICATION of a person’s identity is a needin many social and commercial interactions. It can be achieved in various ways, for example, using a birth certificate, a passport, or the knowledge of a personal identification number. Very often, signatures are used for authentication. In most cases, a given signature is compared to a single reference signature with the naked eye. Thus, this method is prone to impostors. Electronic image-based techniques (so-called offline signature verification) can also be outsmarted quite easily. However, as authentication by signature is widely accepted compared to other techniques such as fingerprint or iris scan, there is a need for a signature verification technique that ensures a high level of security. Biometric signature verification techniques based on the dynamics of a person’s signature, i.e., time series of pen-tip coordinates, writing forces, or inclination angles of a pen, are, in principle, able to provide a reliable authentication (so-called online signature verification). However, in contrast to many other biometric authentication techniques, signatures of a person may be very variable. This makes the verification process—which can basically be seen as a twoclass classification problem (a given signature belongs to a certain person or not)—quite difficult. Signature verification is closely related to signature identification, where the identity of a person must be determined by comparing a signature sample to all reference samples or models in a database, for instance.
Support vector machines (SVM) are widely used to solve classification and regression problems. As they provide very good results in various pattern recognition fields, they also seem to be a good choice for online signature verification. Compared to other classifier paradigms, such as neural networks, fuzzy classifiers, or decision trees, SVM—which are based on the principle of structural risk minimization—have major advantages, such as a convex objective function with efficient training algorithms and good generalization properties. SVM use kernel functions to measure the similarity of two data objects, in our case, time series. However, standard kernel functions such as Gaussian or polynomial kernels are not well suited for timeseries processing. The key contribution of this paper is the definition of a new kernel function for SVM based on an LCSSs detection for time series that even outperforms kernels based on hidden Markov models (HMM) and dynamic time warping (DTW) in the particular application field. An LCSSbased similarity measure is tailored to deal with the specific variability of online signature data, and it could be used in various online signature verification or identification systems.
The remainder of this paper is organized as follows: First, we will discuss some specific characteristics of time series that describe the dynamics of handwriting (online signature data) in Section II. In Section III, related work in the application field and in the field of time-series classification with SVM is presented. Section IV describes the new LCSS-based kernel function for online signature verification. In Section V, this method is evaluated in a number of experiments and also compared to other methods. Finally, Section VI summarizes the major findings."	https://doi.org/10.1109/TSMCB.2009.2034382	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Gruber, et al. [NO], a new technique for online signature verification or identification is proposed. 
Online signature verification and recognition: An approach based on symbolic representation	['There has been much work on online signature verification systems <NO>–<NO>.', 'In addition, Guru and Prakash <NO> derived a symbolic representation of an online signature and introduced the concept of writer independent threshold in order to improve verification accuracy.', 'Some papers <NO>, <NO> randomly select k samples as the training set and then they average over multiple such random selections to derive the final performance result.', 'vector and then employ the classical distance measure such as Euclidean, City-Block for distance computation <NO>, <NO>.', 'Guru and Prakash <NO> represented online signatures by interval-valued symbolic features.']	D.S. Guru, and H.N. Prakash 2009. Online signature verification and recognition: An approach based on symbolic representation. IEEE Transactions on Pattern Analysis and Machine Intelligence 31,	In this paper, we propose a new method of representing online signatures by interval-valued symbolic features. Global features of online signatures are used to form an interval-valued feature vectors. Methods for signature verification and recognition based on the symbolic representation are also proposed. We exploit the notions of writer-dependent threshold and introduce the concept of feature-dependent threshold to achieve a significant reduction in equal error rate. Several experiments are conducted to demonstrate the ability of the proposed scheme in discriminating the genuine signatures from the forgeries. We investigate the feasibility of the proposed representation scheme for signature verification and also signature recognition using all 16,500 signatures from 330 individuals of the MCYT bimodal biometric database. Further, extensive experimentations are conducted to evaluate the performance of the proposed methods by projecting features onto Eigenspace and Fisherspace. Unlike other existing signature verification methods, the proposed method is simple and efficient. The results of the experimentations reveal that the proposed scheme outperforms several other existing verification methods, including the state-of-the-art method for signature verification.	"AN online signature is a behavioral biometric used forpersonal authentication to complete automated transactions, gaining control of computing facilities or physical entry to protected areas. An online signature is more robust compared to an offline signature as it stores dynamic features <NO> like azimuth, elevation, and pressure signals in addition to position trajectories. Nevertheless, it is unlikely that even skilled forgers could imitate online signatures along with their dynamic properties.
Generally, the signature biometric problem has two distinct perspectives: 1) verification and 2) recognition. In verification, features of a test signature are contrasted with features of a limited set of signatures of the class whose identity is claimed, whereas, in recognition, the presence of an identity of a test signature in the database is ascertained. Recognition involves feature-matching stage that extends to entire database.
Online signature verification can generally be divided into two groups: 1) parametric and 2) functional. In a parametric approach, a set of parameters abstracted from the signal represents a signature pattern and the parameters of the reference and test signatures are compared to decide if the test signature is authentic. In this approach, a signature is described compactly so that the enrollment data size is considerably small and constant. More importantly, this method of representation is more stable against the variations in local regions, which are common in signatures. In the literature, we can find hundreds of parameters proposed for signature verification. Several parameters are obtained from time function of the dynamic signals captured by digitizing tablets. Some of the most extensively used parameters are position, displacement, speed, acceleration <NO>, <NO>, number of pen ups and pen downs <NO>, pen down time ratio <NO>, Wavelet transform <NO>, Fourier transform <NO>, and direction-based <NO> parameters. On the other hand, in a function-based approach, the time sequences describing local properties of online signatures are analyzed. In this approach, a signature is characterized by a time function (e.g., position trajectory, velocity, acceleration, pressure, direction of pen movement, and azimuth) <NO>, <NO>, <NO>. In general, function-based systems show better performance than the parameter-based systems but require time-consuming matching/comparison procedures. However, the work <NO> shows that the parametric approaches are equally competitive when compared to function-based approaches.
During matching, the authenticity of a test signature is evaluated by comparing its features against those stored in the knowledgebase. Each matching technique is based on a suitable similarity (or dissimilarity) measure. The matching techniques based on Dynamic time warping (DTW) <NO>, <NO>, <NO>, <NO>, <NO>, Hidden Markov Model (HMM) <NO>, <NO>, <NO>, Support vector machine (SVM) <NO>, <NO>, and Neural Networks (NN) <NO>, <NO>, <NO> are commonly used.
When parameters are used as features, the euclidean <NO>, <NO> distance is the most commonly used dissimilarity measure. When functions are considered, the matching techniques must take into account the variations of signing durations. Elastic matching such as Dynamic Time Warping <NO>, <NO>, <NO> is the best one for this purpose. However, the time complexity of DTW is of Oðn2Þ.
On the other hand, the HMM has also attracted many researchers. An HMM performs stochastic matching of a model and a test signature using a sequence of probability distributions of the features along the signature. This statistical theory of learning has an ability to absorb the variability and similarity between the patterns. The main limitations of HMMs are high computational complexity and large memory requirements. The number of parameters to be set in HMM is more and making a large assumption about the data (regarding transition probabilities and distributions) is required. In addition, large positive data are required to train an HMM.
Another popular matching technique used for signature matching is the SVM. With a set of examples from two classes, an SVM finds the hyperplane that maximizes the distance from either class to the hyperplane and separates the largest number of points belonging to the same class on the same side. Therefore, the misclassification error of data in both the training set and test set is minimized. In <NO>, comparison of SVM classifiers with HMM classifiers in terms of the number of samples used for training and verification using different types of forgeries is carried out. Under both conditions, SVM appears to produce better results. However, the main limitations of SVMs are high algorithmic complexity and extensive memory requirements in large-scale tasks.
Signature verification schemes based on Neural Network are also proposed <NO>, <NO>, <NO>. Although, the neural network-based approaches have the capabilities in generalization, the drawback is the need for a large number of genuine and forgery signatures for training, which is not always practically viable.
There are several other approaches used in signature biometry: the split and merge mechanism <NO>, an elastic local-shape-based model for handwritten curves <NO>, a scheme based on similarity measurement of logarithmic spectrum <NO>, a method for estimating similarity between the input signature and the reference set using string matching <NO>, a relaxation matching technique <NO>, a multiexpert system for signature verification <NO>, a combination of vector quantization and dynamic time warping by means of score fusion <NO>, fusion of function-based methods <NO>, etc.
In this brief survey on signature verification, we understand that almost all works rely on common threshold or global threshold. To the best of our knowledge, the concept of writer-dependent threshold is exploited only in <NO>. Writer-dependent threshold is shown to provide better results than a global threshold. In our work, in addition to adapting writer-dependent threshold, we introduce the concept of feature-dependent threshold for further improvement in results (toward achieving lower equal error rate).
In this work, we propose a simple and novel approach for signature representation based on symbolic data. By the use of the proposed representation and by exploiting the concept of writer-dependent threshold and feature-dependent threshold, we address both the problems of verification and recognition of online signatures. The recent developments in the area of symbolic data analysis have proven that the real-life objects can be better described by the use of symbolic data, which are extensions of classical crisp data <NO>. Symbolic data appear in the form of continuous ratio, discrete absolute interval and multivalued, multivalued with weightage, quantitative, categorical, etc. <NO>. The concept of symbolic data analysis has been extensively studied in the field of cluster analysis <NO>, <NO> and it has been proven both theoretically and experimentally that the clustering approaches based on symbolic data outperform conventional clustering techniques. More details and applications of symbolic data can be found in <NO>. Recently, a symbolic representation model for 2D shapes has been proposed <NO> and it is also shown that symbolic representation effectively captures shape information and the corresponding retrieval methodology outperforms conventional representation techniques <NO>. In signature representation, since sample signatures of each person possess significant variations, features extracted from such samples too vary considerably. Therefore, we felt that it would be more meaningful to capture these variations in the form of interval-valued features and provide an effective representation for signatures. To the best of our knowledge, no work has been reported in the literature which uses symbolic representation for signatures. With this backdrop, in our previous work <NO>, we made an initial attempt toward application of symbolic data concepts for signature verification. In this paper, the same work is extended in many directions. By exploiting the concept of feature-dependent threshold and writer-dependent thresholds, we have obtained significant reduction in equal error rate. Further, the proposed approach is simple and easy to realize. Overall, the following are the contributions of this work:
. a new method of symbolic representation for online signatures; . methods for signature verification and recognition, based on a novel similarity measure; . exploitation of the concept of writer-dependent threshold; . introduction of feature-dependent threshold; . conduction of extensive experiments for both verification and recognition on a large database of 16,500 signatures of 330 individuals (MCYT_ signature database); . achieving a remarkable reduction in EER when compared to the best contemporary works.
All in all, the proposed model being a stand-alone and first of its kind in the literature of signature verification/ recognition is expected to open up a new dimension for further research in the field of signature biometrics by the use of symbolic data.
The rest of the paper is structured as follows: In Section 2, the proposed method of symbolic representation, verification, and recognition of online signatures are presented. In Section 3, the details of the signature verification and recognition experimentations along with results are summarized. A comparative study is presented in Section 4. In Section 5, the proposed approaches of signature verification
and recognition are applied on the features projected onto Eigenspace and Fisherspace. Finally, Section 6 follows with conclusions."	https://doi.org/10.1109/TPAMI.2008.302	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Guru, et al. [NO] proposes a new method of representing online signatures by interval-valued symbolic features. 
Fiberio: A touchscreen that senses fingerprints	[]	C. Holz, and P. Baudisch 2013. Fiberio: A touchscreen that senses fingerprints. In Proc. 26th Annual ACM Symp. on User Interface Software and Technology	We present Fiberio, a rear-projected multitouch table that identifies users biometrically based on their fingerprints during each touch interaction. Fiberio accomplishes this using a new type of screen material: a large fiber optic plate. The plate diffuses light on transmission, thereby allowing it to act as projection surface. At the same time, the plate reflects light specularly, which produces the contrast required for fingerprint sensing. In addition to offering all the functionality known from traditional diffused illumination systems, Fiberio is the first interactive tabletop system that authenticates users during touch interaction—unobtrusively and securely using the biometric features of fingerprints, which eliminates the need for users to carry any identification tokens. Author	"Several researchers have proposed techniques that allow interactive tabletop systems to distinguish users during interaction. The ability to associate each touch with a particular user has allowed such systems to personalize interaction <NO>, log user activity <NO>, and ensure that only the authorized users can access private objects <NO> or perform privileged activities <NO>. 
 A number of existing approaches address this challenge. Unfortunately, they either require users to carry identification tokens, such as RFID tags <NO>, rings <NO>, or marker gloves <NO> or they can only distinguish among a small group of users, for example by recognizing their shoes <NO>, their hand contours <NO>, or the chairs they sit in <NO>. 
 Researchers have therefore pointed to fingerprint recognition as a possible solution to the problem. Fingerprint-based authentication is secure <NO> and—in conjunction with touch interaction—would be unobtrusive for users. First steps in this direction include a separate fingerprint scanner placed next to the touchscreen <NO> and an interactive fingerprint scanner without a screen (Ridgepad <NO>). These prototypes point out the challenge in designing such a system, i.e., to sense fingerprints and display a computergenerated image in the same space at the same time. This challenge boils down to two contradicting requirements with respect to the screen material. On the one hand, the screen has to reveal fingerprints, i.e., produce contrast between the ridges and valleys of the fingerprint. Known solutions require a specular screen surface to accomplish optical fingerprint scanning. On the other hand, to be used as a display, the screen has to allow the rear-projection to produce a visible image, which requires the screen material to be diffuse. Unfortunately, specular and diffuse are contradictory requirements for such a surface. These contradictory requirements eliminate a number of candidate technologies that appear suitable at first glance. Tabletops based on frustrated total internal reflection <NO>, for example, cannot generate the contrast between fingerprint valleys and ridges and thus do not afford scanning users’ fingerprints with sufficient quality. In this paper, we demonstrate how to resolve this contradiction. We present Fiberio, a multitouch table that recognizes fingerprints during touch interaction. As shown in Figure 1, Fiberio authenticates users while interacting with the table."	https://doi.org/10.1145/2501988.2502021	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Holz, et al. [NO] presents fiberio, a rear-projected multitouch table that identifies users biometrically based on their fingerprints during each touch interaction. 
On-line signature verification using 1-D velocity-based directional analysis	[]	M.T. Ibrahim, M. Kyan, M.A. Khan, and L. Guan 2010. On-line signature verification using 1-D velocity-based directional analysis. In 20th International Conference on Pattern Recognition	In this paper, we propose a novel approach for identity verification based on the directional analysis of velocity-based partitions of an on-line signature. First, interfeature dependencies in a signature are exploited by decomposing the shape (horizontal trajectory, vertical trajectory) into two partitions based on the velocity profile of the base-signature for each signer, which offers the flexibility of analyzing both low and high-curvature portions of the trajectory independently. Further, these velocity-based shape partitions are analyzed directionally on the basis of relative angles. Support Vector Machine (SVM) is then used to find the decision boundary between the genuine and forgery class. Experimental results demonstrate the superiority of our approach in on-line signature verification in comparison with other techniques. Keywords-on-line signature; inter-feature dependencies; curvature; support vector machine;	"On-line Signature Verification is a process of verifying user’s identity on the basis of signature’s shape along with some dynamic features such as velocity, acceleration, curvature, pressure, total signature time, RMS speed, average writing speed etc <NO>, <NO>, <NO>. In the past, it has been observed that the on-line signature verification based on a composite feature set containing both shape and dynamic features (velocity, pressure and angle) can outperform verification systems based on the shape alone <NO>, <NO>, <NO>.
This paper also proposes a composite feature set based upon the velocity-based directional analysis of an on-line signature. Recently, a 16-band Decimation-Free Directional Filter Bank (DDFB) was used for the same purpose <NO>. But there are two major problems with the DDFB-based system. The first problem with the DDFB-based system, is that it takes the image as an input. In <NO>, spatial areas corresponding to velocity-based partitions were converted into gray-scale images, due to which, the information of each independent trajectory was lost. The second problem with DDFB or with any filter bank is the selection of the order of the filter. As we know, the more we increase the order of the filter, the more we will get artifacts in the output of the filter. In case of signature, these artifacts can lead to non-existing spatial areas which reduces the verification performance. So, there is a need for a directional analysis tool which can perform the directional analysis in the same manner as DDFB does, but without converting the on-line signature into an image and without any constraint on the order of the filter. In this paper, we provide a novel approach for the directional analysis of a signature based on velocity partitions which first decompose the shape (horizontal and vertical trajectories) of the signature into two velocitybased partitions for analyzing the high-curvature and lowcurvature portions of the trajectory independently. Following this, the low and high-curvature segments are split into two relative angle based partitions respectively. In this work, our proposed directional analysis tool does not need to convert the signature trajectories into an image. Also one can have ideal partitions of signature based on its relative angles - the details of which are deferred to Section III-A.
The remainder of this paper is organized as follows: the second section deals with the acquisition of signature data and preprocessing steps; the third section is dedicated to the design and structure of our proposed system; the experimental results and concluding remarks are presented in the final section of this paper."	https://doi.org/10.1109/ICPR.2010.933	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Ibrahim, et al. [NO] proposes a novel approach for identity verification based on the directional analysis of velocity-based partitions of an on-line signature. 
Automatic signature verification: The state of the art	['These facts have motivated great research efforts in the last decade on dynamic or online signature verification, as reviewed in <NO>.', 'A large variety of local feature-based signature verification systems can be found in the literature and summarized in <NO>.', 'Overviews of techniques for off- or online signature verification can be found in <NO>–<NO>, for instance.', 'For instance, automatic signature verification for personal identification <NO>, <NO> looks for features with both inter-personal discriminative ability and intra-personal stability.', 'The recent state-of-the-art of signature verification is summarized in <NO>.', 'DTW allows the compression or expansion of the time axis of two time sequence representative of signatures to obtain the minimum of a given distance value <NO>.', 'The topic of online signature verification has been a very active area for research exploration, since the past three decades, with the survey of works being well documented in <NO>–<NO>.', 'Although complex theories have been proposed to model the psychophysical mechanisms underlying handwriting and the ink processes, signature verification is still an open challenge since a signature is usually judged to be genuine or a forgery on the basis of only a few reference specimens <NO>.', 'Subsequent works up to 2000 and 2008 are summarized in <NO> and <NO> respectively.', 'Subsequent works are summarized in <NO> and <NO> respectively.']	D. Impedovo, and G. Pirlo 2008. Automatic signature verification: The state of the art. IEEE Transactions on Systems, Man, and Cybernetics 38,	In recent years, along with the extraordinary diffusion of the Internet and a growing need for personal verification in many daily applications, automatic signature verification is being considered with renewed interest. This paper presents the state of the art in automatic signature verification. It addresses the most valuable results obtained so far and highlights the most profitable directions of research to date. It includes a comprehensive bibliography of more than 300 selected references as an aid for researchers working in the field.	"THE SECURITY requirements of the today’s society haveplaced biometrics at the center of a large debate, as it is becoming a key aspect in a multitude of applications <NO>, <NO>, <NO>. The term biometrics refers to individual recognition based on a person’s distinguishing characteristics. While other techniques use the possession of a token (i.e., badge, ID card, etc.) or the knowledge of something (i.e., a password, key phase, etc.) to perform personal recognition, biometric techniques offer the potential to use the inherent characteristics of the person to be recognized to perform this task. Thus, biometric attributes do not suffer from the disadvantages of either the token-based approaches, whose attributes can be lost or stolen, and knowledge-based approaches, whose attributes can be forgotten <NO>, <NO>.
A biometric system can either verify or identify. In verification mode, it authenticates the person’s identity on the basis of his/her claimed identity. Instead, in identification mode, it establishes the person’s identity (among those enrolled in a database) without the subjects having to claim their identity <NO>, <NO>. Depending on the personal traits considered, two types of biometrics can be defined: physiological or behavioral. The former are based on the measurement of biological traits of users, like, for instance, fingerprint, face, hand geometry, retina, and iris. The latter consider behavioral traits of users, such as voice or handwritten signature <NO>, <NO>, <NO>, <NO>, <NO>.
The assessment of biometrics is a multifaceted problem <NO>, <NO>, <NO>. For instance, a biometric trait should be universal,
Manuscript received March 2, 2007; revised August 3, 2007 and November 7, 2007. This paper was recommended by Associate Editor M. Last.
D. Impedovo is with the Dipartimento di Elettrotecnica ed Elettronica, Politecnico di Bari, Bari 70126, Italy and also with the Centro “Rete Puglia,” Università degli Studi di Bari, Bari 70124, Italy (e-mail: impedovo@ deemail.poliba.it).
G. Pirlo is with the Dipartimento di Informatica, Università degli Studi di Bari, Bari 70126, Italy and also with the Centro “Rete Puglia,” Università degli Studi di Bari, Bari 70124, Italy (e-mail: pirlo@di.uniba.it).
Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TSMCC.2008.923866
i.e., each person should possess the trait; unique, i.e., no two persons should share the same trait; permanent, i.e., the trait should neither change nor be alterable; collectable, i.e., the trait can be obtained easily. In addition, biometric system design should also address other desirable features such as accuracy, cost and speed effectiveness, acceptability by the users, and so on <NO>, <NO>.
Although a wide set of biometrics has been considered so far, it is worth noting that no trait is able to completely satisfy all the desirable characteristics required for a biometric system <NO>. Thus, the assessment of a biometric trait is strongly dependent on the specific application since it involves not only technical issues but also social and cultural aspects <NO>, <NO>, <NO>.
Handwritten signatures occupy a very special place in this wide set of biometric traits <NO>, <NO>, <NO>, <NO>, <NO>. This is mainly due to the fact that handwritten signatures have long been established as the most widespread means of personal verification. Signatures are generally recognized as a legal means of verifying an individual’s identity by administrative and financial institutions <NO>, <NO>. Moreover, verification by signature analysis requires no invasive measurements and people are familiar with the use of signatures in their daily life <NO>.
Unfortunately, a handwritten signature is the result of a complex process depending on the psychophysical state of the signer and the conditions under which the signature apposition process occurs. Therefore, although complex theories have been proposed to model the psychophysical mechanisms underlying handwriting <NO>–<NO> and the ink-depository processes <NO>, <NO>, <NO>, <NO>, signature verification still remains an open challenge since a signature is judged to be genuine or a forgery only on the basis of a few reference specimens <NO>. Fig. 1 sketches the three main phases of automatic signature verification: data acquisition and preprocessing, feature extraction, and classification. During enrolment phase, the input signatures are processed and their personal features are extracted and stored into the knowledge base. During the classification phase, personal features extracted from an inputted signature are compared
1094-6977/$25.00 © 2008 IEEE
against the information in the knowledge base, in order to judge the authenticity of the inputted signature.
Automatic signature verification involves aspects from disciplines ranging from human anatomy to engineering, from neuroscience to computer science and system science <NO>. Because of this fact, in recent years, studies on signature verification have attracted researchers from different fields, working for universities and companies, which are interested in not only the scientific challenges but also the valuable applications this field offers <NO>. Comprehensive survey papers reported the progress in the field of automatic signature verification until 1993 <NO>, <NO>, <NO>. In 1994, a special issue and a book collecting the most relevant research activities were published <NO>. Successively, various papers have summarized the increasing research efforts in the field <NO>, <NO>, <NO>, <NO>, <NO> also with respect to the more general area of handwriting analysis and processing <NO>.
In conjunction with the recent and extraordinary growth of the Internet, automatic signature verification is being considered with new interest. The creation of specific laws and regulations, which have been approved in many countries <NO>, <NO>, and the attention that several national associations and international institutes have given to the standardization of signature data interchange formats <NO>, <NO>, <NO> are evidence of the renewed attention in this field. The aim of these efforts is to facilitate the integration of signature verification technologies into other standard equipment to form complete solutions for a wide range of commercial applications such as banking, insurance, health care, ID security, document management, e-commerce, and retail point-of-sale (POS) <NO>, <NO>, <NO>.
This paper presents the state of the art in automatic signature verification, with specific attention to the most recent advancements. Following an introduction of the phases of the signature verification process, the main contributions of research activities in recent years are described and the most promising trends are discussed. Specifically, Section II presents the main aspects related to data acquisition and preprocessing and Section III discusses the feature extraction phase. Section IV describes research activities concerning the classification phase while Section V summarizes the performance of systems for automatic signature verification reported in the literature. A brief discussion on the applications of automatic signature verification and the most promising research directions are reported in Section VI, along with the conclusions of this paper. A bibliography of more than 300 references is also provided for the more interested reader. It includes the most relevant papers recently published as well as some older papers, which can help give a comprehensive outline of developments in this field of research."	https://doi.org/10.1109/TSMCC.2008.923866	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Impedovo, et al. [NO] presents the state of the art in automatic signature verification. 
On-line signature verification by stroke-dependent representation domains	[]	D. Impedovo, and G. Pirlo 2010. On-line signature verification by stroke-dependent representation domains. In 12th Int. Conf. on Frontiers in Handwriting Recognition	In this paper a new system for dynamic signature verification is presented. It is based on the consideration that each region of an handwritten signature can convey personal characteristics in diverse domains. Therefore, a multi-expert approach is considered in which each stroke of the signature is evaluated in the most profitable domain of representation. The experimental results demonstrate the effectiveness of the proposed approach.	"Biometry offers potentials for verifying the identity of a subject by the analysis of physical and behavioural characteristics. Physical characteristics can be obtained from finger-print, palm-print, face gestures, retina, or DNA. Behavioural characteristics can be obtained from key-stroke dynamics, speech, hand-written signature. Among others, hand-written signature is one of the most interesting means for automated personal verification. Signature is the customary way of identifying an individual in our society and it is well-accepted by every user for legal attestation and administrative certification <NO>. In addition, along with the growth of the internet, automatic signature verification is being considered with new interest. The creation of specific laws and regulations, which have been approved in many countries <NO>, and the attention that several national associations and international institutes have given to the standardization of signature data interchange formats <NO> are evidence of the renewed attention in this field. The aim of these efforts is to facilitate the integration of signature verification technologies into other standard equipment to form complete solutions for a wide range of commercial applications such as banking, insurance, health care, ID-security, document management, ecommerce and retail point-of-sale (POS) <NO>.
Unfortunately, handwritten signatures are very complex biometric traits since they are the result of a complex process based on a sequence of predetermined actions, stored in the human brain, and realised by the writing systems of the signers (arms and hands) through ballisticlike movements. Therefore, also signatures written by the same person can be very different depending on the physical and psychological state of the writer. Thus, automatic
signature verification involves aspects from a wide range of disciplines, as computer science, engineering, psychology, neuroscience, human anatomy and system science and several comprehensive survey papers reporting the development of the field have been published <NO>. In order to face with the enormous variability of signatures, multi-expert approach has been often considered and several systems have been proposed in the literature which combine verifiers based on different sets of features, using parallel <NO>, serial <NO> or hybrid <NO> strategies.
This paper presents a multi-expert system for dynamic signature verification. The system uses a segmentation technique of hand-written signatures well-suited for multiexpert verification since it provides compatible strokeoriented descriptions of the test and the reference signatures. Successively, the most profitable domain of representation of each stroke is determined on the basis of its stability characteristics. In other words, each stroke is verified in the domain of representation (i.e. displacement, velocity, acceleration, pressure, direction of pen movement, etc.) in which it is more stable. In this domain, in fact, it is expected that verification is more accurate than in other domains in which the signer is highly variable. The final decision is obtained by simple or weighted averaging of the local decisions on the authenticity of each individual stroke. The experimental results show the effectiveness of the proposed approach.
The paper is organised as follows: Section 2 describes the multi-expert process of signature verification. Section 3 presents the segmentation technique. The selection of the best domain of representation for each stroke and the process of stroke verification is described in Section 4. Section 5 presents the process of decision combination. The experimental results are presented in Section 6."	https://doi.org/10.1109/ICFHR.2010.102	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Impedovo, et al. [NO] a new system for dynamic signature verification is presented. 
Handwritten signature verification: New advancements and open issues	['fested in many publications and surveys <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO> published in the literature during previous decades.']	D. Impedovo, G. Pirlo, and R. Plamondon 2012. Handwritten signature verification: New advancements and open issues. In Int. Conf. on Frontiers in Handwriting Recognition	Recently, research in handwritten signature verification has been considered with renewed interest. In fact, in the age of e-society, handwritten signature still represents an extraordinary means for personal verification and the possibility of using automatic signature verification in a range of applications is becoming a reality. This paper focuses on some of the most remarkable aspects the field and highlights some recent research directions. A list of selected publications is also provided for interested researchers. KeywordsBiometry, Handwritten Signature Verification, Security.	"The recent years have been characterized by the growing interest toward personal identity authentication, along with the spreading of the internet and the increased demand for security issues. Biometrics is an important field that allows personal identity authentication through the analysis of personal characteristics. Two types of biometrics means can be considered: Physiological biometrics, which involves data derived from the direct measurement of some part of the human body; Behavioural biometrics, which involves data derived from an action taken by a person. Examples of physiological biometrics include fingerprint-, face-, palmprint-, retina-based verification. Examples of behavioral biometrics are speech-, keystroke dynamics- and signaturebased verification <NO>.
Among the others, handwritten signature is one of the most widespread means for personal authentication. In all developed countries, people learn to affix the signature in the early age and practiced over a period of years. So handwritten signature is a very personal pattern that originates from a complex generation process involving the instantiation of an action plan stored in the brain of the signer and its execution by his/her body (arm, hand) using suitable writing devices (pen, pencil, paper, etc.) <NO>.
Therefore, it is not surprising that signature analysis is an extremely complex problem that involves aspects of diverse disciplines. Four comprehensive surveys that report the progress in the field of automatic signature verification respectively up to 1989, 1993, 2000 and 2008, provide a comprehensive overview of the efforts done in this research area <NO>.
The aim of this paper is to highlight some of the most relevant issues at the frontier of research in the field of automatic signature verification. Throughout the paper, some of the most promising directions of research will be pointed out and discussed.
The organization of the paper is the following. Section 2 presents some aspects related to signature acquisition and with the use of new acquisition devices. Some of the crucial questions about signature modelling and representation are discussed in Section 3. The verification strategies are illustrated in Section 4. Section 5 introduce the problem of the assessment of signature verification systems. Some crosscultural and health issues are presented in Section 6. Section 7 briefly introduces some inquiries concerning security, privacy and regulatory matters. Section 8 reports the conclusion of the paper."	https://doi.org/10.1109/ICFHR.2012.211	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Impedovo, et al. [NO] focuses on some of the most remarkable aspects the field and highlights some recent research directions. 
Combination of signature verification techniques by SVM	[]	T. Ito, W. Ohyama, T. Wakabayashi, and F. Kimura 2012. Combination of signature verification techniques by SVM. In 13th Int. Conf. on Frontiers in Handwriting Recognition (ICFHR’12)	This paper proposes a new SVM based technique for combining signature verification techniques using off-line features and on-line features. The off-line feature based technique employs gradient feature vector representing the shape of signature image, and the on-line feature based technique employs dynamic programming (DP) matching technique for time series data of the signatures. The final decision (verification) is performed by SVM based on output from those off-line and online techniques. In the evaluation test the proposed technique achieved 92.96% verification accuracy, which is 1.4% higher than the better accuracy obtained by the individual techniques. This result shows that combining multiple techniques by SVM improves signature verification accuracy significantly. Keywords-signature verification; gradient feature; HOG; SVM; DP;	"Automatic signature verification, a behavioral biometrics, has been studied by many researchers<NO>. It can be performed using the scanned signature image or using a tablet with a stylus. The former is called off-line verification and the later is called on-line verification. On-line verification with the availability of dynamic information such as stroke order, velocity, or local pressure has already had commercial applications.
Most of the techniques for the on-line signature verification are based on waveform analysis of time series data in terms of Dynamic Programming (DP) or Hidden Markov Model (HMM) <NO>. However the problems such as accuracy improvement and reduction of required size for training sample are still remaining as the research topics．In particular, the shape of signature image (offline features) is not directly employed for its verification while on-line features are focused on. The characteristic of an individual’s signature can only be established using an appropriate number of signature specimens. Since human signature could vary overtime, too few samples will increase the false rejection rate of genuine signatures while too many samples will be labor intensive for the user.
Aiming to solve these problems this paper proposes a new SVM based technique for combining signature verification techniques using off-line features and on-line features.
The off-line feature based technique employs gradient feature vector representing the shape of signature image<NO>, and the on-line feature based technique employs dynamic programming (DP) matching technique for time series data of the signatures. By combining those two techniques, it is possible to verify the signature synthetically using both offline and on-line features. The proposed technique can be applied to combine more techniques with other features to achieve higher performance.
The rest of this paper is organized as follows: Section 2 describes the signature verification techniques based on the gradient features (2.1), DP matching (2.2) and the combining technique by SVM (2.3). Section 3 details the experimental settings, results and discussions. Finally, Section 4 concludes with future topics."	https://doi.org/10.1109/ICFHR.2012.192	1	['technique', 'svm', 'accuracy']	['based_technique', 'japanese_signature', 'mahalanobis_distance', 'svm_based', 'line_features']	['based_technique_employs', 'decision_verification_performed', 'evaluation_test_proposed', 'feature_based_technique', 'final_decision_verification']	Ito, et al. [NO] proposes a new svm based technique for combining signature verification techniques using off-line features and on-line features. 
Similarity computation based on feature extraction for off-line Chinese signature verification	[]	J. Ji, Z.D. Lu, and X. Chen 2009. Similarity computation based on feature extraction for off-line Chinese signature verification. In 6th Int. Conf. on Fuzzy Systems and Knowledge Discovery,	In this paper we present a new method for off-line Chinese signature verification. The approach is based on feature extraction of every segment segmented from the signature image. After preprocessed and segmented, the signature image is segmented to some segments. Every segment is represented by a set of seven features. By using these segments and its features, the degree of similarity between the questioned sample and the n genuine signature samples is computed. Only a little number (3 to 5) of genuine samples for each writer is needed by using a simplified version of question document expert’s classifier. Experimental results show our approach is promising to distinguish random and simple forgeries from genuine signatures effectively.	"As one of the personal verification in our daily life, signature verification has many metrics, such that signatures are easy to be acquired; the verification is non-intrusive and is widely accepted by people. Therefore signature verification plays an important role in the personal verification field and attracts more and more researchers to work in this field <NO>.
Two-classes of signature verification system are usually distinguished: on-line system for which signature data are captured during the writing process, which makes available the static features and the dynamic features like velocity, acceleration and pressure etc, and off-line system for which signature data takes place on a static image captured once the writing process is over, which only makes available the static features <NO>.
Many approaches are used for solving the involved automatic off-line signature verification problems <NO>. In general, the proposed techniques use either a type of features (global, local, statistical, geometric, pseudo dynamic etc), or a combination of different types of features <NO>, which extracted from the signature images.
A simple and efficient approach is introduced that can be applied to off-line Chinese signature verification. The approach is based on feature extraction of segment of the signature skeleton and the analysis of the matching between two compared signatures’ skeletons. Each extracted segment is represented by three sets of features. In the first set of features, there are two features: its horizontal and vertical center of the segment. Secondly, the number of points that have four kinds of directions neighbors is counted. Lastly, the relative grays of the segment. This information is efficiently compared with the corresponding patterns stored in the signature reference database using the method introduced in ref. <NO>. The degree of similarity between the test sample and the n genuine samples stored in the signature reference database is computed."	https://doi.org/10.1109/FSKD.2009.597	2	['sample']	['signature_identification', 'chain_code', 'code_direction', 'modified_chain', 'line_signatures']	['chain_code_direction', 'modified_chain_code', 'segmented_signature_image', 'signature_identification_proposed', 'line_signature_verification']	Ji, et al. [NO] presents a new method for off-line chinese signature verification. 
Glove-based approach to online signature verification	[]	N.S. Kamel, S. Sayeed, and G.A. Ellis 2008. Glove-based approach to online signature verification. IEEE Transactions on Pattern Analysis and Machine Intelligence 30,	Utilizing the multiple degrees of freedom offered by the data glove for each finger and the hand, a novel online signature verification system using the Singular Value Decomposition (SVD) numerical tool for signature classification and verification is presented. The proposed technique is based on the Singular Value Decomposition in finding r singular vectors sensing the maximal energy of glove data matrixA, called principal subspace, so the effective dimensionality ofA can be reduced. Having modeled the data glove signature through its r-principal subspace, signature authentication is performed by finding the angles between the different subspaces. A demonstration of the data glove is presented as an effective highbandwidth data entry device for signature verification. This SVD-based signature verification technique is tested and its performance is shown to be able to produce Equal Error Rate (EER) of less than 2.37 percent.	"SIGNATURE verification can be split into two categories: Static or offline: In this mode, users write their signature on paper, digitize it through an optical scanner or comer, and a biometric system recognizes the signature by analyzing its shape. Dynamic or online: In this mode, users write their signature on a digitizing tablet, smart pen, or pen tablet. Considering the highest security levels required by the online systems (dynamic), most of the efforts of the researchers in this field address this group.
The design of a dynamic signature verification system initially involves the following four aspects:
1. data acquisition and preprocessing (input device), 2. feature extraction, 3. matching (classification), and 4. decision making.
Fig. 1 shows the general online signature verification process. Although successful in resisting attempts by imposters, the Dynamic Signature Verification (DSV) system still faces serious challenges for various reasons. The large variation in the execution speed of various phases of a signature is one such reason. Another reason is the quality and positions of the physical properties describing the signature themselves. Other factors affecting the difficulty of DSV are the emotional state of the signing person and the accuracy of the input device used. For successful implementation,
This paper is organized as follows: Section 2 describes the details of the proposed SVD-based signature verification technique. Section 3 shows experimental results including the selection of the system parameters and a comparison with other online techniques. Section 4 concludes the paper.
For clarity, an attempt has been made to adhere to a standard notational convention. Lower case boldface characters will generally refer to vectors. Upper case BOLDFACE characters will generally refer to matrices. Vector or matrix transposition will be denoted using ð:ÞT. Rn denotes the real vector space ofndimensions."	https://doi.org/10.1109/TPAMI.2008.32	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Utilizing the multiple degrees of freedom offered by the data glove for each finger and the hand, a novel online signature verification system using the Singular Value Decomposition (SVD) numerical tool for signature classification and verification is presented [NO].
Improvement of Japanese signature verification by segmentation-verification	['The proposed system consists of procedures of signature verification of each image (full name, first name, last name) by off-line verification technique <NO> and full name signature verification by on-line verification technique.']	Y. Kamihira, W. Ohyama, T. Wakabayashi, and F. Kimura 2013. Improvement of Japanese signature verification by segmentation-verification. In 12th Int. Conf. on Document Analysis and Recognition	This paper proposes a new combined signature verification technique called s egmentation-verification based on three different off-line feature vectors extracted from full name Japanese signature image and from the sub-images of the first name and the last name. The Mahalanobis distance for each feature vector is calculated and the final decision (verification) is performed by SVM based on the three Mahalanobis distances. In the evaluation test the proposed technique achieved 94.30% verification accuracy, which is 1.03% higher than the best accuracy obtained for the full name signature image. This result shows that the proposed segmentation-verification approach improves Japanese signature verification accuracy significantly. Keywords—signature verification; gradient feature; SVM; segmentation;	"Automatic signature verification, a behavioral biometrics, has been studied by many researchers<NO>. It can be performed using the scanned signature image or using a tablet with a stylus. The former is called off-line verification and the later is called on-line verification. On-line verification with the availability of dynamic information such as stroke order, velocity, or local pressure has already had commercial applications. Most of the techniques for the on-line signature verification are based on waveform analysis of time series data in terms of Dynamic Programming (DP) or Hidden Markov Model (HMM) <NO>. However the problems such as accuracy improvement and reduction of required size for training sample are still remaining as the research topics. In particular, offline verification using the shape of signature image (off-line features) has still room for improvement. The characteristic of an individuals signature can only be established using an appropriate number of signature specimens. Since human signature could vary overtime, too few samples will increase the false rejection rate of genuine signatures while too many samples will be labor intensive for the user.
Aiming to solve these problems this paper proposes a new SVM based technique for combining verification results obtained by signature verification techniques using off-line features.
The off-line feature based technique employs gradient feature vector representing the shape of signature image<NO>. By combining multiple verification results, it is possible to verify the signature with more features than ever. The proposed technique can be applied to combine multiple results to achieve higher performance.
The rest of this paper is organized as follows: Section 2 describes signature segmentation method (2.1), and the signature verification techniques based on the gradient features (2.2), and the combining technique by SVM (2.3). Section 3 details the experimental settings, results and discussions. Finally, Section 4 concludes with future topics."	https://doi.org/10.1109/ICDAR.2013.83	1	['technique', 'svm', 'accuracy']	['based_technique', 'japanese_signature', 'mahalanobis_distance', 'svm_based', 'line_features']	['based_technique_employs', 'decision_verification_performed', 'evaluation_test_proposed', 'feature_based_technique', 'final_decision_verification']	Kamihira, et al. [NO] proposes a new combined signature verification technique called s egmentation-verification based on three different off-line feature vectors extracted from full name japanese signature image and from the sub-images of the first name and the last name. 
Improvement of Japanese signature verification by combined segmentation verification approach	[]	Y. Kamihira, W. Ohyama, T. Wakabayashi, and F. Kimura 2013. Improvement of Japanese signature verification by combined segmentation verification approach. In 2nd IAPR Asian Conf. on Pattern Recognition	This paper proposes a new signature verification technique called combined segmentation-verification based on off-line features and on-line features. We use three different off-line feature vectors extracted from full name Japanese signature image and from the sub-images of the first name and the last name. The Mahalanobis distance for each offline feature vector is calculated for signature verification. The on-line feature based technique employs dynamic programming (DP) matching technique for time series data of the signatures. The final decision (verification) is performed by SVM based on the three Mahalanobis distances and the dissimilarity of the DP matching. In the evaluation test the proposed technique achieved 97.22% verification accuracy with even FRR and FAR, which is 3.95% higher than the best accuracy obtained by the individual technique. This result shows that the proposed combined segmentation verification approach improves Japanese signature verification accuracy significantly. Keywords—signature verification; gradient feature; HOG; SVM; segmentation; DP;	"Automatic signature verification, a behavioral biometrics, has been studied by many researchers<NO>. It can be performed using the scanned signature image or using a tablet with a stylus. The former is called off-line verification and the later is called on-line verification.
On-line verification with the availability of dynamic information such as stroke order, velocity, or local pressure has already had commercial applications. Most of the techniques for the on-line signature verification are based on waveform analysis of time series data in terms of Dynamic Programming (DP) or Hidden Markov Model (HMM) <NO>. However the problems such as accuracy improvement and reduction of required size for training sample are still remaining as the research topicsIn particular, the shape of signature image (off-line features) is not directly employed for its verification while on-line features are focused on. The characteristic of an individuals signature can only be established using an appropriate number of signature specimens. Since human signature could vary overtime, too few samples will increase the false rejection rate of genuine signatures while too many samples will be labor intensive for the user. Therefore, Aiming to solve these problems this paper proposes a new signature verification technique called combined segmentation-verification based on off-line features and on-line features. We use three different off-line feature vectors extracted from full name Japanese signature image and from the sub-images of the first name and the last name. The Mahalanobis distance for each off-line feature vector is calculated for signature verification. The online feature based technique employs dynamic programming (DP) matching technique for time series data of the signatures. The final decision (verification) is performed by SVM based on the three Mahalanobis distances and the dissimilarity of the DP matching.
The rest of this paper is organized as follows: Section II describes signature verification techniques using off-line feature (II.A), and the signature verification techniques using on-line feature (II.B), and the combining technique by SVM (II.C). Section III details the experimental settings, results and discussions. Finally, Section IV concludes with future topics."	https://doi.org/10.1109/ACPR.2013.46	1	['technique', 'svm', 'accuracy']	['based_technique', 'japanese_signature', 'mahalanobis_distance', 'svm_based', 'line_features']	['based_technique_employs', 'decision_verification_performed', 'evaluation_test_proposed', 'feature_based_technique', 'final_decision_verification']	Kamihira, et al. [NO] proposes a new signature verification technique called combined segmentation-verification based on off-line features and on-line features. 
Improvement of on-line signature verification based on gradient features	[]	Y. Kawazoe, W. Ohyama, T. Wakabayashi, and F. Kimura 2010. Improvement of on-line signature verification based on gradient features. In Int. Conf. on Frontiers in Handwriting Recognition (ICFHR’10)	This paper proposes a new on-line signature verification technique which employs gradient features and a pooled within-covariance matrix of training samples not only of the user but also of the others. Gradient features are extracted from a signature image reflecting the velocity of pen movement as the grayscale so that both on-line and off-line features are exploited. All training samples of different signatures collected in design stage are pooled together with the user’s samples and used for learning within-individual variation to reduce required sample size of the user to minimum number. The result of evaluation test shows that the proposed technique improves the verification accuracy by 4.9% when user’s sample of size three is pooled with samples with others. This result shows that the samples of different signatures are useful for training within-individual variation of a specific user. Keywords-signature verification; gradient feature; pooledwithin covariance matrix;	"Automatic signature verification, a behavioral biometrics, has been studied by many researchers<NO>, <NO>, <NO>, <NO>. It can be performed using a tablet with a stylus or using the scanned signature image. The former is called on-line verification and the later is called off-line verification. Online verification with the availability of dynamic information such as stroke order, velocity, or local pressure has already had commercial applications.
Most of the techniques for the on-line signature verification are based on waveform analysis of time series data in terms of Dynamic Programming (DP) or Hidden Markov Model (HMM)<NO>, <NO>, <NO>. However the research topics on accuracy improvement and reduction of required size for training sample are still remaining to solve the problems such as:
• The shape of signature image (off-line features) is not directly analyzed for its verification while on-line features are focused on, • Signatures can be easily reproduced once a temporal waveform stored as reference in a system is stolen, • The characteristic of an individual’s signature can only be established using an appropriate number of signature specimens. Since human signature could vary overtime, too few samples will increase the false rejection rate of genuine signatures while too many samples will be labor intensive for the user.
Aiming to solve these problems this paper proposes a new on-line signature verification technique which employs gradient features <NO>, <NO> and a pooled within-covariance matrix of training samples not only of the user but also of the others.
Gradient features are extracted from a signature image reflecting the velocity of pen movement as the grayscale so that both on-line and off-line features are exploited. All training samples of different signatures collected in design stage are pooled together with the user’s samples and used for learning within-individual variation to reduce required sample size of the user to minimum number. The gradient features and their statistical parameters have advantage in that the signature can not be reproduced from the features nor parameters. The proposed technique can be combined with other on-line verification techniques based on DP or HMM to achieve higher performance. It can be applied directly to off-line signature verification, too.
The remaining of this paper is organized as follows: The next section presents the signature verification technique consisting of image generation II-A, gradient feature extraction II-B, verification II-C and training II-D. Section III details the experimental settings, results and discussions. Finally, Section IV concludes with future topics."	https://doi.org/10.1109/ICFHR.2010.70	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Kawazoe, et al. [NO] proposes a new on-line signature verification technique which employs gradient features and a pooled within-covariance matrix of training samples not only of the user but also of the others. 
Online signature verification based on biometric features	[]	N. Li, J. Liu, Q. Li, X. Luo, and J. Duan 2016. Online signature verification based on biometric features. In 49th Hawaii Int. Conf. on System Sciences	Since current signatures are generally not verified carefully, frauds by forging others signature always happen. This paper tried to authenticate user automatically with electronic signatures on mobile device. We collected coordinates, pressure, contact area and other biometric data when users sign their name on touch screen smart phone. Then we used four different classification algorithms, Support Vector Machine, Logistic Regression, AdaBoost and Random Forest to build a specific signature verification model for each user, and compared the verification accuracy of these algorithms. The experimental result on 42 persons’ dataset shows that these four algorithms have satisfactory performance on Chinese signature verification, and Adaboost has the best performance with error rate of 2.375%. Keywords-Electronic Signature; Signature Verification; Adaboost; Random Forest;	"With the continuous advance of the paperless office, electronic signatures are gradually replacing handwritten signatures in various fields. The most beneficial reason for electronic signatures taking place of handwritten signatures is resource saving. To get electronic signatures, users are required to sign his/her name on the touch screen of a digital panel, a tablet or a smart phone rather than on paper. For those companies that have a large requirement for receipts or contracts, electronic signatures can help them reduce considerable expenditure. Nowadays, many communications, retailing, hotels and many other serviceoriented industries are beginning to use electronic signature to authenticate user or produce non-repudiation evidence. Especially in financial industries such as banks and insurance which are in greater demand for signatures, electronic signatures are widely adopted in China. So if the reliability of electronic signatures can be guaranteed, more companies and industries are willing to use electronic signatures instead of handwritten ones.
In terms of security, electronic signatures have better reliability than handwritten signatures. Generally handwritten signatures are highly vulnerable to imitate. People have to authenticate handwritten signatures by hand, and it is neither reliable nor efficient because of judges background knowledge and limited experience. Even if we can turn
Corresponding author: Jiafen Liu.
the handwritten signature to an image and authenticate it automatically, it is unreliable. Because offline-handwritten signatures contain no information of writing process, we can only authenticate it by computing the similarity of shape and structure. As for electronic signatures, we could collect more information with touchscreen besides shape and structure of characters, such as writing speed, pressure, size of contact area and other biometric features. These biometric features can be used to reproduce the writing process and often reflect users writing habits. Since each person has unique writing habit and it is hard to imitate, signature verification based on biometric features is feasible.
This paper explored the feasibility of automatic signature identification on a smart device to tell whether the signer is the real user. The rest of this paper is organized as follows. We first briefly describe the related work in Section 2. The design details for our signature verification experiment are presented in Section 3. We then compared performance of 4 different classification methods in signatures verification using our experimental data in Section 4. This paper is concluded with speculation on how the current prototype can be further improved in Section 5."	https://doi.org/10.1109/HICSS.2016.683	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Li, et al. [NO] tries to authenticate user automatically with electronic signatures on mobile device. 
Signature verification competition for online and offline skilled forgeries (SigComp2011)	[]	M. Liwicki, M. Imran Malik, C.E. van den Heuvel, X. Chen, C. Berger, R. Stoel, M. Blumenstein, and B. Found 2011. Signature verification competition for online and offline skilled forgeries (SigComp2011). In 11th Int. Conf. on Document Analysis and Recognition	The Netherlands Forensic Institute and the Institute for Forensic Science in Shanghai are in search of a signature verification system that can be implemented in forensic casework and research to objectify results. We want to bridge the gap between recent technological developments and forensic casework. In collaboration with the German Research Center for Artificial Intelligence we have organized a signature verification competition on datasets with two scripts (Dutch and Chinese) in which we asked to compare questioned signatures against a set of reference signatures. We have received 12 systems from 5 institutes and performed experiments on online and offline Dutch and Chinese signatures. For evaluation, we applied methods used by Forensic Handwriting Examiners (FHEs) to assess the value of the evidence, i.e., we took the likelihood ratios more into account than in previous competitions. The data set was quite challenging and the results are very interesting.	"The topic of writer identification and verification has been addressed in the literature for several decades <NO>, <NO>. Usually, the task is to identify the writer of a handwritten text or signature or to verify his or her identity. Work in writer verification can be differentiated according to the available data. If only a scanned or a camera captured image of the handwriting is available then writer classification is performed with offline data. Otherwise, if temporal and spatial information about the writing is available, writer classification is performed with online data. Usually, the former task is considered to be less difficult than the offline classification <NO>. Surveys covering work in automatic writer identification and signature verification until 1993 are given in <NO>. Subsequent works up to 2000 are summarized in <NO>. Most approaches are tested on specially collected data sets which were acquired in controlled environments. In the past, several competitions were organized to measure the detection rate of several classifiers:
• First international Signature Verification Competition
(SVC 2004), online data, 5 reference signatures
• BioSecure Signature Evaluation Campaign 2009, online
data, 5 reference signatures
• SigComp 2009 <NO>, online and offline data, 1 reference signature
Most of the current research in the field of signature verification does not take the real needs of Forensic Handwriting Experts (FHEs) into account. In their casework they often work with signatures produced in various real world environments. These signatures are more difficult to analyze compared to the signatures produced in controlled environments. FHEs also have to deal with possibly disguised signatures, where the author tries to disguise his or her handwriting in order to make it seem to be a forgery. The 4NSigComp2010 <NO> was the first signature verification competition focusing explicitly the classification of disguised, simulated and genuine signatures.
We have now organized the Signature Verification Competition for Online and Offline Skilled Forgeries (SigComp2011). The major emphasis of this competition is not the possibility of disguised signatures but to motivate the signature verification community to enable their systems to compute the likelihood ratios instead of just computing the evidence (for more details see <NO>). This is very important as it allows one to combine the FHE’s evidence (from the results of an automated system) with other evidence presented in a court of law. In this competition we ask to produce a comparison score (e.g. a degree of similarity or difference), and the evidential value of that score, expressed as the ratio of the probabilities of finding that score when the questioned signature is a genuine signature and when it is a forgery (i.e. the likelihood ratio). Note that this competition has introduced a paradigm shift from the “decision paradigm” to an evidential value that impacts the task in the competition.
 The issue is not the pure classification, since 
• The FHE cannot and was never asked to decide on authorship,
• the FHE cannot know the probability of authorship based on handwriting comparison alone, and
• classification brings with it the probability of an error of which the cost is undefined.
The true issue is to find the likelihood ratio (LR) for a comparison: the probability of finding a particular score given that Hypothesis H1 is true, divided by the probability of finding the score when the alternative Hypothesis H2 is true. H1 corresponds to intra-source scores (same author) and H2 to inter-source scores (different authors).
The relevant graphs therefore show histograms of some measure of similarity (or difference; or any continuous measure that used to be compared to some threshold in a classification task) for intra-source and inter-source comparisons. Such graphs make it possible to assess the value of the evidence given both hypotheses, which is of major importance to forensic experts and the courts. Therefore, in this competition we have had a closer look at the likelihood ratios."	https://doi.org/10.1109/ICDAR.2011.294	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Liwicki, et al. [NO] wants to bridges the gap between recent technological developments and forensic casework. 
Forensic signature verification competition 4NSigComp2010 - detection of simulated and disguised signatures	['The 4NSigComp2010 <NO> was the first signature verification competition focusing explicitly the classification of disguised, simulated and genuine signatures.', 'Although in forensic examination, disguised signatures are of high importance <NO>, yet they are often neglected by PR researchers <NO>.', 'Disguised signatures are usually difficult to identify <NO>, as they are written by the genuine author but with intention to deny its authorship.', 'Existing systems for automatic signature verification are already performing very well if the goal is to identify genuine and forged signature <NO>.', 'This is a realistic forensic scenario <NO> and is posed in the 4NSigComp2010.', '• First International Signature Verification Competition (SVC 2004), online data, 100 signature sets: 20 (genuine, forged)/author <NO> • BioSecure Signature Evaluation Campaign 2009, online data from DS2-382, DS3-382 <NO> • SigComp 2009 <NO>, online and offline data, 1 reference signature (several authors) • 4NSigComp 2010 <NO>, offline data, 25 reference signatures (1 author) • SigComp 2011 <NO>, online and offline data Chinese and Dutch signatures, 12 reference signatures (several authors)', 'In order to investigate these signatures and evaluate the performance of some of the stateof-the-art systems on data containing disguised signatures, we previously organized the 4NsigComp2010 <NO>.', 'Moreover, significant advancements in system benckmarking and performance evaluation has been achieved through international competitions for on-line and off-line signature verification systems, such as: SVC 2004 <NO>, BioSecure Signature Evaluation Campaign 2009 <NO>, SigComp 2009 <NO>, 4NSigComp2010 <NO>, SigComp2011 <NO>.', '• 4NSigComp 2010 <NO>, offline data, 1 reference author • SigComp 2011 <NO>, online and offline data, dutch and chinese; several authors.', 'The 4NSigComp2010 <NO> was the first signature verification competition focusing explicitly the classification of disguised, simulated and genuine signatures.']	M. Liwicki, C.E. van den Heuvel, B. Found, and M.I. Malik 2010. Forensic signature verification competition 4NSigComp2010 - detection of simulated and disguised signatures. In 12th Int. Conf. on Frontiers in Handwriting Recognition	This competition scenario aims at a performance comparison of several automated systems for the task of signature verification. The systems have to rate the probability of authorship and non-authorship of signatures. In particular they have to determine whether questioned signatures are simulated disguised or the normal signature of the reference writer. Furthermore, the results will be compared to forensic handwriting examiners (FHEs) opinions on the same tasks. As such, to the best of the authors’ knowledge, this scenario will be the first attempt in literature to relate system performances to the performance of FHEs who gave their opinion on exactly the the same signatures.	"The topic of writer identification and verification has been addressed in the literature for several decades <NO>, <NO>. Usually the task is to identify the writer of a handwritten text or signature or to verify his or her identity. Work in writer verification can be differentiated according to the available data. If only a scanned image of the handwriting is available then writer classification is performed with offline data. Otherwise, if temporal and spatial information about the writing is available, writer classification is performed with online data. Usually, the former task is considered to be less difficult than offline classification <NO>.
Surveys covering work in automatic writer identification and signature verification until 1993 are given in <NO>. Subsequent works up to 2000 are summarized in <NO>. Most approaches are tested on specially collected data sets which were acquired in controlled environments. In the past, several competitions were organized to measure the detection rate of several classifiers:
• First international Signature Verification Competition
(SVC 2004), online data, 5 reference signatures
• BioSecure Signature Evaluation Campaign 2009, online data, 5 reference signatures
• SigComp 2009 <NO>, online and offline data, 1 reference signature
Unfortunately, current research in the field of signature verification does not take the real needs of Forensic Handwriting Experts (FHEs) into account. In their real casework they often work with offline signatures produced in different environments. The most crucial fact is that they also have to deal with disguised signatures, where the author tries to disguise his or her handwriting in order to make it seem to be a simulated signature. To the best of the authors’ knowledge there has been no reported signature verification competition where disguised signatures were also present in the testing data.
The task considered in this paper aims at a comparison between FHEs opinions on authorship of signatures and the systems performances to determine whether questioned signatures are simulated disguised or the normal signature of the reference writer."	https://doi.org/10.1109/ICFHR.2010.116	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	This competition scenario aims at a performance comparison of several automated systems for the task of signature verification [NO].
Cancelable templates for sequence-based biometrics with application to on-line signature recognition	['For instance, Maiorana et al <NO> have proposed a convolution scheme to protect the original signature sequence of a user, that can be directly applied to any function based approach.', 'has been applied with HMM and DTW based verification systems <NO>–<NO>.', 'Other papers <NO>, <NO>, <NO>, <NO>, and <NO> choose the first k samples, according to the original order in which the data was acquired, as the training set.', 'In <NO>, noninvertible transforms designed for biometrics which can be expressed in terms of a set of sequences have also been described, and a detailed security and renewability analysis is provided.', 'A feature transformation approach named Bioconvolving, which relies on the use of convolutions to modify a set of signature time sequences, has been proposed in <NO>, and its renewability has been discussed in <NO>.']	E. Maiorana, P. Campisi, J. Fierrez, J. Ortega-Garcia, and A. Neri 2010. Cancelable templates for sequence-based biometrics with application to on-line signature recognition. IEEE Transactions on Systems, Man, and Cybernetics Part A: Systems and Humans 40,	Recent years have seen the rapid spread of biometric technologies for automatic people recognition. However, security and privacy issues still represent the main obstacles for the deployment of biometric-based authentication systems. In this paper, we propose an approach, which we refer to as BioConvolving, that is able to guarantee security and renewability to biometric templates. Specifically, we introduce a set of noninvertible transformations, which can be applied to any biometrics whose template can be represented by a set of sequences, in order to generate multiple transformed versions of the template. Once the transformation is performed, retrieving the original data from the transformed template is computationally as hard as random guessing. As a proof of concept, the proposed approach is applied to an on-line signature recognition system, where a hidden Markov model-based matching strategy is employed. The performance of a protected on-line signature recognition system employing the proposed BioConvolving approach is evaluated, both in terms of authentication rates and renewability capacity, using the MCYT signature database. The reported extensive set of experiments shows that protected and renewable biometric templates can be properly generated and used for recognition, at the expense of a slight degradation in authentication performance.	"BIOMETRIC person recognition refers to the use of phys-iological or behavioral characteristics of people in an automated way to identify them or verify who they claim to be <NO>. Biometric recognition systems are typically able to provide improved comfort and security to their users, when compared to traditional authentication methods, typically based on something that you have (e.g., a token) or something that you know (e.g., a password).
Unfortunately, biometrics-based people authentication poses new challenges related to personal data protection, not existing in traditional authentication methods. In fact, if biometric data are stolen by an attacker, this can lead to identity theft. Moreover, users’ biometrics cannot be changed, and they may reveal sensitive information about personality and health, which can be processed and distributed without the users’ authorization <NO>. An unauthorized tracking of the enrolled subjects can also be done when a cross-matching among different biometric databases is performed, since personal biometric traits are permanently associated with the users. This would lead to users’ privacy loss.
Because of these security and privacy issues, there are currently many research efforts toward protecting biometric systems against possible attacks which can be perpetrated at their vulnerable points (see <NO>). In essence, the adopted security measures should be able to enhance biometric systems’ resilience against attacks while allowing the matching to be performed efficiently, thus guaranteeing acceptable recognition performance.
In this paper, we introduce a novel noninvertible transformbased approach, namely, BioConvolving, which provides both protection and renewability for any biometric template which can be expressed in terms of a set of discrete sequences related to the temporal, spatial, or spectral behavior of the considered biometrics. The proposed approach can be therefore applied to a variety of biometric modalities, for example, speech biometrics <NO>, where spectral or temporal analysis of the voice signal produces discrete sequences, or to signature <NO> and handwriting <NO> recognition, where the extracted sequences are related to the pen’s position, applied pressure, and inclination. Moreover, when performing gait recognition <NO>, temporal sequences describing the trajectories of the ankle, knee, and hip of walking people can be considered as templates. A set of discrete finite sequences representing the potentials of brain electrical activity, generated as a response to visual stimuli, can also be employed as a template, when performing brain-activity-based identification <NO>. This is also the case when performing iris recognition, since the normalized template can be decomposed into 1-D intensity signals, which retain the local variations of the iris <NO>.
It is worth pointing out that some methods for the protection of templates extracted from the aforementioned biometrics act on sets of parametric features derived from the originally acquired data, thus limiting the kind of matching which can be performed <NO>. Since our BioConvolving approach deals with discrete sequences instead of parametric features, it allows using sophisticated matching schemes such as dynamic time warping (DTW) or hidden Markov models (HMMs). As a proof of concept, we apply the proposed BioConvolving protection scheme to signature biometrics, extending the authors’ works presented in <NO> and <NO>. Specifically, the signature representation here employed comprises a higher number of signature sequences, and a detailed renewability and security analysis is carried out.
Specifically, this paper is organized as follows. In Section II, the different solutions which have been investigated in the recent past to secure biometric templates are analyzed. The proposed approach for the protection of sequence-based biometric templates is illustrated in Section III, and its security analysis is outlined in Section IV. The state of the art on signature-based authentication schemes and on signature template protection approaches is outlined in Section V. The application of the proposed scheme to on-line signature biometrics is presented in Section VI, which details both the employed signature representation and the employed template matcher. The experimental setup is described in Section VII. The authentication and the renewability performances of the proposed protection approach are discussed in Sections VIII and IX, respectively. Eventually, some conclusions are drawn in Section X."	https://doi.org/10.1109/TSMCA.2010.2041653	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Maiorana, et al. [NO] proposes an approach, which Maiorana, et al. [NO] refer to as bioconvolving, that is able to guarantee security and renewability to biometric templates. 
A signature verification framework for digital pen applications	[]	M.I. Malik, S. Ahmed, A. Dengel, and M. Liwicki 2012. A signature verification framework for digital pen applications. In 10th IAPR International Workshop on Document Analysis Systems (DAS’12)	In this paper we present a framework for realtime online signature verification scenarios. The proposed framework is based on state-of-the-art feature extraction and Gaussian Mixture Model (GMM) classification. While our signature verification library is generally applicable to any input device using digital pens, we have implemented verification scenarios using the Anoto digital pen. As such our automated signature verification framework becomes an interesting commodity for industry, because the Anoto SDK is easy to apply and the GMM-based classification can be seamlessly integrated. The novelty of this work is the application of our framework that takes real-time online signature verification to every scenario where digital pens may potentially be used. In this paper we describe several scenarios where our framework has been applied, including signatures in financial contracts or ordering processes. We also propose a general approach to integrate the GMM-descriptions into electronic ID-cards in order to also store behavioral biometrics on these cards. In experiments we have measured the performance of the signature verification system when skilled forgeries were present. The interest shown by our partner financial institutions and the results of our initial evaluations indicate that our signature verification framework suits exactly the demands of our clients. Keywords-Online signatures, Verification, Biometric authentication, Mixture models, Anoto, Digital pens, Forged signatures, Skilled forgeries, Handwriting analysis	"Automated signature verification is in focus of research since decades. In many recent works automatic signature verification is described as a two-class pattern classification problem <NO>. Here an automated system has to decide whether or not a given signature belongs to a referenced authentic author. If the system could not find enough evidence of a forgery from the questioned signature feature vector, it considers the signature as genuine belonging to the referenced authentic author; otherwise it declares the signature as forged.
Handwritten signatures are biometric attributes <NO>. They suffer severely from intra-writer/within writer variations. Unlike some of the other biometric attributes, signatures are influenced by many factors from aging to psychological conditions of individuals. Despite of their difficult nature, signatures are considered as an important modality for person identification <NO>.
In general, automated signature verification is divided into two broad categories, online and offline, depending on the mode of the handwritten input. If both the spatial as well as temporal information regarding signatures are available to the systems, verification is performed on online data. In the case where temporal information is not available and the system can only utilize the spatial information gleaned through scanned or even camera captured documents, verification is performed on offline data <NO>, <NO>.
The main motivation of this paper is to take signature verification to the most commonly occurring real world scenarios, particularly in industry, where signature verification is required. Two of the most important areas where signature verification is highly demanded are, forensic signature analysis and signature verification in financial institutions. Until now online signature verification is not a common type of criminal casework for a forensic expert because the questioned signatures and the collected reference signatures (known) are commonly supplied offline <NO>. However, in most of the today’s modern financial institutions there is an increasing demand to perform signature verification in realtime right after signing contracts, etc. We focus explicitly on the online signature verification keeping in view its various applications in these institutions.
In this paper we apply our signature verification framework in different real world scenarios in connection with the Anoto digital pen. Note, however, that our framework can also be integrated with other hardware like digitizing tablets.
Figure 1 illustrates the hardware layout of the Anoto digital pen. This pen specializes in providing the look and feel of regular pens. It only demands to add Anoto dot pattern to any paper and data can be digitized seamlessly. The Anoto pattern makes it possible for the Anoto pen’s built-in camera to detect strokes and record signatures that then can be stored in an internal memory or sent via communication unit using Bluetooth/USB. Due to this ease of use, Anoto pens are finding applications in fields from health care to finance. Our signature verification framework is an attempt to take signature verification to every area where the Anoto pen finds an application. In particular our system has already been applied in test scenarios for finance institutions and product manufacturing companies."	https://doi.org/10.1109/DAS.2012.10	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Malik, et al. [NO] presents a framework for realtime online signature verification scenarios. 
Freak for real time forensic signature verification	[]	M.I. Malik, S. Ahmed, M. Liwicki, and A. Dengel 2013. Freak for real time forensic signature verification. In 12th Int. Conf. on Document Analysis and Recognition (ICDAR’13)	This paper presents a novel signature verification system based on local features of signatures. The proposed system uses Fast Retina Keypoints (FREAK) which represent local features and are inspired by the human visual system, particularly the retina. To locate local points of interest in signatures, two local keypoint detectors, i.e., Features from Accelerated Segment Test (FAST) and Speeded-up Robust Features (SURF), have been used and their performance comparison in terms of Equal Error Rate (EER) and time is presented. The proposed system has been evaluated on publicly available dataset of forensic signature verification competition, 4NSigComp2010, which contains genuine, forged, and disguised signatures. The proposed system achieved an EER of 30%, which is considerably very low when compared against all the participants of the said competition. In addition to EER, the proposed system requires only 0.6 seconds on average to verify a 3000*1500 scanned signature. This shows that the proposed system has a potential and suitability for forensic signature verification as well as real time applications.	"Automatic signature verification is needed in various different areas of our daily life. These include banks, governmental, security, and financial institutions, etc. Since the last few decades, researchers from Pattern Recognition (PR) community are developing and continuously improving different automatic signature verification systems for both offline (where only spatial information of signatures is available) and online (where both spatial and temporal information of signatures is available) cases. In either case, (online or offline), the verification problem is usually solved by classifying signatures into two classes, i.e., either as genuine or forged. This classification is helpful in many cases, but in some areas, e.g., forensic handwriting/signature analysis, another important genre of signatures, i.e., disguised signatures, needs classification. Although in forensic examination, disguised signatures are of high importance <NO>, yet they are often neglected by PR researchers <NO>.
Disguised signatures are usually difficult to identify <NO>, as they are written by the genuine author but with intention to deny its authorship. This could be done mainly for fraud e.g., a disguised signature made on a bank check can be used to withdraw cash and later on it can be claimed that the check did not contain original signatures.. In such a case, it is very difficult for bank/financial institution officers to distinguish between genuine and disguised signatures. In addition, it is also not possible to have an expert forensic examiner available in all of the institutes, which require authentication via signatures, who can first analyze signatures and then allow the next step in the routine work flow. It is, therefore, required to enable automatic systems classify the three different genres of signatures, i.e., genuine, forged, and disguised, so that different types of such frauds can be prevented.
Existing systems for automatic signature verification are already performing very well if the goal is to identify genuine and forged signature <NO>. There are already some commercial systems available which are also being used in banking and other financial and sensitive institutions. However, to the best of our knowledge, there are only a few systems capable of disguise classification and most of them have appeared first time in the 4NSigComp2010 signature verification competition. Note that, these systems performed quite naively when disguised signatures were present in the test set and the best of these systems could reach an EER of 55%. Considering the above mentioned scenario, there is a strong need for a system which is capable of dealing with genuine, forged, and disguised signatures at the same time with reasonably low EER.
In this paper we propose a novel method for automatic signature verification, which is able to deal with genuine, forged, and disguised signatures at the same time, with comparatively low EER and very high time efficiency. The proposed method is based on part-based analysis of signature images. In particular FREAK features are used which are inspired by human visual perception. In addition to EER, the proposed method is also computationally efficient and only requires 0.6 seconds, on average, to verify an offline signature of dimensions nearly 3000∗1500. This shows that there is a strong potential for using the proposed method in various real time scenarios, such as the bank scenarios noted earlier. Furthermore, the proposed method is evaluated on the publicly available dataset of 4NSigComp2010 (the first ever signature verification competition with data containing disguised along with genuine and forged signatures). This data is collected by Forensic Handwriting Examiners (FHEs) from forensic-like situations <NO>.
The rest of the paper is organized as follows. Section II summarizes different automatic verification systems available for signatures. Section III provides details about the proposed system for forensic signature verification. Section IV details the dataset used for evaluation. Section V presents evaluation results in terms of EER, and execution speed on the said publicly available dataset. Finally, Section VI concludes the paper and provides hints for possible future improvements."	https://doi.org/10.1109/ICDAR.2013.196	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Malik, et al. [NO] presents a novel signature verification system based on local features of signatures. 
Techniques for static handwriting trajectory recovery: A survey	['Certainly, to estimate the pseudo-dynamic information needed to make the handwritten trajectories is a classic challenge <NO>.']	V. Nguyen, and M. Blumenstein 2010. Techniques for static handwriting trajectory recovery: A survey. In Proc. of the 9th IAPR Int. Workshop on Document Analysis Systems	On-line handwriting recognition systems are usually better than their off-line counterparts thanks to the accessibility of dynamic information such as stroke order, velocity, acceleration, and pressure. Whilst the exact value of velocity as well as acceleration or pressure is unlikely to be recoverable, the temporal order of the strokes or the pen trajectory is shown to be more promising for recovery. The published experimental results suggest that the recovered pen trajectory information actually improves the off-line recognition accuracy. This paper presents an overview and discussion of pen trajectory recovery methods developed to date.	"The research in automatic handwriting recognition has been intensively pursued for nearly four decades and has obtained some significant achievements <NO>. Successful applications include: postal address recognition, handwriting recognition and signature verification using tablets, historical document recognition, and form processing.
Automatic recognition systems can be categorised as being on-line or off-line based on the availability of dynamic information. On-line recognition is usually performed using temporal spatial information generated from the movement of a stylus on the surface of an electrostatic or electromagnetic tablet. Depending on the hardware, this signal stream of information may include: pen-inclination, pressure, velocity, acceleration, movement direction, number of strokes. From such information, the corresponding static image could
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. DAS ’10, June 9-11, 2010, Boston, MA, USA Copyright 2010 ACM 978-1-60558-773-8/10/06 ...$10.00
be simulated <NO> using ink deposition models in conjunction with trajectory interpolation functions.
Unlike its on-line counterpart, off-line recognition employs only the static images captured by optical devices such as a camera or scanner. Due to the absence of dynamic information, the accuracies of off-line recognition systems could not be as high as on-line recognition <NO>. As a trade-off, the on-the-fly collection of dynamic information restricts the applications of on-line recognition and gives off-line recognition certain unique advantages such as the ability to capture information remotely and conveniently.
The success of on-line systems <NO> encourages the recovery and utilisation of dynamic information, such as pressure <NO> and, especially, stroke order to improve the performance of off-line recognition systems. Research in the field of psychology also suggests that humans’ perception of dynamic information from static images assists in the recognition of characters <NO>. It is strongly believed that if the trajectories are properly recovered, the performance of automatic off-line handwriting recognition systems could significantly be improved <NO>. Handler et al. <NO> reported a recognition performance downgrade when off-line data was simulated using on-line data. Experimental results from <NO> later confirmed that the time ordering of the signal contains important information for the recognition of handwriting. In the literature, time ordering information has been used for word segmentation and recognition <NO>, character recognition <NO>, numeral recognition <NO>, writer identification <NO>. The application of recovered trajectories in signature verification has also been investigated. In, <NO>, on-line data which was previously obtained in a registration process provided valuable clues for the recovery of signature trajectory. In another attempt to extract and utilise the temporal order, Munich and Perona <NO> tracked the signature images as writers sign with the assistance of a camera. Despite the large number of potential applications, trajectory recovery remains an open problem.
There are two major processes in a trajectory recovery system: local examination and global reconstruction. Local examination provides the essential information which will be referred to in the global reconstruction phase. This often includes the detection and analysis of junctions or ambiguous zones, endpoints, double-traced lines but can also be extended to gray level consistency, striations, feathering, pressures, and accelerations <NO>. In global reconstruction,
the overall trajectory is determined using the information obtained from local examination. The outcome of this process can be a list of ranked trajectory candidates which may further be analysed using a knowledge-based module <NO>. The principle components of a trajectory recovery system is presented in Figure 1.
The remainder of this paper is organized as follows: The next section, Section 2, presents both the advantages and disadvantages of input material, the skeleton and the contour. Section 3 briefly mentions the preprocessing of input images. Local examination is then detailed in Section 4. Section 5 is devoted to global reconstruction techniques. Finally, issues concerning performance evaluation are discussed in Section 6."	https://doi.org/10.1145/1815330.1815390	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Nguyen, et al. [NO] presents an overview and discussion of pen trajectory recovery methods developed to date. 
Global features for the off-line signature verification problem	['<NO> developed an verification system based on global features Vector Machine classifier.', 'We compared the performance of two feature extraction techniques, the gradient feature and the Modified Direction Feature <NO>, using a public database GPDS-160 <NO> with similar experimental settings.']	V. Nguyen, M. Blumenstein, and G. Leedham 2009. Global features for the off-line signature verification problem. In 10th Int. Conf. on Document Analysis and Recognition	Global features based on the boundary of a signature and its projections are described for enhancing the process of automated signature verification. The first global feature is derived from the total ’energy’ a writer uses to create their signature. The second feature employs information from the vertical and horizontal projections of a signature, focusing on the proportion of the distance between key strokes in the image, and the height/width of the signature. The combination of these features with the Modified Direction Feature (MDF) and the ratio feature showed promising results for the off-line signature verification problem. When being trained using 12 genuine specimens and 400 random forgeries taken from a publicly available database, the Support Vector Machine (SVM) classifier obtained an average error rate (AER) of 17.25%. The false acceptance rate (FAR) for random forgeries was also kept as low as 0.08%.	"Handwritten signatures have long been accepted as an official means to verify personal identity for legal purposes on such documents as cheques, credit cards, contracts and wills. The handwritten signature is therefore well established and accepted as a behavioural biometric. Considering the large number of signatures verified daily through visual inspection by people, the construction of a robust and accurate automatic signature verification system has many potential benefits for ensuring authenticity of signatures and reducing fraud and other crimes. As a consequence, research into signature verification has been vigorously pursued for several decades, particularly with reference to offline verification <NO>.
Off-line verification refers to when the signature is only available as a static image, typically obtained after it has been written on paper using a variety of writing instruments, with no reference to the sequence and timing of the pen-strokes, which created the signature. When the sequence of the pen-strokes is available the process is referred to as on-line signature verification.
The off-line signature verification problem is more challenging due to the lack of valuable behavioural information about how the person created the signature in terms of penpoint velocity and accelerations, writing pressure and stroke sequence. Access to this on-line information during the training phase, has been demonstrated to result in improved performance in off-line signature verification systems <NO>.
Like any other pattern recognition scheme, one crucial aspect of a signature verification system includes appropriate feature extraction procedures. Thus, new feature extraction techniques are being extensively explored. The survey by Weiping et al. <NO> summarises some previously investigated features and approaches. In this paper we report our latest results in the pursuit of new global and local features for addressing the off-line signature verification problem."	https://doi.org/10.1109/ICDAR.2009.123	2	['sample']	['signature_identification', 'chain_code', 'code_direction', 'modified_chain', 'line_signatures']	['chain_code_direction', 'modified_chain_code', 'segmented_signature_image', 'signature_identification_proposed', 'line_signature_verification']	Global features based on the boundary of a signature and its projections are described for enhancing the process of automated signature verification [NO].
Off-line signature identification using background and foreground information	[]	S. Pal, A. Alaei, U. Pal, and M. Blumenstein 2011. Off-line signature identification using background and foreground information. In Int. Conf. on Digital Image Computing Techniques and Applications (DICTA’11)	Biometric systems play an important role in the field of information security as they are extremely required for user authentication. Automatic signature recognition and verification is one of the biometric techniques, which is currently receiving renewed interest and is only one of several techniques used to verify the identities of individuals. Signatures provide a secure means for confirmation and authorization in legal documents. So nowadays, signature identification and verification becomes an essential component in automating the rapid processing of documents containing embedded signatures. In this paper, a technique for a bi-script off-line signature identification system is proposed. In the proposed signature identification system, the signatures of English and Bengali (Bangla) are considered for the identification process. Different features such as undersampled bitmaps, modified chain-code direction features and gradient features computed from both background and foreground components are employed for this purpose. Support Vector Machines (SVMs) and Nearest Neighbour (NN) techniques are considered as classifiers for signature identification in the proposed system. A database of 1554 English signatures and 1092 Bengali signatures are used to generate the experimental results. Various results based on different features are calculated and analysed. The highest accuracies of 99.41%, 98.45% and 97.75% are obtained based on the modified chain-code direction, under-sampled bitmaps and gradient features respectively using 1800 (1100 English+700 Bengali) samples for training and 846 (454 English+392 Bengali) samples for testing. KeywordsOffline systems, Signature identification, Biometrics, Authentication systems, Modified chain-code directions, SVMs.	"Today, biometric technologies are increasingly and more commonly being used to ensure identity verification or the authorisation of access to sensitive data. For historical reasons, the handwritten signature continues to be the most commonly accepted form of transaction confirmation, as well as being used in civil law contracts, acts of volition, or authenticating one's identity. Signature verification has been a topic of intensive research during the past several years <NO> due to the important role it plays in numerous areas, including the financial system.
Signatures have been accepted as an official means to verify personal identity for legal purposes on such documents such as cheques, credit cards, wills etc. The handwritten signature is therefore well established and accepted as a behavioral biometric. Considering the large number of signatures verified daily through visual inspection by people, the construction of a robust and accurate automatic signature verification system has many potential benefits for ensuring authenticity of signatures and reducing fraud and other crimes.
The goal of a signature verification system is to verify the identity of an individual based on an analysis of his or her signature through a process that discriminates a genuine signature from a forgery. The verification of human signatures is particularly concerned with the improvement of the interface between human-beings and computers <NO>. A signature verification system and the associated techniques used to solve the inherent problems of authentication can be divided into two classes <NO>: (a) on-line methods <NO> to measure temporal and sequential data by utilizing intelligent algorithms <NO> and (b) off-line methods <NO> that use an optical scanner to obtain handwriting data written on paper. Off-line signature verification deals with the verification of signatures, which appear in a static format <NO>. On-line signature verification has been shown to achieve much higher verification rates than off-line verification <NO>, as a considerable amount of dynamic information is lost in the off-line mode. But off-line systems have a significant advantage as they do not require access to special processing devices when the signatures are produced. Moreover, the offline method has many more practical application areas than that of the on-line variety.
Signatures represent a particular writing style and very often are a combination of symbols and strokes. So it is obviously necessary to deal with a signature as a complete image with a special distribution of pixels, representing a particular writing style and is not considered as a collection of letters and words <NO>. It is often difficult for a human to instantly verify two signatures of the same person because signature samples from the same person are similar but not identical, and signatures can change depending on elements such as mood, fatigue and time. In addition, a person’s signature often changes radically during their lifetime. Great inconsistency can even be observed in sign to country, habits, psychological or mental s practical conditions <NO>.
Numerous techniques for feature classification have been put forward in the processing of signatures. Justino et al. <NO> line signature verification system based on Models (HMMs) to detect random, cas forgeries. Three features: a pixel density distribution feature and an axial slant feat from a grid segmentation scheme. Lv et al. Chinese off-line signature verification syst data base of 1100 signatures. Support Vecto employed for classification. Four different such as Moment feature, Direction feature, and Stroke width distribution feature we research. Nguyen et al. <NO> developed an verification system based on global features Vector Machine classifier. In their paper, th the Modified Direction Feature (MDF) w features: feature from Energy information, and ratio feature are reported. In addition Weiping et al. <NO> summarises some additi approaches that have been previously investi
Although, many systems involving o recognition and verification have been devel systems have solely considered single-s However, signatures may be written in dif and there is a need to undertake a systema area. In the field of signature verificatio published work has been undertaken only signatures. Only a few studies have bee Chinese, Japanese, Persian and Arabic indicated earlier, researchers have used diff signature verification and it was noted that work is based on foreground information. Moreover, to the best of our knowle published work on signatures written in I India is a multi-lingual and multi-script co people write signatures in local state lan Hindi, Bangla, Telugu, and Tamil. Thus th work on signatures written in Indian lang off-line signature identification considering or more languages. This area of signature i verification would be considered as a nove the field. In this paper we propose a signat system for two scripts: Bangla and English our knowledge, background information ha used in signature identification research, a system is the first work of its kind that uses background and foreground information results. Some Bengali and English signat shown in Table I and Table II, respectively.
The remainder of this paper is organ Section II deals the feature extraction techn Details of the classifiers used are presented in Section III. Results and discussion are given in Section IV and error analysis is detailed in Section V. Finally, conclusions and future work are discussed in Section VI."	https://doi.org/10.1109/DICTA.2011.119	2	['sample']	['signature_identification', 'chain_code', 'code_direction', 'modified_chain', 'line_signatures']	['chain_code_direction', 'modified_chain_code', 'segmented_signature_image', 'signature_identification_proposed', 'line_signature_verification']	Pal, et al. [NO], a technique for a bi-script off-line signature identification system is proposed. 
Matrix-based hierarchical graph matching in off-line handwritten signatures recognition	[]	M. Piekarczyk, and M.R. Ogiela 2013. Matrix-based hierarchical graph matching in off-line handwritten signatures recognition. In 2nd IAPR Asian Conf. on Pattern Recognition	In this paper, a graph-based off-line handwritten signature verification system is proposed. The system can automatically identify some global and local features which exist within different signatures of the same person. Based on these features it is possible to verify whether a signature is a forgery or not. The structural description in the form of hierarchical attributed random graph set is transformed into matrix-vector structures. These structures can be directly used as matching pattern when examined signature is analyzed. The proposed approach can be applied to off-line signature verification systems especially for kanji-like or ideogram-based structurally complex signatures. Keywords— handwritten signatures verification; attributed IE graph; matrix-based matching; hierarchical random graph; ambiguous signature patterns;	"The common approach in handwritten signatures verification systems is using of statistical pattern recognition methods <NO>. The most popular models are HMM <NO>, SVM <NO>, ANN or DTW. Though the syntactic methods in pattern recognition can be very effective <NO> these are not very popular in biometrics. Also, graph-based modeling techniques are rather rarely used by researchers in this field. Among possible reasons of the situation some difficulties related to analysis of such complicated structures as graphs, can be indicated. These types of problems appear both in syntacticoriented and template matching approaches. If the formal graph languages are used, it is necessary to cope with the high complexity membership problem (parsing) and extremely hard designing of the grammatical inference procedures (learning). On the other side, using the template matching model faced with the issue of testing the graph isomorphism in the effective way. In spite of that, some proposals of HSV systems basing on graph-based modeling has been appeared in the recent years including <NO> for on-line and <NO> for off-line signatures recognition. In both cases the graph-based modeling is used to handle the distinctive features connected with handwritten signatures.
The paper <NO> introduces a mathematical linguistic based model designed for distorted or ambiguous patterns, where a graph based approach is used for structure representation. The concept basing on the use of hierarchical random IE graphs and two-level probabilistic grammars belonging to the ETPL(k) class with the aim of gathering and analyzing the knowledge about the structure and the features of ambiguous patterns (signatures). The knowledge about variability of a specimen is created just on the basis of finite number of patterns treated as positive samples of unknown graph language. Additionally, the usage of attributed graphs enables the storage of additional semantic information describing the local properties of the signature. The graph linguistic formalisms applied – the IE graphs and attribute-controlled ETPL(k) grammar <NO> – are characterized by considerable descriptive strength and a polynomial membership problem of the syntactic analysis. However, the effective mechanism of machine learning through grammatical inference process has been defined <NO>. This model is very comprehensive but its constructional complexity may be treated as significant disadvantage.
In the approach designed to on-line handwritten signatures analysis <NO> different type of graph-based modeling has been presented. On-line signatures are represented by series of graphs, whose nodes and edges describe certain properties of sample points and relationship between points respectively. The collection of graphs is stored in the matrix-oriented form not as a formal graph grammar like in <NO>. Then, graph matching techniques are introduced to compute edit distance between graphs. This way it is possible to measure the similarity of the graphs. Calculating the edit distance between the graphs on the basis of adjacency matrices allows to avoid a difficulties related to syntactic approach (like complexity of parsing) and give the opportunity to obtain considerable simplification of the classifier construction in relation to mathematical linguistics approach.
Due to the known instability of the signing process and consequently signature features in both systems the inexact graph matching algorithms have been used. In <NO> it is done by utilization of the random graphs while in <NO> the classifier is responsible to handle it.
The aim of this paper is to introduce a matrix-oriented classifier for the graph-based model of a signature representation proposed in <NO>. It seems that simplification of the classifier construction can be useful and profitable despite existing an effective solution basing on mathematical linguistics. In the paper novel matrix-based representation of hierarchical and probabilistic graph structures is well defined and a discussion on matrix-oriented matching scheme is investigated."	https://doi.org/10.1109/ACPR.2013.164	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Piekarczyk, et al. [NO], a graph-based off-line handwritten signature verification system is proposed. 
A new method for the synthesis of signature data with natural variability	['There are other proposals in the literature focused on the generation of signature images from on-line signatures <NO>, <NO>, <NO>, <NO>, <NO>.', ', 2008 <NO> transformations On-2-Off 2 Sign.', 'other duplicators <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', ', <NO>) may also help to improve the classification performance in a “customization” phase (start-up phase).', 'In <NO> new static signatures are generated from two dynamic samples from the same user while in <NO> a cognitive model is proposed for duplicating off-line signatures from one dynamic sample.', 'samples from the real ones of a given individual – distortionbased techniques using elastic matching procedures <NO> and variability estimation <NO> have been considered for this purpose.', 'Moreover, significant advancements in system benckmarking and performance evaluation has been achieved through international competitions for on-line and off-line signature verification systems, such as: SVC 2004 <NO>, BioSecure Signature Evaluation Campaign 2009 <NO>, SigComp 2009 <NO>, 4NSigComp2010 <NO>, SigComp2011 <NO>.']	C. Rabasse, R.M. Guest, and M.C. Fairhurst 2008. A new method for the synthesis of signature data with natural variability. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics 38,	The collection of human biometric test data for system development and evaluation within any chosen modality generally requires significant time and effort if data are to be obtained in workable quantities. To overcome this problem, techniques to generate synthetic data have been developed. This paper describes a novel technique for the automatic synthesis of human handwritten-signature images, which introduces modeled variability within the generated output based on positional variation that is naturally found within genuine source data. The synthesized data were found to generate similar verification rates to those obtained using genuine data with the use of a commercial verification engine, thereby indicating the suitability of the data synthesized by using this method for a wide range of application scenarios.	"THE operational performance of all biometric systemsused for the verification and identification of humans can be assessed by investigating the successful classification of genuine or nongenuine (e.g., forged) data <NO>. Within the biometric research community, a constant goal is to identify new and improved sensors, performance-defining features, and classification algorithms to improve this operational system performance. A low error rate is generally a main priority and is critical, given the increasing prevalence of biometric systems deployed across ever larger populations.
The practical context of synthesizing these signatures is to enable the effective training and testing of biometric systems where test data are limited. Biometric systems are conventionally assessed by using reference data sets comprising samples captured from human participants for a biometric modality of interest. A meaningful evaluation of a system, with respect to both enrolment and usage issues, which is to be deployed in a large population, therefore requires a considerable amount of test data. Many classifiers (for example, neural networks) also generally require a large quantity of data to ensure an optimally trained and stable system prior to classification. To obtain this large-scale data from human participants requires significant time and effort (often with considerable financial implications), but subjects are also often unwilling to donate large numbers
Manuscript received November 29, 2006; revised May 15, 2007. This work was supported by the EU Erasmus Exchange Program. This paper was recommended by Associate Editor V. Govindaraju.
C. Rabasse was with Laboratoire d’Informatique, de Traitement de l’Information et des Systèmes, University of Rouen, 76821 Rouen, France.
R. M. Guest and M. C. Fairhurst are with the Department of Electronics, University of Kent, CT2 7NZ Canterbury, U.K. (e-mail: r.m.guest@kent.ac.uk).
Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TSMCB.2008.918575
of biometric samples due to concerns about possible security compromise, specifically if the data are to be released into the public domain. Similarly, the quality of samples may often be compromised through repetition and fatigue, particularly if a subject is asked to repetitively donate samples within a single session. To overcome a potential “data shortage” problem in practical situations, a number of studies have investigated synthesizing biometric data using either actual human samples as “seed” references or, in other approaches, data generated using typical patterns/images found within a modality but without reference to a particular individual <NO>. Examples of synthesized modality studies (typically focused on image-based systems) include fingerprints <NO>, <NO>, iris <NO>, <NO>, and face <NO>.
Ma et al. <NO> proposed three criteria to ensure that synthesized biometric data sets are representative of the real-life modality under study and that the artificially generated data have the “appearance” of being generated from a natural source. These criteria are flexibility, parsimony, and consistency. Flexibility implies a parameterization of the synthesis mechanism enabling variations in data representation. Parsimony aims at building a generation procedure that is as simple as the inherent complexity of data will allow. Once a generation model has been found to be flexible and simple, it is necessary to assess how well it fits the original data. The generated data must produce repeatable results within a verification system and must be representative of the original data that they aim to simulate (the consistency criterion). If this is not the case, the simulation study may result in unreliable output.
This paper describes a method of automatically generating static image-based representations of human handwritten signatures (i.e., representations based solely on visual appearance) for use in biometric-system analysis, using (two) reference signatures from an individual signer as models for the generation of synthetic data. Automatic signature verification is a commonly used form of biometric verification and identification <NO> and is a modality which can offer many advantages over other forms of biometric authentication, among which is the familiarity among the general public for the everyday use of signatures to authorize transactions and verify identity. While other studies have outlined simple methods for generating synthetic signatures (such as fitting a series of Bezier curves between two sample images <NO>) or have assessed the properties of signatures with reference to parameter modeling of forged data <NO>, <NO>, the approach outlined in this paper is based on dynamic time warping (DTW) techniques <NO> and includes novel methods for dealing with sample signatures from the same signer but with different numbers of pen strokes. It also importantly introduces spatial variability throughout synthesized signatures based on that found within genuine
1083-4419/$25.00 © 2008 IEEE
signatures, resulting in images which encapsulate the natural variability found within hand-drawn signatures rather than a simple morphological mapping between two images. Our approach uses both the static (spatial) and dynamic (temporal) information within signatures, which is stored during online capture of data to generate the static synthesized images and which is generally included in sample databases. The temporal data enable the construction order to be assessed, which enables an accurate mapping between drawing sequences. Following this mapping, all subsequent operations occur in the spatial domain.
Finally, we assess the quality of the resulting synthesized images using samples from a standard online public-domain signature database (Signature Verification Competition 2004 (SVC2004) <NO>) as seed signatures and a commercial static signature verification engine as a “black-box” recognition system. In the experiment, we assess the verification performance achieved when using both genuine and forged signature images, synthesis images with variability, and, as a comparison, a second simpler method for synthesizing signature images."	https://doi.org/10.1109/TSMCB.2008.918575	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Rabasse, et al. [NO] describes a novel technique for the automatic synthesis of human handwritten-signature images, which introduces modeled variability within the generated output based on positional variation that is naturally found within genuine source data. 
Online signature verification on mobile devices	['The second on-line classifier <NO> is based on the', 'vector and then employ the classical distance measure such as Euclidean, City-Block for distance computation <NO>, <NO>.']	N. Sae-Bae, and N. Memon 2014. Online signature verification on mobile devices. IEEE Transactions on Information Forensics and Security 9,	This paper studies online signature verification on touch interface-based mobile devices. A simple and effective method for signature verification is developed. An online signature is represented with a discriminative feature vector derived from attributes of several histograms that can be computed in linear time. The resulting signature template is compact and requires constant space. The algorithm was first tested on the well-known MCYT-100 and SUSIG data sets. The results show that the performance of the proposed technique is comparable and often superior to state-of-the-art algorithms despite its simplicity and efficiency. In order to test the proposed method on finger drawn signatures on touch devices, a data set was collected from an uncontrolled environment and over multiple sessions. Experimental results on this data set confirm the effectiveness of the proposed algorithm in mobile settings. The results demonstrate the problem of within-user variation of signatures across multiple sessions and the effectiveness of cross session training strategies to alleviate these problems.	"AHANDWRITTEN signature is a socially and legallyaccepted biometric trait for authenticating an individual. Typically, there are two types of handwritten signature verification systems: off-line and online systems. In an off-line system, just an image of the user’s signature is acquired without additional attributes, whereas, in an online system, a sequence of x-y coordinates of the user’s signature, along with associated attributes like pressure, time, etc., are also acquired. As a result, an online signature verification system usually achieves better accuracy than an off-line system <NO>.
The increasing number of personal computing devices that come equipped with a touch sensitive interface and the difficulty of entering a password on such devices <NO> have led to an interest in developing alternative authentication mechanisms on them <NO>, <NO>. In this context, an online signature is a plausible candidate given the familiarity users have with the concept of using a signature for the purpose of authentication.
There has been much work on online signature verification systems <NO>–<NO>. However, none of this has been directed to the context of authentication on mobile devices. Previous work has addressed online signatures acquired from traditional digitizers in a controlled environment. These are different from those acquired from mobile devices in dynamic environments. First, on mobile devices, a user performs his signatures in various contexts, i.e., sitting or standing, mobile or immobile, and holding a device at different angles and orientations. Secondly, availability of computational resources may differ from one signature instance to another and it could result in greater variation of input resolution when compared to that of stand-alone acquisition devices. Last, signatures on mobile devices are often drawn using a finger instead of a stylus resulting in less precise signals. An example of finger drawn signatures acquired from mobile devices is depicted in Figure 1.
Consequently, verification performance derived from traditional datasets, collected using stylus-based devices in a controlled environment, may not carry over to online signature verification on mobile device setting <NO>–<NO>. In addition, other characteristics of the system, i.e., a template aging <NO> and effectiveness of cross-sessions training, may be different when signatures are obtained from a mobile device.
This paper proposes an online signature verification algorithm that is suitable to deploy on mobile devices. It is a computationally and space efficient algorithm for enrolling and verifying signatures. In addition, a signature template is stored in an irreversible form thereby providing privacy protection to an original online signature. The proposed method was evaluated on public datasets as well as new dataset collected in in uncontrolled setting from user owned mobile devices. The verification performance obtained is promising. The key contributions made by this paper are as follows:
1) A method to extract a model-free non-invertible feature set from an online signature is proposed. The feature set comprises of sets of histograms that capture distributions of attributes generated from raw signature data sequences and their combinations. By evaluating the proposed method on public datasets, its verification performance is superior to several state of the art algorithms. 2) A new dataset was collected from 180 users in a mobile device verification environment. The signatures in this dataset were drawn with a fingertip, in an uncontrolled setting on user owned iOS devices and over six separate sessions with intervals ranging from 12 to 96 hours. 3) By applying the proposed method on the above dataset, the following aspects of online signature verification on mobile devices were investigated:
• impact of template aging on online signatures, • effectiveness of using cross-session samples, or
samples from multiple sessions, to train a classifier, and • security of the system against random forgery, or zero-effort attack, and its comparison to that of 4-digit PIN.
The rest of this paper is organized as follows. Section II presents a process of deriving a set of histograms from an online signature, gives details of the proposed online signature verification system, and analyzes its complexity. Section III provides experimental results on public datasets. In section IV, a method and apparatus for collecting a new dataset as well as experimental results and analysis on this dataset is presented. Lastly, in section V, conclusions and future work are discussed."	https://doi.org/10.1109/TIFS.2014.2316472	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Sae-Bae, et al. [NO] studies online signature verification on touch interface-based mobile devices. 
User authentication using combination of behavioral biometrics over the touchpad acting like touch screen of mobile device	['In <NO>, keystroke dynamics were used to authenticate 10 users entering digits on a touch pad.']	H. Saevanee, and P. Bhatarakosol 2008. User authentication using combination of behavioral biometrics over the touchpad acting like touch screen of mobile device. In Int. Conf. on Computer and Electrical Engineering (ICCEE’08)	Now mobile devices are developed to serve various functions, storing the sensitive information. In order to protect those information and mobile systems from unauthorized users, the authentication system must be installed unavoidably. Additionally, the development of the mobile system is moving forward to the touch screen system for user friendly and quick access mechanism. In this paper, we proposed behavioral manners of users over the touchpad acting like touch screen that is able to detect the finger pressure. These behaviors are keystroke dynamics and the finger pressure. The finding has shown that, the finger pressure gives the discriminative information more than keystroke dynamics with the k-NN analytical method. Moreover, using only the finger pressure produces high accuracy rate of 99%.	"Nowadays, Mobile devices, such as cellular phones and Personal Digital Assistants (PDAs), become widespread in excess over 3 billion users <NO>. Most of them are operated by touching a display commonly used because the touch screen interface is easy to use and user-friendly operates. Currently, mobile devices are used to not only make or receive a call, take photos, and play video games, but also give the special assistance in the business, such as providing internet access, directing access to e-mail and cooperating data, transferring money, and managing bank account. As a consequence, the authentication of users for mobile devices has become an important issue.
According to <NO>, the authentication on mobile devices can be classified in three fundamental approaches. The first approach is using a PIN (Personal Identification Number) or a password which is a secrete-knowledge based technique. This technique
offers a standard level of protection and provide cheap and quick authentication. Unfortunately, it is not enough to the safeguard mobile device and data access through them because passwords have never been completely protected by the owners; sharing passwords with friends or any other systems are unavoidable problems. Moreover, the result of a survey from <NO> has shown that most users agree that using PIN is very inconvenient and they do not have confidence in the protection of the PIN facility provides.
The second approach is the token-based technique or SIM (Subscriber Identification Module). In this approach, when users do not want to use the mobile, the mobile’s SIM must be removed. However, removing SIM is not recommended due to inconvenient manners.
The last approach is applying the biometric technique. This technique is based on a unique characteristic of a person that provides an improvement on the current authentication. Biometrics relevance with the identification and verification of individual based on human characteristics. Biometric approaches are typically subdivided into two categories: physiological and behavioral biometrics. Physiological biometric is based on bodily characteristics, such as fingerprint, facial recognition, and iris scanning. Behavioral biometric is based on the way people do things, such as keystroke dynamics, mouse movement, and speech recognition.
Using any kind of mobile phones, people cannot avoid interact with keystroke dynamics. However, each person may have different styles to press the key because the typing style is based on user’s experience and individual skill which is difficult to imitate.
The purpose of this paper is to investigate the behavioral manner of users when dialing the phone number on touchpad acting like touch screen on the mobile touch screen detected force in the future <NO>. Using keystroke dynamics and the finger pressure information are the features to authenticate users to increase the accuracy using the combination of behavioral biometrics.
The remaining of this paper is organized in six sections. In the Section II is presented works published in the area. In the Section III, the purpose of methodology is discussed: gathering the data, extracting the features, and data structuring and anglicizing. The results are presented in Section IV and discuss in Section V. Finally, the conclusions and future work are presented in Section VI."	https://doi.org/10.1109/ICCEE.2008.157	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Saevanee, et al. [NO] proposes behavioral manners of users over the touchpad acting like touch screen that is able to detect the finger pressure. 
A novel online signature verification system based on GMM features in a DTW framework	[]	A. Sharma, and S. Sundaram 2017. A novel online signature verification system based on GMM features in a DTW framework. IEEE Transactions on Information Forensics and Security 12,	This paper presents a novel online signature verification system based on the extension of the traditional dynamic time warping (DTW) matching scheme. We propose the use of a set of features derived from a Gaussian mixture model (GMM) for the alignment of the signatures using DTW. These features aid in capturing signature-dependent characteristics of a user in the feature space with a probabilistic framework. In addition, we explore the characteristics of the warping path of DTW, by employing the proposed GMM features. We derive a score for the warping path, and fuse it to that of the DTW score for verifying the authenticity of a test signature. To the best of our knowledge, this paper is the first of its kind that uses the features of the GMM, a model-based classifier into the framework of the DTW technique for online signature verification. The experiments are conducted on the publicly available MCYT database for both common and user thresholds. The results obtained are promising over prior works for this database.	"IN RECENT times, owing to security concerns, authen-tication of a person has become a necessity. Traditional means of authentication include the use of password and/or PIN number. These have a disadvantage that they can be either forgotten or stolen. Bio-metrics refers to identification of a person based on their physical or behavioral characteristics <NO>. It is inherently, more reliable than traditional systems such as passwords. It is desired that a candidate biometric used in an authentication system demonstrates the properties of universality, distinctiveness, permanence, and acceptability <NO>.
From the works in literature, the field of bio-metrics is categorized to one of two types, namely (i) physiological and (ii) behavioral. The physiological bio-metrics employ direct measurements of a part of the human body, such as fingerprint, face, iris and hand-scan for identification. On the other hand, behavioral bio-metrics operate on features derived from data obtained from an action performed by the user. A signature is one such bio-metric that falls into this category. In a signature verification system, features of a test signature are contrasted against those from a set of enrolled genuine signatures of an user whose identity is to be authenticated. This leads to one of two possible outcomes: either the claimed signature is accepted as genuine or it is rejected as forgery.
The recent trend in technology has led to the release of devices offering a pen based interface to input handwritten data. Such devices, referred to as hand-held devices are manufactured small, portable and are easy to use. They primarily utilize an electronic pen (referred to as stylus) to capture data on a pressure-sensitive screen. The dynamic information of handwriting, such as (x, y) spatial location, pressure, azimuth and inclination are recorded in the device by sensing the movements of the pen-tip. In the literature, the processing of such data is referred to as online. This makes it different with regards to the off-line case, where-in only the completed writing in the form of a bitmap image is available for analysis using image processing techniques.
The input to any online signature verification system comprises a set of strokes, each of which in turn are a sequence of points. A stroke starts with a pen-down movement and ends with the next pen-up movement. It is presented by its duration and a sequence of x and y coordinates, along with other dynamic information."	https://doi.org/10.1109/TIFS.2016.2632063	0	['user', 'recognition', 'algorithm']	['handwritten_signatures', 'skill_forgeries', 'state_art', 'the_results', 'dynamic_signatures']	['online_signature_verification', 'handwritten_signature_verification', 'equal_error_rate', 'in_paper_propose', 'methods_signature_verification']	Sharma, et al. [NO] presents a novel online signature verification system based on the extension of the traditional dynamic time warping (dtw) matching scheme. 
Offline loop investigation for handwriting analysis	['<NO>, the contour was used to classify holes, identify hidden loops and hidden natural sub-loops.', 'A loop, which is made of several strokes, is formed when the writing instrument revisits a previous position while touching the writing surface continuously <NO>.', '<NO> used contour bank segments to determine the next course of the pen trajectory.', '<NO> 2009 Contour Loops 540 loops from IRONOFF database 80.']	T. Steinherz, D. Doermann, E. Rivlin, and N. Intrator 2009. Offline loop investigation for handwriting analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 31,	Resolution of different types of loops in handwritten script presents a difficult task and is an important step in many classic word recognition systems, writer modeling, and signature verification. When processing a handwritten script, a great deal of ambiguity occurs when strokes overlap, merge, or intersect. This paper presents a novel loop modeling and contour-based handwriting analysis that improves loop investigation. We show excellent results on various loop resolution scenarios, including axial loop understanding and collapsed loop recovery. We demonstrate our approach for loop investigation on several realistic data sets of static binary images and compare with the ground truth of the genuine online signal.	"SHARING many of the trajectory singularities, loops appearas one of the most dominant features available in cursive handwriting processing <NO>, <NO>, <NO>, <NO>, <NO>. In particular, loops are the key to successful offline-to-online-based word recognition systems, i.e., those mapping a static (bitmap) image to an ordered list of pixel locations along a time axis <NO>, <NO>, <NO>, <NO>, <NO>. It is useful to investigate how improved loop detection and recognition can facilitate not only character recognition but also writer modeling for identification and examination <NO>, <NO>, <NO> and script or style identification <NO>, <NO>, <NO>. Similarly, many other applications in forensic science, such as signature verification, could benefit from loop analysis <NO>, <NO>, <NO>, <NO>, <NO>.
The dominance of loops in these tasks reflects in part on their frequent presence in handwritten cursive words and their parameterizable descriptive nature. J.C. Simon first elucidated the elementary nature of loops and provided an intuitive definition of the types of loops <NO>: “Displacing a pen from left to right in an oscillating movement, with loops, descendants (legs), and ascendants (poles).” Moreover, in the common case of pure cursive handwriting, its continuous nature constrains many ascending and descending strokes in a loop form. Therefore, we consider an extended definition of loops to contain all kinds of uninterrupted enclosures <NO>, including those with invisible “holes.” Thus, loops can be
found in the usual letters like a, d, e, g, o, p, q, and in letters like b, f , h, j, k, l, s, t, y, and z. In most cases, any stroke intersection, excluding delayed strokes, relates to some kind of a loop.
The significance of loops increases because of their parameterizable nature, which enables the transformation of a static loop image into a quantified feature vector. Thus, the loop provides information in a format usable in machine learning algorithms. Given the ground truth for genuine loops provided by the online signal, loop investigation essentially tries to understand the isomorphism between the offline image and the online signal. Unfortunately, such a transformation is not straightforward <NO>.
Loop investigation has been considered in the context of enhancing offline handwritten word representation and the reconstruction of the genuine ordered list of strokes. It has been done mostly by using temporal (dynamic) information recovery techniques such as contour analysis <NO>, <NO>, gray-scale examination <NO>, <NO>, and path minimization <NO>, <NO>, <NO>. Other methods include thinning/skeletonization <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO> and morphological loop investigation <NO>, <NO>, <NO>. This paper improves aspects of former solutions; we detect and resolve the structure of most loops. Our method uses a sophisticated contour analysis we call The Multipartite Matching Approach. Our algorithm is beneficial in cases, where a complete offline to online transformation is desired.
This paper has four main sections: Section 2 introduces the theory of loops; in Sections 3 and 4, The Multipartite Matching Approach and its implementation are demonstrated; Section 5 provides experimental results. The concluding section provides a final discussion."	https://doi.org/10.1109/TPAMI.2008.68	1	['technique', 'svm', 'accuracy']	['based_technique', 'japanese_signature', 'mahalanobis_distance', 'svm_based', 'line_features']	['based_technique_employs', 'decision_verification_performed', 'evaluation_test_proposed', 'feature_based_technique', 'final_decision_verification']	Steinherz, et al. [NO] presents a novel loop modeling and contour-based handwriting analysis that improves loop investigation. 
On-line signature verification using graph representation	[]	K. Wang, Y. Wang, and Z. Zhang 2011. On-line signature verification using graph representation. In 6th Int. Conf. on Image and Graphics	This paper proposes a novel approach of on-line signature verification. Firstly, on-line signatures are represented by a series of graphs, whose nodes and edges describe certain properties at sample points and relationship between points respectively. Then, graph matching techniques are introduced to compute edit distance between graphs, which measures the similarity of graphs. Finally, having been able to compare any two signatures through the last two steps, user-dependent classifiers are trained using limited genuine signatures. The proposed method is tested on SUSIG on-line signature database and shows promising performance. Keywords-signature verification, on-line signature, graph representation, graph matching, biometrics	"Signatures are widely accepted as the common method of identity verification. On-line signatures make use of the dynamic features extracted by pressure sensitive tablets or digital pens, exploiting more useful information than traditional off-line signature, thus provide more reliable protection against forgeries. Examples of traditional signatures and on-line signatures are given in Fig.1, and each point in on-line signature contains abundant information like pen position and pen-tip pressure. To verify the authenticity of signatures, signature verification problem could be attributed to a two-class pattern recognition problem, the process of which is shown in Fig.2.
A wide range of pattern recognition methods can be applied to signature verification. As one of the most popular method in signal processing, dynamic time warping (DTW) has been applied to signatures verification in many researches. DTW allows the compression or expansion of the time axis of two time sequence representative of signatures to obtain the minimum of a given distance value <NO>. It has been
Figure 2. Framework of signature verification
extensively used in the field of on-line signature verification and has derived a bunch of methods taking advantages of other methods <NO><NO><NO>. Improvements have also been made to deal with DTW’s disadvantage of time consuming.
Meanwhile, intensive research has been devoted to hidden Markov model (HMM) as well. An HMM is a double stochastic approach in which one underlying yet unobservable process may be estimated through a set of processes that produce a sequence of observations. The topological structures of left-to-right, ring and ergodic have been respectively looked into <NO><NO><NO>. However, HMM makes a large assumption about the data, requires a lot of positive sample and needs too many parameters.
As in many other aspects of pattern recognition, support vector machine (SVM) turns out to be a promising approach to signature verification. It maps input vectors to a higher dimensional space in which clusters may be determined by a maximal separating hyperplane. When applied to on-line signature verification, SVM gives convincing results. New kernel functions have also been proposed to improve the performance <NO>.
However, it has come to our attention that some methods involving graph representation of signature and graph matching had been applied to off-line signature verification <NO>, while to our knowledge, there has been few works on on-line signature verification in such ways. However, there have been some trials on string matching <NO>, which can be seen as special cases of graph matching.
In this paper, we propose an approach of on-line signature verification using graph representation. Represented by graphs, signatures are compared using graph matching methods. As is NP-complete, graph matching is a classic problem in the field of algorithms. Therefore, there have been many efficient graph matching algorithms that can be made use of.
The organization of the remainder of this paper is as follows. Section 2 describes the graph representation and graph matching method adopted in our work. The details of the training and testing procedure will be given in Section 3. Section 4 demonstrates the experiment results while Section 5 draws the conclusion."	https://doi.org/10.1109/ICIG.2011.57	2	['sample']	['signature_identification', 'chain_code', 'code_direction', 'modified_chain', 'line_signatures']	['chain_code_direction', 'modified_chain_code', 'segmented_signature_image', 'signature_identification_proposed', 'line_signature_verification']	Wang, et al. [NO] proposes a novel approach of on-line signature verification. 
