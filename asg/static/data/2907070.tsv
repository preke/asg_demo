	ref_title	ref_context	ref_entry	abstract	intro	ref_link
0	Distance Based Chyi	[]	Torgo 2013. Distance Based Chyi			http://scholar.google.com/scholar?hl=en&q=Ying+Mi.+2013.+Imbalanced+classification+based+on+active+learning+SMOTE.+Res.+J.+Appl.+Sci.+5+%282013%29.
1	Combination of Methods	[]	Sáez 2015. Combination of Methods			http://scholar.google.com/scholar?hl=en&q=Vincent+Barnab-Lortie%2C+Colin+Bellinger%2C+and+Nathalie+Japkowicz.+2015.+Active+learning+for+one-class+classification.+In+Proceedings+of+ICMLA%272015.
2	size, while the second one adds data, increasing the sample size. In random under-sampling, a random set of majority class examples are discarded. This may eliminate useful examples leading to a worse performance. Oppositely, in random oversampling, a random set of copies of minority class examples is added to the data	[]	Zadrozny 2003. size, while the second one adds data, increasing the sample size. In random under-sampling, a random set of majority class examples are discarded. This may eliminate useful examples leading to a worse performance. Oppositely, in random oversampling, a random set of copies of minority class examples is added to the data			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
3	The notion of the Condensed Nearest Neighbour Rule	[]	classes [Batista 2004. The notion of the Condensed Nearest Neighbour Rule			https://doi.org/10.1007/s11063-014-9376-3
4	Recognition-based methods as one-class learning or autoencoders offer the possibility	[]	Batista 2004. Recognition-based methods as one-class learning or autoencoders offer the possibility			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
5	2006a, 2006b], and Lee and Cho [2006]) and the use of an autoencoder (or autoassociator	[]	P. Branco et al. Yousef, Raskutti, Kowalczyk, Zhuang, and Dai 2000. 2006a, 2006b], and Lee and Cho [2006]) and the use of an autoencoder (or autoassociator			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
6	made one of the first contributions with a new evolutionary method proposed for balancing the dataset. The presented method uses a new fitness function designed to perform a prototype selection process. Some proposals have also emerged in the area of heuristics and metrics for improving several genetic programming classifiers performance in imbalanced domains [Doucette and Heywood 2008	[]	Garcı́a 2006. made one of the first contributions with a new evolutionary method proposed for balancing the dataset. The presented method uses a new fitness function designed to perform a prototype selection process. Some proposals have also emerged in the area of heuristics and metrics for improving several genetic programming classifiers performance in imbalanced domains [Doucette and Heywood 2008			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
7	Genetic Algorithms (GA) and clustering techniques were combined to perform both under- and over-sampling [Maheshwari et al	[]	EA However 2011. Genetic Algorithms (GA) and clustering techniques were combined to perform both under- and over-sampling [Maheshwari et al			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
8	The denoising autoencoders are neural networks	[]	Vincent 2010. The denoising autoencoders are neural networks			https://doi.org/10.1007/s11063-014-9376-3
9	2014] use a random-walk-based approach as an over-sampling strategy	[]	matrix. Zhang, and Li 2014. 2014] use a random-walk-based approach as an over-sampling strategy			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
10	2012a]. An exception is the Fuzzy Rough Imbalanced	[]	SMOTE+RSB) [Ramentol 2012. 2012a]. An exception is the Fuzzy Rough Imbalanced			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
11	Adaptive Synthetic (ADASYN	[]	Borderline-SMOTE [Han 2005. Adaptive Synthetic (ADASYN. [He et al			http://scholar.google.com/scholar?hl=en&q=Colin+Bellinger%2C+Nathalie+Japkowicz%2C+and+Christopher+Drummond.+2015.+Synthetic+oversampling+for+advanced+radioactive+threat+detection.+In+Proceedings+ICML%272015.
12	Safe Level Graph [Bunkhumpornpat	[]	Level-SMOTE [Bunkhumpornpat 2009. Safe Level Graph [Bunkhumpornpat			http://scholar.google.com/scholar?hl=en&q=Chumphol+Bunkhumpornpat%2C+Krung+Sinapiromsaran%2C+and+Chidchanok+Lursinsap.+2011.+MUTE%3A+Majority+under-sampling+technique.+In+2011+8th+International+Conference+on+Information%2C+Communications+and+Signal+Processing+%28ICICS%29.+IEEE%2C+1%2D%2D4.
13	For regression problems, only one method for generating new synthetic data	[]	Bunkhumpornpat 2012. For regression problems, only one method for generating new synthetic data			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
14	An interesting approach that combines clustering	[]	Chen 2010. An interesting approach that combines clustering			https://doi.org/10.1007/s11063-014-9376-3
15	2006], where an ensemble of under-sampled SVMs is presented	[]	of Kang, and Cho 2006. 2006], where an ensemble of under-sampled SVMs is presented			https://doi.org/10.1007/s11063-014-9376-3
16	conversion: in a transparent box or in a black box way. In the first, the weights are provided to the classifier, while for the second a careful sub-sampling is performed according to the same weights. The first approach cannot be applied to an arbitrary learner, while the second one results in severe overfitting if sampling with replacement is used	[]	Zadrozny 2003. conversion: in a transparent box or in a black box way. In the first, the weights are provided to the classifier, while for the second a careful sub-sampling is performed according to the same weights. The first approach cannot be applied to an arbitrary learner, while the second one results in severe overfitting if sampling with replacement is used			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
17	Regarding neural networks, the possibility of making them	[]	others [Weiguo 2012. Regarding neural networks, the possibility of making them			https://doi.org/10.1007/s11063-014-9376-3
18	proposes a method called Weighted Random Forest (WRF) for dealing	[]	Chen 2004. proposes a method called Weighted Random Forest (WRF) for dealing			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
19	2003] present a weighted distance function to be used	[]	problem. Barandela 2003. 2003] present a weighted distance function to be used			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
20	CCPDT is robust and insensitive to class	[]	Liu 2010. CCPDT is robust and insensitive to class			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
21	recommended the use of bagged HDDTs as the preferred method	[]	Cieslak 2012. recommended the use of bagged HDDTs as the preferred method			https://doi.org/10.1007/s11063-014-9376-3
22	The Kernel Boundary Alignment algorithm (KBA) is proposed in Wu and Chang	[]	Ribeiro, and Torgo 2003. The Kernel Boundary Alignment algorithm (KBA) is proposed in Wu and Chang			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
23	Cost-sensitive Post-processing	[]	Hernández-Orallo 2012. Cost-sensitive Post-processing			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo+Eleuterio%2C+Jos%C3%A9+Mart%C3%ADnez+Sotoca%2C+Vicente+Garc%C3%ADa+Jim%C3%A9nez%2C+and+Rosa+Mar%C3%ADa+Valdovinos+Rosas.+2011.+Back+propagation+with+balanced+MSE+cost+function+and+nearest+neighbor+editing+for+handling+class+overlap+and+class+imbalance.+%282011%29.
24	threshold method: uses the ranking provided by a score that expresses the degree	[]	Hernández-Orallo 2014. threshold method: uses the ranking provided by a score that expresses the degree			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
25	explores several threshold choice methods and provides an interesting interpretation for a diversity of performance metrics. The threshold choice methods are categorized according to the operating conditions. Guidelines are provided regarding the performance metric that should be used based on the information available on the threshold choice method	[]	Hernández-Orallo 2012. explores several threshold choice methods and provides an interesting interpretation for a diversity of performance metrics. The threshold choice methods are categorized according to the operating conditions. Guidelines are provided regarding the performance metric that should be used based on the information available on the threshold choice method			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
26	It is an issue still under-explored with few limited solutions. Similarly to what happens in classification, no progress has yet been made for evaluating these solutions in imbalanced domains. However, one interesting proposal called reframing [Hernández-Orallo	[]	Zhao 2008. It is an issue still under-explored with few limited solutions. Similarly to what happens in classification, no progress has yet been made for evaluating these solutions in imbalanced domains. However, one interesting proposal called reframing [Hernández-Orallo			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
27	the reframing of an enriched soft regression model to new contexts by an instancedependent optimization of the expected loss derived from the conditional normal distribution	[]	Barnab-Lortie 2015. the reframing of an enriched soft regression model to new contexts by an instancedependent optimization of the expected loss derived from the conditional normal distribution			https://doi.org/10.1007/978-3-642-01307-2_43
28	The motivation for this proposal is related to the fact that a perfectly balanced data may not be optimal and that the right amount of over-/undersample to apply is difficult to determine. To overcome these difficulties, a mixture-ofexperts framework was proposed in an architecture with three levels: a classifier	[]	Estabrooks 2004. The motivation for this proposal is related to the fact that a perfectly balanced data may not be optimal and that the right amount of over-/undersample to apply is difficult to determine. To overcome these difficulties, a mixture-ofexperts framework was proposed in an architecture with three levels: a classifier			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
29	The proposed approach uses a facilitator agent and three learning agents, each one with its own learning system. The facilitator starts by filtering the features of the dataset. The filtered data are then passed to the three learning agents. Each learning agent samples the dataset, learns using the respective system (Naive Bayes, C4.5, and 5-NN), and then returns the predictions	[]	Kotsiantis, and Pintelas 2003. The proposed approach uses a facilitator agent and three learning agents, each one with its own learning system. The facilitator starts by filtering the features of the dataset. The filtered data are then passed to the three learning agents. Each learning agent samples the dataset, learns using the respective system (Naive Bayes, C4.5, and 5-NN), and then returns the predictions			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
30	2011a, 2011b] presented a new approach that also uses active	[]	Ghasemi 2011. 2011a, 2011b] presented a new approach that also uses active			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
31	compared three types of classifiers (SVM, decision	[]	López 2013. compared three types of classifiers (SVM, decision			https://doi.org/10.5555/1768409.1768431
32	For decision trees and k-NN, the best performing strategy was smoteBagging, while for SVMs SMOTE obtained the best performance closely followed by the remaining evaluated pre-processing strategies	[]	López 2013. For decision trees and k-NN, the best performing strategy was smoteBagging, while for SVMs SMOTE obtained the best performance closely followed by the remaining evaluated pre-processing strategies			https://doi.org/10.1007/978-3-642-13059-5_22
33	2016] studies the relation between the dominant type	[]	outliers. Then, and Stefanowski 2016. 2016] studies the relation between the dominant type			https://doi.org/10.1007/s11063-014-9376-3
34	1984]. However, for real-world applications, Weiss [2013] has shown that the equivalence frequently does not hold. Consider, for instance, a binary classification problem with 1,100 examples and an imbalanced domain with a class distribution of 10:1	[]	Breiman 2013. 1984]. However, for real-world applications, Weiss [2013] has shown that the equivalence frequently does not hold. Consider, for instance, a binary classification problem with 1,100 examples and an imbalanced domain with a class distribution of 10:1			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
35	A theoretical analysis of imbalance	[]	Wallace 2007. A theoretical analysis of imbalance. (e.g., Van Hulse et al			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
36	contributed theoretical advances regarding imbalanced domains. The focus of this work was also in the under-sampling strategy. In this article, the authors study two aspects that are consequences of ACM	[]	More recently, and Dal Pozzolo 2016. contributed theoretical advances regarding imbalanced domains. The focus of this work was also in the under-sampling strategy. In this article, the authors study two aspects that are consequences of ACM. Computing Surveys,			http://scholar.google.com/scholar?hl=en&q=Chao+Chen%2C+Andy+Liaw%2C+and+Leo+Breiman.+2004.+Using+random+forest+to+learn+imbalanced+data.+University+of+California%2C+Berkeley+%282004%29.
37	2011]), while for the second issue it is necessary to calibrate the probability of the new priors	[]	Wallace 2015. 2011]), while for the second issue it is necessary to calibrate the probability of the new priors. Dal Pozzolo et al			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
38	Several proposals exist for handling the imbalance problem in conjunction with the high-dimensionality problem, all using a feature selection strategy [Zheng et al	[]	Van Someren 2003. Several proposals exist for handling the imbalance problem in conjunction with the high-dimensionality problem, all using a feature selection strategy [Zheng et al. Del Castillo and Serrano 2004; Forman and Cohen 2004; Chu et al			https://doi.org/10.5555/1768409.1768431
39	fore they will tend to overfit and misclassify the cases in the small disjuncts. Due to the importance of these two problems, several works address the relation between the problem of small disjuncts and the class imbalance problem (e.g., Japkowicz	[]	Weiss, Provost, Jo, Japkowicz, and Pearson 2001. fore they will tend to overfit and misclassify the cases in the small disjuncts. Due to the importance of these two problems, several works address the relation between the problem of small disjuncts and the class imbalance problem (e.g., Japkowicz. and Prati et al			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
40	The data shift problem has also deserved the attention of the research community. The problem of data shift occurs when there is a difference in the distribution of the train and test sets. The data shift occurs frequently, and it usually leads to a small performance degradation	[]	López 2013. The data shift problem has also deserved the attention of the research community. The problem of data shift occurs when there is a difference in the distribution of the train and test sets. The data shift occurs frequently, and it usually leads to a small performance degradation			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
41	perspectives of this problem under imbalanced domains: intrinsic and induced data shift. The first one regards shifts in the data distribution that are already present in the data. This is an unexplored issue that still has no solution. As for induced data shift, it is related with the evaluation techniques	[]	Moreno-Torres 2012. perspectives of this problem under imbalanced domains: intrinsic and induced data shift. The first one regards shifts in the data distribution that are already present in the data. This is an unexplored issue that still has no solution. As for induced data shift, it is related with the evaluation techniques			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
42	2014], multi-label classification [Charte et al. 2015b], association	[]	Pérez-Ortiz 2015. 2014], multi-label classification [Charte et al. 2015b], association			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
43	2015a, 2015b]), multi-instance learning (e.g., Wang et al. [2013a, 2013b]), and mining association rules	[]	Ortiz 2014. 2015a, 2015b]), multi-instance learning (e.g., Wang et al. [2013a, 2013b]), and mining association rules. (e.g., Mangat and Vig [2014], and Luna et al			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
44	Applying support vector machines to imbalanced datasets	['Weighting methods either assign heavier weights to the minority training instances or penalties for misclassifications of minority instances <NO>.', 'However, SVMs are also sensitive to class imbalance <NO><NO>.', 'As a remedy, researchers have used alternative performance measures, such as Geometric-mean (Gm) <NO>,<NO> and F-measure (Fm) <NO>,<NO> for classifier performance evaluation in imbalanced dataset learning.', 'In order to overcome this problem, other performance measures, such as the Gm <NO>,<NO> and Fm <NO>,<NO>, have been used in class imbalance learning research.', 'In <NO> the behavior of Support Vector Machines (SVM) with imbalanced data is investigated.', ', the separating hyperplane of an SVM model, which was developed with an imbalanced dataset, can be skewed toward the minority (positive) class <NO>, <NO>.', 'This was following the findings reported in <NO>, where the optimal results for the DEC method could be obtained when C−/C+ equals to the minority-to-majority class ratio.', 'It has been well-studied that when the training dataset is imbalanced, the commonly used performance measure accuracy, which is the proportion of correctly classified instances, could lead to suboptimal models <NO>, <NO>.', 'Therefore, we used the geometric mean of sensitivity (SE = proportion of the positives correctly recognized) and specificity (SP = proportion of the negatives correctly recognized), given by Gm = √ SE × SP, for the classifier performance evaluation in this study, as commonly used in class imbalanced-learning research <NO>, <NO>, <NO>.', 'When applying the DEC method, the value of C−/C+ was set to the positive-to-negative class ratio of the dataset by following the findings in <NO>.']	Rehan Akbani, Stephen Kwek, and Nathalie Japkowicz. 2004. Applying support vector machines to imbalanced datasets. Machine Learning: ECML 2004. Springer, 39–50.			http://scholar.google.com/scholar?hl=en&q=Rehan+Akbani%2C+Stephen+Kwek%2C+and+Nathalie+Japkowicz.+2004.+Applying+support+vector+machines+to+imbalanced+datasets.+In+Machine+Learning%3A+ECML+2004.+Springer%2C+39%2D%2D50.
45	Assessments metrics for multi-class imbalance learning: A preliminary study	[]	Roberto Alejo, J.A. Antonio, Rosa Maria Valdovinos, and J. Horacio Pacheco-Sánchez. 2013. Assessments metrics for multi-class imbalance learning: A preliminary study. Pattern Recognition. Springer, 335– 343.			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo%2C+J.+A.+Antonio%2C+Rosa+Maria+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2013.+Assessments+metrics+for+multi-class+imbalance+learning%3A+A+preliminary+study.+In+Pattern+Recognition.+Springer%2C+335%2D%2D343.
46	An efficient over-sampling approach based on mean square error back-propagation for dealing with the multi-class imbalance problem	[]	Roberto Alejo, Vicente Garcı́a, and J. Horacio Pacheco-Sánchez 2014. An efficient over-sampling approach based on mean square error back-propagation for dealing with the multi-class imbalance problem			https://doi.org/10.1007/s11063-014-9376-3
47	Improving the performance of the RBF neural networks trained with imbalanced samples	[]	Roberto Alejo, Vicente Garcı́a, José Martı́nez Sotoca, Ramón Alberto Mollineda, and José Salvador Sánchez 2007. Improving the performance of the RBF neural networks trained with imbalanced samples. In Computational and Ambient Intelligence. Springer,			https://doi.org/10.5555/1768409.1768431
48	A hybrid method to face class overlap and class imbalance on neural networks and multi-class scenarios	[]	Roberto Alejo, Rosa Maria Valdovinos, Vicente Garcı́a, and J. Horacio Pacheco-Sanchez 2013. A hybrid method to face class overlap and class imbalance on neural networks and multi-class scenarios. Pattern Recogn. Lett. 34,			https://doi.org/10.1016/j.patrec.2012.09.003
49	Back propagation with balanced MSE cost function and nearest neighbor editing for handling class overlap and class imbalance	[]	Roberto Alejo Eleuterio, José Martı́nez Sotoca, Vicente Garcı́a Jiménez, and Rosa Marı́a Valdovinos Rosas 2011. Back propagation with balanced MSE cost function and nearest neighbor editing for handling class overlap and class imbalance			http://scholar.google.com/scholar?hl=en&q=Roberto+Alejo+Eleuterio%2C+Jos%C3%A9+Mart%C3%ADnez+Sotoca%2C+Vicente+Garc%C3%ADa+Jim%C3%A9nez%2C+and+Rosa+Mar%C3%ADa+Valdovinos+Rosas.+2011.+Back+propagation+with+balanced+MSE+cost+function+and+nearest+neighbor+editing+for+handling+class+overlap+and+class+imbalance.+%282011%29.
50	Class imbalance and active learning	[]	Josh Attenberg, and Seyda Ertekin. 2013. Class imbalance and active learning. Imbalanced Learning: Foundations, Algorithms, and Applications, Haibo He and Yunqian Ma (Eds.). John Wiley & Sons.			http://scholar.google.com/scholar?hl=en&q=Josh+Attenberg+and+Seyda+Ertekin.+2013.+Class+imbalance+and+active+learning.+In+Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications%2C+Haibo+He+and+Yunqian+Ma+%28Eds.%29.+John+Wiley+%26+Sons.
51	Evaluation measures for ordinal regression	[]	Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Evaluation measures for ordinal regression. Ninth International Conference on Intelligent Systems Design and Applications, 2009. ISDA’09. IEEE, 283–287.	Ordinal regression (OR – also known as ordinal classification) has received increasing attention in recent times, due to its importance in IR applications such as learning to rank and product review rating. However, research has not paid attention to the fact that typical applications of OR often involve datasets that are highly imbalanced. An imbalanced dataset has the consequence that, when testing a system with an evaluation measure conceived for balanced datasets, a trivial system assigning all items to a single class (typically, the majority class) may even outperform genuinely engineered systems. Moreover, if this evaluation measure is used for parameter optimization, a parameter choice may result that makes the system behave very much like a trivial system. In order to avoid this, evaluation measures that can handle imbalance must be used. We propose a simple way to turn standard measures for OR into ones robust to imbalance. We also show that, once used on balanced datasets, the two versions of each measure coincide, and therefore argue that our measures should become the standard choice for OR. 	"Are the standard evaluation measures for OR robust to imbalance? The most commonly used such measures are
1) Mean Absolute Error (here denoted MAEµ, and also called ranking loss – see e.g., <NO>), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>. MAEµ is defined as the average deviation of the predicted class from the true class, i.e.,
MAEµ(Φ̂, T e) = 1 |Te| ∑ xi∈Te |Φ̂(xi)− Φ(xi)| (1)
where Te denotes the test set and the n classes in Y are assumed to be real numbers, so that |Φ̂(xi)−Φ(xi)| exactly quantifies the distance between the true and the predicted rank (the meaning of the µ superscript will be clarified later). 2) Mean Squared Error (MSEµ – also called Squared Error Loss), as used e.g., in <NO>, defined as
MSEµ(Φ̂, T e) = 1 |Te| ∑ xi∈Te (Φ̂(xi)− Φ(xi))2 (2)
A variant is Root Mean Square Error, as used e.g., in <NO>, which corresponds to the square root of MSEµ. 3) Mean Zero-One Error (more frequently known as Error Rate), as used e.g., in <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, and simply defined as the fraction of incorrect predictions, i.e.,
MZOEµ(Φ̂, T e) = |{xi ∈ Te : Φ̂(xi) 6= Φ(xi}|
|Te| (3)
Unlike MSEµ and MAEµ, MZOEµ has the disadvantage that all errors are treated alike, and thus insufficiently penalizes algorithms that incur into blatant errors. MSEµ penalizes blatant mistakes (e.g., misplacing an item into a rank faraway from the correct one) more than MAEµ, due
4Here we assume that the positives are the minority and the negatives are the majority, which is usually the case in binary classification.
to the presence of squaring; as such, it has been argued (see e.g., <NO>) that MSEµ is more adequate for measuring systems that classify product reviews, since different reviewers might attribute identical reviews to different but neighbouring classes.
It is quite evident that none of these measures is robust to imbalance, since they are all based on a sum of the classification errors across documents. Since the majorityclass classifier incurs in zero error for all the documents whose true class is the majority class, and since in an imbalanced dataset these documents are many, this trivial policy tends to be fairly “error-free”.
To make this problem even worse, it is easy to show that for all these error measures the “trivial class” Φ̃k need not be the majority class; in other words, there may exist trivial classifiers that are even more “error-free” than the majority-class classifier. For instance, in the TripAdvisor15763 dataset mentioned above, assuming that the class distribution in the test set is the same as that in the training set, by assigning all test documents 4 stars we obtain lower MAEµ than by assigning all of them 5 stars, which is the majority class. This is because 4 stars is only marginally less frequent than 5 stars, but in misclassifying all of the documents belonging to the lower classes (1 stars to 3 stars) as 4 stars we make a smaller mistake than in misclassifying them as 5 stars.
Little research has been performed in order to identify evaluation measures that overcome the shortcomings of measures (1)-(3). Gaudette and Japkovicz <NO> acknowledge that these and other measures are somehow problematic but do not concretely propose alternatives. Waegeman et al. <NO> instead propose an evaluation method based on ROC analysis. The problem with their method is that, like all methods based on ROC analysis, it is more apt to evaluate the ability of a classifier at correctly ranking the objects (i.e., at placing 5 stars reviews higher than 4 stars reviews) than to evaluate the ability of the classifier to classify an object into its true (or into a nearby) class. In other words, the ROC measure of <NO> does not reward the ability of a learning device to correctly identify the thresholds τj that separate a class yj from its successor class yj+1, for all j = 1, . . . , (n− 1)."	https://doi.org/10.1109/ISDA.2009.230
52	Tuning data mining methods for cost-sensitive regression: A study in loan charge-off forecasting	[]	Gaurav Bansal, Atish P. Sinha, and Huimin Zhao. 2008. Tuning data mining methods for cost-sensitive regression: A study in loan charge-off forecasting. J. Manag. Inform. Syst. 25, 3 (2008), 315–336.			https://doi.org/10.2753/MIS0742-1222250309
53	Strategies for learning in class imbalance problems	['learning toward the minority class <NO>–<NO>.']	Ricardo Barandela, José Salvador Sánchez, Vicente Garcia, and Edgar Rangel. 2003. Strategies for learning in class imbalance problems. Pattern Recogn. 36, 3 (2003), 849–851.			http://scholar.google.com/scholar?hl=en&q=Ricardo+Barandela%2C+Jos%C3%A9+Salvador+S%C3%A1nchez%2C+Vicente+Garcia%2C+and+Edgar+Rangel.+2003.+Strategies+for+learning+in+class+imbalance+problems.+Pattern+Recogn.+36%2C+3+%282003%29%2C+849%2D%2D851.
54	Active learning for one-class classification	[]	Vincent Barnab-Lortie, Colin Bellinger, and Nathalie Japkowicz. 2015. Active learning for one-class classification. Proceedings of ICMLA’2015.			http://scholar.google.com/scholar?hl=en&q=Vincent+Barnab-Lortie%2C+Colin+Bellinger%2C+and+Nathalie+Japkowicz.+2015.+Active+learning+for+one-class+classification.+In+Proceedings+of+ICMLA%272015.
55	MWMOTE-majority weighted minority oversampling technique for imbalanced data set learning	[]	Sukarna Barua, Monirul Islam, Xin Yao, and Kazuyuki Murase. 2012. MWMOTE-majority weighted minority oversampling technique for imbalanced data set learning. IEEE Transactions on Knowledge and Data Engineering (2012), 1.			https://doi.org/10.1109/TKDE.2012.232
56	An experimental design to evaluate class imbalance treatment methods	[]	Guilherme Batista, Danilo Silva, and Ronaldo Prati. 2012. An experimental design to evaluate class imbalance treatment methods. 2012 11th International Conference on Machine Learning and Applications (ICMLA), Vol. 2. IEEE, 95–101.	In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition applications; therefore, these research communities have responded to such interest with literally dozens of methods and techniques. Surprisingly, there are still many fundamental open-ended questions such as “Are all learning paradigms equally affected by class imbalance?”, “What is the expected performance loss for different imbalance degrees?” and “How much of the performance losses can be recovered by the treatment methods?”. In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods. This experimental setup uses real data sets with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We employ such experimental design in a large-scale experimental evaluation with twenty-two data sets and seven learning algorithms from different paradigms. Our results indicate that the expected performance loss, as a percentage of the performance obtained with the balanced distribution, is quite modest (below 5%) for the most balanced distributions up to 10% of minority examples. However, the loss tends to increase quickly for higher degrees of class imbalance, reaching 20% for 1% of minority class examples. Support Vector Machine is the classifier paradigm that is less affected by class imbalance, being almost insensitive to all but the most imbalanced distributions. Finally, we show that the sampling algorithms only partially recover the performance losses. On average, typically about 30% or less of the performance that was lost due to class imbalance was recovered by random oversampling and SMOTE. 	"Due to lack of space we assume the reader is familiar with the class imbalance problem and methods. In the case this assumption does not hold, the literature has several comprehensible surveys, we recommend <NO>. For the same reason, we are not able to include here all numerical results obtained in our experiments. Therefore, we decided to create a paper website <NO> that has detailed results, including tables and data; however, we note this paper is totally selfcontained.
Our experimental design is inspired by the design used in <NO>. The central idea is to generate several training set
distributions with increasing degrees of class imbalance while maintaining the training sets with a fixed number of examples. Our training set distributions range from the balanced distribution (denoted as 50/501) to the imbalanced distributions of 1/99 and 99/1. In order to generate these class distributions, we restricted the training sets to have a total number of instances equals to the number of instances of the minority class. We avoid using extremely imbalanced data sets; in particular, the combination of a small data set with large class imbalance would result in a training set with too few examples. We return to this discussion in Section V, where we comment possible limitations of this work.
The test sets have the naturally occurring class distributions. We reserved 25% of the original data as test set using a stratified sample, and did not touch this portion of the data until the final evaluation; the remaining 75% was used as training set, with class distributions manipulated as previously described. In order to reduce variance and increase statistical significance, we repeated the whole process 100 times using different train and test sample partitions.
For this specific study we assembled a database with twenty data sets. Since we want to promote reproducibility, most of them are public domain benchmark data sets available in repositories such as UCI Machine Learning Repository <NO> or used in projects like Statlog <NO>. A few data sets are not public domain, but we decided to include them since they are frequently used in studies involving class imbalance. These data sets include tumour identification in mammography images <NO>, <NO>. We also included data sets obtained in past research, and we make them publicly available for the first time in the paper website.
We use the area under the ROC curve (AUC) <NO> as the main measure to assess our results. Since we chose AUC and also because we want to control the degree of class imbalance, we converted all non-binary class data sets into binary class, associating a positive label to one of the classes (usually the minority one) and aggregating all remaining classes into a negative class. Table I presents a summarised description of the data sets included in our study. The table lists the data sets full names, as well as a short identifier used in this paper, the number of instances and attributes, labels associated with the positive and negative classes and attribute class distribution. The data sets are listed in increasing order of class imbalance.
Two data sets resulted in two entries each in Table I, because different classes were used as positive class. For the Letter dataset, Letter-a is the variation in which the positive class is the original letter “a” class; and Letter-vowel has the positive class composed by all classes that represent vowels. For the Splice-junction Gene Sequences dataset, one entry has the intron-exon (“ie”) boundaries as positive class
1We use the notation X/Y , with X + Y = 100 to denote that for a set of 100 instances, X belongs to the positive class and Y belongs to the negative class.
and the other has the exon-intron (“ei”) boundaries. The final number of data sets is twenty-two, considering the four entries generated from these two data sets.
We use this experimental design to answer the questions raised in abstract and introduction. We start discussing the expected performance loss for different learning paradigms in the next section."	https://doi.org/10.1109/ICMLA.2012.162
57	A study of the behavior of several methods for balancing machine learning training data	['Furthermore, there exist several comparisons between different external techniques in different frameworks <NO>, <NO>, <NO>.', '5 <NO> as base classifier for our experiments since it has been widely used in imbalanced domains <NO>, <NO>–<NO>; besides, most of the proposals we are studying were tested with C4.', '2) Data level (or external) approaches rebalance the class distribution by resampling the data space <NO>, <NO>, <NO>,', 'empirically proved that the application of a preprocessing step in order to balance the class distribution is usually a positive solution <NO>, <NO>.', 'Previous works have shown the positive synergy of this combination leading to significant improvements <NO>, <NO>.', 'The Wilcoxon test shows, in concordance with previous studies <NO>, <NO>, that making use of SMOTE as a preprocessing technique significantly outperforms C4.', 'So the traditional classification algorithms cannot achieve ideal effects because positive class is the more valuable class <NO> <NO>.', 'Drummond <NO> proposed that the performance of classifiers which are built based on under-sampling technology is superior to the performance of classifiers which are built based on over-sampling technology, Chris Seiffert <NO> put forward a similar view from the model training complexity and training time, GEBatista <NO> thought that over-sampling technique was better than under-sampling techniques when there are overlaps in the data-set.', 'Some sampling methods first use clustering to partition the data set and then apply undersampling and/or oversampling on different partitions’ data <NO>, <NO>, <NO>.', 'A cluster-based oversampling method was proposed in <NO>, which randomly oversampled both the minority class and majority class samples in such a way that all clusters became the same size.', 'There is a significant body of research comparing the various sampling methods <NO>.', 'The main drawback associated with undersampling is the loss of information that comes with deleting examples from the training data <NO>.', 'However, the training datasets are not always balance, and imbalanced class distribution problem in real applications hinders the application of the traditional classifiers <NO>.', 'Some other methods combine different sampling strategies to achieve further improvement <NO>.', 'Many researchers, suach as Batista, G <NO>, Estabrooks <NO> and Japkowicz <NO>, proposes various approach about two resampling strategies.', 'Some studies have shown that the degradation of classification performance attributed to imbalanced data is not necessarily the result of relative imbalances but rather due to the lack of representative examples (absolute imbalances) <NO>, <NO>, <NO>, <NO>, <NO>.', 'Other popular approaches include the DataBoost-IM (imbalanced learning) <NO>, AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (JOUS-Boost approach) <NO>, and the integration of SMOTE with Tomek links and edited nearest neighbor <NO>.', 'Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>.', 'However, they do not imply that classifiers cannot learn from imbalanced data sets; on the contrary, studies have also shown that classifiers induced from certain imbalanced data sets are comparable to classifiers induced from the same data set balanced by sampling techniques <NO>, <NO>.', 'Some representative work in this area includes the OSS method <NO>, the condensed nearest neighbor rule and Tomek Links ðCNNþTomek LinksÞ integration method <NO>, the neighborhood cleaning rule (NCL) <NO> based on the edited nearest neighbor (ENN) rule—which removes examples that differ from two of its three nearest neighbors, and the integrations of SMOTE with ENN ðSMOTEþENNÞ and SMOTE with Tomek links ðSMOTEþTomekÞ <NO>.', 'Some representative work in this area includes the OSS method <NO>, the condensed nearest neighbor rule and Tomek Links ðCNNþTomek LinksÞ integration method <NO>, the neighborhood cleaning rule (NCL) <NO> based on the edited nearest neighbor (ENN) rule—which removes examples that differ from two of its three nearest neighbors, and the integrations of SMOTE with ENN ðSMOTEþENNÞ and SMOTE with Tomek links ðSMOTEþTomekÞ <NO>.']	Gustavo E.A.P.A. Batista, Ronaldo C. Prati, and Maria Carolina Monard. 2004. A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 20–29.	There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.	"Learning from imbalanced data sets is often reported as being a difficult task. In order to better understand this problem, imagine the situation illustrated in Figure 1. In Fig. 1(a) there is a large imbalance between the majority class (-) and the minority class (+), and the data set presents some degree of class overlapping. A much more comfortable situation for learning is represented in Fig. 1(b), where the classes are balanced with well-defined clusters.
In a situation similar to the one illustrated in Fig. 1(a), spare cases from the minority class may confuse a classifier like k-Nearest Neighbor (k-NN). For instance, 1-NN may incorrectly classify many cases from the minority class because the nearest neighbors of these cases are examples belonging to the majority class. In a situation where the imbalance is very high, the probability of the nearest neighbor of a minority class case is a case of the majority class is likely to be high, and the minority class error rate will tend to have high values, which is unacceptable.
Figure 1: Many negative cases against some spare positive cases (a) balanced data set with well-defined clusters (b).
Decision trees also experience a similar problem. In the presence of class overlapping, decision trees may need to create many tests to distinguish the minority class cases from majority class cases. Pruning the decision tree might not necessarily alleviate the problem. This is due to the fact that pruning removes some branches considered too specialized, labelling new leaf nodes with the dominant class on this node. Thus, there is a high probability that the majority class will also be the dominant class of those leaf nodes."	https://doi.org/10.1145/1007730.1007735
58	A new performance measure for class imbalance learning	[]	Rukshan Batuwita, and Vasile Palade. 2009. A new performance measure for class imbalance learning. Application to bioinformatics problems. In International Conference on Machine Learning and Applications, 2009. ICMLA’09. IEEE, 545–550.			https://doi.org/10.1109/ICMLA.2009.126
59	Efficient resampling methods for training support vector machines with imbalanced datasets	[]	Rukshan Batuwita, and Vasile Palade. 2010. Efficient resampling methods for training support vector machines with imbalanced datasets. The 2010 International Joint Conference on Neural Networks (IJCNN). IEEE, 1–8.			http://scholar.google.com/scholar?hl=en&q=Rukshan+Batuwita+and+Vasile+Palade.+2010a.+Efficient+resampling+methods+for+training+support+vector+machines+with+imbalanced+datasets.+In+The+2010+International+Joint+Conference+on+Neural+Networks+%28IJCNN%29.+IEEE%2C+1%2D%2D8.
60	FSVM-CIL: Fuzzy support vector machines for class imbalance learning	[]	Rukshan Batuwita, and Vasile Palade. 2010. FSVM-CIL: Fuzzy support vector machines for class imbalance learning. IEEE Trans. Fuzzy Syst. 18, 3 (2010), 558–571.	Support vector machines (SVMs) is a popular machine learning technique, which works effectively with balanced datasets. However, when it comes to imbalanced datasets, SVMs produce suboptimal classification models. On the other hand, the SVM algorithm is sensitive to outliers and noise present in the datasets. Therefore, although the existing class imbalance learning (CIL) methods can make SVMs less sensitive to class imbalance, they can still suffer from the problem of outliers and noise. Fuzzy SVMs (FSVMs) is a variant of the SVM algorithm, which has been proposed to handle the problem of outliers and noise. In FSVMs, training examples are assigned different fuzzy-membership values based on their importance, and these membership values are incorporated into the SVM learning algorithm to make it less sensitive to outliers and noise. However, like the normal SVM algorithm, FSVMs can also suffer from the problem of class imbalance. In this paper, we present a method to improve FSVMs for CIL (called FSVM-CIL), which can be used to handle the class imbalance problem in the presence of outliers and noise. We thoroughly evaluated the proposed FSVM-CIL method on ten real-world imbalanced datasets and compared its performance with five existing CIL methods, which are available for normal SVM training. Based on the overall results, we can conclude that the proposed FSVMCIL method is a very effective method for CIL, especially in the presence of outliers and noise in datasets.	"In this section, we briefly review the learning algorithm of SVMs, which has been initially proposed in <NO>, <NO>. Consider we have a binary classification problem, which is represented by a dataset {(x1 , y1), (x2 , y2), . . . , (xl,yl)}, where xi ∈ n represents an n-dimensional data point, and yi ∈ {−1, 1} represents the class of that data point, for i = 1, . . . , l. The goal of the SVM learning algorithm is to find a separating hyperplane that separates these data points into two classes. In order to find a better separation of classes, the data are first transformed into a higher dimensional feature space by a mapping function Φ. Then, a possible separating hyperplane, which resides in the higher dimensional feature space, can be represented by
w · Φ(x) + b = 0. (1) If the dataset is completely linearly separable, the separating hyperplane with the maximum margin can be found by solving the following maximal-margin optimization problem:
Min (
1 2 w · w
)
s.t. yi(w · Φ(xi) + b) ≥ 1 i = 1, . . . , l. (2)
However, in most real-world problems, the datasets are not completely linearly separable, although they are mapped into a higher dimensional feature space. Therefore, the constrains in the aforementioned optimization problem in (2) are relaxed by introducing a slack variable εi ≥ 0, and then, the soft-margin optimization problem is formulated as follows:
Min (
1 2 w · w + C l∑ i=1 εi
)
s.t. yi(w · Φ(xi) + b) ≥ 1 − εi εi ≥ 0, i = 1, . . . , l. (3)
The slack variables εi > 0 hold for misclassified examples, and therefore, ∑l i=1 εi can be thought of as a measure of the amount of misclassifications. This new objective function in (3) has two goals. One is to maximize the margin, and the other one is to minimize the number of misclassifications. The parameter C controls the tradeoff between these two goals, and it can also be treated as the misclassification cost of a training
example. This quadratic-optimization problem can be solved by constructing a Lagrangian representation and transforming it into the following dual problem:
Max W (α) = l∑
i=1
αi − 1 2 l∑ i=1 l∑ j=1 αiαjyiyjΦ(xi) · Φ(xj )
s.t. l∑
i=1
yiαi = 0, 0 ≤ αi ≤ C, i = 1, . . . , l (4)
where αis are Lagrange multipliers, which should satisfy the following Karush–Kuhn–Tucker (KKT) conditions:
αi(yi(w · Φ(xi) + b) − 1 + εi) = 0, i = 1, . . . , l (5) (C − αi)ξi = 0, i = 1, . . . , l. (6)
An important property of SVMs is that it is not necessary to know the mapping function Φ(x) explicitly. By applying a kernel function, such that K(xi, xj ) = Φ(xi) · Φ(xj ), we would be able to transform the dual-optimization problem in (4) into
MaxW (α) = l∑
i=1
αi − 1 2 l∑ i=1 l∑ j=1 αiαjyiyjK(xi, xj )
s.t. l∑
i=1
yiαi = 0, 0 ≤ αi ≤ C, i = 1, . . . , l. (7)
By solving (7) and finding the optimal values for αi , w can be recovered as follows:
w = l∑
i=1
αiyiΦ(xi) (8)
and b can be determined from the KKT conditions in (5). The data points having nonzero αi values are called support vectors. Finally, the SVM decision function is given by
f(x) = sign(w · Φ(x) + b) = sign ( l∑
i=1
αiyiK(xi, x) + b ) .
(9)"	https://doi.org/10.1109/TFUZZ.2010.2042721
61	Adjusted geometric-mean: A novel performance measure for imbalanced bioinformatics datasets learning	[]	Rukshan Batuwita, and Vasile Palade. 2012. Adjusted geometric-mean: A novel performance measure for imbalanced bioinformatics datasets learning. J. Bioinform. Comput. Biol. 10, 4 (2012).			http://scholar.google.com/scholar?hl=en&q=Rukshan+Batuwita+and+Vasile+Palade.+2012.+Adjusted+geometric-mean%3A+A+novel+performance+measure+for+imbalanced+bioinformatics+datasets+learning.+J.+Bioinform.+Comput.+Biol.+10%2C+4+%282012%29.
62	Synthetic oversampling for advanced radioactive threat detection	[]	Colin Bellinger, Nathalie Japkowicz, and Christopher Drummond. 2015. Synthetic oversampling for advanced radioactive threat detection. Proceedings ICML’2015.			http://scholar.google.com/scholar?hl=en&q=Colin+Bellinger%2C+Nathalie+Japkowicz%2C+and+Christopher+Drummond.+2015.+Synthetic+oversampling+for+advanced+radioactive+threat+detection.+In+Proceedings+ICML%272015.
63	One-class versus binary classification: Which and when? In 2012 11th International Conference on Machine Learning and Applications (ICMLA), Vol	[]	Colin Bellinger, Shiven Sharma, and Nathalie Japkowicz. 2012. One-class versus binary classification: Which and when? In 2012 11th International Conference on Machine Learning and Applications (ICMLA), Vol. 2. IEEE, 102–106.			https://doi.org/10.1109/ICMLA.2012.212
64	Regression error characteristic curves	['We present the notion of Regression Error Characteristic (REC) surfaces, that are a generalization of REC curves <NO>.', 'Bi and Bennet <NO> have presented REC curves.', 'to non-experts and it is possible to obtain the same quantitative information given by prediction error statistics by calculating the Area Over the Curve (AOC), which Bi and Bennet <NO> have proved to be a biased estimate of the expected error of a model.', 'In order to plot a REC surface we use an algorithm with some similarities to the one presented by Bi and Bennet <NO> for REC curves.', 'The same observation was made by Bi and Bennet <NO> regarding REC curves.', 'If we want to compare the performance of several models over the same Y range, we plot their respective partial REC curves and then proceed with the same analysis as with any REC curve <NO>.', 'Moreover, they allow easy comparison of the performance of several models using the same sort of analysis described by Bi and Bennet <NO>.']	Jinbo Bi, and Kristin P. Bennett. 2003. Regression error characteristic curves. Proc. of the 20th Int. Conf. on Machine Learning. 43–50.			http://scholar.google.com/scholar?hl=en&q=Jinbo+Bi+and+Kristin+P.+Bennett.+2003.+Regression+error+characteristic+curves.+In+Proc.+of+the+20th+Int.+Conf.+on+Machine+Learning.+43%2D%2D50.
65	Neighbourhood sampling in bagging for imbalanced data	[]	Jerzy Błaszczyński, and Jerzy Stefanowski. 2015. Neighbourhood sampling in bagging for imbalanced data. Neurocomputing 150 (2015), 529–542.			http://scholar.google.com/scholar?hl=en&q=Jerzy+B%C5%82aszczy%C5%84ski+and+Jerzy+Stefanowski.+2015.+Neighbourhood+sampling+in+bagging+for+imbalanced+data.+Neurocomputing+150+%282015%29%2C+529%2D%2D542.
66	The use of the area under the ROC curve in the evaluation of machine learning algorithms	['To do so, according to the imbalance framework, we use the area under the ROC curve (AUC) <NO>, <NO> as the evaluation criterion.', 'Some other evaluation measures, such as recall, precision, F-measure, G-mean and Receiver Operation Characteristic (ROC) Curve Analysis, are then explored and proposed as more proper evaluation measures <NO>.', 'Another often used method for evaluating classification of unbalanced data is ROC analysis <NO>.', 'Furthermore, an area under a receiver operating characteristic curve (AUC-ROC) can also indicate a balanced classification ability between sensitivity and specificity as a function of varying a classification threshold <NO>', 'Often, the area under the curve is used as a simple metric to define how an algorithm performs over the whole space (Bradley, 1997; Davis et al., 2005; Goadrich et al., 2004; Kok & Domingos, 2005; Macskassy & Provost, 2005; Singla & Domingos, 2005).', 'The AUC can be interpreted as the probability of ranking a true positive example ahead of a false positive when ordering examples according to decreasing likelihood of being positive <NO>.', 'One reason for choosing to compare models with respect to AUC instead of accuracy is that the former is not sensitive to differences between the class distribution of the training examples and of the examples on which the model is applied <NO>.', 'In this paper, we use F-measure, G-mean, and AUC <NO> as performance evaluation measures.', 'Consequently, robust evaluation techniques like the ROC convex hull method <NO> or the area under the ROC curve <NO> have been proposed to enable classifier assessment in accordance with managerial objectives.', 'A quantitative representation of a ROC curve is the area under it (AUC) <NO>.', 'Perhaps the most common is ROC analysis and the associated use of the area under the ROC curve (AUC) to assess overall classification performance <NO>.']	Andrew P. Bradley. 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recogn. 30, 7 (1997), 1145–1159.			https://doi.org/10.1016/S0031-3203(96)00142-2
67	Re-sampling Approaches for Regression Tasks under Imbalanced Domains	[]	Paula Branco. 2014. Re-sampling Approaches for Regression Tasks under Imbalanced Domains. Master’s thesis. Dept. Computer Science, Faculty of Sciences, University of Porto.			http://scholar.google.com/scholar?hl=en&q=Paula+Branco.+2014.+Re-sampling+Approaches+for+Regression+Tasks+under+Imbalanced+Domains.+Master%27s+thesis.+Dept.+Computer+Science%2C+Faculty+of+Sciences%2C+University+of+Porto.
68	Classification and regression trees	['Probability estimation trees (PETs) <NO> are classification trees <NO> that have a class probability distribution at each leaf instead of only a single class label.', '1) CART: classification and regression trees <NO>.', 'In particular, Breiman <NO> establishes the connection between the distribution of training-set examples, the prior probability of each class, the costs of mistakes on each class and the placement of the decision threshold.']	Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Classification and regression trees. Wadsworth & Brooks, Monterey, CA (1984).			http://scholar.google.com/scholar?hl=en&q=Leo+Breiman%2C+Jerome+H.+Friedman%2C+Richard+A.+Olshen%2C+and+Charles+J.+Stone.+1984.+Classification+and+regression+trees.+Wadsworth+%26+Brooks%2C+Monterey%2C+CA+%281984%29.
69	Safe-level-smote: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem	[]	Chumphol Bunkhumpornpat, Krung Sinapiromsaran, and Chidchanok Lursinsap. 2009. Safe-level-smote: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem. Advances in Knowledge Discovery and Data Mining. Springer, 475–482.			https://doi.org/10.1007/978-3-642-01307-2_43
70	MUTE: Majority under-sampling technique	[]	Chumphol Bunkhumpornpat, Krung Sinapiromsaran, and Chidchanok Lursinsap. 2011. MUTE: Majority under-sampling technique. 2011 8th International Conference on Information, Communications and Signal Processing (ICICS). IEEE, 1–4.			http://scholar.google.com/scholar?hl=en&q=Chumphol+Bunkhumpornpat%2C+Krung+Sinapiromsaran%2C+and+Chidchanok+Lursinsap.+2011.+MUTE%3A+Majority+under-sampling+technique.+In+2011+8th+International+Conference+on+Information%2C+Communications+and+Signal+Processing+%28ICICS%29.+IEEE%2C+1%2D%2D4.
71	DBSMOTE: Density-based synthetic minority over-sampling technique	[]	Chumphol Bunkhumpornpat, Krung Sinapiromsaran, and Chidchanok Lursinsap. 2012. DBSMOTE: Density-based synthetic minority over-sampling technique. Applied Intelligence 36, 3 (2012), 664–684.			https://doi.org/10.1007/s10489-011-0287-y
72	Safe level graph for synthetic minority over-sampling techniques	[]	Chumphol Bunkhumpornpat, and Sitthichoke Subpaiboonkit. 2013. Safe level graph for synthetic minority over-sampling techniques. 2013 13th International Symposium on Communications and Information Technologies (ISCIT). IEEE, 570–575.			http://scholar.google.com/scholar?hl=en&q=Chumphol+Bunkhumpornpat+and+Sitthichoke+Subpaiboonkit.+2013.+Safe+level+graph+for+synthetic+minority+over-sampling+techniques.+In+2013+13th+International+Symposium+on+Communications+and+Information+Technologies+%28ISCIT%29.+IEEE%2C+570%2D%2D575.
73	Real estate price prediction under asymmetric loss	[]	Michael Cain, and Christian Janssen. 1995. Real estate price prediction under asymmetric loss. Ann. Inst. Stat. Math. 47, 3 (1995), 401–414.			http://scholar.google.com/scholar?hl=en&q=Michael+Cain+and+Christian+Janssen.+1995.+Real+estate+price+prediction+under+asymmetric+loss.+Ann.+Inst.+Stat.+Math.+47%2C+3+%281995%29%2C+401%2D%2D414.
74	A PSO-based cost-sensitive neural network for imbalanced data classification	[]	Peng Cao, Dazhe Zhao, and Osmar R. Zaı̈ane 2013. A PSO-based cost-sensitive neural network for imbalanced data classification. In Trends and Applications in Knowledge Discovery and Data			https://doi.org/10.1007/978-3-642-40319-4_39
75	Novel cost-sensitive approach to improve the multilayer perceptron performance on imbalanced data	[]	Cristiano Leite Castro, and Antônio de Pádua Braga. 2013. Novel cost-sensitive approach to improve the multilayer perceptron performance on imbalanced data. IEEE Trans. Neur. Netw. Learn. Syst. 24, 6 (2013), 888–899.			http://scholar.google.com/scholar?hl=en&q=Cristiano+Leite+Castro+and+Ant%C3%B4nio+de+P%C3%A1dua+Braga.+2013.+Novel+cost-sensitive+approach+to+improve+the+multilayer+perceptron+performance+on+imbalanced+data.+IEEE+Trans.+Neur.+Netw.+Learn.+Syst.+24%2C+6+%282013%29%2C+888%2D%2D899.
76	Statistical learning for effective visual information retrieval	[', Asymmetric Bagging <NO> and QuasiBagging <NO>.']	Edward Y. Chang, Beitao Li, Gang Wu, and Kingshy Goh. 2003. Statistical learning for effective visual information retrieval. ICIP (3). 609–612.			http://scholar.google.com/scholar?hl=en&q=Edward+Y.+Chang%2C+Beitao+Li%2C+Gang+Wu%2C+and+Kingshy+Goh.+2003.+Statistical+learning+for+effective+visual+information+retrieval.+In+ICIP+%283%29.+609%2D%2D612.
77	Addressing imbalance in multilabel classification: Measures and random resampling	[]	Francisco Charte, Antonio J. Rivera, Marı́a J. del Jesus, and Francisco Herrera 2015. Addressing imbalance in multilabel classification: Measures and random resampling. algorithms. Neurocomputing			https://doi.org/10.1016/j.neucom.2014.08.091
78	2015b. MLSMOTE: Approaching imbalanced multilabel learning through synthetic instance generation	[]	Francisco Charte, Antonio J. Rivera, Marı́a J. del Jesus, and Francisco Herrera 2015. 2015b. MLSMOTE: Approaching imbalanced multilabel learning through synthetic instance generation			https://doi.org/10.1016/j.knosys.2015.07.019
79	SMOTE: Synthetic minority over-sampling technique	['Many works have been developed studying the suitability of data preprocessing techniques to deal with imbalanced data-sets <NO>, <NO>, <NO>.', '<NO>: It is an oversampling method, whose main idea is to create new minority class examples by interpolating several minority class instances that lie together.', 'Among them undersampling (US) and oversampling method (SMOTE <NO>) are examples of resampling techniques.', 'Oversampling <NO> the minority class and undersampling <NO> the majority class are the data level approaches.', 'SMOTE algorithm <NO> is a classic oversampling algorithm.', 'SMOTE <NO> is another over-sampling method which focuses on the generation of new synthetic minority examples rather than directly duplicating the existing ones.', '<NO> used a Synthetic Minority Oversampling TEchnique (SMOTE) <NO> oversampling and also a random undersampling for SVM modeling on an imbalanced intestinal-contraction-detection task.', '2) SVM-SMOTE: SVM-SMOTE adopts the SMOTE algorithm <NO> to generate more pseudopositive samples and then builds an SVM on the oversampling data set <NO>.', 'The second is to resample the original training dataset, either by over-sampling the minority class and/or under-sampling the majority class until the classes are approximately equally represented <NO>.', 'They applied <NO>’s SMOTE algorithm to oversample the data and trained SVM with different error costs.', 'One of the famous over-sampling methods is the synthetic minority over-sampling technique (SMOTE) <NO>.', '<NO> 2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery', 'Furthermore, it tends to lead to overfitting since it repeats minority class examples <NO>, <NO>.', 'SMOTE <NO> added new synthetic minority class examples by randomly interpolating pairs of closest neighbors in the minority class.', 'SMOTE adds synthetic minority class examples <NO>.', 'Details for implementing SMOTE and SMOTE-NC can be found in <NO>.', 'For example, the work on cost sensitive learning <NO> aimed at reducing total cost, and sampling approaches <NO> to favour the minority class are usually demonstrated with decision tree algorithms and/or naive Bayes.', 'Recent work on skewed data sets was evaluated using better performance metrics such as Area Under Curve (AUC) <NO>, cost curves <NO>, and Receiver Operating Characteristic (ROC) analysis <NO>.', 'But for Experiment XII, the minority class is innovatively oversampled by using the Synthetic Minority Oversampling TEchnique (SMOTE) <NO>.', 'LIMITATIONS This paper should have used superior approaches such as SMOTE <NO> to create the oversampled data partitions, and multiresponse model trees <NO> as the meta-classifier.', 'Typically, this is done by under-sampling the larger class <NO> or by over-sampling the smaller one <NO> or by combination of these techniques <NO>.', 'Above we show the effects of two popular strategies for handling imbalance – undersamping/bagging and SMOTE <NO> – on both classification performance (left panels) and probability estimation (right panels) over 16 datasets.', 'The top and bottom panels show the improvements achieved over standard SVM achieved using undersampling/bagging and SMOTE <NO>, respectively.', 'This can be seen in Figure 1, which shows the results achieved using two popular methods for handling imbalance – undersampling/bagging and SMOTE <NO> – both in terms of classification and probability estimation for minority instances over 16 imbalanced datasets.', 'Both of the methods for handling imbalance shown (bagging classifiers induced on undersampled training datasets <NO> and SMOTE <NO>) improve the sensitivity of the model, as can be seen in the left-hand sub-plots.', 'These data sets include tumour identification in mammography images <NO>, <NO>.', 'We chose to analyse Random Over-sampling and SMOTE <NO> for the following reasons: first, our previous experience with sampling methods <NO> shows that these two methods outperform several other (over and under) sampling methods; second, SMOTE has became one of the most popular sampling methods for class imbalance, and it is frequently used as counterpart in empirical evaluations and has several proposed extensions and variations (for instance <NO>, <NO>).', 'For example, the synthetic minority oversampling technique (SMOTE) algorithm <NO> was proposed to search for the nearest neighbors of every minority instance and generates synthetic minority data by calculating linear interpolations between an original minority class instance and a randomly selected neighbor.', 'Before presenting the details of the RAMOBoost algorithm, we briefly discuss the data generation mechanisms of SMOTE <NO> and ADASYN <NO>, which motivated the work presented', 'Motivated by the SMOTE <NO>, SMOTEBoost <NO>, and ADASYN <NO> algorithms, RAMOBoost facilitates imbalanced learning by iteratively developing an ensemble of hypotheses.', 'Here we follow the suggestions of <NO>, <NO>, and <NO> and use the minority class as the positive class and majority class as the negative class.', 'It is possible to extend RAMOBoost to handle datasets with nominal features by adopting the techniques used in the SMOTE-N method <NO>.', 'Consider the “Mammography Data Set,” a collection of images acquired from a series of mammography exams performed on a set of distinct patients, which has been widely used in the analysis of algorithms addressing the imbalanced learning problem <NO>, <NO>, <NO>.', 'In reality, we find that classifiers tend to provide a severely imbalanced degree of accuracy, with the majority class having close to 100 percent accuracy and the minority class having accuracies of 0-10 percent, for instance <NO>, <NO>.', 'In regards to synthetic sampling, the synthetic minority oversampling technique (SMOTE) is a powerful method that has shown a great deal of success in various applications <NO>.', 'In <NO> Bagging has better results than Boosting <NO>, Majority Bagging <NO>, SMOTE <NO> and a combination of undersampling and SMOTE <NO>.', 'Worse yet, because over-sampling often involves making exact copies of examples, it may lead to overfitting <NO>.', 'SMOTE <NO>, on the other hand, over-samples by introducing new, non-replicated minorityclass examples.', 'However, this could be addressed by generating new examples using a technique such as SMOTE <NO>.', 'Instead of changing the distribution of training data by updating the weights associated with each example, SMOTEBoost alters the distribution by adding new minority-class examples using the SMOTE algorithm <NO>.', 'One practical consideration is that the “precise relationship among these things is complex and task- and methodspecific” <NO>.', 'Synthetic minority oversampling technique (SMOTE) <NO> is an oversampling method, where new synthetic examples are generated in the neighbor-', 'Although some studies have shown that oversampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, it should be noted that oversampling usually increases the training time and may lead to overfitting since it involves making exact copies of examples <NO>, <NO>.', 'The detailed description of the algorithm can be found in <NO>.', 'cluster-based oversampling <NO>, Synthetic Minority Oversampling Technique (SMOTE) <NO>, and Borderline-SMOTE <NO>.', 'The “SMOTE,” <NO> creates new artificial (synthetic) minority data instances rather than simply duplicating from the existing instances.']	Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W.P. Kegelmeyer. 2002. SMOTE: Synthetic minority over-sampling technique. JAIR 16 (2002), 321–357.			https://doi.org/10.5555/1622407.1622416
80	Automatically countering imbalance and its empirical relationship to cost	['transformations (by adding costs to instances) and algorithm level modifications (by modifying the learning process to accept costs) <NO>, <NO>, <NO>.', 'Although it is impossible to predict what true class distribution should be <NO>, <NO> it is observed that classifiers learn well from a balanced distribution than from an imbalanced one <NO>, <NO>, <NO>.', '<NO> perform a detailed evaluation of a wrapper-based sampling approach to minimize misclassification cost.', 'This experiment is motivated by <NO>, which suggested', 'In <NO> Bagging has better results than Boosting <NO>, Majority Bagging <NO>, SMOTE <NO> and a combination of undersampling and SMOTE <NO>.']	Nitesh V. Chawla, David A. Cieslak, Lawrence O. Hall, and Ajay Joshi. 2008. Automatically countering imbalance and its empirical relationship to cost. Data Min. Knowl. Discov. 17, 2 (2008), 225–252.			https://doi.org/10.1007/s10618-008-0087-0
81	Wrapper-based computation and evaluation of sampling methods for imbalanced datasets	[]	Nitesh V. Chawla, Lawrence O. Hall, and Ajay Joshi. 2005. Wrapper-based computation and evaluation of sampling methods for imbalanced datasets. Proceedings of the 1st International Workshop on UtilityBased Data Mining. ACM, New York, NY, 24–33.	Learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. When the imbalance is large, classification accuracy on the smaller class(es) tends to be lower. In particular, when a class is of great interest but occurs relatively rarely such as cases of fraud, instances of disease, and regions of interest in largescale simulations, it is important to accurately identify it. It then becomes more costly to misclassify the interesting class. In this paper, we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (SMOTE) to improve minority class accuracy. The f-value serves as the evaluation function. Experimental results show the wrapper approach is effective in optimization of the composite f-value, and reduces the average cost per test example for the datasets considered. We report both average cost per test example and the cost curves in the paper. The true positive rate of the minority class increases significantly without causing a significant change in the f-value. We also obtain the lowest cost per test example, compared to any result we are aware of for the KDD Cup-99 intrusion detection data set.	"Researchers in the machine learning community have dealt with the problem of class imbalance by using various approaches like over-sampling the minority classes, undersampling the majority classes, assigning different costs for different misclassification errors, learning by recognition as opposed to discrimination, etc <NO>. There is a significant body of research comparing the various sampling methods <NO>. Sampling strategies have almost become the de facto standard for countering the imbalance in datasets <NO>. With all this there is still no answer on how to do the sampling required for obtaining good classifier accuracies on minority classes.
There are a number of different approaches that can be applied to build classifiers on imbalanced data sets. In this work, we examined under sampling and over-sampling by creating synthetic examples of minority classes. Under-
sampling the majority class can reduce the bias of the learned classifier towards it and thus improve the accuracy on the minority classes.
Some studies <NO> have been done which combined under-sampling of majority classes with over sampling by replication of minority classes. While Japkowicz <NO> found this approach very effective, Ling and Li <NO> were not able to get significant improvement in their performance measures. Japkowicz experimented with only one-dimensional artificial data of varying complexity whereas Ling and Li used real data from a Direct Marketing problem. This might have been the reason for the discrepancy between their results. On the whole, from the body of literature, it was found that under-sampling of majority classes was better than over-sampling with replication of minority classes <NO> and that the combination of the two did not significantly improve the performance over under sampling alone.
Chawla et al. <NO> introduced a new over-sampling approach for two class problems that over-sampled the minority class by creating synthetic examples rather than replicating examples. They pointed out the limitation of oversampling with replication in terms of the decision regions in feature space for decision trees. They showed that as the minority class was over sampled by increasing amounts, for decision trees, the result was to identify similar but more specific regions in the feature space. A preferable approach is to build generalized regions around minority class examples.
The synthetic minority over-sampling technique (SMOTE) was introduced to provide synthetic minority class examples which were not identical but came from the same region in feature space. The over-sampling was done by selecting each minority class example and creating a synthetic example along the line segment joining the selected example and any/all of the k minority class nearest neighbors. In the calculations of the nearest neighbors for the minority class examples a Euclidean distance for continuous features and the value Distance Metric (with the Euclidean assumption) for nominal features was used. For examples with continuous features, the synthetic examples are generated by taking the difference between the feature vectors of selected examples under consideration and their nearest neighbors. The difference between the feature vectors is multiplied by a random number between 0 and 1 and then added to the feature vector of the example under consideration to get a new synthetic example. For nominal valued features, a majority vote for the feature value is taken between the example under consideration and its k nearest neighbors. This approach effectively selects a random point along the line segment between the two feature vectors. This strategy forces the decision regions of the minority class learned by the classifier to become more general and effectively provides better generalization performance on unseen data.
However, an investigation into how to choose the number of examples to be added was not done. In addition, the amount of under-sampling also needs to be determined. Given the various costs of making errors, it is important to identify potentially optimal values for both SMOTE and under-sampling. This is equivalent to discovering the operating point in the ROC space giving the best trade-off between True Positives and False Positives. In this paper, we develop an approach to automatically set the parameters. We discuss a wrapper framework using cross-validation that
performs a step-wise and greedy search for the parameters. Note that while the computational aspects of the automated approach induces certain costs, we do not incorporate that into our framework. We optimize based on the different types of errors made. However, we do try to restrict our search space. We show that this approach works on three highly skewed datasets. We also utilized a cost-matrix to indicate the costs per test example based on the different kinds of errors."	https://doi.org/10.1145/1089827.1089830
82	Editorial: Special issue on learning from imbalanced data sets	['Reported solutions for the bi-class applications can be categorized as data level and algorithm level approaches <NO>.', 'At the data level, the objective is to re-balance the class distribution by re-sampling the data space including oversampling instances of the positive class and undersampling instances of the negative class, sometimes, uses the combination of the two techniques <NO>.', 'Mining highly unbalanced data sets, particularly in a cost-sensitive environment, is among the leading challenges for knowledge discovery and data mining <NO>, <NO>.', 'Many methods have been proposed for imbalanced classification, and some good results have been reported <NO>.', 'Lots of work has been done to deal with this problem by adjusting the class proportions <NO>,<NO>,<NO>,<NO>.', 'Learning algorithms that do not consider class imbalance tend to be overwhelmed by the majority class and ignore the minority class <NO>.', 'On the other hand, these opportunities also raise many new challenges for the research community in general <NO>–<NO>.', 'This increased interest is reflected in the recent installment of several major workshops, conferences, and special issues including the American Association for Artificial Intelligence (now the Association for the Advancement of Artificial Intelligence) workshop on Learning from Imbalanced Data Sets (AAAI ’00) <NO>, the International Conference on Machine Learning workshop on Learning from Imbalanced Data Sets (ICML’03) <NO>, and the Association for Computing Machinery Special Interest Group on Knowledge Discovery and Data Mining Explorations (ACM SIGKDD Explorations ’04) <NO>.', 'anced learning problems <NO>, <NO>, <NO>.', 'For instance, the one-class learning or novelty detection methods have also attracted much attention in the community <NO>.', 'Generally, these methods can be divided into two categories: external methods and internal methods <NO>, <NO>, <NO>.']	Nitesh V. Chawla, Nathalie Japkowicz, and Aleksander Kotcz. 2004. Editorial: Special issue on learning from imbalanced data sets. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 1–6.			https://doi.org/10.1145/1007730.1007733
83	SMOTEBoost: Improving prediction of the minority class in boosting	['The modification of the ensemble learning algorithm usually includes data level approaches to preprocess the data before learning each classifier <NO>–<NO>.', 'On the other hand, with regard to ensemble learning methods, a large number of different approaches have been proposed in the literature, including but not limited to SMOTEBoost <NO>, RUSBoost <NO>, IIVotes <NO>, EasyEnsemble <NO>, or SMOTEBagging <NO>.', 'to a small number of methods and by the usage of limited sets of problems <NO>–<NO>.', 'Inside this family, we include SMOTEBoost <NO>, MSMOTEBoost <NO>, RUSBoost <NO>, and DataBoost-IM <NO> algorithms.', 'Thus, boosting and oversampling together provide a good option for efficiently learning imbalanced data <NO>, <NO>, <NO>, <NO>.', 'M2 is chosen for ensemble because of its better performance for imbalance problems <NO>, <NO>, <NO>.', 'We compare the performance of RUSBoost to that of SMOTEBoost <NO>, which is another algorithm that combines boosting with data sampling.', 'One of the most promising of these techniques is SMOTEBoost <NO>.', '<NO>, combines the SMOTE algorithm with AdaBoost, resulting in a hybrid sampling/boosting algorithm that outperforms both SMOTE and AdaBoost.', 'on the SMOTEBoost algorithm <NO>, which is, in turn, based on the AdaBoost.', 'Algorithmic methods include adjusting the costs associated with misclassification so as to improve performance <NO>, shifting the bias of a classifier to favor the rare class <NO>, creating Adaboost-like boosting schemes <NO>, and learning from one class <NO>.', 'We chose to analyse Random Over-sampling and SMOTE <NO> for the following reasons: first, our previous experience with sampling methods <NO> shows that these two methods outperform several other (over and under) sampling methods; second, SMOTE has became one of the most popular sampling methods for class imbalance, and it is frequently used as counterpart in empirical evaluations and has several proposed extensions and variations (for instance <NO>, <NO>).', 'Motivated by the SMOTE <NO>, SMOTEBoost <NO>, and ADASYN <NO> algorithms, RAMOBoost facilitates imbalanced learning by iteratively developing an ensemble of hypotheses.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'For instance, the SMOTEBoost <NO> algorithm is based HE AND GARCIA: LEARNING FROM IMBALANCED DATA 1269', 'Many representative works on the ineffectiveness of accuracy in the imbalanced learning scenario exist in the community <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', ', Bagging, Boosting) has given good results <NO>, <NO>, <NO>.', 'A second algorithm that uses boosting to address the problems with rare classes is SMOTEBoost <NO>.', 'Empirical results indicate that this approach allows SMOTEBoost to achieve higher F-values than Adacost <NO>.', 'Moreover, different boosting methods have also been adopted for CIL in <NO> and <NO>.']	Nitesh V. Chawla, Aleksandar Lazarevic, Lawrence O. Hall, and Kevin W. Bowyer. 2003. SMOTEBoost: Improving prediction of the minority class in boosting. Knowledge Discovery in Databases: PKDD 2003. Springer, 107–119.			http://scholar.google.com/scholar?hl=en&q=Nitesh+V.+Chawla%2C+Aleksandar+Lazarevic%2C+Lawrence+O.+Hall%2C+and+Kevin+W.+Bowyer.+2003.+SMOTEBoost%3A+Improving+prediction+of+the+minority+class+in+boosting.+In+Knowledge+Discovery+in+Databases%3A+PKDD+2003.+Springer%2C+107%2D%2D119.
84	Using random forest to learn imbalanced data	['Lots of work has been done to deal with this problem by adjusting the class proportions <NO>,<NO>,<NO>,<NO>.', 'Balanced Random Forests <NO>, EasyEnsemble generates T balanced subproblems.']	Chao Chen, Andy Liaw, and Leo Breiman. 2004. Using random forest to learn imbalanced data. University of California, Berkeley (2004).			http://scholar.google.com/scholar?hl=en&q=Chao+Chen%2C+Andy+Liaw%2C+and+Leo+Breiman.+2004.+Using+random+forest+to+learn+imbalanced+data.+University+of+California%2C+Berkeley+%282004%29.
85	Ramoboost: Ranked minority oversampling in boosting	['Some of the most popular approaches to deal with imbalanced learning problems are based on the synthetic oversampling methods <NO>, <NO>, <NO>, <NO>.', 'Thus, boosting and oversampling together provide a good option for efficiently learning imbalanced data <NO>, <NO>, <NO>, <NO>.', 'Synthetic oversampling methods have been shown to be very successful in dealing with imbalance data <NO>, <NO>, <NO>, <NO>.', 'Oversampling methods ADASYN <NO> and RAMOBoost <NO> try to avoid the aforementioned problem by adaptively assigning weights to the minority class samples.', 'A large weight helps in generating many synthetic samples from the corresponding minority class sample <NO> or enhances the chance for the minority class sample as a participant in the synthetic sample generation process <NO>.', 'To assign the weight, both <NO> and <NO> use a parameter , defining the number of the majority class samples among the k-nearest neighbors of the minority class sample.', ', <NO>, <NO>, <NO>) employ the k-nearest neighbor (also called k-NN)-based approach.', ', <NO>, <NO>, <NO>) do not explicitly identify the hard-to-learn samples.', 'SMOTE <NO>, ADASYN <NO>, and RAMO <NO>.', 'The RAMO method is found from <NO> by excluding its boosting part.', 'M2 is chosen for ensemble because of its better performance for imbalance problems <NO>, <NO>, <NO>.', 'TABLE 2 Accuracy, Precision, and Recall Performance Values of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using Single Neural Network Classifier, and AdaBoost.', 'TABLE 3 F-Measure, G-Mean, and AUC Performance Values of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using Single Neural Network Classifier, and AdaBoost.', 'TABLE 4 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using k-Nearest Neighbor, and Decision Tree', 'Averaged ROC curves of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE for single neural network classifier ((a)-(d)) and for AdaBoost.', 'Tables 2, 3, and 4 summarize the results of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on four', 'TABLE 6 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Real-World Data Sets Using Single Neural Network Classifier, and AdaBoost.', 'Averaged ROC curves of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE for Neural Network classifier ((a)-(d)) and for ensemble of Neural Network classifiers ((e)-(h)): (a) Glass data set, (b) Libra data set, (c) Vehicle data set, (d) Satimage data set, (e) Breast Cancer Original data set, (f) Breast Tissue data set, (g) Pima data set, (h) Yeast data set.', 'M2 Ensemble of Neural Network: Significance Test of Averaged AUC of MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO> TABLE 7 Simulation 1 on Real-World Data Sets Using Single Neural Network: Significance Test of Averaged AUC between MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO>', 'M2 Ensemble of Neural Network: Significance Test of Averaged AUC of MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO> TABLE 7 Simulation 1 on Real-World Data Sets Using Single Neural Network: Significance Test of Averaged AUC between MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO>', 'TABLE 9 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Real-World Data Sets Using k-NN, and Decision Tree as Base Classifiers']	Sheng Chen, Haibo He, and Edwardo A. Garcia. 2010. Ramoboost: Ranked minority oversampling in boosting. IEEE Trans. Neural Networks 21, 10 (2010), 1624–1642.	In recent years, learning from imbalanced data has attracted growing attention from both academia and industry due to the explosive growth of applications that use and produce imbalanced data. However, because of the complex characteristics of imbalanced data, many real-world solutions struggle to provide robust efficiency in learning-based applications. In an effort to address this problem, this paper presents Ranked Minority Oversampling in Boosting (RAMOBoost), which is a RAMO technique based on the idea of adaptive synthetic data generation in an ensemble learning system. Briefly, RAMOBoost adaptively ranks minority class instances at each learning iteration according to a sampling probability distribution that is based on the underlying data distribution, and can adaptively shift the decision boundary toward difficult-to-learn minority and majority class instances by using a hypothesis assessment procedure. Simulation analysis on 19 real-world datasets assessed over various metrics—including overall accuracy, precision, recall, F-measure, G-mean, and receiver operation characteristic analysis—is used to illustrate the effectiveness of this method.	A comprehensive review of the development of research in learning from imbalanced data, including the nature of the problem, the state-of-the-art approaches, the assessment metrics, and the major opportunities and challenges, has been presented in <NO>. Interested readers can refer to that article for details. In this section, we provide a focused review of four major categories of research activity in imbalanced learning.	https://doi.org/10.1109/TNN.2010.2066988
86	Fast: A roc-based feature selection metric for small samples and imbalanced data classification problems	[]	Xue-wen Chen, and Michael Wasikowski. 2008. Fast: A roc-based feature selection metric for small samples and imbalanced data classification problems. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, 124–132.	The class imbalance problem is encountered in a large number of practical applications of machine learning and data mining, for example, information retrieval and filtering, and the detection of credit card fraud. It has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifier’s suboptimal performance. This is even more true when the imbalanced data are also high dimensional. In such cases, feature selection methods are critical to achieve optimal performance. In this paper, we propose a new feature selection method, Feature Assessment by Sliding Thresholds (FAST), which is based on the area under a ROC curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. FAST is compared to two commonly-used feature selection methods, correlation coefficient and RELevance In Estimating Features (RELIEF), for imbalanced data classification. The experimental results obtained on text mining, mass spectrometry, and microarray data sets showed that the proposed method outperformed both RELIEF and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred, the classification performance of the proposed method was significantly improved compared to correlation and RELIEF-based methods.	In this section, we briefly review two commonly-used feature selection methods, CC and RELIEF.	https://doi.org/10.1145/1401890.1401910
87	Further results on forecasting and model selection under asymmetric loss	['These costs arising from over- and underprediction are typically not quadratic in form and frequently non-symmetric <NO>.', 'Based upon modest research in non-quadratic error functions in NN theory <NO> and asymmetric costs in prediction theory <NO>, a set of asymmetric cost functions was recently proposed as objective functions for neural network training <NO>.', 'Linear, non-linear and mixed ACFs have been specified in literature <NO> while variable or dynamic objective functions to account for varying or heteroscedastic training objectives have not yet been developed for NN-training, as shown in Table 1.']	Peter F. Christoffersen, and Francis X. Diebold. 1996. Further results on forecasting and model selection under asymmetric loss. J. Appl. Econom. 11, 5 (1996), 561–571.			http://scholar.google.com/scholar?hl=en&q=Peter+F.+Christoffersen+and+Francis+X.+Diebold.+1996.+Further+results+on+forecasting+and+model+selection+under+asymmetric+loss.+J.+Appl.+Econom.+11%2C+5+%281996%29%2C+561%2D%2D571.
88	Optimal prediction under asymmetric loss	['Based upon modest research in non-quadratic error functions in NN theory <NO> and asymmetric costs in prediction theory <NO>, a set of asymmetric cost functions was recently proposed as objective functions for neural network training <NO>.', 'Linear, non-linear and mixed ACFs have been specified in literature <NO> while variable or dynamic objective functions to account for varying or heteroscedastic training objectives have not yet been developed for NN-training, as shown in Table 1.']	Peter F. Christoffersen, and Francis X. Diebold. 1997. Optimal prediction under asymmetric loss. Econom. Theor. 13, 6 (1997), 808–817.			http://scholar.google.com/scholar?hl=en&q=Peter+F.+Christoffersen+and+Francis+X.+Diebold.+1997.+Optimal+prediction+under+asymmetric+loss.+Econom.+Theor.+13%2C+6+%281997%29%2C+808%2D%2D817.
89	A new feature weighting method based on probability distribution in imbalanced text classification	[]	Leilei Chu, Hui Gao, and Wenbo Chang. 2010. A new feature weighting method based on probability distribution in imbalanced text classification. 2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), Vol. 5. IEEE, 2335–2339.			http://scholar.google.com/scholar?hl=en&q=Leilei+Chu%2C+Hui+Gao%2C+and+Wenbo+Chang.+2010.+A+new+feature+weighting+method+based+on+probability+distribution+in+imbalanced+text+classification.+In+2010+Seventh+International+Conference+on+Fuzzy+Systems+and+Knowledge+Discovery+%28FSKD%29%2C+Vol.+5.+IEEE%2C+2335%2D%2D2339.
90	Classification analysis techniques for skewed class distribution problems	[]	Yu-Meei Chyi. 2003. Classification analysis techniques for skewed class distribution problems. Master Thesis, Department of Information Management, National Sun Yat-Sen University (2003).			http://scholar.google.com/scholar?hl=en&q=Yu-Meei+Chyi.+2003.+Classification+analysis+techniques+for+skewed+class+distribution+problems.+Master+Thesis%2C+Department+of+Information+Management%2C+National+Sun+Yat-Sen+University+%282003%29.
91	Learning decision trees for unbalanced data	['use of a classification tree algorithm, such as Hellinger distance tree <NO>, that is specifically designed for the solution of im-', 'However, in <NO>, the authors show that it often experiences a reduction in performance when sampling techniques are applied, which is the base of the majority of the studied techniques; moreover, being more robust (less weak) than C4.', 'Algorithmic techniques have been developed for the different classification algorithms, such as neural networks <NO>, decision trees <NO>, fuzzy systems <NO>, <NO> etc.']	David A. Cieslak, and Nitesh V. Chawla. 2008. Learning decision trees for unbalanced data. Machine Learning and Knowledge Discovery in Databases. Springer, 241–256.			http://scholar.google.com/scholar?hl=en&q=David+A.+Cieslak+and+Nitesh+V.+Chawla.+2008.+Learning+decision+trees+for+unbalanced+data.+In+Machine+Learning+and+Knowledge+Discovery+in+Databases.+Springer%2C+241%2D%2D256.
92	Hellinger distance decision trees are robust and skew-insensitive	[]	David A. Cieslak, Thomas R. Hoens, Nitesh V. Chawla, and W. Philip Kegelmeyer. 2012. Hellinger distance decision trees are robust and skew-insensitive. Data Min. Knowl. Discov. 24, 1 (2012), 136–158.			https://doi.org/10.1007/s10618-011-0222-1
93	Learning from imbalanced data in surveillance of nosocomial infection	[', in text classification and natural language problems <NO> and medical applications <NO>.']	Gilles Cohen, Mélanie Hilario, Hugo Sax, Stéphane Hugonnet, and Antoine Geissbuhler. 2006. Learning from imbalanced data in surveillance of nosocomial infection. Artif. Intell. Med. 37, 1 (2006), 7–18.			https://doi.org/10.1016/j.artmed.2005.03.002
94	Utility based data mining for time series analysis: Cost-sensitive learning for neural network predictors	[]	Sven F. Crone, Stefan Lessmann, and Robert Stahlbock. 2005. Utility based data mining for time series analysis: Cost-sensitive learning for neural network predictors. Proceedings of the 1st International Workshop on Utility-based Data Mining. ACM, New York, NY, 59–68.			https://doi.org/10.1145/1089827.1089835
95	When is undersampling effective in unbalanced classification tasks? In Machine Learning and Knowledge Discovery in Databases	[]	Andrea Dal Pozzolo, Olivier Caelen, and Gianluca Bontempi. 2015. When is undersampling effective in unbalanced classification tasks? In Machine Learning and Knowledge Discovery in Databases. Springer, 200–215.			http://scholar.google.com/scholar?hl=en&q=Andrea+Dal+Pozzolo%2C+Olivier+Caelen%2C+and+Gianluca+Bontempi.+2015.+When+is+undersampling+effective+in+unbalanced+classification+tasks%3F+In+Machine+Learning+and+Knowledge+Discovery+in+Databases.+Springer%2C+200%2D%2D215.
96	Evaluation of classifiers for an uneven class distribution problem	['As claimed by many authors <NO>, the use of plain accuracy (error) rates to evaluate the classification performance in imbalanced domains might produce misleading conclusions, since they do not take misclassification costs into account, are strongly biased to favor the majority class, and are sensitive to class skews.']	Sophia Daskalaki, Ioannis Kopanas, and Nikolaos M. Avouris. 2006. Evaluation of classifiers for an uneven class distribution problem. Appl. Artif. Intell. 20, 5 (2006), 381–417.			http://scholar.google.com/scholar?hl=en&q=Sophia+Daskalaki%2C+Ioannis+Kopanas%2C+and+Nikolaos+M.+Avouris.+2006.+Evaluation+of+classifiers+for+an+uneven+class+distribution+problem.+Appl.+Artif.+Intell.+20%2C+5+%282006%29%2C+381%2D%2D417.
97	The relationship between Precision-Recall and ROC curves	['Similar to AUC-ROC, an area under precision/recall curve (AUC-PR) can be used to indicate the detection ability of a classifier between precision and recall as a function of varying a decision threshold <NO>', 'For example, researchers have recently argued that precision-recall curves are preferable when dealing with highly skewed datasets <NO>.']	Jesse Davis, and Mark Goadrich. 2006. The relationship between Precision-Recall and ROC curves. ICML’06: Proc. of the 23rd Int. Conf. on Machine Learning (ACM ICPS). ACM, New York, NY, 233– 240.	Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm’s performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.	"In a binary decision problem, a classifier labels examples as either positive or negative. The decision made by the classifier can be represented in a structure known as a confusion matrix or contingency table. The confusion matrix has four categories: True positives (TP) are examples correctly labeled as positives. False positives (FP) refer to negative examples incorrectly labeled as positive. True negatives (TN) correspond to negatives correctly labeled as negative. Finally, false negatives (FN) refer to positive examples incorrectly labeled as negative.
A confusion matrix is shown in Figure 2(a). The confusion matrix can be used to construct a point in either ROC space or PR space. Given the confusion matrix, we are able to define the metrics used in each space as in Figure 2(b). In ROC space, one plots the False Positive Rate (FPR) on the x-axis and the True Positive Rate (TPR) on the y-axis. The FPR measures the fraction of negative examples that are misclassified as positive. The TPR measures the fraction of positive examples that are correctly labeled. In PR space, one plots Recall on the x-axis and Precision on the y-axis. Recall is the same as TPR, whereas Precision measures that fraction of examples classified as positive that are truly positive. Figure 2(b) gives the definitions for each metric. We will treat the metrics as functions that act on the underlying confusion matrix which defines a point in either ROC space or PR space. Thus, given a confusion matrix A, RECALL(A) returns the Recall associated with A."	https://doi.org/10.1145/1143844.1143874
98	A multistrategy approach for digital text categorization from imbalanced documents	[]	Marı́a Dolores Del Castillo, and José Ignacio Serrano 2004. A multistrategy approach for digital text categorization from imbalanced documents. ACM SIGKDD Explor. Newslett	The goal of the research described here is to develop a multistrategy classifier system that can be used for document categorization. The system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample. The learners work in a parallel manner, where each learner carries out its own feature selection based on evolutionary techniques and then obtains a classification model. In classifying documents, the system combines the predictions of the learners by applying evolutionary techniques as well. The system relies on a modular, flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain.	"HYCLA operates in two stages, learning and integration. In the learning stage, learners apply an evolutionary technique to obtain their own feature set, and then they are trained to obtain their classification model. In the integration stage, individual learned models are evaluated on a test set, and the predictions made are combined in order to achieve the best classification of test documents. The subsections below describe the modules and procedures of this system.
The underlying architecture of HYCLA can be instantiated to approach a different text mining task by upgrading its modules."	https://doi.org/10.1145/1007730.1007740
99	Overlap versus imbalance	[]	Misha Denil, and Thomas Trappenberg. 2010. Overlap versus imbalance. Advances in Artificial Intelligence. Springer, 220–231.			https://doi.org/10.1007/978-3-642-13059-5_22
100	MetaCost: A general method for making classifiers cost-sensitive	['One is to assign distinct costs to the classification errors <NO>.', 'MetaCost <NO> or cost sensitive boosting <NO>.', 'For example, the work on cost sensitive learning <NO> aimed at reducing total cost, and sampling approaches <NO> to favour the minority class are usually demonstrated with decision tree algorithms and/or naive Bayes.', 'For example, the work on cost sensitive learning <NO> aimed at reducing total cost, and sampling approaches <NO> to favour the minority class are usually demonstrated with decision tree algorithms and/or naive Bayes.', 'Algorithmic methods include adjusting the costs associated with misclassification so as to improve performance <NO>, shifting the bias of a classifier to favor the rare class <NO>, creating Adaboost-like boosting schemes <NO>, and learning from one class <NO>.', 'Moreover, Domingos <NO> reports that concerning concept learning problems, C4.', 'where P jjx ð Þ represents the probability of each class j for a given example x <NO>, <NO>.', 'Both of these classes have rich theoretical foundations that justify their approaches, with cost-sensitive dataspace weighting methods building on the translation theorem <NO>, and cost-sensitive Metatechniques building on the Metacost framework <NO>.', 'We additionally note that it is generally very straightforward to make hard-type classifiers provide softtype outputs based on the observations of the intrinsic characteristics of those classifiers <NO>, <NO>, <NO>, <NO>.', 'It is worth noting that the original MetaCost method <NO> does not explicitly manipulate the outputs of the classifier.']	Pedro Domingos. 1999. MetaCost: A general method for making classifiers cost-sensitive. KDD’99: Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining. ACM Press, New York, NY, 155–164.			https://doi.org/10.1145/312129.312220
101	GP classification under imbalanced data sets: Active subsampling and AUC approximation	[]	John Doucette, and Malcolm I. Heywood. 2008. GP classification under imbalanced data sets: Active subsampling and AUC approximation. Genetic Programming. Springer, 266–277.			https://doi.org/10.5555/1792694.1792719
102	Evolutionary sampling and software quality modeling of high-assurance systems	[]	Dennis J. Drown, Taghi M. Khoshgoftaar, and Naeem Seliya. 2009. Evolutionary sampling and software quality modeling of high-assurance systems. IEEE Trans. Syst. Man Cybernet. A 39, 5 (2009), 1097– 1107.	Software quality modeling for high-assurance systems, such as safety-critical systems, is adversely affected by the skewed distribution of fault-prone program modules. This sparsity of defect occurrence within the software system impedes training and performance of software quality estimation models. Data sampling approaches presented in data mining and machine learning literature can be used to address the imbalance problem. We present a novel genetic algorithm-based data sampling method, named Evolutionary Sampling, as a solution to improving software quality modeling for high-assurance systems. The proposed solution is compared with multiple existing data sampling techniques, including random undersampling, one-sided selection, Wilson’s editing, random oversampling, cluster-based oversampling, Synthetic Minority Oversampling Technique (SMOTE), and Borderline-SMOTE. This paper involves case studies of two realworld software systems and builds C4.5and RIPPER-based software quality models both before and after applying a given data sampling technique. It is empirically shown that Evolutionary Sampling improves performance of software quality models for high-assurance systems and is significantly better than most existing data sampling techniques.		https://doi.org/10.1109/TSMCA.2009.2020804
103	Explicitly representing expected cost: An alternative to ROC representation	['Drummond and Holte <NO> introduced the cost space representation that allows for comparing different classifiers in terms of the expected cost.', 'We also implemented cost-curves over the range of PCF (+) established by varying C(+|−) and C(−|+) <NO>.', 'In order to provide a more comprehensive evaluation metric to address these issues, cost curves were proposed in <NO>, <NO>, <NO>.', 'such as ROC curve <NO> or cost curve <NO> can be used to', 'useful nontrivial classifiers can be identified <NO>.']	Chris Drummond, and Robert C. Holte. 2000. Explicitly representing expected cost: An alternative to ROC representation. Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, 198–207.	This paper proposes an alternative to ROC representation, in which the expected cost of a classi er is represented explicitly . This expected cost representation maintains many of the adv an tagesof R OCrepresen tation, but is easier to understand. It allows the experimenter to immediately see the range of costs and class frequencies where a particular classi er is the best and quantitatively how much better it is than other classi ers. This paper demonstrates there is a point/line duality between the tw o represen tations.A point in ROC space representing a classi er becomes a line segment spanning the full range of costs and class frequencies. This duality produces equivalen t operations in the tw o spaces, allowing most techniques used in ROC analysis to be readily reproduced in the cost space.	In this section we brie y review ROC analysis and how it is used in evaluating or comparing a classi er's performance. We then introduce our alternative dual representation, which maintains these advantages but by making explicit the expected cost is much easier to understand. In both representations, the analysis is restricted to two class problems which are referred to as the positive and negative class.	https://doi.org/10.1145/347090.347126
104	C4	[]	Chris Drummond, and Robert C. Holte. 2003. C4. 5, class imbalance, and cost sensitivity: Why under-sampling beats over-sampling. In Workshop on Learning from Imbalanced Datasets II, Vol. 11. Citeseer. James P. Egan. 1975. Signal detection theory and {ROC} analysis. (1975).			http://scholar.google.com/scholar?hl=en&q=Chris+Drummond+and+Robert+C.+Holte.+2003.+C4.+5%2C+class+imbalance%2C+and+cost+sensitivity%3A+Why+under-sampling+beats+over-sampling.+In+Workshop+on+Learning+from+Imbalanced+Datasets+II%2C+Vol.+11.+Citeseer.
105	The foundations of cost-sensitive learning	['As stated in <NO>, given a set of cost setups, the decisions are unchanged if each one in the set is multiplied by a positive constant or added with a constant.', 'Since boosting assigns higher weights to misclassified examples and minority class examples are those most likely to be misclassified, it stands to reason that minority class examples will receive higher weights during the boosting process, making it similar in many ways to cost-sensitive classification <NO> (a technique for alleviating the class imbalance problem that is not discussed in this paper).', 'Cost-sensitive learning <NO>, <NO> is another important class of class-imbalance learning methods.', 'Algorithmic methods include adjusting the costs associated with misclassification so as to improve performance <NO>, shifting the bias of a classifier to favor the rare class <NO>, creating Adaboost-like boosting schemes <NO>, and learning from one class <NO>.', 'Indeed, the motivation for attaining probability estimates is often to classify instances as belonging to the majority class only when we are quite sure of it (this is the basic idea behind cost-sensitive learning <NO>, <NO>).', 'where P jjx ð Þ represents the probability of each class j for a given example x <NO>, <NO>.', 'lished in <NO>, which generalizes the effects of decision tree', 'In this case cost-sensitivity was obtained by altering the ratio of positive to negative examples in the training data, or, equivalently, by adjusting the probability thresholds used to assign class labels <NO>.', '1 to account for the differences between the training distribution and the underlying distribution <NO>.', 'Elkan indicated <NO>, sampling can be done either randomly or deterministically.']	Charles Elkan. 2001. The foundations of cost-sensitive learning. IJCAI’01: Proc. of 17th Int. Joint Conf. of Artificial Intelligence, Vol. 1. Morgan Kaufmann Publishers, 973–978.			https://doi.org/10.5555/1642194.1642224
106	Adaptive oversampling for imbalanced data classification	[]	Şeyda Ertekin. 2013. Adaptive oversampling for imbalanced data classification. Information Sciences and Systems 2013. Springer, 261–269.			http://scholar.google.com/scholar?hl=en&q=%C5%9Eeyda+Ertekin.+2013.+Adaptive+oversampling+for+imbalanced+data+classification.+In+Information+Sciences+and+Systems+2013.+Springer%2C+261%2D%2D269.
107	Learning on the border: Active learning in imbalanced data classification	['For example, an SVM-based active learning approach for imbalanced datasets was proposed in <NO> and <NO>.', 'Recently, however, various issues on active learning from imbalanced data sets have been discussed in literature <NO>, <NO>, <NO>, <NO>.', '8 illustrates the motivation for the selection procedure for imbalanced data sets <NO>.', '<NO> and <NO> proposed an efficient SVM-based active learning method which queries a small pool of data at each iterative step of active learning instead of querying the entire data set.', '<NO> and <NO> also point out that the search process for the most informative instances can be computationally expensive because, for each instance of unseen data, the algorithm needs to recalculate the distance between each instance and the current hyperplane.', 'To solve this problem, they proposed a method to effectively select such informative instances from a random set of training populations to reduce the computational cost for large-scale imbalanced data sets <NO>, <NO>.', 'Data imbalance ratio within and outside the margin <NO>.']	Şeyda Ertekin, Jian Huang, Leon Bottou, and Lee Giles. 2007. Learning on the border: Active learning in imbalanced data classification. Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management. ACM, New York, NY, 127–136.	This paper is concerned with the class imbalance problem which has been known to hinder the learning performance of classification algorithms. The problem occurs when there are significantly less number of observations of the target concept. Various real-world classification tasks, such as medical diagnosis, text categorization and fraud detection suffer from this phenomenon. The standard machine learning algorithms yield better prediction performance with balanced datasets. In this paper, we demonstrate that active learning is capable of solving the class imbalance problem by providing the learner more balanced classes. We also propose an efficient way of selecting informative instances from a smaller pool of samples for active learning which does not necessitate a search through the entire dataset. The proposed method yields an efficient querying system and allows active learning to be applied to very large datasets. Our experimental results show that with an early stopping criteria, active learning achieves a fast solution with competitive prediction performance in imbalanced data classification.	"Recent research on class imbalance problem has focused on several major groups of techniques. One is to assign distinct costs to the classification errors <NO>. In this method, the misclassification penalty for the positive class is assigned a higher value than that of the negative class. This method requires tuning to come up with good penalty parameters for the misclassified examples. The second is to resample the original training dataset, either by over-sampling the minority class and/or under-sampling the majority class until the classes are approximately equally represented <NO>. Both resampling methods introduce additional computational costs of data preprocessing and oversampling can be overwhelming in the case of very large scale training data. Undersampling has been proposed as a good means of increasing the sensitivity of a classifier. However this method may discard potentially useful data that could be important for the learning process therefore significant decrease in the prediction performance may be observed. Discarding the redundant examples in undersampling has been discussed in <NO> but since it is an adaptive method for ensemble learning and does not involve an external preprocessing step it can not be applied to other types of algorithms. Oversampling has been proposed to create synthetic positive instances from the existing positive samples to increase the representation of the class. Nevertheless, oversampling may suffer from overfitting and due to the increase in the number of
samples, the training time of the learning process gets longer. If a complex oversampling method is used, it also suffers from high computational costs during preprocessing data. In addition to those, oversampling methods demand more memory space for the storage of newly created instances and the data structures based on the learning algorithm (i.e., extended kernel matrix in kernel classification algorithms). Deciding on the oversampling and undersampling rate is also another issue of those methods. Another technique suggested for class imbalance problem is to use a recognition-based, instead of discrimination-based inductive learning <NO>. These methods attempt to measure the amount of similarity between a query object and the target class, where classification is accomplished by imposing a threshold on the similarity measure. The major drawback of those methods is the need for tuning the similarity threshold of which the success of the method mostly relies on. On the other hand, discrimination-based learning algorithms have been proved to give better prediction performance in most domains.
In <NO> the behavior of Support Vector Machines (SVM) with imbalanced data is investigated. They applied <NO>’s SMOTE algorithm to oversample the data and trained SVM with different error costs. SMOTE is an oversampling approach in which the minority class is oversampled by creating synthetic examples rather than with replacement. The k nearest positive neighbors of all positive instances are identified and synthetic positive examples are created and placed randomly along the line segments joining the k minority class nearest neighbors. Preprocessing the data with SMOTE may lead to improved prediction performance at the classifiers, however it also brings more computational cost to the system for preprocessing and yet the increased number of training data makes the SVM training very costly since the training time at SVMs scales quadratically with the number of training instances. In order to cope with today’s tremendously growing dataset sizes, we believe that there is a need for more computationally efficient and scalable algorithms. We show that such a solution can be achieved by using active learning strategy."	https://doi.org/10.1145/1321440.1321461
108	Active learning for class imbalance problem	['For example, an SVM-based active learning approach for imbalanced datasets was proposed in <NO> and <NO>.', 'This algorithm locates the “most informative” sample by evaluating a small fixed number of randomly selected examples instead of the entire dataset <NO>.', 'Recently, however, various issues on active learning from imbalanced data sets have been discussed in literature <NO>, <NO>, <NO>, <NO>.', 'SVM-based active learning aims to select the most informative instances from the unseen training data in order to retrain the kernel-based model <NO>, i.', '<NO> and <NO> proposed an efficient SVM-based active learning method which queries a small pool of data at each iterative step of active learning instead of querying the entire data set.', '<NO> and <NO> also point out that the search process for the most informative instances can be computationally expensive because, for each instance of unseen data, the algorithm needs to recalculate the distance between each instance and the current hyperplane.', 'To solve this problem, they proposed a method to effectively select such informative instances from a random set of training populations to reduce the computational cost for large-scale imbalanced data sets <NO>, <NO>.']	Şeyda Ertekin, Jian Huang, and C. Lee Giles. 2007. Active learning for class imbalance problem. Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New York, NY, 823–824.	The class imbalance problem has been known to hinder the learning performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem.	"The basic SVM based active learning selects the closest instance to the current hyperplane from the unseen training data and adds it to the training set to retrain the model. In classical active learning <NO>, the search for the most informative (closest) instance is done through the entire unseen dataset. Each iteration of active learning involves the recomputation of the distances of each instance to the new hyperplane. Thus, for large datasets, searching the entire training set is very time-consuming and computationally expensive.
We propose a selection method which will not necessitate a full search through the entire dataset but locates an approximate most informative sample by examining a small constant number of randomly chosen samples. The method picks L (L # training instances) random training samples in each iteration and selects the best (closest to the hyperplane) among them. Suppose, instead of picking the closest instance among all the training samples XN = (x1, x2, · · · , xN) at each iteration, we first pick a random subset XL, L N and select the closest sample xi from XL based on the condition that xi is among the top p% closest instances in XN with probability (1− η). Any numerical modification to these constraints can be met by varying the size of L, and is independent of N . To demonstrate, the probability that at least one of the L instances is among the closest p% is 1 − (1 − p%)L. Due to the requirement of (1 − η) probability, we have
1 − (1 − p%)L = 1 − η (1) which follows the solution of L in terms of η and p
L = log η / log(1 − p%) (2) For example, the active learner will pick one instance, with 95% probability, that is among the top 5% closest instances to the hyperplane, by randomly sampling only log(.05)/ log(.95) = 59 instances regardless of the training set size. This approach scales well since the size of the subset L is independent of the training set size N , requires significantly less training time and does not have an adverse effect on the classification performance of the learner. In our experiments, we set L = 59.
Early Stopping: In SVM learning the classification boundary (hyperplane) is only determined by support vectors. This means that there is no point of adding new instances to the model after the
number of support vectors saturates. A practical implementation of this idea is to count the number of support vectors during the active learning training process. If the number of the support vectors stabilizes, it implies that all possible support vectors have been selected by the active learning method and the rest of the training instances are redundant. Therefore, we choose our stopping point where the number of support vectors saturates."	https://doi.org/10.1145/1277741.1277927
109	A mixture-of-experts framework for learning from imbalanced data sets	[]	Andrew Estabrooks, and Nathalie Japkowicz. 2001. A mixture-of-experts framework for learning from imbalanced data sets. Advances in Intelligent Data Analysis. Springer, 34–43.			https://doi.org/10.5555/647967.741623
110	A multiple resampling method for learning from imbalanced data sets	['Many works have been developed studying the suitability of data preprocessing techniques to deal with imbalanced data-sets <NO>, <NO>, <NO>.', 'Although it is impossible to predict what true class distribution should be <NO>, <NO> it is observed that classifiers learn well from a balanced distribution than from an imbalanced one <NO>, <NO>, <NO>.', 'Sampling methods aim to modify the original training dataset to provide a balanced distribution <NO>.', 'Many researchers, suach as Batista, G <NO>, Estabrooks <NO> and Japkowicz <NO>, proposes various approach about two resampling strategies.', 'Typically, this is done by under-sampling the larger class <NO> or by over-sampling the smaller one <NO> or by combination of these techniques <NO>.', 'Studies have shown that for several base classifiers, a balanced data set provides improved overall classification performance compared to an imbalanced data set <NO>, <NO>, <NO>.', '<NO> and <NO> that directly relate to the aforementioned', '” In <NO>, the rate of oversampling and under-']	Andrew Estabrooks, Taeho Jo, and Nathalie Japkowicz. 2004. A multiple resampling method for learning from imbalanced data sets. Comput. Intell. 20, 1 (2004), 18–36.			http://scholar.google.com/scholar?hl=en&q=Andrew+Estabrooks%2C+Taeho+Jo%2C+and+Nathalie+Japkowicz.+2004.+A+multiple+resampling+method+for+learning+from+imbalanced+data+sets.+Comput.+Intell.+20%2C+1+%282004%29%2C+18%2D%2D36.
111	An introduction to ROC analysis	['Unlike G-mean and F-measure, AUC is not sensitive to the distribution between the positive class and negative class samples, thus suitable for performance comparison of different classifiers <NO>, <NO>.', 'Traditionally, metrics which are used by the most standard algorithm are accuracy and error rate <NO><NO><NO><NO>.', 'We use the area under the ROC curve (AUC) <NO> as the main measure to assess our results.', 'In order to overcome such issues, the ROC assessment technique <NO>, <NO> makes use of the proportion of two single-column-based evaluation metrics, namely, true posi-', 'In order to assess different classifiers’ performance in this case, one generally uses the area under the curve (AUC) as an evaluation criterion <NO>, <NO>.', 'Of course, one should also note that it is possible for a high AUC classifier to perform worse in a specific region in ROC space than a low AUC classifier <NO>, <NO>.', 'For instance, Fawcett <NO> and <NO> discussed multiclass ROC graphs.', 'A straightforward way of doing this is to generate n different ROC graphs, one for each class <NO>, <NO>.']	Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recogn. Lett. 27, 8 (2006), 861–874.			https://doi.org/10.1016/j.patrec.2005.10.010
112	On the 2-tuples based genetic tuning performance for fuzzy rule based classification systems in imbalanced data-sets	[]	Alberto Fernández, Marı́a José del Jesus, and Francisco Herrera 2010. On the 2-tuples based genetic tuning performance for fuzzy rule based classification systems in imbalanced data-sets. Inform. Sci. 180,			https://doi.org/10.1016/j.ins.2009.12.014
113	A study of the behaviour of linguistic fuzzy rule based classification systems in the framework of imbalanced data-sets	[]	Alberto Fernández, Salvador Garcı́a, Marı́a José del Jesus, and Francisco Herrera 2008. A study of the behaviour of linguistic fuzzy rule based classification systems in the framework of imbalanced data-sets. Fuzzy Sets Syst. 159,			https://doi.org/10.1016/j.fss.2007.12.023
114	Multi-class boosting for imbalanced data	[]	Antonio Fernández-Baldera, José M. Buenaposada, and Luis Baumela. 2015. Multi-class boosting for imbalanced data. Pattern Recognition and Image Analysis. Springer, 57–64.			http://scholar.google.com/scholar?hl=en&q=Antonio+Fern%C3%A1ndez-Baldera%2C+Jos%C3%A9+M.+Buenaposada%2C+and+Luis+Baumela.+2015.+Multi-class+boosting+for+imbalanced+data.+In+Pattern+Recognition+and+Image+Analysis.+Springer%2C+57%2D%2D64.
115	Modifying ROC curves to incorporate predicted probabilities	[]	César Ferri, Peter Flach, José Hernández-Orallo, and Athmane Senad. 2005. Modifying ROC curves to incorporate predicted probabilities. Proceedings of the Second Workshop on ROC Analysis in Machine Learning. 33–40.			http://scholar.google.com/scholar?hl=en&q=C%C3%A9sar+Ferri%2C+Peter+Flach%2C+Jos%C3%A9+Hern%C3%A1ndez-Orallo%2C+and+Athmane+Senad.+2005.+Modifying+ROC+curves+to+incorporate+predicted+probabilities.+In+Proceedings+of+the+Second+Workshop+on+ROC+Analysis+in+Machine+Learning.+33%2D%2D40.
116	Brier curves: A new cost-based visualisation of classifier performance	[]	César Ferri, José Hernández-orallo, and Peter A. Flach. 2011. Brier curves: A new cost-based visualisation of classifier performance. Proceedings of the 28th International Conference on Machine Learning (ICML-11). 585–592.			http://scholar.google.com/scholar?hl=en&q=C%C3%A9sar+Ferri%2C+Jos%C3%A9+Hern%C3%A1ndez-orallo%2C+and+Peter+A.+Flach.+2011a.+Brier+curves%3A+A+new+cost-based+visualisation+of+classifier+performance.+In+Proceedings+of+the+28th+International+Conference+on+Machine+Learning+%28ICML-11%29.+585%2D%2D592.
117	A coherent interpretation of AUC as a measure of aggregated classification performance	['A commonly used measure for imbalanced data is the AUC: Area Under the ROC (Receiver Operation Characteristic) Curve <NO>.']	César Ferri, José Hernández-Orallo, and Peter A. Flach. 2011. A coherent interpretation of AUC as a measure of aggregated classification performance. Proceedings of the 28th International Conference on Machine Learning (ICML-11). 657–664.			http://scholar.google.com/scholar?hl=en&q=C%C3%A9sar+Ferri%2C+Jos%C3%A9+Hern%C3%A1ndez-Orallo%2C+and+Peter+A.+Flach.+2011b.+A+coherent+interpretation+of+AUC+as+a+measure+of+aggregated+classification+performance.+In+Proceedings+of+the+28th+International+Conference+on+Machine+Learning+%28ICML-11%29.+657%2D%2D664.
118	An experimental comparison of performance measures for classification	['As claimed by many authors <NO>, the use of plain accuracy (error) rates to evaluate the classification performance in imbalanced domains might produce misleading conclusions, since they do not take misclassification costs into account, are strongly biased to favor the majority class, and are sensitive to class skews.']	César Ferri, José Hernández-Orallo, and R. Modroiu. 2009. An experimental comparison of performance measures for classification. Pattern Recogn. Lett. 30, 1 (2009), 27–38.			https://doi.org/10.1016/j.patrec.2008.08.010
119	An extensive empirical study of feature selection metrics for text classification	['Their experiments show that when deprived of negative features, the performance of all feature selection metrics degrades, which indicates negative features are essential to high quality classification <NO>.', 'As Forman <NO> argued, feature selection should be relatively more important than classification algorithms in highly imbalanced situations.']	George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. J. Mach. Learn. Res. 3 (2003), 1289–1305.			https://doi.org/10.5555/944919.944974
120	Learning from little: Comparison of classifiers given little training	[]	George Forman, and Ira Cohen. 2004. Learning from little: Comparison of classifiers given little training. Knowledge Discovery in Databases: PKDD 2004. Springer, 161–172.			https://doi.org/10.5555/1053072.1053089
121	A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches	['Literally hundreds of research papers have reported that imbalanced data lead to performance losses, and some sort of treatment (such as sampling <NO>, cost-sensitive learning <NO>, ensembles <NO>, among others) are able to improve classification.', ', Bagging, Boosting) has given good results <NO>, <NO>, <NO>.']	Mikel Galar, Alberto Fernández, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. 2012. A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches. IEEE Trans. Syst. Man Cybernet. C 42, 4 (2012), 463–484.	Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.		https://doi.org/10.1109/TSMCC.2011.2161285
122	Eusboost: Enhancing ensembles for highly imbalanced data-sets by evolutionary undersampling	['In an imbalanced scenario, the traditional classification algorithms are biased toward the negative class, because it is easier to learn <NO>.']	Mikel Galar, Alberto Fernández, Edurne Barrenechea, and Francisco Herrera. 2013. Eusboost: Enhancing ensembles for highly imbalanced data-sets by evolutionary undersampling. Pattern Recogn. (2013).			https://doi.org/10.1016/j.patcog.2013.05.006
123	PDFOS: PDF estimation based over-sampling for imbalanced two-class problems	[]	Ming Gao, Xia Hong, Sheng Chen, Chris J. Harris, and Emad Khalaf. 2014. PDFOS: PDF estimation based over-sampling for imbalanced two-class problems. Neurocomputing 138 (2014), 248–259.			http://scholar.google.com/scholar?hl=en&q=Ming+Gao%2C+Xia+Hong%2C+Sheng+Chen%2C+Chris+J.+Harris%2C+and+Emad+Khalaf.+2014.+PDFOS%3A+PDF+estimation+based+over-sampling+for+imbalanced+two-class+problems.+Neurocomputing+138+%282014%29%2C+248%2D%2D259.
124	Evolutionary-based selection of generalized instances for imbalanced classification	[]	Joaquı́n Garcı́a, Salvador Derrac, Isaac Triguero, Cristobal J. Carmona, and Francisco Herrera 2012. Evolutionary-based selection of generalized instances for imbalanced classification. Knowl.-Based Syst. 25,			https://doi.org/10.1016/j.knosys.2011.01.012
125	A proposal of evolutionary prototype selection for class imbalance problems. In Intelligent Data Engineering and Automated Learning–IDEAL	[]	Salvador Garcı́a, José Ramón Cano, Alberto Fernández, and Francisco Herrera 2006. A proposal of evolutionary prototype selection for class imbalance problems. In Intelligent Data Engineering and Automated Learning–IDEAL			https://doi.org/10.1007/11875581_168
126	Evolutionary undersampling for classification with imbalanced datasets: Proposals and taxonomy	[]	Salvador Garcı́a, and Francisco Herrera 2009. Evolutionary undersampling for classification with imbalanced datasets: Proposals and taxonomy. Evol. Comput. 17,			https://doi.org/10.1162/evco.2009.17.3.275
127	Combined effects of class imbalance and class overlap on instance-based classification. In Intelligent Data Engineering and Automated Learning–IDEAL	[]	Vicente Garcı́a, Roberto Alejo, José Salvador Sánchez, José Martı́nez Sotoca, and Ramón Alberto Mollineda 2006. Combined effects of class imbalance and class overlap on instance-based classification. In Intelligent Data Engineering and Automated Learning–IDEAL			https://doi.org/10.1007/11875581_45
128	A new performance evaluation method for two-class imbalanced problems. In Structural, Syntactic, and Statistical Pattern	[]	Vicente Garcı́a, Ramón Alberto Mollineda, and José Salvador Sánchez 2008. A new performance evaluation method for two-class imbalanced problems. In Structural, Syntactic, and Statistical Pattern			https://doi.org/10.1007/978-3-540-89689-0_95
129	Index of balanced accuracy: A performance measure for skewed class distributions	['This section provides a generalization of a primary index reported in <NO>, named Index of Balanced Accuracy (IBA).']	Vicente Garcı́a, Ramón Alberto Mollineda, and José Salvador Sánchez 2009. Index of balanced accuracy: A performance measure for skewed class distributions. In Pattern Recognition and Image			https://doi.org/10.1007/978-3-642-02172-5_57
130	Theoretical analysis of a performance measure for imbalanced data	[]	Vicente Garcı́a, Ramón Alberto Mollineda, and José Salvador Sánchez 2010. Theoretical analysis of a performance measure for imbalanced data. In 2010 20th International Conference on Pattern Recognition (ICPR)	This paper analyzes a generalization of a new metric to evaluate the classification performance in imbalanced domains, combining some estimate of the overall accuracy with a plain index about how dominant the class with the highest individual accuracy is. A theoretical analysis shows the merits of this metric when compared to other well-known measures.	"Traditionally, classification accuracy (Acc) and/or error rates have been the standard metrics used to estimate the performance of learning systems. For a twoclass problem, they can be easily derived from a 2 × 2 confusion matrix as that given in Table 1.
However, empirical and theoretical evidences show that these measures are biased with respect to data imbalance and proportions of correct and incorrect classifications. These shortcomings have motivated a search for new metrics based on simple indexes, such as the true positive rate (TPrate) and the true negative rate (TNrate). The TPrate (TNrate) is the percentage of positive (negative) examples correctly classified.
One of the most widely-used evaluation methods in the context of class imbalance is the ROC curve, which is a tool for visualizing and selecting classifiers based on their trade-offs between benefits (true positives) and costs (false positives). A quantitative representation of a ROC curve is the area under it (AUC) <NO>. For just one run of a classifier, the AUC can be computed as <NO> AUC = (TPrate+ TNrate)/2.
1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.156
62117
Kubat et al. <NO> use the geometric mean of accuracies measured separately on each class, with the aim of maximizing the accuracies of both classes while keeping them balanced, Gmean = √ TPrate · TNrate.
Both AUC and Gmean minimize the negative influence of skewed distributions of classes, but they do not show up the contribution of each class to the overall performance, nor which is the prevalent class. This means that different combinations of TPrate and TNrate may produce the same result for those metrics.
Recently, Ranawana and Palade <NO> introduced the optimized precision, which can be computed as,
OP = Acc− |TNrate− TPrate| TNrate+ TPrate
(1)
This represents the difference between the global accuracy and a second term that computes how balanced both class accuracies are. High OP values require high global accuracy and well-balanced class accuracies. However, OP can be strongly affected by the biased influence of the global accuracy."	https://doi.org/10.1109/ICPR.2010.156
131	Active one-class learning by kernel density estimation	[]	Alireza Ghasemi, Mohammad T. Manzuri, Hamid R. Rabiee, Mohammad H. Rohban, and Siavash Haghiri. 2011. Active one-class learning by kernel density estimation. 2011 IEEE International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 1–6.			http://scholar.google.com/scholar?hl=en&q=Alireza+Ghasemi%2C+Mohammad+T.+Manzuri%2C+Hamid+R.+Rabiee%2C+Mohammad+H.+Rohban%2C+and+Siavash+Haghiri.+2011a.+Active+one-class+learning+by+kernel+density+estimation.+In+2011+IEEE+International+Workshop+on+Machine+Learning+for+Signal+Processing+%28MLSP%29.+IEEE%2C+1%2D%2D6.
132	Active learning from positive and unlabeled data	[]	Alireza Ghasemi, Hamid R. Rabiee, Mohsen Fadaee, Mohammad T. Manzuri, and Mohammad H. Rohban. 2011. Active learning from positive and unlabeled data. 2011 IEEE 11th International Conference on Data Mining Workshops (ICDMW). IEEE, 244–250.	During recent years, active learning has evolved into a popular paradigm for utilizing user’s feedback to improve accuracy of learning algorithms. Active learning works by selecting the most informative sample among unlabeled data and querying the label of that point from user. Many different methods such as uncertainty sampling and minimum risk sampling have been utilized to select the most informative sample in active learning. Although many active learning algorithms have been proposed so far, most of them work with binary or multi-class classification problems and therefore can not be applied to problems in which only samples from one class as well as a set of unlabeled data are available. Such problems arise in many real-world situations and are known as the problem of learning from positive and unlabeled data. In this paper we propose an active learning algorithm that can work when only samples of one class as well as a set of unlabeled data are available. Our method works by separately estimating probability density of positive and unlabeled points and then computing expected value of informativeness to get rid of a hyper-parameter and have a better measure of informativeness. Experiments and empirical analysis show promising results compared to other similar methods. 	"Many active learning algorithms have been proposed in the literature so far. <NO> is a comprehensive survey of recent works in this field. Among the earliest and most popular active learning paradigms is the uncertainty sampling approach which is based on selecting the least confident sample for querying. The definition of confidence depends on the base classifier in use. For example, <NO> proposes an active learning approach for SVM which selects for querying the sample which is closest to the separating hyperplane. selecting the sample with minimum margin <NO> and the data sample with maximum entropy <NO> are other approaches which have been applied to active learning problems.
For classifiers that are unable to define a similarity measure over their predictions, committee-based active learning methods have been proposed. these methods form an ensemble or committee of diverse classifiers and measure uncertainty by the amount of disagreement between committee members’ votes for a data sample <NO>.
In problems where samples of only one class are available, traditional uncertainty assessment methods can not work since they require information about at least two of the classes or their separating hyperplane . Therefore, specific active learning methods are required for one-class problems. One of the earlier works is <NO> which uses active learning for outlier detection. This methods works in two stages: First, a number of unlabeled samples are selected as negative samples by means of statical methods. Then a traditional committee based active learning algorithm is used to perform active learning on the rest of samples. The main advantage of this approach is that it’s flexible and can utilize a wide range of traditional active learning algorithms. However, <NO> approaches the problem of one-class learning by a traditional binary classification method. This causes degradation in accuracy of the resulting classifier since the two classes have very different characteristics and should not be treated equally. moreover, because of using two learning algorithms, the runtime complexity of this approach is much higher than other similar methods.
Another method for active learning from positive and unlabeled data has been proposed by <NO>. This paper suggests that the best choice for active learning is selecting the most relevant data sample. the justification behind this claim comes from the nature of relevance feedback in image retrieval applications. In other words, the most informative data will be chosen by the following rule:
x∗ = argmaxx∈U f(x), (1)
in which f(.) is the scoring function of one-class learning which is used to rank data samples by their likelihood to the target (positive) class. The main advantages of this method lie in its simplicity and speed. However, since this method does not consider uncertainty in choosing samples, the selected data point may lack informativeness.
A more recent approach has been proposed in <NO>, which tries to apply active learning to the well-known SVDD method. <NO> considers likelihood as well as local density of data point to assess their uncertainty. First, the algorithm constructs a neighborhood graph over all data samples. Then, the most informative sample is selected using the following rule:
x∗ =
argminxi∈U σ ||d(xi,C)−R|| c + 1−σ 2k Σxj∈L∪U (yj + 1)aij
(2)
In (2), parameters c and σ are used to manipulate the significance of any of two factors in the final decision measure, d(xi, C) is the distance between xi and center of sphere formed by the SVDD approach. R is radius of that sphere. y is 0 for unlabeled data, +1 for positive and −1 for negative samples. a is the adjacency matrix of the data neighborhood graph. aij = 1 if there is an edge between xi and xj , and 0 otherwise.
The main advantage of <NO> is that it considers both selection based on uncertainty of data, and exploring unknown regions of the feature space. This fact can be easily inferred from the two terms of equation 2. However, this methods is biased toward exploring regions containing negative data in the feature space. This causes algorithm to be biased toward selecting data which are more likely negative samples. Due to the nature of one-class learning, positive data are much more valuable than negative data samples and therefore selecting negative samples may not be much helpful in improving classification accuracy. Moreover, constructing the neighborhood graph is a time consuming task and makes the algorithm infeasible for real-time applications."	https://doi.org/10.1109/ICDMW.2011.20
133	Online neural network model for nonstationary and imbalanced data stream classification	[]	Adel Ghazikhani, Reza Monsefi, and Hadi Sadoghi Yazdi. 2014. Online neural network model for nonstationary and imbalanced data stream classification. Int. J. Mach. Learn. Cybernet. 5, 1 (2014), 51–62.			http://scholar.google.com/scholar?hl=en&q=Adel+Ghazikhani%2C+Reza+Monsefi%2C+and+Hadi+Sadoghi+Yazdi.+2014.+Online+neural+network+model+for+non-stationary+and+imbalanced+data+stream+classification.+Int.+J.+Mach.+Learn.+Cybernet.+5%2C+1+%282014%29%2C+51%2D%2D62.
134	Outline of forecast theory using generalized cost functions	[]	Clive W. Granger. 1999. Outline of forecast theory using generalized cost functions. Span. Econ. Rev. 1, 2 (1999), 161–173.			http://scholar.google.com/scholar?hl=en&q=Clive+W.+Granger.+1999.+Outline+of+forecast+theory+using+generalized+cost+functions.+Span.+Econ.+Rev.+1%2C+2+%281999%29%2C+161%2D%2D173.
135	Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning	['We chose to analyse Random Over-sampling and SMOTE <NO> for the following reasons: first, our previous experience with sampling methods <NO> shows that these two methods outperform several other (over and under) sampling methods; second, SMOTE has became one of the most popular sampling methods for class imbalance, and it is frequently used as counterpart in empirical evaluations and has several proposed extensions and variations (for instance <NO>, <NO>).', 'To this end, various adaptive sampling methods have been proposed to overcome this limitation; some representative work includes the Borderline-SMOTE <NO> and Adaptive Synthetic Sampling (ADASYN) <NO> algorithms.', 'Equation (2) suggests that only those xi that have more majority class neighbors than minority class neighbors are selected to form the set “DANGER” <NO>.']	Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. 2005. Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning. Advances in Intelligent Computing. Springer, 878–887.			https://doi.org/10.1007/11538059_91
136	Measuring classifier performance: A coherent alternative to the area under the ROC curve	['Traditionally, metrics which are used by the most standard algorithm are accuracy and error rate <NO><NO><NO><NO>.', 'For instance, it was recently presented in <NO> that H -measure could be a qualified alternative metric of AUC.', 'The interested reader can refer to <NO> for further details on H -measure and <NO> for a critical review of the assessment metrics for imbalanced learning.']	David J. Hand. 2009. Measuring classifier performance: A coherent alternative to the area under the ROC curve. Machine Learn. 77, 1 (2009), 103–123.			https://doi.org/10.1007/s10994-009-5119-5
137	The condensed nearest neighbor rule	['Condensed Nearest Neighbor Rule Hart’s Condensed Nearest Neighbor Rule (CNN) <NO> is used to find a consistent subset of examples.']	Peter. E. Hart. 1968. The condensed nearest neighbor rule. IEEE Transactions on Information Theory 14 (1968), 515–516.			https://doi.org/10.1109/TIT.1968.1054155
138	ADASYN: Adaptive synthetic sampling approach for imbalanced learning	['Some of the most popular approaches to deal with imbalanced learning problems are based on the synthetic oversampling methods <NO>, <NO>, <NO>, <NO>.', 'Depending on the technique of how synthetic samples will be generated, various methods exist in the literature such as Synthetic Minority Oversampling TEchnique (SMOTE) <NO>, Borderline-SMOTE <NO>, and Adaptive Synthetic Sampling Technique (ADASYN) <NO>.', 'Synthetic oversampling methods have been shown to be very successful in dealing with imbalance data <NO>, <NO>, <NO>, <NO>.', 'Oversampling methods ADASYN <NO> and RAMOBoost <NO> try to avoid the aforementioned problem by adaptively assigning weights to the minority class samples.', 'A large weight helps in generating many synthetic samples from the corresponding minority class sample <NO> or enhances the chance for the minority class sample as a participant in the synthetic sample generation process <NO>.', 'To assign the weight, both <NO> and <NO> use a parameter , defining the number of the majority class samples among the k-nearest neighbors of the minority class sample.', ', <NO>, <NO>, <NO>) employ the k-nearest neighbor (also called k-NN)-based approach.', ', <NO>, <NO>, <NO>) do not explicitly identify the hard-to-learn samples.', 'SMOTE <NO>, ADASYN <NO>, and RAMO <NO>.', 'For SMOTE and ADASYN, the value of the nearest neighbors, k, is set to 5 <NO>, <NO>.', 'TABLE 2 Accuracy, Precision, and Recall Performance Values of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using Single Neural Network Classifier, and AdaBoost.', 'TABLE 3 F-Measure, G-Mean, and AUC Performance Values of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using Single Neural Network Classifier, and AdaBoost.', 'TABLE 4 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Artificial Domains Using k-Nearest Neighbor, and Decision Tree', 'Averaged ROC curves of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE for single neural network classifier ((a)-(d)) and for AdaBoost.', 'Tables 2, 3, and 4 summarize the results of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on four', 'TABLE 6 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Real-World Data Sets Using Single Neural Network Classifier, and AdaBoost.', 'Averaged ROC curves of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE for Neural Network classifier ((a)-(d)) and for ensemble of Neural Network classifiers ((e)-(h)): (a) Glass data set, (b) Libra data set, (c) Vehicle data set, (d) Satimage data set, (e) Breast Cancer Original data set, (f) Breast Tissue data set, (g) Pima data set, (h) Yeast data set.', 'M2 Ensemble of Neural Network: Significance Test of Averaged AUC of MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO> TABLE 7 Simulation 1 on Real-World Data Sets Using Single Neural Network: Significance Test of Averaged AUC between MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO>', 'M2 Ensemble of Neural Network: Significance Test of Averaged AUC of MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO> TABLE 7 Simulation 1 on Real-World Data Sets Using Single Neural Network: Significance Test of Averaged AUC between MWMOTE versus SMOTE <NO>, ADASYN <NO>, and RAMO <NO>', 'TABLE 9 Performance of SMOTE <NO>, ADASYN <NO>, RAMO <NO>, and MWMOTE on Real-World Data Sets Using k-NN, and Decision Tree as Base Classifiers', 'However, the drawbacks of over-sampling approaches are enlarging the size of the original training dataset, and leading to overfitting<NO><NO>.', 'The calculation of density is the same as in <NO>.', 'Our previous work adaptive synthetic (ADASYN) <NO> also applies a similar idea to assess the difficulty level of minority data.', 'Before presenting the details of the RAMOBoost algorithm, we briefly discuss the data generation mechanisms of SMOTE <NO> and ADASYN <NO>, which motivated the work presented', 'The difference from SMOTE is, instead of applying a uniform distribution for data generation like SMOTE, ADASYN uses a density distribution {r̂i } as a criterion to automatically decide the number of synthetic samples that need to be generated for each minority example <NO>.', 'Motivated by the SMOTE <NO>, SMOTEBoost <NO>, and ADASYN <NO> algorithms, RAMOBoost facilitates imbalanced learning by iteratively developing an ensemble of hypotheses.', 'Consequently, ADASYN also has the ability to push the learning algorithm to be more focused on the difficult regions of the decision boundary <NO>.', 'To this end, various adaptive sampling methods have been proposed to overcome this limitation; some representative work includes the Borderline-SMOTE <NO> and Adaptive Synthetic Sampling (ADASYN) <NO> algorithms.', 'ADASYN, on the other hand, uses a systematic method to adaptively create different amounts of synthetic data according to their distributions <NO>.']	Haibo He, Yang Bai, Edwardo A. Garcia, and Shutao Li. 2008. ADASYN: Adaptive synthetic sampling approach for imbalanced learning. IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence). IEEE, 1322–1328.			http://scholar.google.com/scholar?hl=en&q=Haibo+He%2C+Yang+Bai%2C+Edwardo+A.+Garcia%2C+and+Shutao+Li.+2008.+ADASYN%3A+Adaptive+synthetic+sampling+approach+for+imbalanced+learning.+In+IEEE+International+Joint+Conference+on+Neural+Networks%2C+2008.+IJCNN+2008.+%28IEEE+World+Congress+on+Computational+Intelligence%29.+IEEE%2C+1322%2D%2D1328.
139	Learning from imbalanced data	['itself <NO>, <NO>, the issue is that usually a series of difficulties related to this problem turn up.', 'Most of these works fall under four different categories: sampling-based methods, cost-based methods, kernel-based methods, and active learning-based methods <NO>.', 'Details of work performed on the other categories can be found in <NO>.', 'The main problem associates with the accuracy measure is its dependence on the distribution of positive class and negative class samples in the data set, thus not suitable for imbalanced learning problems <NO>.', 'They are precision, recall, geometric-mean (G-mean), and F-measure <NO>.', 'For several base classifiers, studies have shown that a balanced training dataset provides high prediction accuracy and good generalization capability compared to an imbalanced dataset <NO> <NO>.', 'However, the drawbacks of over-sampling approaches are enlarging the size of the original training dataset, and leading to overfitting<NO><NO>.', 'Traditionally, metrics which are used by the most standard algorithm are accuracy and error rate <NO><NO><NO><NO>.', 'Indeed, there are at least three surveys of methods for improving classification performance on imbalanced datasets <NO>, <NO>, <NO>.', 'In the case this assumption does not hold, the literature has several comprehensible surveys, we recommend <NO>.', 'LEARNING from imbalanced data (imbalanced learning) <NO>, <NO> has become a critical and significant research issue in many of today’s data-intensive applications, such as financial engineering, anomaly detection, biomedical data analysis, and many others.', 'The imbalance learning problem generally manifests itself in two forms: relative imbalances and absolute imbalances <NO>, <NO>.', 'Some studies have shown that the degradation of classification performance attributed to imbalanced data is not necessarily the result of relative imbalances but rather due to the lack of representative examples (absolute imbalances) <NO>, <NO>, <NO>, <NO>, <NO>.', 'In particular, for a given dataset that contains several sub-concepts, the distribution of minority examples over the minority class concepts may yield clusters with insufficient representative examples to form a classification rule <NO>.', 'A comprehensive review of the development of research in learning from imbalanced data, including the nature of the problem, the state-of-the-art approaches, the assessment metrics, and the major opportunities and challenges, has been presented in <NO>.', 'These techniques have shown great success when applied to imbalanced learning problems <NO>.', 'Kernel-based methods have recently become very popular across various fields including imbalanced learning <NO>.', 'Recently, active learning methods have found increased use in imbalanced learning applications <NO>.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'The detailed discussions on these metrics and their applications for imbalanced learning can be found in <NO>.', 'The interested reader can refer to <NO> for further details on H -measure and <NO> for a critical review of the assessment metrics for imbalanced learning.', 'Hence, several specific approaches have been proposed <NO>.', 'Although the SVM algorithm works effectively with balanced datasets, when it comes to imbalanced datasets, it could often produce suboptimal results <NO>–<NO>, i.', 'It has been well-studied that the SVM algorithm can be sensitive to class imbalance <NO>–<NO>, i.', 'Generally, these methods can be divided into two categories: external methods and internal methods <NO>, <NO>, <NO>.', 'A comprehensive review of different CIL methods can be found in <NO>.', 'until a particular class ratio is met <NO>.', 'Therefore, we used the geometric mean of sensitivity (SE = proportion of the positives correctly recognized) and specificity (SP = proportion of the negatives correctly recognized), given by Gm = √ SE × SP, for the classifier performance evaluation in this study, as commonly used in class imbalanced-learning research <NO>, <NO>, <NO>.']	Haibo He, and Edwardo A. Garcia. 2009. Learning from imbalanced data. IEEE Knowl. Data Eng. 21, 9 (2009), 1263–1284.	With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.	"Technically speaking, any data set that exhibits an unequal distribution between its classes can be considered imbalanced. However, the common understanding in the community is that imbalanced data correspond to data sets exhibiting significant, and in some cases extreme, imbalances. Specifically, this form of imbalance is referred to as a between-class imbalance; not uncommon are betweenclass imbalances on the order of 100:1, 1,000:1, and 10,000:1, where in each case, one class severely outrepresents another <NO>, <NO>, <NO>. Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. In this paper, we only briefly touch upon the multiclass imbalanced learning problem, focusing instead on the two-class imbalanced learning problem for space considerations.
In order to highlight the implications of the imbalanced learning problem in the real world, we present an example from biomedical applications. Consider the “Mammography Data Set,” a collection of images acquired from a series of mammography exams performed on a set of distinct patients, which has been widely used in the analysis of algorithms addressing the imbalanced learning problem <NO>, <NO>, <NO>. Analyzing the images in a binary sense, the natural classes (labels) that arise are “Positive” or “Negative” for an image representative of a “cancerous” or “healthy” patient, respectively. From experience, one would expect the number of noncancerous patients to exceed greatly the number of cancerous patients; indeed, this data
set contains 10,923 “Negative” (majority class) samples and 260 “Positive” (minority class) samples. Preferably, we require a classifier that provides a balanced degree of predictive accuracy (ideally 100 percent) for both the minority and majority classes on the data set. In reality, we find that classifiers tend to provide a severely imbalanced degree of accuracy, with the majority class having close to 100 percent accuracy and the minority class having accuracies of 0-10 percent, for instance <NO>, <NO>. Suppose a classifier achieves 10 percent accuracy on the minority class of the mammography data set. Analytically, this would suggest that 234 minority samples are misclassified as majority samples. The consequence of this is equivalent to 234 cancerous patients classified (diagnosed) as noncancerous. In the medical industry, the ramifications of such a consequence can be overwhelmingly costly, more so than classifying a noncancerous patient as cancerous <NO>. Therefore, it is evident that for this domain, we require a classifier that will provide high accuracy for the minority class without severely jeopardizing the accuracy of the majority class. Furthermore, this also suggests that the conventional evaluation practice of using singular assessment criteria, such as the overall accuracy or error rate, does not provide adequate information in the case of imbalanced learning. Therefore, more informative assessment metrics, such as the receiver operating characteristics curves, precision-recall curves, and cost curves, are necessary for conclusive evaluations of performance in the presence of imbalanced data. These topics will be discussed in detail in Section 4 of this paper. In addition to biomedical applications, further speculation will yield similar consequences for domains such as fraud detection, network intrusion, and oil-spill detection, to name a few <NO>, <NO>, <NO>, <NO>, <NO>.
Imbalances of this form are commonly referred to as intrinsic, i.e., the imbalance is a direct result of the nature of the dataspace. However, imbalanced data are not solely restricted to the intrinsic variety. Variable factors such as time and storage also give rise to data sets that are imbalanced. Imbalances of this type are considered extrinsic, i.e., the imbalance is not directly related to the nature of the dataspace. Extrinsic imbalances are equally as interesting as their intrinsic counterparts since it may very well occur that the dataspace from which an extrinsic imbalanced data set is attained may not be imbalanced at all. For instance, suppose a data set is procured from a continuous data stream of balanced data over a specific interval of time, and if during this interval, the transmission has sporadic interruptions where data are not transmitted, then it is possible that the acquired data set can be imbalanced in which case the data set would be an extrinsic imbalanced data set attained from a balanced dataspace.
In addition to intrinsic and extrinsic imbalance, it is important to understand the difference between relative imbalance and imbalance due to rare instances (or “absolute rarity”) <NO>, <NO>. Consider a mammography data set with 100,000 examples and a 100:1 between-class imbalance. We would expect this data set to contain 1,000 minority class examples; clearly, the majority class dominates the minority class. Suppose we then double the sample space by testing more patients, and suppose further that the distribution
does not change, i.e., the minority class now contains 2,000 examples. Clearly, the minority class is still outnumbered; however, with 2,000 examples, the minority class is not necessarily rare in its own right but rather relative to the majority class. This example is representative of a relative imbalance. Relative imbalances arise frequently in real-world applications and are often the focus of many knowledge discovery and data engineering research efforts. Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>. These results are particularly suggestive because they show that the degree of imbalance is not the only factor that hinders learning. As it turns out, data set complexity is the primary determining factor of classification deterioration, which, in turn, is amplified by the addition of a relative imbalance.
Data complexity is a broad term that comprises issues such as overlapping, lack of representative data, small disjuncts, and others. In a simple example, consider the depicted distributions in Fig. 2. In this figure, the stars and circles represent the minority and majority classes, respectively. By inspection, we see that both distributions in Figs. 2a and 2b exhibit relative imbalances. However, notice how Fig. 2a has no overlapping examples between its classes and has only one concept pertaining to each class, whereas Fig. 2b has both multiple concepts and severe overlapping. Also of interest is subconcept C in the distribution of Fig. 2b. This concept might go unlearned by some inducers due to its lack of representative data; this issue embodies imbalances due to rare instances, which we proceed to explore.
Imbalance due to rare instances is representative of domains where minority class examples are very limited, i.e., where the target concept is rare. In this situation, the lack of representative data will make learning difficult regardless of the between-class imbalance <NO>. Furthermore, the minority concept may additionally contain a subconcept with limited instances, amounting to diverging degrees of classification difficulty <NO>, <NO>. This, in fact, is the result of another form of imbalance, a within-class imbalance, which concerns itself with the distribution of representative data for subconcepts within a class <NO>, <NO>, <NO>. These ideas are again highlighted in our simplified example in Fig. 2. In Fig. 2b, cluster B represents the dominant minority class concept and cluster C represents a subconcept of the minority class. Cluster D represents two subconcepts of the majority class and cluster A (anything
not enclosed) represents the dominant majority class concept. For both classes, the number of examples in the dominant clusters significantly outnumber the examples in their respective subconcept clusters, so that this dataspace exhibits both within-class and between-class imbalances. Moreover, if we completely remove the examples in cluster B, the dataspace would then have a homogeneous minority class concept that is easily identified (cluster C), but can go unlearned due to its severe underrepresentation.
The existence of within-class imbalances is closely intertwined with the problem of small disjuncts, which has been shown to greatly depreciate classification performance <NO>, <NO>, <NO>, <NO>. Briefly, the problem of small disjuncts can be understood as follows: A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept <NO>, <NO>, <NO>. In the case of homogeneous concepts, the classifier will generally create large disjuncts, i.e., rules that cover a large portion (cluster) of examples pertaining to the main concept. However, in the case of heterogeneous concepts, small disjuncts, i.e., rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts <NO>, <NO>, <NO>. Moreover, since classifiers attempt to learn both majority and minority concepts, the problem of small disjuncts is not only restricted to the minority concept. On the contrary, small disjuncts of the majority class can arise from noisy misclassified minority class examples or underrepresented subconcepts. However, because of the vast representation of majority class data, this occurrence is infrequent. A more common scenario is that noise may influence disjuncts in the minority class. In this case, the validity of the clusters corresponding to the small disjuncts becomes an important issue, i.e., whether these examples represent an actual subconcept or are merely attributed to noise. For example, in Fig. 2b, suppose a classifier generates disjuncts for each of the two noisy minority samples in cluster A, then these would be illegitimate disjuncts attributed to noise compared to cluster C, for example, which is a legitimate cluster formed from a severely underrepresented subconcept.
The last issue we would like to discuss is the combination of imbalanced data and the small sample size problem <NO>, <NO>. In many of today’s data analysis and knowledge discovery applications, it is often unavoidable to have data with high dimensionality and small sample size; some specific examples include face recognition and gene expression data analysis, among others. Traditionally, the small sample size problem has been studied extensively in the pattern recognition community <NO>. Dimensionality reduction methods have been widely adopted to handle this issue, e.g., principal component analysis (PCA) and various extension methods <NO>. However, when the representative data sets’ concepts exhibit imbalances of the forms described earlier, the combination of imbalanced data and small sample size presents a new challenge to the community <NO>. In this situation, there are two critical issues that arise simultaneously <NO>. First, since the sample size is small, all of the issues related to absolute rarity and within-class imbalances are applicable. Second and more importantly, learning algorithms often fail to
Fig. 2. (a) A data set with a between-class imbalance. (b) A highcomplexity data set with both between-class and within-class imbalances, multiple concepts, overlapping, noise, and lack of representative data.
generalize inductive rules over the sample space when presented with this form of imbalance. In this case, the combination of small sample size and high dimensionality hinders learning because of difficultly involved in forming conjunctions over the high degree of features with limited samples. If the sample space is sufficiently large enough, a set of general (albeit complex) inductive rules can be defined for the dataspace. However, when samples are limited, the rules formed can become too specific, leading to overfitting. In regards to learning from such data sets, this is a relatively new research topic that requires much needed attention in the community. As a result, we will touch upon this topic again later in our discussions."	https://doi.org/10.1145/1007730.1007733
140	Imbalanced Learning: Foundations, Algorithms, and Applications	[]	Haibo He, and Yunqian Ma. 2013. Imbalanced Learning: Foundations, Algorithms, and Applications. John Wiley & Sons.			http://scholar.google.com/scholar?hl=en&q=Josh+Attenberg+and+Seyda+Ertekin.+2013.+Class+imbalance+and+active+learning.+In+Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications%2C+Haibo+He+and+Yunqian+Ma+%28Eds.%29.+John+Wiley+%26+Sons.
141	Soft (gaussian CDE) regression models and loss functions	[]	José Hernández-Orallo. 2012. Soft (gaussian CDE) regression models and loss functions. arXiv Preprint arXiv:1211.1043 (2012).			http://scholar.google.com/scholar?hl=en&q=Jos%C3%A9+Hern%C3%A1ndez-Orallo.+2012.+Soft+%28gaussian+CDE%29+regression+models+and+loss+functions.+arXiv+Preprint+arXiv%3A1211.1043+%282012%29.
142	ROC} curves for regression	[]	José Hernández-Orallo. 2013. ROC} curves for regression. Pattern Recogn. 46, 12 (2013), 3395–3411. DOI:http://dx.doi.org/10.1016/j.patcog.2013.06.014			https://doi.org/10.1016/j.patcog.2013.06.014
143	Probabilistic reframing for cost-sensitive regression	[]	José Hernández-Orallo. 2014. Probabilistic reframing for cost-sensitive regression. ACM Trans. Knowl. Discov. Data 8, 4, Article 17 (Aug. 2014), 55 pages. DOI:http://dx.doi.org/10.1145/2641758			https://doi.org/10.1145/2641758
144	A unified view of performance metrics: Translating threshold choice into expected classification loss	[]	José Hernández-Orallo, Peter Flach, and César Ferri. 2012. A unified view of performance metrics: Translating threshold choice into expected classification loss. J. Mach. Learn. Res. 13, 1 (2012), 2813–2869.			https://doi.org/10.5555/2503308.2503332
145	Concept learning and the problem of small disjuncts	['However, imbalance learning problems pose a great challenge to the classifier as it becomes very hard to learn the minority class samples <NO>, <NO>, <NO>.', 'This kind of oversampling sometimes creates very specific rules, leading to overfitting <NO>.', 'Furthermore, the minority concept may additionally contain a subconcept with limited instances, amounting to diverging degrees of classification difficulty <NO>, <NO>.', 'Briefly, the problem of small disjuncts can be understood as follows: A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept <NO>, <NO>, <NO>.', ', rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts <NO>, <NO>, <NO>.', 'However, this commonality is only superficial, each method introduces its own set of problematic consequences that can potentially hinder learning <NO>, <NO>, <NO>.', 'In particular, overfitting in oversampling occurs when classifiers produce multiple clauses in a rule for multiple copies of the same example which causes the rule to become too specific; although the training accuracy will be high in this scenario, the classification performance on the unseen testing data is generally far worse <NO>.', 'Small disjuncts are those disjuncts in the learned classifier that cover few training examples <NO>.', ', small disjuncts) generally have a much higher error rate than large disjuncts <NO>.', 'If a data set has two classes and an equal number of training examples in each, then a disjunct is 99% significant if and only if it covers at least 7 training examples <NO>.', 'Empirical results <NO> show that the strategy of eliminating all small disjuncts results in an increase in overall error rate and hence is not a good strategy.', 'Volume 6, Issue 1 - Page 10 Consider a learner with a maximum-generality bias <NO>.', 'Unfortunately, this approach was shown not only to degrade performance with respect to rarity, but also to degrade overall classification performance <NO>.', 'Holte, Acker and Porter <NO> changed the bias of an existing learner, CN2, to make the bias more specific.']	Robert C. Holte, Liane E. Acker, and Bruce W. Porter. 1989. Concept learning and the problem of small disjuncts. IJCAI, Vol. 89. Citeseer, 813–818.			https://doi.org/10.5555/1623755.1623884
146	Active learning for imbalance problem using L-GEM of RBFNN	[]	Junjie Hu. 2012. Active learning for imbalance problem using L-GEM of RBFNN. ICMLC. 490–495.			http://scholar.google.com/scholar?hl=en&q=Junjie+Hu.+2012.+Active+learning+for+imbalance+problem+using+L-GEM+of+RBFNN.+In+ICMLC.+490%2D%2D495.
147	MSMOTE: Improving classification performance when training data is imbalanced	['4) Modified synthetic minority oversampling technique (MSMOTE) <NO>: It is a modified version of SMOTE.', 'Inside this family, we include SMOTEBoost <NO>, MSMOTEBoost <NO>, RUSBoost <NO>, and DataBoost-IM <NO> algorithms.']	Shengguo Hu, Yanfeng Liang, Lintao Ma, and Ying He. 2009. MSMOTE: Improving classification performance when training data is imbalanced. Second International Workshop on Computer Science and Engineering, 2009. WCSE’09, Vol. 2. IEEE, 13–17.			https://doi.org/10.5555/1681514.1682710
148	Learning classifiers from imbalanced data based on biased minimax probability machine	['to adapt a boosting algorithm to cost-sensitive problems is to change the weights of the weak classifiers in forming the final ensemble classifier, such as BMPM <NO> and LAC <NO>.', 'Further improvements are possible by learning αi,j , as shown in <NO> and <NO>.', 'Algorithmic methods include adjusting the costs associated with misclassification so as to improve performance <NO>, shifting the bias of a classifier to favor the rare class <NO>, creating Adaboost-like boosting schemes <NO>, and learning from one class <NO>.']	Kaizhu Huang, Haiqin Yang, Irwin King, and Michael R. Lyu. 2004. Learning classifiers from imbalanced data based on biased minimax probability machine. Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004. Vol. 2. IEEE, II–558.			https://doi.org/10.5555/1896300.1896381
149	A new weighted approach to imbalanced data classification problem via support vector machine with quadratic cost function	[]	Jae Pil Hwang, Seongkeun Park, and Euntai Kim. 2011. A new weighted approach to imbalanced data classification problem via support vector machine with quadratic cost function. Expert Syst. Appl. 38, 7 (2011), 8580–8585.			https://doi.org/10.1016/j.eswa.2011.01.061
150	z-SVM: An SVM for improved classification of imbalanced data	['For training SVMs with imbalanced dataset, there exist algorithmic techniques, such as Different Error Cost (DEC) <NO> and zSVM <NO>.', 'zSVM is another algorithmic modification, which is proposed for SVMs in <NO> for learning-imbalance datasets.', 'In <NO>, the value of z, which gives the best prediction for the training dataset, was selected as the optimal value of z to be used in (12).']	Tasadduq Imam, Kai Ming Ting, and Joarder Kamruzzaman. 2006. z-SVM: An SVM for improved classification of imbalanced data. AI 2006: Advances in Artificial Intelligence. Springer, 264–273.			https://doi.org/10.1007/11941439_30
151	Learning from imbalanced data sets: A comparison of various strategies	[', (Drummond & Holte, 2003), (Kubat & Matwin, 1997), (Maloof, 2003), (Japkowicz, 2000), (Weiss & Provost, 2003), (Chawla et al.', 'According to <NO>, the number of synthetic samples to be generated is set to 200 percent of the original minority class samples.', 'Japkowicz <NO> presents another study addressing the issue of class imbalance, including an investigation of the types of imbalance that most negatively impact classification performance, and a small case study comparing several techniques for alleviating the problem.', 'Many researchers, suach as Batista, G <NO>, Estabrooks <NO> and Japkowicz <NO>, proposes various approach about two resampling strategies.', 'On the other hand, these opportunities also raise many new challenges for the research community in general <NO>–<NO>.', 'Here we follow the suggestions of <NO>, <NO>, and <NO> and use the minority class as the positive class and majority class as the negative class.', 'The number of synthetic data generated at each boosting iteration is set to 200% of the number of the minority instances <NO>.', 'Representative works in this area include the one-class SVMs <NO>, <NO>, <NO>, <NO>, <NO>, <NO> and the autoassociator (or autoencoder) method <NO>, <NO>, <NO>, <NO>.', 'In <NO>, a comparison of different sampling methods and the one-class autoassociator method was presented, which provides useful suggestions about the advantages and limitations of both methods.', 'Recently, the class imbalance problem has been recognized as a crucial problem in machine learning and data mining because such a problem is encountered in a large number of domains and, in certain cases, it causes seriously negative effects on the performance of learning methods that assume a balanced distribution of classes <NO>, <NO>.', 'Although some studies have shown that oversampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, it should be noted that oversampling usually increases the training time and may lead to overfitting since it involves making exact copies of examples <NO>, <NO>.', 'Nevertheless, some studies have shown that undersampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, sometimes even stronger than oversampling, especially on large data sets <NO>, <NO>.', 'Nevertheless, some studies have shown that undersampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, sometimes even stronger than oversampling, especially on large data sets <NO>, <NO>.', 'applied to multiclass problems <NO>, in fact, little work has']	Nathalie Japkowicz. 2000. Learning from imbalanced data sets: A comparison of various strategies. AAAI Workshop on Learning from Imbalanced Data Sets, Vol. 68. Menlo Park, CA.			http://scholar.google.com/scholar?hl=en&q=Nathalie+Japkowicz.+2000.+Learning+from+imbalanced+data+sets%3A+A+comparison+of+various+strategies.+In+AAAI+Workshop+on+Learning+from+Imbalanced+Data+Sets%2C+Vol.+68.+Menlo+Park%2C+CA.
152	Concept-learning in the presence of between-class and within-class imbalances	[]	Nathalie Japkowicz. 2001. Concept-learning in the presence of between-class and within-class imbalances. Advances in Artificial Intelligence. Springer, 67–77.			https://doi.org/10.5555/647462.726288
153	Class imbalances: Are we focusing on the right issue	['Japkowicz <NO> discussed about the cause for lower performance in standard classifiers is actually small disjuncts of within-class.', 'Moreover, in <NO> Japkowicz performed several experiments on artificial data sets and concluded that class imbalances do not seem to systematically cause performance degradation.']	Nathalie Japkowicz. 2003. Class imbalances: Are we focusing on the right issue. Workshop on Learning from Imbalanced Data Sets II, Vol. 1723. 63.			http://scholar.google.com/scholar?hl=en&q=Nathalie+Japkowicz.+2003.+Class+imbalances%3A+Are+we+focusing+on+the+right+issue.+In+Workshop+on+Learning+from+Imbalanced+Data+Sets+II%2C+Vol.+1723.+63.
154	Assessment metrics for imbalanced learning	[]	Natalie Japkowicz. 2013. Assessment metrics for imbalanced learning. Imbalanced Learning: Foundations, Algorithms, and Applications, Haibo He and Yunqian Ma (Eds.). John Wiley & Sons.			http://scholar.google.com/scholar?hl=en&q=Natalie+Japkowicz.+2013.+Assessment+metrics+for+imbalanced+learning.+In+Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications%2C+Haibo+He+and+Yunqian+Ma+%28Eds.%29.+John+Wiley+%26+Sons.
155	A novelty detection approach to classification	['This becomes very costly when identification of the minority class samples is crucial <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Another technique suggested for class imbalance problem is to use a recognition-based, instead of discrimination-based inductive learning <NO>.', 'Representative works in this area include the one-class SVMs <NO>, <NO>, <NO>, <NO>, <NO>, <NO> and the autoassociator (or autoencoder) method <NO>, <NO>, <NO>, <NO>.', 'One data mining system that utilizes this recognition-based approach is Hippo <NO>.']	Nathalie Japkowicz, Catherine Myers, and Mark Gluck. 1995. A novelty detection approach to classification. IJCAI. 518–523.			https://doi.org/10.5555/1625855.1625923
156	Evaluating Learning Algorithms: A Classification Perspective	[]	Nathalie Japkowicz, and Mohak Shah. 2011. Evaluating Learning Algorithms: A Classification Perspective. Cambridge University Press.			https://doi.org/10.5555/1964882
157	The class imbalance problem: A systematic study	['In <NO>, the authors reported that the error rate caused by imbalanced class distribution decreases when the number of examples of', 'Recently, reports from both academy and industry indicate that the imbalanced class distribution of a data set has posed a serious difficulty to most classifier learning algorithms which assume a relatively balanced distribution <NO>.', 'in what domains do class imbalances most hinder the performance of a standard classifier? <NO>; and 3) what are the possible solutions in dealing with the class imbalance problem? With regard to the first aspect, it is stated that accuracy is traditionally the most commonly used measure in both assessing the classification models and guiding the search algorithms.', 'With respect to the second aspect, a thorough study can be found in <NO>.', 'have been found to depreciate in the presence of within-class imbalance and small disjuncts problems <NO>, <NO>, <NO>.', 'the classifiers’ poor performance <NO>.', 'It has been shown that oversampling is lot more useful than undersampling and oversampling dramatically improves classifiers performance even for complex data <NO>.', 'Mining highly unbalanced data sets, particularly in a cost-sensitive environment, is among the leading challenges for knowledge discovery and data mining <NO>, <NO>.', '2) Most previous works use decision trees as the basic classifier <NO>.', 'Random undersampling was studied in <NO>, <NO>.', 'Attempts have been made to deal with this problem in diverse domains such as fraud detection <NO>, in-flight helicopter gearbox fault monitoring <NO>, and text categorization <NO>.', 'There is a significant body of research comparing the various sampling methods <NO>.', 'Recent research showed that oversampling at random does not help to improve prediction performance <NO> therefore we use a more complex oversampling Table 2: Overview of the datasets.', 'For example, the work on cost sensitive learning <NO> aimed at reducing total cost, and sampling approaches <NO> to favour the minority class are usually demonstrated with decision tree algorithms and/or naive Bayes.', 'Skewed data is transformed into partitions with more of the rarer examples and fewer of the common examples using the procedure known as minority oversampling with replacement/replication <NO>.', 'Indeed, there are at least three surveys of methods for improving classification performance on imbalanced datasets <NO>, <NO>, <NO>.', 'A possible explanation is that SVM frequently uses few support vectors to determine the separation between classes, as previously observed by <NO>.', 'On the other hand, Japkowicz and Stephen <NO> compare several methods of over and under-sampling on a series of artificial data sets and conclude that over-sampling is more effective than under-sampling at reducing error rate.', 'Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>.', 'The existence of within-class imbalances is closely intertwined with the problem of small disjuncts, which has been shown to greatly depreciate classification performance <NO>, <NO>, <NO>, <NO>.', 'In this case, imbalanced data sets exploit inadequacies in the splitting criterion at each node of the decision tree <NO>, <NO>, <NO>.', 'However, they do not imply that classifiers cannot learn from imbalanced data sets; on the contrary, studies have also shown that classifiers induced from certain imbalanced data sets are comparable to classifiers induced from the same data set balanced by sampling techniques <NO>, <NO>.', 'The representative kernel-based learning paradigm, support vector machines (SVMs), can provide relatively robust classification results when applied to imbalanced data sets <NO>.', 'A two-class data set is said to be imbalanced when one of the classes is heavily under-represented as regards the other class <NO>.', 'Many systems have used some variation of precision and recall to guide the data mining process and evaluate the end result <NO>.', 'However, a study that used artificial domains came to the opposite conclusion <NO>.', 'Sampling techniques have generated the most research in this area and there are a few studies <NO> that compare sampling methods.', 'This recommendation is also supported by research that shows that such methods outperform over-sampling and under-sampling <NO>.', 'Although some studies have shown that oversampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, it should be noted that oversampling usually increases the training time and may lead to overfitting since it involves making exact copies of examples <NO>, <NO>.', 'Nevertheless, some studies have shown that undersampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, sometimes even stronger than oversampling, especially on large data sets <NO>, <NO>.']	Nathalie Japkowicz, and Shaju Stephen. 2002. The class imbalance problem: A systematic study. Intell. Data Anal. 6, 5 (2002), 429–449.			https://doi.org/10.5555/1293951.1293954
158	Classification of imbalanced data by combining the complementary neural network and SMOTE algorithm	[]	Piyasak Jeatrakul, Kok Wai Wong, and Chun Che Fung. 2010. Classification of imbalanced data by combining the complementary neural network and SMOTE algorithm. Neural Information Processing. Models and Applications. Springer, 152–159.			https://doi.org/10.5555/1939751.1939773
159	Class imbalances versus small disjuncts	['Japkowicz <NO> discussed about the cause for lower performance in standard classifiers is actually small disjuncts of within-class.', 'have been found to depreciate in the presence of within-class imbalance and small disjuncts problems <NO>, <NO>, <NO>.', 'then the imbalance problem becomes very severe <NO>, <NO>.', 'Random oversampling is a nonheuristic method that adds samples through the random replication of the minority class samples <NO>, <NO>.', 'Regardless of the method used to counter the imbalance problem, factors such as the uncertainty of the true distribution between the samples of the minority class and majority class samples, complexity of data, and noise in data may pose a limit on the classifiers’ performance <NO>, <NO>, <NO>.', 'This, in fact, is the result of another form of imbalance, a within-class imbalance, which concerns itself with the distribution of representative data for subconcepts within a class <NO>, <NO>, <NO>.', 'The existence of within-class imbalances is closely intertwined with the problem of small disjuncts, which has been shown to greatly depreciate classification performance <NO>, <NO>, <NO>, <NO>.', 'In <NO>, the cluster-based oversampling (CBO) algorithm is proposed to effectively deal with the within-class imbalance problem in tandem with the between-class imbalance problem.', 'For instance, Jo and Japkowicz <NO> used the', 'A clustering-based sampling method has been proposed in <NO>, while a genetic algorithmbased sampling method has been proposed in <NO>.', 'cluster-based oversampling <NO>, Synthetic Minority Oversampling Technique (SMOTE) <NO>, and Borderline-SMOTE <NO>.', 'classes but also helps balance instance grouping within the two classes <NO>.']	Taeho Jo, and Nathalie Japkowicz. 2004. Class imbalances versus small disjuncts. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 40–49.			https://doi.org/10.1145/1007730.1007737
160	Evaluating boosting algorithms to classify rare classes: Comparison and improvements	['way that they modify the weight update rule, among this family AdaCost <NO>, CSB1, CSB2 <NO>, RareBoost <NO>, AdaC1, AdaC2, and AdaC3 <NO> are the most representative approaches.', 'However, for a classification model induced from a data set with imbalanced class distribution, accuracy is no longer a proper measure since rare classes have very few impact on accuracy than prevalent classes <NO>.', 'Within the bi-class applications, some variants of the AdaBoost algorithm in tackling the imbalance problem are reported, such as AdaCost <NO>, CSB1 and CSB2 <NO>, RareBoost <NO> and AdaC1, AdaC2 and AdaC3 <NO>.', 'Several variations have been proposed to make AdaBoost cost sensitive <NO>–<NO> or to improve its performance on imbalanced data <NO>–<NO>.', 'Many representative works on the ineffectiveness of accuracy in the imbalanced learning scenario exist in the community <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.']	Mahesh V. Joshi, Vipin Kumar, and Ramesh C. Agarwal. 2001. Evaluating boosting algorithms to classify rare classes: Comparison and improvements. Proceedings IEEE International Conference on Data Mining, 2001. ICDM 2001. IEEE, 257–264.			https://doi.org/10.5555/645496.658037
161	EUS SVMs: Ensemble of under-sampled SVMs for data imbalance problems	[]	Pilsung Kang, and Sungzoon Cho. 2006. EUS SVMs: Ensemble of under-sampled SVMs for data imbalance problems. Neural Information Processing. Springer, 837–846.			https://doi.org/10.1007/11893028_93
162	Learning with limited minority class data	['to 35 : 65 (minority:majority) may result in better classification performance <NO>.', 'Another experimental work is presented in <NO>.', 'An interesting feature of the experimental design used in <NO> is to allow the analysis of classification performance factorized by class rarity and class distribution.']	Taghi M. Khoshgoftaar, Chris Seiffert, Jason Van Hulse, Amri Napolitano, and Andres Folleco. 2007. Learning with limited minority class data. Sixth International Conference on Machine Learning and Applications, 2007. ICMLA 2007. IEEE, 348–353.			https://doi.org/10.1109/ICMLA.2007.64
163	Handling imbalanced datasets: A review	[]	Sotiris Kotsiantis, Dimitris Kanellopoulos, and Panayiotis Pintelas. 2006. Handling imbalanced datasets: A review. GESTS Int. Trans. Comput. Sci. Eng. 30, 1 (2006), 25–36.			http://scholar.google.com/scholar?hl=en&q=Sotiris+Kotsiantis%2C+Dimitris+Kanellopoulos%2C+and+Panayiotis+Pintelas.+2006.+Handling+imbalanced+datasets%3A+A+review.+GESTS+Int.+Trans.+Comput.+Sci.+Eng.+30%2C+1+%282006%29%2C+25%2D%2D36.
164	Mixture of expert agents for handling imbalanced data sets	[]	Sotiris Kotsiantis, and Panagiotis Pintelas. 2003. Mixture of expert agents for handling imbalanced data sets. Ann. Math. Comput. Teleinform. 1, 1 (2003), 46–55.			http://scholar.google.com/scholar?hl=en&q=Sotiris+Kotsiantis+and+Panagiotis+Pintelas.+2003.+Mixture+of+expert+agents+for+handling+imbalanced+data+sets.+Ann.+Math.+Comput.+Teleinform.+1%2C+1+%282003%29%2C+46%2D%2D55.
165	Machine learning for the detection of oil spills in satellite radar images	['For instance, some applications are known to suffer from this problem, fault diagnosis <NO>, <NO>, anomaly detection <NO>, <NO>, medical diagnosis <NO>, e-mail foldering <NO>, face recognition <NO>, or detection of oil spills <NO>, among others.', 'Recently, reports from both academy and industry indicate that the imbalanced class distribution of a data set has posed a serious difficulty to most classifier learning algorithms which assume a relatively balanced distribution <NO>.', 'For the bi-class scenario, Kubat et al <NO> suggested the G-mean as the geometric means of recall values of two classes.', '<NO>, data mining from direct marketing <NO>, and helicopter', 'This becomes very costly when identification of the minority class samples is crucial <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Researchers in the machine learning community have dealt with the problem of class imbalance by using various approaches like over-sampling the minority classes, undersampling the majority classes, assigning different costs for different misclassification errors, learning by recognition as opposed to discrimination, etc <NO>.', 'Examples of applications which may have class imbalance problem include, but are not limited to, predicting pre-term births <NO>, identifying fraudulent credit card transactions <NO>, text categorization <NO>, classification of protein databases <NO> and detecting certain objects from satellite images <NO>.', '(In the remainder of the paper negative is always taken to be the majority class and positive is the minority class) Example applications include vision recognition <NO>, bioinformatics<NO>, credit card fraud detection<NO>, the detection of oil spills<NO> and so on.', 'Our own experience with im balanced classes<NO> dealt with the detection of oil spills and the number of non-spills far outweighed the number of spills.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Specifically, this form of imbalance is referred to as a between-class imbalance; not uncommon are betweenclass imbalances on the order of 100:1, 1,000:1, and 10,000:1, where in each case, one class severely outrepresents another <NO>, <NO>, <NO>.', 'In addition to biomedical applications, further speculation will yield similar consequences for domains such as fraud detection, network intrusion, and oil-spill detection, to name a few <NO>, <NO>, <NO>, <NO>, <NO>.', 'Examples of such domains include those in which there are almost an infinite number of instances from the outlier classes, such as in typist recognition <NO>, or those in which obtaining instances from the outlier classes is dependent upon the occurrence of a rare event1, such as the detection of oil spills <NO> or the inclusion of journal articles for systematic reviews <NO>.', 'Examples abound and include identifying fraudulent credit card transactions <NO>, learning word pronunciations <NO>, predicting pre-term births <NO>, predicting telecommunication equipment failures <NO>, and detecting oil spills from satellite images <NO>.', 'ROC analysis has been used by many systems designed to deal with rarity, such as the Shrink data mining system <NO>.', 'Many systems have used some variation of precision and recall to guide the data mining process and evaluate the end result <NO>.', 'The advantage of Brute’s approach is summarized nicely in <NO>, “by measuring performance only of the positive predicting rules Brute is not influenced by the invariably high accuracy of the negative examples that are not covered by the positive predicting rules.']	Miroslav Kubat, Robert C. Holte, and Stan Matwin. 1998. Machine learning for the detection of oil spills in satellite radar images. Mach. Learn. 30, 2–3 (1998), 195–215.			https://doi.org/10.1023/A:1007452223027
166	Addressing the curse of imbalanced training sets: One-sided selection	['Oversampling <NO> the minority class and undersampling <NO> the majority class are the data level approaches.', 'As a remedy, researchers have used alternative performance measures, such as Geometric-mean (Gm) <NO>,<NO> and F-measure (Fm) <NO>,<NO> for classifier performance evaluation in imbalanced dataset learning.', 'It has been well-studied in the past research that the most widely used performance measure, the Accuracy (Acc), can lead to sub-optimal classification models, if it is used for imbalanced dataset learning <NO>-<NO>.', 'In order to overcome this problem, other performance measures, such as the Gm <NO>,<NO> and Fm <NO>,<NO>, have been used in class imbalance learning research.', 'Gm metric has been introduced in <NO> to overcome the problems associated with Acc metric in imbalanced dataset', 'Some informed undersampling methods also apply data cleaning techniques <NO> to further refine the majority class samples <NO>, <NO>.', 'Based on these two metrics, G-mean was proposed at (4), which is the geometric mean of sensitivity and specificity <NO>.', 'Random undersampling was studied in <NO>, <NO>.', 'The one-sided selection procedures <NO> tried to find a representative subset of majority class examples by only removing “borderline” and “noisy” majority examples.', 'Here we follow the suggestions of <NO>, <NO>, and <NO> and use the minority class as the positive class and majority class as the negative class.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'For instance, the one-sided selection (OSS) method <NO> selects a representative subset of the majority class E and combines it with the set of all minority examples Smin to form a preliminary set N;N 1⁄4 E [ Smin f g.', 'Some representative work in this area includes the OSS method <NO>, the condensed nearest neighbor rule and Tomek Links ðCNNþTomek LinksÞ integration method <NO>, the neighborhood cleaning rule (NCL) <NO> based on the edited nearest neighbor (ENN) rule—which removes examples that differ from two of its three nearest neighbors, and the integrations of SMOTE with ENN ðSMOTEþENNÞ and SMOTE with Tomek links ðSMOTEþTomekÞ <NO>.', '<NO> use the geometric mean of accuracies measured separately on each class, with the aim of maximizing the accuracies of both classes while keeping them balanced, Gmean = √ TPrate · TNrate.', 'The performance measure we use is the geometric mean of the per-class accuracies <NO>.', 'One under-sampling strategy <NO> only removes majority-class examples that are redundant with other examples or border regions with minority-class examples, figuring that they may be the result of noise.', 'Therefore, we used the geometric mean of sensitivity (SE = proportion of the positives correctly recognized) and specificity (SP = proportion of the negatives correctly recognized), given by Gm = √ SE × SP, for the classifier performance evaluation in this study, as commonly used in class imbalanced-learning research <NO>, <NO>, <NO>.', 'Here, a routine similar to that used in <NO> is employed, which removes redundant examples at first and then removes borderline examples and examples suffering from the class label noise.']	Miroslav Kubat, and Stan Matwin. 1997. Addressing the curse of imbalanced training sets: One-sided selection. Proc. of the 14th Int. Conf. on Machine Learning. Morgan Kaufmann, 179–186.			http://scholar.google.com/scholar?hl=en&q=Miroslav+Kubat+and+Stan+Matwin.+1997.+Addressing+the+curse+of+imbalanced+training+sets%3A+One-sided+selection.+In+Proc.+of+the+14th+Int.+Conf.+on+Machine+Learning.+Morgan+Kaufmann%2C+179%2D%2D186.
167	Improving Identification of Difficult Small Classes by Balancing Class Distribution	['Laurikkala <NO> is an under-sampling method.', 'Although it is impossible to predict what true class distribution should be <NO>, <NO> it is observed that classifiers learn well from a balanced distribution than from an imbalanced one <NO>, <NO>, <NO>.', 'Neighborhood Cleaning Rule Neighborhood Cleaning Rule (NCL) <NO> uses the Wilson’s Edited Nearest Neighbor Rule (ENN) <NO> to remove majority class examples.', 'Studies have shown that for several base classifiers, a balanced data set provides improved overall classification performance compared to an imbalanced data set <NO>, <NO>, <NO>.', 'Some representative work in this area includes the OSS method <NO>, the condensed nearest neighbor rule and Tomek Links ðCNNþTomek LinksÞ integration method <NO>, the neighborhood cleaning rule (NCL) <NO> based on the edited nearest neighbor (ENN) rule—which removes examples that differ from two of its three nearest neighbors, and the integrations of SMOTE with ENN ðSMOTEþENNÞ and SMOTE with Tomek links ðSMOTEþTomekÞ <NO>.']	Jorma Laurikkala. 2001. Improving Identification of Difficult Small Classes by Balancing Class Distribution. Springer.			http://scholar.google.com/scholar?hl=en&q=Jorma+Laurikkala.+2001.+Improving+Identification+of+Difficult+Small+Classes+by+Balancing+Class+Distribution.+Springer.
168	The novelty detection approach for different degrees of class imbalance	[]	Hyoung-joo Lee, and Sungzoon Cho. 2006. The novelty detection approach for different degrees of class imbalance. Neural Information Processing. Springer, 21–30.			https://doi.org/10.1007/11893257_3
169	Regularization in skewed binary classification	[]	Sauchi Stephen Lee. 1999. Regularization in skewed binary classification. Comput. Stat. 14, 2 (1999), 277.			http://scholar.google.com/scholar?hl=en&q=Sauchi+Stephen+Lee.+1999.+Regularization+in+skewed+binary+classification.+Comput.+Stat.+14%2C+2+%281999%29%2C+277.
170	Noisy replication in skewed binary classification	[]	Sauchi Stephen Lee. 2000. Noisy replication in skewed binary classification. Comput. Stat. Data Anal. 34, 2 (2000), 165–191.			https://doi.org/10.1016/S0167-9473(99)00095-X
171	Loss functions in time series forecasting	[]	Tae-Hwy Lee. 2008. Loss functions in time series forecasting. International Encyclopedia of the Social Sciences (2008).			http://scholar.google.com/scholar?hl=en&q=Tae-Hwy+Lee.+2008.+Loss+functions+in+time+series+forecasting.+International+Encyclopedia+of+the+Social+Sciences+%282008%29.
172	An improved P-SVM method used to deal with imbalanced data sets	[]	Chen Li, Chen Jing, and Gao Xin-tao. 2009. An improved P-SVM method used to deal with imbalanced data sets. IEEE International Conference on Intelligent Computing and Intelligent Systems, 2009. ICIS 2009, Vol. 1. IEEE, 118–122.			http://scholar.google.com/scholar?hl=en&q=Chen+Li%2C+Chen+Jing%2C+and+Gao+Xin-tao.+2009.+An+improved+P-SVM+method+used+to+deal+with+imbalanced+data+sets.+In+IEEE+International+Conference+on+Intelligent+Computing+and+Intelligent+Systems%2C+2009.+ICIS+2009%2C+Vol.+1.+IEEE%2C+118%2D%2D122.
173	An improved SMOTE imbalanced data classification method based on support degree	[]	Kewen Li, Wenrong Zhang, Qinghua Lu, and Xianghua Fang. 2014. An improved SMOTE imbalanced data classification method based on support degree. 2014 International Conference on Identification, Information and Knowledge in the Internet of Things (IIKI). IEEE, 34–38.	Imbalanced data-set Classification has become a hotspot problem in Data Mining. The essential assumption of the traditional classification algorithms is that the distribution of the classes is balanced, therefore the algorithms used in Imbalanced data-set Classification cannot achieve an ideal effect. In view of imbalance date-set classification, we propose an oversampling method based on support degree in order to guide people to select minority class samples and generate new minority class samples. In the light of support degree, it is now possible to identify minority class boundary samples, then produce a number of new samples between the boundary samples and their neighbors, finally add the synthetic samples to the original data-set to participate in training and testing. Experimental results show that the method has an obvious advantage in dealing with imbalanced data-set. 	"In data level, people mainly use sampling techniques to deal with imbalanced data. The basic idea of sampling is that we change the distribution of training samples to overcome the imbalance of data-set. Data sampling techniques include three types: under-sampling, over-sampling, mixed sampling. Under-sampling removes some majority class samples in order to achieve balanced data-set; oversampling increases the number of minority class samples to change the distribution of data-set; mix sampling uses both over-sampling and undersampling techniques to deal with data-set.
1) Under-sampling technique Random under-sampling <NO> is the most simple and common method in under-sampling technique, it changes the distribution of data-set by removing some negative class samples randomly, but this method also exists shortcomings, such as deleting samples artificially may lose the samples with important information and reduce the performance of classifiers.
NCR(Neighborhood Cleaning Rule, NCR) proposed by J.Laurikkala <NO> is an under-sampling method. It uses the nearest neighbor thought to remove negative class samples. Its basic idea is as follows: select a sample iX from data-set randomly, then find its three nearest neighbors and their categories, compare iX with the three neighbors: if iX is a negative class sample, at least two of the three samples are positive class samples, then remove iX from the data-set; if
iX is a positive class sample, at least two of the three samples are negative class samples, then remove the three neighbors from the data-set. So you can use this method to under-sample negative class samples.
978-1-4799-8003-1/14 $31.00 © 2014 IEEE DOI 10.1109/IIKI.2014.14
34
2) Over-sampling technique Random over-sampling <NO> is the most simple and common method in oversampling technique. It increases the number of positive class by copying positive class samples randomly. This method really changes the distribution of dataset, but it has some shortcomings: copying too many positive class samples may cause classifier over-fitting , the time required for building classifiers becomes longer.
SMOTE algorithm <NO> is a classic oversampling algorithm. The basic idea of SMOTE is that new positive class samples are synthesized through linear interpolation between two near positive class samples, then add them to the original data-set. The two classes could be balanced by increasing new minority class samples. The specific approach is: for a positive class sample iX , calculate its distance from other samples of positive class, then select a sample jX from the k-nearest neighbor samples of positive class randomly, finally generate new samples as the following manner:
(0,1) ( )new i j iX X rand X X
According to Eq. 1, newX is added to participate in the training and testing. The method can prevent the occurrence of over-fitting effectively because it is not just copying positive class samples, but it cannot provide a scalar control of the number of new samples, and it cannot select positive class samples and synthesize new samples with guidance, so the quality of the new samples is not very good.
3) Mix sampling technique Both over-sampling and under-sampling are able to reduce the imbalance of data-set, but they have some drawbacks inevitably. C.Drummond <NO> proposed that the performance of classifiers which are built based on under-sampling technology is superior to the performance of classifiers which are built based on over-sampling technology, Chris Seiffert <NO> put forward a similar view from the model training complexity and training time, GEBatista <NO> thought that over-sampling technique was better than under-sampling techniques when there are overlaps in the data-set. There is not a uniform conclusion about which is better method. Therefore, combination of the two techniques is a common approach to imbalanced data classification."	https://doi.org/10.1109/IIKI.2014.14
174	A hybrid re-sampling method for SVM learning from imbalanced data sets	[]	Peng Li, Pei-Li Qiao, and Yuan-Chao Liu. 2008. A hybrid re-sampling method for SVM learning from imbalanced data sets. Fifth International Conference on Fuzzy Systems and Knowledge Discovery, 2008. FSKD’08. Vol. 2. IEEE, 65–69.	Support Vector Machine (SVM) has been widely studied and shown success in many application fields. However, the performance of SVM drops significantly when it is applied to the problem of learning from imbalanced data sets in which negative instances greatly outnumber the positive instances. This paper analyzes the intrinsic factors behind this failure and proposes a suitable re-sampling method. We re-sample the imbalance data by using variable SOM clustering so as to overcome the flaws of the traditional re-sampling methods, such as serious randomness, subjective interference and information loss. Then we prune the training set by means of K-NN rule to solve the problem of data confusion, which improves the generalization ability of SVM. Experiment results show that our method obviously improves the performance of the SVM on imbalanced data sets.	"Imbalanced datasets have two inner factors, namely, imbalance ratio (IR) and lack of information (LI). Imbalance ratio is the value of Number of Majority/ Number of Minority and LI is the lack of information for the monority class. For a data set consisting of 100:10 majority : minority examples the imbalance factor IR is the same as in a data set of 1000:100, but the intuition implicate us there are several defference in them. In the first case the minority class is poorly respresented and suffers more from the LI factor than in the second case. Both the above inherent factors are present in every IDS learning problem, in combination with other external factors, such as overlap, complexity, size of the data and high dimension etc.
We theoretically analyze Influencing factors by means of linear separable imbalanced datasets. In figure 1(a), SVM learning from an imbanlanced data set, the result show that the learning hyperplane has almost the same orientation as the ideal hyperplane, but the distance of the learning hyperplane is far away from the ideal hyperplane. Furthermore, learning hyperplane is too close to the positive support vectors. In figure 1(b), the learning hyperplane will lean toward the negative instances and mis-identify the positive instances to negative ones in the process of testing. The phenomemon is data-whelming, when the training data gets more imbalanced, the ratio between the positive and negative support vectors also becomes more skewed. They dicide the learning hyperplane far off the ideal hyperplane, the neighborhood of a testing instances close to the boundary is more likely to be dominated by negative support vectors and hence the decision function is more likely to classify a boundary point negative and led to the majority whelm the minority.
If we randomly under-sampe the majority instances of imbalance training data, until their numbers are equal to the minority instances in gross. In figure 2(a), we can
see that the learning hyperplane is close to ideal hyperplane but the orientation of the learning hyperplane is no longer accurate. In figure 2(b), the learning hyperplane will lead to severely wrong classification result in the process of testing. This phenomemon is information loss. The reason is that mass negative instances are cut down randomly lead to many valuable information is lossing, the remainder negative instances can no longer give good cues about the orientation of the hyperplane and there is a greater degree of freedom for the orientation to vary.
From above analysis, we get the guiding principle to our re-sampling method that is to find a strategy to filter large number of majority instances, which are far away from the target boundary, without losing too many minority instances. This allows us to concentrate on distinguishing the more difficult boundary instances and reduce the imbablance ratio which makes the learning task more tractable."	https://doi.org/10.1109/FSKD.2008.407
175	UCI Machine Learning Repository	['Since we want to promote reproducibility, most of them are public domain benchmark data sets available in repositories such as UCI Machine Learning Repository <NO> or used in projects like Statlog <NO>.', 'Many datasets in these two collections are available or are modifications of datasets in the UCI Repository <NO>.']	M. Lichman. 2013. UCI Machine Learning Repository. University of California, Irvine, School of Information and Computer Sciences. http://archive.ics.uci.edu/ml.			http://scholar.google.com/scholar?hl=en&q=M.+Lichman.+2013.+UCI+Machine+Learning+Repository.+University+of+California%2C+Irvine%2C+School+of+Information+and+Computer+Sciences.+http%3A%2F%2Farchive.ics.uci.edu%2Fml.
176	Fuzzy support vector machines	['Fuzzy support vector machines (FSVMs) is a variant of the SVM learning algorithm, which was originally proposed in <NO> in order to handle the problem of outliers and noise.', 'Lin and Wang <NO> applies a fuzzymembership value for each training example, which is based on the importance of that example of its class, and reformulates the SVM learning algorithm such that different input points can make different contributions when finding the separating hyperplane.', 'initially proposed in <NO>, as a solution for the problem of outliers and noise.', 'As mentioned earlier, the FSVM method proposed in <NO> assigns different fuzzy-membership values mi (or weights) for different examples to reflect their importance for their own class, where more important examples are assigned higher membership values, while the less-important ones, such as outliers and noise, are assigned lower membership values.', 'In order to solve the FSVM optimization problem, (13) is transformed into the following dual Lagrangian <NO>:', '1) f(xi) is Based on the Distance from the Own Class Center: In this method, f(xi) is defined with respect to dcen i , which is the distance between xi and its own class center, as previously proposed for FSVMs in <NO>.']	Chun-Fu Lin, and Sheng-De Wang. 2002. Fuzzy support vector machines. IEEE Trans. Neur. Network. 13, 2 (2002), 464–471.	A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different constributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).	In this section we briefly review the basis of the theory of SVM in classification problems <NO>–<NO>.	https://doi.org/10.1109/TFUZZ.2010.2042721
177	Generative oversampling for mining imbalanced datasets	[]	Alexander Liu, Joydeep Ghosh, and Cheryl E. Martin. 2007. Generative oversampling for mining imbalanced datasets. DMIN. 66–72.			http://scholar.google.com/scholar?hl=en&q=Alexander+Liu%2C+Joydeep+Ghosh%2C+and+Cheryl+E.+Martin.+2007.+Generative+oversampling+for+mining+imbalanced+datasets.+In+DMIN.+66%2D%2D72.
178	A robust decision tree algorithm for imbalanced data sets	[]	Wei Liu, Sanjay Chawla, David A. Cieslak, and Nitesh V. Chawla. 2010. A robust decision tree algorithm for imbalanced data sets. SDM, Vol. 10. SIAM, 766–777.			http://scholar.google.com/scholar?hl=en&q=Wei+Liu%2C+Sanjay+Chawla%2C+David+A.+Cieslak%2C+and+Nitesh+V.+Chawla.+2010.+A+robust+decision+tree+algorithm+for+imbalanced+data+sets.+In+SDM%2C+Vol.+10.+SIAM%2C+766%2D%2D777.
179	Exploratory undersampling for class-imbalance learning	['Literally hundreds of research papers have reported that imbalanced data lead to performance losses, and some sort of treatment (such as sampling <NO>, cost-sensitive learning <NO>, ensembles <NO>, among others) are able to improve classification.']	Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. 2009. Exploratory undersampling for class-imbalance learning. IEEE Trans. Syst. Man Cybernet. B 39, 2 (2009), 539–550.	Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. EasyEnsemble samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. BalanceCascade trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the ROC Curve, F-measure, and G-mean values than many existing classimbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods.	"As mentioned in the previous section, many existing classimbalance learning methods manipulate the following four components: training set size, class prior, cost matrix, and placement of decision boundary. Here, we pay special attention to two classes of methods that are most widely used: sampling
1083-4419/$25.00 © 2008 IEEE
and cost-sensitive learning. For other methods, we refer the readers to <NO> for a more complete and detailed review.
Sampling is a class of methods that alters the size of training sets. Undersampling and oversampling change the training sets by sampling a smaller majority training set and repeating instances in the minority training set, respectively <NO>. The level of imbalance is reduced in both methods, with the hope that a more balanced training set can give better results. Both sampling methods are easy to implement and have been shown to be helpful in imbalanced problems <NO>, <NO>. Undersampling requires shorter training time, at the cost of ignoring potentially useful data. Oversampling increases the training set size and thus requires longer training time. Furthermore, it tends to lead to overfitting since it repeats minority class examples <NO>, <NO>. Aside from the basic undersampling and oversampling methods, there are also methods that sample in more complex ways. SMOTE <NO> added new synthetic minority class examples by randomly interpolating pairs of closest neighbors in the minority class. The one-sided selection procedures <NO> tried to find a representative subset of majority class examples by only removing “borderline” and “noisy” majority examples. Some other methods combine different sampling strategies to achieve further improvement <NO>. In addition, researchers have studied the effect of varying the level of imbalance and how to find the best ratio when a C4.5 tree classifier was used <NO>.
Cost-sensitive learning <NO>, <NO> is another important class of class-imbalance learning methods. Although many learning algorithms have been adapted to accommodate class-imbalance and cost-sensitive problems, variants of AdaBoost appear to be the most popular ones. Many cost-sensitive boosting algorithms have been proposed <NO>. A common strategy of these variants was to intentionally increase the weights of examples with higher misclassification cost in the boosting process. In <NO>, the initial weights of high cost examples were increased. It was reported that, however, the weight differences between examples in different classes disappear quickly when the boosting process proceeds <NO>. Thus, many algorithms raised high cost examples’ weights in every iteration of the boosting process, for example, AsymBoost <NO>, AdaCost <NO>, CSB <NO>, DataBoost <NO>, and AdaUBoost <NO>, just to name a few. Another way to adapt a boosting algorithm to cost-sensitive problems is to change the weights of the weak classifiers in forming the final ensemble classifier, such as BMPM <NO> and LAC <NO>. Unlike the heuristic methods mentioned earlier, Asymmetric Boosting <NO> directly minimized a cost-sensitive loss function in the statistical interpretation of boosting.
SMOTEBoost <NO> is designed for class-imbalance learning, which is very similar to AsymBoost. Both methods alter the distribution for the minority class and majority class in separate ways. The only difference is how these distributions are altered. AsymBoost directly updates instance weights for the majority class and minority class differently in each iteration, while SMOTEBoost alters distribution by first updating instance weights for majority class and minority class equally and then using SMOTE to get new minority class instances.
Chan and Stolfo <NO> introduced an approach to explore majority class examples. They split the majority class into several nonoverlapping subsets, with each subset having ap-
proximately the same number of examples as the minority class. One classifier was trained from each of these subsets and the minority class. The final classifier ensembled these classifiers using stacking <NO>. However, when a data set is highly imbalanced, this approach requires a much longer training time than undersampling. Moreover, since the minority class examples are used by every classifier, stacking these classifiers will have a high probability of suffering from overfitting when the number of minority class examples is limited.
III. EasyEnsemble AND BalanceCascade
As was shown by Drummond and Holte <NO>, undersampling is an efficient strategy to deal with class imbalance. However, the drawback of undersampling is that it throws away many potentially useful data. In this section, we propose two strategies to explore the majority class examples ignored by undersampling: EasyEnsemble and BalanceCascade."	https://doi.org/10.1109/TSMCB.2008.2007853
180	Boosting prediction accuracy on imbalanced datasets with SVM ensembles	[]	Yang Liu, Aijun An, and Xiangji Huang. 2006. Boosting prediction accuracy on imbalanced datasets with SVM ensembles. Advances in Knowledge Discovery and Data Mining. Springer, 107–118.			https://doi.org/10.1007/11731139_15
181	An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics	[]	Victoria López, Alberto Fernández, Salvador Garcı́a, Vasile Palade, and Francisco Herrera 2013. An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics. Inform. Sci			http://scholar.google.com/scholar?hl=en&q=Victoria+L%C3%B3pez%2C+Alberto+Fern%C3%A1ndez%2C+Salvador+Garc%C3%ADa%2C+Vasile+Palade%2C+and+Francisco+Herrera.+2013.+An+insight+into+classification+with+imbalanced+data%3A+Empirical+results+and+current+trends+on+using+data+intrinsic+characteristics.+Inform.+Sci.+250+%282013%29%2C+113%2D%2D141.
182	On the importance of the validation technique for classification with imbalanced datasets: Addressing covariate shift when data is skewed	[]	Victoria López, Alberto Fernández, and Francisco Herrera. 2014. On the importance of the validation technique for classification with imbalanced datasets: Addressing covariate shift when data is skewed. Inform. Sci. 257 (2014), 1–13.			https://doi.org/10.1016/j.ins.2013.09.038
183	An evolutionary algorithm for the discovery of rare class association rules in learning management systems	[]	José Marı́a Luna, Cristóbal Romero, José Raúl Romero, and Sebastián Ventura 2015. An evolutionary algorithm for the discovery of rare class association rules in learning management systems. Appl. Intell. 42,			https://doi.org/10.1007/s10489-014-0603-4
184	Local neighbourhood extension of SMOTE for mining imbalanced data	[]	Tomasz Maciejewski, and Jerzy Stefanowski. 2011. Local neighbourhood extension of SMOTE for mining imbalanced data. 2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM). IEEE, 104–111.			http://scholar.google.com/scholar?hl=en&q=Tomasz+Maciejewski+and+Jerzy+Stefanowski.+2011.+Local+neighbourhood+extension+of+SMOTE+for+mining+imbalanced+data.+In+2011+IEEE+Symposium+on+Computational+Intelligence+and+Data+Mining+%28CIDM%29.+IEEE%2C+104%2D%2D111.
185	A new approach for classification of highly imbalanced datasets using evolutionary algorithms	[]	Satyam Maheshwari, Jitendra Agrawal, and Sanjeev Sharma. 2011. A new approach for classification of highly imbalanced datasets using evolutionary algorithms. Intl. J. Sci. Eng. Res 2 (2011), 1–5.			http://scholar.google.com/scholar?hl=en&q=Satyam+Maheshwari%2C+Jitendra+Agrawal%2C+and+Sanjeev+Sharma.+2011.+A+new+approach+for+classification+of+highly+imbalanced+datasets+using+evolutionary+algorithms.+Intl.+J.+Sci.+Eng.+Res+2+%282011%29%2C+1%2D%2D5.
186	Learning when data sets are imbalanced and when costs are unequal and unknown	[', (Drummond & Holte, 2003), (Kubat & Matwin, 1997), (Maloof, 2003), (Japkowicz, 2000), (Weiss & Provost, 2003), (Chawla et al.', 'There is a significant body of research comparing the various sampling methods <NO>.', '(In the remainder of the paper negative is always taken to be the majority class and positive is the minority class) Example applications include vision recognition <NO>, bioinformatics<NO>, credit card fraud detection<NO>, the detection of oil spills<NO> and so on.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'anced learning problems <NO>, <NO>, <NO>.', ', only an informal assertion is known, such as misclassifications on the positive class are more expensive than the negative class <NO>.', 'However, Maloof <NO> notes that the precise definition', 'Many representative works on the ineffectiveness of accuracy in the imbalanced learning scenario exist in the community <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'In particular, it has been indicated that learning from imbalanced data sets and learning when costs are unequal and unknown can be handled in a similar manner <NO>, and cost-sensitive learning is a good solution to the class imbalance problem <NO>.', 'Although some studies have shown that oversampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, it should be noted that oversampling usually increases the training time and may lead to overfitting since it involves making exact copies of examples <NO>, <NO>.', 'Nevertheless, some studies have shown that undersampling is effective in learning with imbalanced data sets <NO>, <NO>, <NO>, sometimes even stronger than oversampling, especially on large data sets <NO>, <NO>.', 'A recent study has shown that threshold-moving is as effective as sampling methods in addressing the class imbalance problem <NO>.']	Marcus A. Maloof. 2003. Learning when data sets are imbalanced and when costs are unequal and unknown. ICML-2003 Workshop on Learning from Imbalanced Data Sets II, Vol. 2. 2–1.			http://scholar.google.com/scholar?hl=en&q=Marcus+A.+Maloof.+2003.+Learning+when+data+sets+are+imbalanced+and+when+costs+are+unequal+and+unknown.+In+ICML-2003+Workshop+on+Learning+from+Imbalanced+Data+Sets+II%2C+Vol.+2.+2%2D%2D1.
187	One-class SVMs for document classification	['Representative works in this area include the one-class SVMs <NO>, <NO>, <NO>, <NO>, <NO>, <NO> and the autoassociator (or autoencoder) method <NO>, <NO>, <NO>, <NO>.', 'Meanwhile, Manevitz and Yousef <NO> and <NO> presented the successful applications of the one-class learning approach to the document classification domain based on SVMs and autoencoder, respectively.', 'One-class classification has seen a rise in application over the years, for example, in the use of document classification <NO>, typist recognition <NO> and compliance verification of the CTBT <NO>.']	Larry Manevitz, and Malik Yousef. 2002. One-class SVMs for document classification. J. Mach. Learn. Res. 2 (2002), 139–154.			https://doi.org/10.5555/944790.944808
188	Proximal support vector machine classifiers	[]	Olvi L. Mangasarian, and Edward W. Wild. 2001. Proximal support vector machine classifiers. Proceedings KDD-2001: Knowledge Discovery and Data Mining. Citeseer.			https://doi.org/10.1145/502512.502527
189	Intelligent rule mining algorithm for classification over imbalanced data	[]	Veenu Mangat, and Renu Vig. 2014. Intelligent rule mining algorithm for classification over imbalanced data. J. Emerg. Technol. Web Intell. 6, 3 (2014), 373–379.			http://scholar.google.com/scholar?hl=en&q=Veenu+Mangat+and+Renu+Vig.+2014.+Intelligent+rule+mining+algorithm+for+classification+over+imbalanced+data.+J.+Emerg.+Technol.+Web+Intell.+6%2C+3+%282014%29%2C+373%2D%2D379.
190	kNN approach to unbalanced data distributions: A case study involving information extraction	['This reduction can be done randomly in which case it is called random undersampling <NO> or it can be done by using some statistical knowledge in which case it is called informed undersampling <NO>.', 'Based on the characteristics of the given data distribution, four KNN undersampling methods were proposed in <NO>, namely, NearMiss-1, NearMiss-2, NearMiss-3, and the “most distant” method.', 'In addition, several informed sampling methods have been introduced in <NO>.']	Inderjeet Mani, and Jianping Zhang. 2003. kNN approach to unbalanced data distributions: A case study involving information extraction. Proceedings of Workshop on Learning from Imbalanced Datasets.			http://scholar.google.com/scholar?hl=en&q=Inderjeet+Mani+and+Jianping+Zhang.+2003.+kNN+approach+to+unbalanced+data+distributions%3A+A+case+study+involving+information+extraction.+In+Proceedings+of+Workshop+on+Learning+from+Imbalanced+Datasets.
191	SNEOM: A sanger network based extended over-sampling method. application to imbalanced biomedical datasets	[]	José Manuel Martı́nez-Garcı́a, Carmen Paz Suárez-Araujo, and Patricio Garcı́a Báez 2012. SNEOM: A sanger network based extended over-sampling method. application to imbalanced biomedical datasets. In Neural Information			https://doi.org/10.1007/978-3-642-34478-7_71
192	Cost-weighted boosting with jittering and over/under-sampling: JOUS-boost	[]	David Mease, Abraham Wyner, and Andreas Buja. 2007. Cost-weighted boosting with jittering and over/under-sampling: JOUS-boost. J. Mach. Learn. Res. 8 (2007), 409–439.			http://scholar.google.com/scholar?hl=en&q=David+Mease%2C+Abraham+Wyner%2C+and+Andreas+Buja.+2007.+Cost-weighted+boosting+with+jittering+and+over%2Funder-sampling%3A+JOUS-boost.+J.+Mach.+Learn.+Res.+8+%282007%29%2C+409%2D%2D439.
193	Training and assessing classification rules with imbalanced data	[]	Giovanna Menardi, and Nicola Torelli. 2010. Training and assessing classification rules with imbalanced data. Data Min. Knowl. Discov. (2010), 1–31.			https://doi.org/10.1007/s10618-012-0295-5
194	Basic principles of ROC analysis	[]	Charles E. Metz. 1978. Basic principles of ROC analysis. Seminars in Nuclear Medicine, Vol. 8. Elsevier, 283–298.			http://scholar.google.com/scholar?hl=en&q=Charles+E.+Metz.+1978.+Basic+principles+of+ROC+analysis.+In+Seminars+in+Nuclear+Medicine%2C+Vol.+8.+Elsevier%2C+283%2D%2D298.
195	Imbalanced classification based on active learning SMOTE	[]	Ying Mi. 2013. Imbalanced classification based on active learning SMOTE. Res. J. Appl. Sci. 5 (2013).			http://scholar.google.com/scholar?hl=en&q=Ying+Mi.+2013.+Imbalanced+classification+based+on+active+learning+SMOTE.+Res.+J.+Appl.+Sci.+5+%282013%29.
196	Feature selection for unbalanced class distribution and naive bayes	[]	Dunja Mladenic, and Marko Grobelnik. 1999. Feature selection for unbalanced class distribution and naive bayes. ICML, Vol. 99. 258–267.			https://doi.org/10.5555/645528.657649
197	A unifying view on dataset shift in classification	[]	Jose G. Moreno-Torres, Troy Raeder, Rocı́o Alaiz-Rodrı́guez, Nitesh V. Chawla, and Francisco Herrera 2012. A unifying view on dataset shift in classification. Pattern Recogn. 45,			https://doi.org/10.1016/j.patcog.2011.06.019
198	Three-way rocs	[]	Douglas Mossman. 1999. Three-way rocs. Med. Dec. Mak. 19, 1 (1999), 78–89.			http://scholar.google.com/scholar?hl=en&q=Douglas+Mossman.+1999.+Three-way+rocs.+Med.+Dec.+Mak.+19%2C+1+%281999%29%2C+78%2D%2D89.
199	A novel framework for class imbalance learning using intelligent under-sampling	[]	Satuluri Naganjaneyulu, and Mrithyumjaya Rao Kuppa. 2013. A novel framework for class imbalance learning using intelligent under-sampling. Progr. Artif. Intell. 2, 1 (2013), 73–84.			http://scholar.google.com/scholar?hl=en&q=Satuluri+Naganjaneyulu+and+Mrithyumjaya+Rao+Kuppa.+2013.+A+novel+framework+for+class+imbalance+learning+using+intelligent+under-sampling.+Progr.+Artif.+Intell.+2%2C+1+%282013%29%2C+73%2D%2D84.
200	LVQ-SMOTE– learning vector quantization based synthetic minority over–sampling technique for biomedical data	[]	Munehiro Nakamura, Yusuke Kajiwara, Atsushi Otsuka, and Haruhiko Kimura. 2013. LVQ-SMOTE– learning vector quantization based synthetic minority over–sampling technique for biomedical data. BioData Min. 6, 1 (2013), 16.			http://scholar.google.com/scholar?hl=en&q=Munehiro+Nakamura%2C+Yusuke+Kajiwara%2C+Atsushi+Otsuka%2C+and+Haruhiko+Kimura.+2013.+LVQ-SMOTE%2D%2Dlearning+vector+quantization+based+synthetic+minority+over%2D%2Dsampling+technique+for+biomedical+data.+BioData+Min.+6%2C+1+%282013%29%2C+16.
201	Learning from imbalanced data in presence of noisy and borderline examples	[]	Krystyna Napierała, Jerzy Stefanowski, and Szymon Wilk. 2010. Learning from imbalanced data in presence of noisy and borderline examples. Rough Sets and Current Trends in Computing. Springer, 158–167.			https://doi.org/10.5555/1876210.1876233
202	Diversified sensitivity-based undersampling for imbalance classification problems	[]	Wing WY Ng, Jiankun Hu, Daniel S. Yeung, Sha Yin, and Fabio Roli. 2014. Diversified sensitivity-based undersampling for imbalance classification problems. (2014).			http://scholar.google.com/scholar?hl=en&q=Wing+WY+Ng%2C+Jiankun+Hu%2C+Daniel+S.+Yeung%2C+Sha+Yin%2C+and+Fabio+Roli.+2014.+Diversified+sensitivity-based+undersampling+for+imbalance+classification+problems.+%282014%29.
203	Error back-propagation algorithm for classification of imbalanced data	[]	Sang-Hoon Oh. 2011. Error back-propagation algorithm for classification of imbalanced data. Neurocomputing 74, 6 (2011), 1058–1061.			https://doi.org/10.1016/j.neucom.2010.11.024
204	Imbalanced clustering for microarray time-series	['Specifically, this form of imbalance is referred to as a between-class imbalance; not uncommon are betweenclass imbalances on the order of 100:1, 1,000:1, and 10,000:1, where in each case, one class severely outrepresents another <NO>, <NO>, <NO>.']	Ronald Pearson, Gregory Goney, and James Shwaber. 2003. Imbalanced clustering for microarray time-series. Proceedings of the ICML, Vol. 3.			http://scholar.google.com/scholar?hl=en&q=Ronald+Pearson%2C+Gregory+Goney%2C+and+James+Shwaber.+2003.+Imbalanced+clustering+for+microarray+time-series.+In+Proceedings+of+the+ICML%2C+Vol.+3.
205	Projection-based ensemble learning for ordinal regression	[]	Marı́a Pérez-Ortiz, Pedro Antonio Gutiérrez, and César Hervás-Martı́nez 2014. Projection-based ensemble learning for ordinal regression. IEEE Trans. Cybernet. 44,			http://scholar.google.com/scholar?hl=en&q=Mar%C3%ADa+P%C3%A9rez-Ortiz%2C+Pedro+Antonio+Guti%C3%A9rrez%2C+and+C%C3%A9sar+Herv%C3%A1s-Mart%C3%ADnez.+2014.+Projection-based+ensemble+learning+for+ordinal+regression.+IEEE+Trans.+Cybernet.+44%2C+5+%282014%29%2C+681%2D%2D694.
206	Minority report in fraud detection: Classification of skewed data	['Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition, as it has been extensively documented in the literature with applications such as diagnostics of rare diseases <NO>, fraud detection <NO>, identification of oil spills in satellite radar images <NO>, and many others.', 'In addition to biomedical applications, further speculation will yield similar consequences for domains such as fraud detection, network intrusion, and oil-spill detection, to name a few <NO>, <NO>, <NO>, <NO>, <NO>.']	Clifton Phua, Damminda Alahakoon, and Vincent Lee. 2004. Minority report in fraud detection: Classification of skewed data. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 50–59.	This paper proposes an innovative fraud detection method, built upon existing fraud detection research and Minority Report, to deal with the data mining problem of skewed data distributions. This method uses backpropagation (BP), together with naive Bayesian (NB) and C4.5 algorithms, on data partitions derived from minority oversampling with replacement. Its originality lies in the use of a single meta-classifier (stacking) to choose the best base classifiers, and then combine these base classifiers’ predictions (bagging) to improve cost savings (stacking-bagging). Results from a publicly available automobile insurance fraud detection data set demonstrate that stacking-bagging performs slightly better than the best performing bagged algorithm, C4.5, and its best classifier, C4.5 (2), in terms of cost savings. Stackingbagging also outperforms the common technique used in industry (BP without both sampling and partitioning). Subsequently, this paper compares the new fraud detection method (meta-learning approach) against C4.5 trained using undersampling, oversampling, and SMOTEing without partitioning (sampling approach). Results show that, given a fixed decision threshold and cost matrix, the partitioning and multiple algorithms approach achieves marginally higher cost savings than varying the entire training data set with different class distributions. The most interesting find is confirming that the combination of classifiers to produce the best cost savings has its contributions from all three algorithms.		https://doi.org/10.1145/1007730.1007738
207	Class imbalances versus class overlapping: An analysis of a learning system behavior	['have been found to depreciate in the presence of within-class imbalance and small disjuncts problems <NO>, <NO>, <NO>.', 'then the imbalance problem becomes very severe <NO>, <NO>.', 'Some informed undersampling methods also apply data cleaning techniques <NO> to further refine the majority class samples <NO>, <NO>.', 'Regardless of the method used to counter the imbalance problem, factors such as the uncertainty of the true distribution between the samples of the minority class and majority class samples, complexity of data, and noise in data may pose a limit on the classifiers’ performance <NO>, <NO>, <NO>.', 'In <NO> we developed a systematic study aiming to question whether class imbalances hinder classifier induction or whether these deficiencies might be explained in other ways.', 'The results obtained in the UCI data sets seem to be compatible with previous work of the authors <NO> conducted on a series of experiments with artificial domains, in which we varied the degree of overlapping between the classes.', 'This, in fact, is the result of another form of imbalance, a within-class imbalance, which concerns itself with the distribution of representative data for subconcepts within a class <NO>, <NO>, <NO>.', 'The existence of within-class imbalances is closely intertwined with the problem of small disjuncts, which has been shown to greatly depreciate classification performance <NO>, <NO>, <NO>, <NO>.']	Ronaldo C. Prati, Gustavo E.A.P.A. Batista, and Maria Carolina Monard. 2004. Class imbalances versus class overlapping: An analysis of a learning system behavior. MICAI 2004: Advances in Artificial Intelligence. Springer, 312–321.			http://scholar.google.com/scholar?hl=en&q=Ronaldo+C.+Prati%2C+Gustavo+E.+A.+P.+A.+Batista%2C+and+Maria+Carolina+Monard.+2004a.+Class+imbalances+versus+class+overlapping%3A+An+analysis+of+a+learning+system+behavior.+In+MICAI+2004%3A+Advances+in+Artificial+Intelligence.+Springer%2C+312%2D%2D321.
208	Learning with class skews and small disjuncts	[]	Ronaldo C. Prati, Gustavo E.A.P.A. Batista, and Maria Carolina Monard. 2004. Learning with class skews and small disjuncts. Advances in Artificial Intelligence–SBIA 2004. Springer, 296–306.			http://scholar.google.com/scholar?hl=en&q=Ronaldo+C.+Prati%2C+Gustavo+E.+A.+P.+A.+Batista%2C+and+Maria+Carolina+Monard.+2004b.+Learning+with+class+skews+and+small+disjuncts.+In+Advances+in+Artificial+Intelligence%2D%2DSBIA+2004.+Springer%2C+296%2D%2D306.
209	Class imbalance revisited: A new experimental setup to assess the performance of treatment methods	[]	Ronaldo C. Prati, Gustavo E.A.P.A. Batista, and Diego F. Silva. 2014. Class imbalance revisited: A new experimental setup to assess the performance of treatment methods. Knowl. Inform. Syst. (2014), 1–24.			https://doi.org/10.1007/s10115-014-0794-3
210	Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions	['Some other evaluation measures, such as recall, precision, F-measure, G-mean and Receiver Operation Characteristic (ROC) Curve Analysis, are then explored and proposed as more proper evaluation measures <NO>.', 'Consequently, robust evaluation techniques like the ROC convex hull method <NO> or the area under the ROC curve <NO> have been proposed to enable classifier assessment in accordance with managerial objectives.', 'ROC (Receiver Operating Characteristic) graphs <NO> can be used to analyze the relationship between FNrate and FPrate (or TNrate and TPrate) for a classifier.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'In addition to these singular assessment metrics, we also adopted the ROC graph <NO>, <NO> for evaluation in this paper.', 'A detailed discussion of ROC analysis and its assessment for classifier performances can be found in <NO> and <NO>.', 'Many representative works on the ineffectiveness of accuracy in the imbalanced learning scenario exist in the community <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'such as ROC curve <NO> or cost curve <NO> can be used to']	Foster J. Provost, and Tom Fawcett. 1997. Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions. KDD, Vol. 97. 43–48.			http://scholar.google.com/scholar?hl=en&q=Foster+J.+Provost+and+Tom+Fawcett.+1997.+Analysis+and+visualization+of+classifier+performance%3A+Comparison+under+imprecise+class+and+cost+distributions.+In+KDD%2C+Vol.+97.+43%2D%2D48.
211	The case against accuracy estimation for comparing induction algorithms	['<NO>) in classification tasks, but for regression problems.', '<NO>) and may even require specific modeling techniques (e.', 'One reason for choosing to compare models with respect to AUC instead of accuracy is that the former is not sensitive to differences between the class distribution of the training examples and of the examples on which the model is applied <NO>.', 'In addition, the costs associated with true versus false prediction of positives and negatives are often asymmetric <NO> and are routinely used to guide the parameterisation and selection process of a wide range of classifiers, e.', '<NO> propose an alternative averaging 0 0.', 'Many representative works on the ineffectiveness of accuracy in the imbalanced learning scenario exist in the community <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.']	Foster J Provost, Tom Fawcett, and Ron Kohavi. 1998. The case against accuracy estimation for comparing induction algorithms. ICML’98: Proc. of the 15th Int. Conf. on Machine Learning. Morgan Kaufmann Publishers, 445–453.			https://doi.org/10.5555/645527.657469
212	Learning from imbalanced data: Evaluation matters	[]	Troy Raeder, George Forman, and Nitesh V. Chawla. 2012. Learning from imbalanced data: Evaluation matters. Data Mining: Foundations and Intelligent Paradigms. Springer, 315–331.			http://scholar.google.com/scholar?hl=en&q=Troy+Raeder%2C+George+Forman%2C+and+Nitesh+V.+Chawla.+2012.+Learning+from+imbalanced+data%3A+Evaluation+matters.+In+Data+Mining%3A+Foundations+and+Intelligent+Paradigms.+Springer%2C+315%2D%2D331.
213	SMOTE-RSB*: A hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory	[]	Enislay Ramentol, Yailé Caballero, Rafael Bello, and Francisco Herrera. 2012. SMOTE-RSB*: A hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory. Knowl. Inform. Syst. 33, 2 (2012), 245–265.			http://scholar.google.com/scholar?hl=en&q=Enislay+Ramentol%2C+Yail%C3%A9+Caballero%2C+Rafael+Bello%2C+and+Francisco+Herrera.+2012a.+SMOTE-RSB%26ast%3B%3A+A+hybrid+preprocessing+approach+based+on+oversampling+and+undersampling+for+high+imbalanced+data-sets+using+SMOTE+and+rough+sets+theory.+Knowl.+Inform.+Syst.+33%2C+2+%282012%29%2C+245%2D%2D265.
214	SMOTE-FRST: A new resampling method using fuzzy rough set theory	[]	Enislay Ramentol, Nelle Verbiest, Rafael Bello, Yailé Caballero, Chris Cornelis, and Francisco Herrera. 2012. SMOTE-FRST: A new resampling method using fuzzy rough set theory. 10th International FLINS Conference on Uncertainty Modelling in Knowledge Engineering and Decision Making (to Appear).			http://scholar.google.com/scholar?hl=en&q=Enislay+Ramentol%2C+Nelle+Verbiest%2C+Rafael+Bello%2C+Yail%C3%A9+Caballero%2C+Chris+Cornelis%2C+and+Francisco+Herrera.+2012b.+SMOTE-FRST%3A+A+new+resampling+method+using+fuzzy+rough+set+theory.+In+10th+International+FLINS+Conference+on+Uncertainty+Modelling+in+Knowledge+Engineering+and+Decision+Making+%28to+Appear%29.
215	Optimized precision-a new measure for classifier performance evaluation	[]	Romesh Ranawana, and Vasile Palade. 2006. Optimized precision-a new measure for classifier performance evaluation. IEEE Congress on Evolutionary Computation, 2006. CEC 2006. IEEE, 2254–2261.			http://scholar.google.com/scholar?hl=en&q=Romesh+Ranawana+and+Vasile+Palade.+2006.+Optimized+precision-a+new+measure+for+classifier+performance+evaluation.+In+IEEE+Congress+on+Evolutionary+Computation%2C+2006.+CEC+2006.+IEEE%2C+2254%2D%2D2261.
216	Extreme re-balancing for SVMs: A case study	['There have been some recent works in improving the classification performance of SVMs on unbalanced data sets <NO>–<NO>.', 'Raskutti and Kowalczyk <NO> demonstrated that a one-class SVM that learned only from the minority class can sometimes perform better than an SVM modeled from two classes.', 'One-class SVM actually performs worse in many cases compared with a standard twoclass SVM <NO>.', 'While there are some recent papers on SVM for imbalanced classification <NO>–<NO>, <NO>, the application of SVM is still not completely explored, particularly the realm of undersampling of SVs.', 'Another technique suggested for class imbalance problem is to use a recognition-based, instead of discrimination-based inductive learning <NO>.', 'The effects of imbalanced data on SVMs exploit inadequacies of the soft-margin maximization paradigm <NO>, <NO>.', 'In this case, it might occur that the support vectors representing the minority concept are “far away” from this “ideal” line, and as a result, will contribute less to the final hypothesis <NO>, <NO>, <NO>.', '<NO> consider both sampling and dataspace weighting', 'Representative works in this area include the one-class SVMs <NO>, <NO>, <NO>, <NO>, <NO>, <NO> and the autoassociator (or autoencoder) method <NO>, <NO>, <NO>, <NO>.', 'Specifically, Raskutti and Kowalcyzk <NO> suggested that one-class learning is particularly useful in dealing with extremely imbalanced data sets with high feature space dimensionality.', 'Support vector machines have also utilized this same approach to learn rare classes, with some success <NO>.']	Bhavani Raskutti, and Adam Kowalczyk. 2004. Extreme re-balancing for SVMs: A case study. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 60–69.			https://doi.org/10.1145/1007730.1007739
217	Utility-based Regression	[]	Rita P. Ribeiro. 2011. Utility-based Regression. Ph.D. Dissertation. Dep. Computer Science, Faculty of Sciences, University of Porto.			http://scholar.google.com/scholar?hl=en&q=Rita+P.+Ribeiro.+2011.+Utility-based+Regression.+Ph.D.+Dissertation.+Dep.+Computer+Science%2C+Faculty+of+Sciences%2C+University+of+Porto.
218	Predicting harmful algae blooms	['harmful algae blooms in rivers <NO>), or the prediction of unusual high (low) returns in financial quote data.']	Rita P. Ribeiro, and Luı́s Torgo 2003. Predicting harmful algae blooms. In Progress in Artificial Intelligence			http://scholar.google.com/scholar?hl=en&q=Rita+P.+Ribeiro+and+Lu%C3%ADs+Torgo.+2003.+Predicting+harmful+algae+blooms.+In+Progress+in+Artificial+Intelligence.+Springer%2C+308%2D%2D312.
219	Information Retrieval	['F1 has been widely used in information retrieval, which is: 2·TP 2·TP+FP+FN <NO>.', '0, attach different weights to precision(p) and recall(r) <NO>:', 'To measure the performance, we use both precision (p) and recall (r) in their combined from F1 : 2pr p+r <NO>.']	Cornelis V. Rijsbergen. 1979. Information Retrieval. Dept. of Computer Science, University of Glasgow, 2nd edition. (1979).			http://scholar.google.com/scholar?hl=en&q=Edward+Y.+Chang%2C+Beitao+Li%2C+Gang+Wu%2C+and+Kingshy+Goh.+2003.+Statistical+learning+for+effective+visual+information+retrieval.+In+ICIP+%283%29.+609%2D%2D612.
220	Disturbing neighbors ensembles of trees for imbalanced data	[]	Juan J. Rodrı́guez, José-Francisco Dı́ez-Pastor, Jesús Maudes, and César Garcı́a-Osorio 2012. Disturbing neighbors ensembles of trees for imbalanced data. In 2012 11th International Conference on Machine Learning and Applications (ICMLA),	Disturbing Neighbors (DN ) is a method for generating classifier ensembles. Moreover, it can be combined with any other ensemble method, generally improving the results. This paper considers the application of these ensembles to imbalanced data: classification problems where the class proportions are significantly different. DN ensembles are compared and combined with Bagging, using three tree methods as base classifiers: conventional decision trees (C4.5), Hellinger distance decision trees (HDDT) —a method designed for imbalance data— and model trees (M5P) —trees with linear models at the leaves—. The methods are compared using two collections of imbalanced datasets, with 20 and 66 datasets, respectively. The best results are obtained combining Bagging and DN , using conventional decision trees. 	"As many other ensemble methods (e.g., Bagging, Random Subspaces) the Disturbing Neighbors (DN ) method is based on training each base classifier with a random transformation of the dataset.
In this method, the transformation adds new attributes. These attributes are obtained from a small sample of randomly selected training examples. These examples are called the Disturbing Neighbors.
The additional attributes are based on which one of the Disturbing Neighbors is the nearest. The first added attribute is nominal, the possible values are the class labels. For a binary class data set, the attribute will be binary. For a given example, the value of this attribute is the class label of the nearest Disturbing Neighbor.
For each Disturbing Neighbor an additional boolean attribute is included. The value of the attribute is true for a given example if the corresponding Disturbing Neighbor is the nearest one.
As an additional source of diversity (a necessary ingredient of successful ensembles) for each base classifier the distances are calculated in a random subspace. This random subspace is only used for determining the nearest Disturbing Neighbor, the transformed dataset contains all the original attributes.
It is possible to obtain ensembles using only the DN method, but usually is possible to obtain better results combining DN with other ensemble methods. The combination generally is better than the other ensemble method and the DN ensemble <NO>.
In Figure 1 the tree at the right was obtained with the same method and from the same dataset (glass4) as the other trees but with the additional attributes from the Disturbing Neighbors. One of this attributes, Nearest 8, was selected as the root of the tree. This attribute indicates if the nearest DN is the eighth.
From the considered sources of diversity, in <NO> it was determined that for non particularly imbalanced data sets the most important source was the use of the boolean features, the impact of the nominal feature and the calculation of the distances in a random subspace were less important."	https://doi.org/10.1109/ICMLA.2012.181
221	SMOTE–IPF: Addressing the noisy and borderline examples problem in imbalanced classification by a re-sampling method with filtering	[]	José A. Sáez, Julián Luengo, Jerzy Stefanowski, and Francisco Herrera. 2015. SMOTE–IPF: Addressing the noisy and borderline examples problem in imbalanced classification by a re-sampling method with filtering. Inform. Sci. 291 (2015), 184–203.			https://doi.org/10.1016/j.ins.2014.08.051
222	Empirical analysis of assessments metrics for multi-class imbalance learning on the back-propagation context	[]	Juan Pablo Sánchez-Crisostomo, Roberto Alejo, Erika López-González, Rosa Marı́a Valdovinos, and J. Horacio Pacheco-Sánchez 2014. Empirical analysis of assessments metrics for multi-class imbalance learning on the back-propagation context. In Advances in Swarm Intelligence			http://scholar.google.com/scholar?hl=en&q=Juan+Pablo+S%C3%A1nchez-Crisostomo%2C+Roberto+Alejo%2C+Erika+L%C3%B3pez-Gonz%C3%A1lez%2C+Rosa+Mar%C3%ADa+Valdovinos%2C+and+J.+Horacio+Pacheco-S%C3%A1nchez.+2014.+Empirical+analysis+of+assessments+metrics+for+multi-class+imbalance+learning+on+the+back-propagation+context.+In+Advances+in+Swarm+Intelligence.+Springer%2C+17%2D%2D23.
223	Evolutionary ordinal extreme learning machine	[]	Javier Sánchez-Monedero, Pedro Antonio Gutiérrez, and Cesar Hervás-Martı́nez 2013. Evolutionary ordinal extreme learning machine. In Hybrid Artificial Intelligent Systems			http://scholar.google.com/scholar?hl=en&q=Javier+S%C3%A1nchez-Monedero%2C+Pedro+Antonio+Guti%C3%A9rrez%2C+and+Cesar+Herv%C3%A1s-Mart%C3%ADnez.+2013.+Evolutionary+ordinal+extreme+learning+machine.+In+Hybrid+Artificial+Intelligent+Systems.+Springer%2C+500%2D%2D509.
224	Estimating the support of a high-dimensional distribution	['Representative works in this area include the one-class SVMs <NO>, <NO>, <NO>, <NO>, <NO>, <NO> and the autoassociator (or autoencoder) method <NO>, <NO>, <NO>, <NO>.', 'Yet another example of a OC classifier is the OC Support Vector Machine (OCSVM) <NO>.']	Bernhard Schölkopf, John C. Platt, John Shawe-Taylor, Alex J. Smola, and Robert C. Williamson. 2001. Estimating the support of a high-dimensional distribution. Neur. Comput. 13, 7 (2001), 1443–1471.			https://doi.org/10.1162/089976601750264965
225	An empirical study of the classification performance of learners on imbalanced and noisy software quality data	[]	Chris Seiffert, Taghi M. Khoshgoftaar, Jason Van Hulse, and Andres Folleco. 2011. An empirical study of the classification performance of learners on imbalanced and noisy software quality data. Inform. Sci. (2011).			https://doi.org/10.1016/j.ins.2010.12.016
226	RUSBoost: A hybrid approach to alleviating class imbalance	['On the other hand, with regard to ensemble learning methods, a large number of different approaches have been proposed in the literature, including but not limited to SMOTEBoost <NO>, RUSBoost <NO>, IIVotes <NO>, EasyEnsemble <NO>, or SMOTEBagging <NO>.', 'have arisen as a possible solution to the class imbalance problem attracting great interest among researchers <NO>, <NO>, <NO>, <NO>.', 'Inside this family, we include SMOTEBoost <NO>, MSMOTEBoost <NO>, RUSBoost <NO>, and DataBoost-IM <NO> algorithms.', ', Bagging, Boosting) has given good results <NO>, <NO>, <NO>.']	Chris Seiffert, Taghi M. Khoshgoftaar, Jason Van Hulse, and Amri Napolitano. 2010. RUSBoost: A hybrid approach to alleviating class imbalance. IEEE Trans.Syst. Man Cybernet. A 40, 1 (2010), 185–197.	Class imbalance is a problem that is common to many application domains. When examples of one class in a training data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classification models. Several techniques have been used to alleviate the problem of class imbalance, including data sampling and boosting. In this paper, we present a new hybrid sampling/boosting algorithm, called RUSBoost, for learning from skewed training data. This algorithm provides a simpler and faster alternative to SMOTEBoost, which is another algorithm that combines boosting and data sampling. This paper evaluates the performances of RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using 15 data sets from various application domains, four base learners, and four evaluation metrics. RUSBoost and SMOTEBoost both outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a simpler and faster technique. Given these experimental results, we highly recommend RUSBoost as an attractive alternative for improving the classification performance of learners built using imbalanced data.	"Much research has been performed with respect to the class imbalance problem. Weiss <NO> provides a survey of the class imbalance problem and techniques for reducing the negative impact imbalance that has on classification performance. The study identifies many methods for alleviating the problem of class imbalance, including data sampling and boosting, which are the two techniques investigated in this paper. Japkowicz <NO> presents another study addressing the issue of class imbalance, including an investigation of the types of imbalance that most negatively impact classification performance, and a small case study comparing several techniques for alleviating the problem.
Data sampling has received much attention in research related to class imbalance. Data sampling attempts to overcome imbalanced class distributions by adding examples to (oversampling) or removing examples from (undersampling) the data set. The simplest form of undersampling is RUS. RUS randomly removes examples from the majority class until a desired class distribution is found. While there is no universally accepted optimal class distribution, a balanced (50 : 50) distribution is often considered to be near optimal <NO>. However, when
examples from the minority class are very rare, a ratio closer to 35 : 65 (minority:majority) may result in better classification performance <NO>.
In addition to random data sampling techniques, several more “intelligent” algorithms for resampling data have been proposed. Barandela et al. <NO> and Han et al. <NO> examine the performance of some of these “intelligent” data sampling techniques, such as SMOTE, borderline SMOTE, and Wilson’s editing. Van Hulse et al. <NO> examine the performance of seven different data sampling techniques (both “intelligent” and random) using a large number of different learning algorithms and experimental data sets, finding both RUS and SMOTE to be very effective data sampling techniques.
Another technique for dealing with class imbalance is boosting. While boosting is not specifically designed to handle the class imbalance problem, it has been shown to be very effective in this regard <NO>. The most commonly used boosting algorithm is AdaBoost <NO>, which has been shown to improve the performance of any weak classifier, provided that the classifier results in better performance than random guessing. Several variations have been proposed to make AdaBoost cost sensitive <NO>–<NO> or to improve its performance on imbalanced data <NO>–<NO>. One of the most promising of these techniques is SMOTEBoost <NO>. SMOTEBoost combines an intelligent oversampling technique (SMOTE) with AdaBoost, resulting in a highly effective hybrid approach to learning from imbalanced data. Our proposed technique, which is RUSBoost, is based on the SMOTEBoost algorithm but provides a faster and simpler alternative for learning from imbalanced data with performance that is usually as good (and often better) than that of SMOTEBoost.
Closely related to the issue of class imbalance is costsensitive learning. Weiss et al. <NO> compare the performances of oversampling, undersampling, and cost-sensitive learning when dealing with data that have both an imbalanced class distribution and unequal error costs. Sun et al. <NO> present an in-depth examination of cost-sensitive boosting. Chawla et al. <NO> perform a detailed evaluation of a wrapper-based sampling approach to minimize misclassification cost. A detailed evaluation of RUSBoost as a cost-sensitive learning technique, including a comparison to existing methodologies, is left as a future work."	https://doi.org/10.1109/TSMCA.2009.2029559
227	Clustering based one-class classification for compliance verification of the comprehensive nuclear-test-ban treaty	[]	Shiven Sharma, Colin Bellinger, and Nathalie Japkowicz. 2012. Clustering based one-class classification for compliance verification of the comprehensive nuclear-test-ban treaty. Advances in Artificial Intelligence. Springer, 181–193.			https://doi.org/10.1007/978-3-642-30353-1_16
228	Evaluating and tuning predictive data mining models using receiver operating characteristic curves	[]	Atish P. Sinha, and Jerrold H. May. 2004. Evaluating and tuning predictive data mining models using receiver operating characteristic curves. J. Manag. Inform. Syst. 21, 3 (2004), 249–280.			https://doi.org/10.1080/07421222.2004.11045815
229	Learning from imbalanced data using ensemble methods and cluster-based undersampling	[]	Parinaz Sobhani, Herna Viktor, and Stan Matwin. 2014. Learning from imbalanced data using ensemble methods and cluster-based undersampling. New Frontiers in Mining Complex Patterns. Springer, 69–83.			http://scholar.google.com/scholar?hl=en&q=Parinaz+Sobhani%2C+Herna+Viktor%2C+and+Stan+Matwin.+2014.+Learning+from+imbalanced+data+using+ensemble+methods+and+cluster-based+undersampling.+In+New+Frontiers+in+Mining+Complex+Patterns.+Springer%2C+69%2D%2D83.
230	A systematic analysis of performance measures for classification tasks	[]	Marina Sokolova, and Guy Lapalme. 2009. A systematic analysis of performance measures for classification tasks. Inform. Process. Manag. 45, 4 (2009), 427–437.			https://doi.org/10.1016/j.ipm.2009.03.002
231	An improved AdaBoost algorithm for unbalanced classification data	[]	Jie Song, Xiaoling Lu, and Xizhi Wu. 2009. An improved AdaBoost algorithm for unbalanced classification data. Sixth International Conference on Fuzzy Systems and Knowledge Discovery, 2009. FSKD’09. Vol. 1. IEEE, 109–113.	AdaBoost algorithm is proved to be a very efficient classification method for the balanced dataset with all classes having similar proportions. However, in real application, it is quite common to have unbalanced dataset with a certain class of interest having very small size. It will be problematic since the algorithm might predict all the cases into majority classes without loss of overall accuracy. This paper proposes an improved AdaBoost algorithm called BABoost (Balanced AdaBoost), which gives higher weights to the misclassified examples from the minority class. Empirical results show that the new method decreases the prediction error of minority class significantly with increasing the prediction error of majority class a little bit. It can also produce higher values of margin which indicates a better classification method	"In this paper, we propose a new algorithm called BABoost based on dividing the overall misclassification error into several parts. We want to utilize BABoost to improve the accuracy over the minority class without much sacrificing the accuracy over the majority class.
Adaboost algorithm gives equal weight to each misclassified example. But the misclassification error of each class is not same. Generally, the misclassification error of the minority class will larger than the majority’s. So Adaboost algorithm will lead to higher bias and smaller margin when encountering skew distribution. Our goal is to reduce the bias of Adaboost algorithm and increase the margin between each two classes. The proposed BABoost algorithm (Figure 1) in each round of boosting assigns more weights to the misclassified examples, especially those in the minority class. More generally, we focus on multiclass (J-class) problems in which in this paper. Following Schapire and Singer’s <NO>, <NO> approach to multiclass problems, we change the multiclass problem into two-class problems. That is, if equals , we set it equal to a 1 by vector with the element as +1 and -1 for others; if equals , then it equals a 1 by J-1 vector with all elements as -1. D denotes the resampling probabilities for each case and H is the matrix for the ensemble predictor.
The differences between BABoost and Adaboost are the calculation of , , and the update of the H. indicates the prediction error for the predictor when predicting the cases into the class. is the weight for the
predictor and class. For example, instead of the overall error in the AdaBoost, it will be two within group prediction errors: and for binary problems.
Except that and will lead to different weights to majority class and minority class, also give a strong impact to reweight the distribution of samples. Different class corresponds to its own . The value of should be positive and larger for majority class, smaller for minority class in order to emphasize more on minority class: The larger the is, the smaller the is. So the corresponding misclassified objects of the class get lower weights."	https://doi.org/10.1109/FSKD.2009.608
232	SMOUTE: Synthetics minority over-sampling and under-sampling techniques for class imbalanced problem	[]	Panote Songwattanasiri, and Krung Sinapiromsaran. 2010. SMOUTE: Synthetics minority over-sampling and under-sampling techniques for class imbalanced problem. Proceedings of the Annual International Conference on Computer Science Education: Innovation and Technology, Special Track: Knowledge Discovery. 78–83.			http://scholar.google.com/scholar?hl=en&q=Panote+Songwattanasiri+and+Krung+Sinapiromsaran.+2010.+SMOUTE%3A+Synthetics+minority+over-sampling+and+under-sampling+techniques+for+class+imbalanced+problem.+In+Proceedings+of+the+Annual+International+Conference+on+Computer+Science+Education%3A+Innovation+and+Technology%2C+Special+Track%3A+Knowledge+Discovery.+78%2D%2D83.
233	Dealing with data difficulty factors while learning from imbalanced data	[]	Jerzy Stefanowski. 2016. Dealing with data difficulty factors while learning from imbalanced data. Challenges in Computational Statistics and Data Mining. Springer, 333–363.			http://scholar.google.com/scholar?hl=en&q=Jerzy+Stefanowski.+2016.+Dealing+with+data+difficulty+factors+while+learning+from+imbalanced+data.+In+Challenges+in+Computational+Statistics+and+Data+Mining.+Springer%2C+333%2D%2D363.
234	Selective pre-processing of imbalanced data for improving classification performance	['Many works have been developed studying the suitability of data preprocessing techniques to deal with imbalanced data-sets <NO>, <NO>, <NO>.', '2) Data level (or external) approaches rebalance the class distribution by resampling the data space <NO>, <NO>, <NO>,', '5) Selective preprocessing of imbalanced data (SPIDER) <NO>: It combines local oversampling of the minority class with filtering difficult examples from the majority class.']	Jerzy Stefanowski, and Szymon Wilk. 2008. Selective pre-processing of imbalanced data for improving classification performance. Data Warehousing and Knowledge Discovery. Springer, 283–292.			https://doi.org/10.1007/978-3-540-85836-2_27
235	Boosting for learning multiple classes with imbalanced class distribution	['Algorithmic methods include adjusting the costs associated with misclassification so as to improve performance <NO>, shifting the bias of a classifier to favor the rare class <NO>, creating Adaboost-like boosting schemes <NO>, and learning from one class <NO>.', 'Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', '<NO> extend the G-mean definition (see (17)) to the geometric means of recall values']	Yanmin Sun, Mohamed S. Kamel, and Yang Wang. 2006. Boosting for learning multiple classes with imbalanced class distribution. Sixth International Conference on Data Mining, 2006. ICDM’06. IEEE, 592–602.	Classification of data with imbalanced class distribution has posed a significant drawback of the performance attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. This learning difficulty attracts a lot of research interests. Most efforts concentrate on bi-class problems. However, bi-class is not the only scenario where the class imbalance problem prevails. Reported solutions for bi-class applications are not applicable to multi-class problems. In this paper, we develop a cost-sensitive boosting algorithm to improve the classification performance of imbalanced data involving multiple classes. One barrier of applying the cost-sensitive boosting algorithm to the imbalanced data is that the cost matrix is often unavailable for a problem domain. To solve this problem, we apply Genetic Algorithm to search the optimum cost setup of each class. Empirical tests show that the proposed cost-sensitive boosting algorithm improves the classification performances of imbalanced data sets significantly.	"The original AdaBoost algorithm reported in <NO> takes as input a training set {(x1, y1), · · ·, (xm, ym)} where each xi is an n-tuple of attribute values belonging to a certain domain or instance space X , and yi is a label in a label set Y = {−1, +1} in the context of bi-class applications. The Pseudocode for AdaBoost is given in Figure 1.
It has been shown in <NO> that the training error of the final classifier is bounded as
1 m |{i : H(xi) 6= yi}| ≤ ∏ t Zt (3)
Let
f(x) = T∑ t=1 αtht(x)
By unraveling the update rule of Equation 1, we have that
Dt+1(i) = exp(−
∑ t αtht(xi)yi)
m ∏
t Zt
= exp(−yif(xi))
m ∏
t Zt
(4)
By the definition of the final hypothesis of Equation 2, if H(xi) 6= yi, the yif(xi) ≤ 0 implying that exp(−yif(xi)) ≥ 1. Thus,
<NO> ≤ exp(−yif(xi)). (5)
where for any predicate π,
<NO> = { 1 if π holds 0 otherwise
(6)
Combining Equation 4 and 5 gives the error upper bound of Equation 3 since
1
m ∑ i <NO> ≤ 1 m ∑ i exp(−yif(xi)) (7)
= ∑
i
( ∏
t
Zt)D t+1(i) = ∏ t Zt (8)
To minimize the error upper-bound, on each boosting round, the learning objective is to minimize
Zt = ∑
i
Dt(i)exp(−αtyiht(xi)) (9)
= ∑
i
Dt(i)( 1 + yiht(xi) 2 e−α + 1− yiht(xi) 2 eα) (10)
Then, by minimizing Zt on each round, αt is induced as
αt = 1
2 log(
∑ i,yi=ht(xi) Dt(i)
∑ i,yi 6=ht(xi) Dt(i) ) (11)
The sample weight updating goal of AdaBoost is to decrease the weight of training samples which are correctly classified and increase the weights of the opposite part. Therefore, αt should be a positive value demanding that the training error should be less than randomly guessing (0.5) based on the current data distribution. That is
∑ i,yi=ht(xi) Dt(i) > ∑ i,yi 6=ht(xi) Dt(i) (12)"	https://doi.org/10.1109/ICDM.2006.29
236	Cost-sensitive boosting for classification of imbalanced data	['However, other proposals consider the embedding of the cost-sensitive framework in the ensemble learning process <NO>–<NO>.', 'have arisen as a possible solution to the class imbalance problem attracting great interest among researchers <NO>, <NO>, <NO>, <NO>.', 'way that they modify the weight update rule, among this family AdaCost <NO>, CSB1, CSB2 <NO>, RareBoost <NO>, AdaC1, AdaC2, and AdaC3 <NO> are the most representative approaches.', '4) AdaC1: This algorithm is one of the three modifications of AdaBoost proposed in <NO>.', 'ough empirical study was presented in <NO>.', 'Several variations have been proposed to make AdaBoost cost sensitive <NO>–<NO> or to improve its performance on imbalanced data <NO>–<NO>.', '<NO> present an in-depth examination of cost-sensitive boosting.', 'Three cost-sensitive boosting methods, AdaC1, AdaC2, and AdaC3, were proposed in <NO> which introduce cost items into the weight updating strategy of AdaBoost.', 'An empirical comparison over four imbalanced data sets of AdaC1, AdaC2, AdaC3, and AdaCost and two other similar algorithms, CSB1 and CSB2 <NO>, was performed in <NO> using decision trees and a rule association system as the base classifiers.', 'Many representative works on the ineffectiveness of accuracy in the imbalanced learning scenario exist in the community <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.']	Yanmin Sun, Mohamed S. Kamel, Andrew K.C. Wong, and Yang Wang. 2007. Cost-sensitive boosting for classification of imbalanced data. Pattern Recogn. 40, 12 (2007), 3358–3378.			https://doi.org/10.1016/j.patcog.2007.04.009
237	Classification of imbalanced data: A review	['Anyway, neither uniform distributions nor skewed distributions have to imply additional difficulties to the classifier learning task by themselves <NO>–<NO>.', 'itself <NO>, <NO>, the issue is that usually a series of difficulties related to this problem turn up.', 'This is known as the imbalance problem <NO>.']	Yanmin Sun, Andrew K.C. Wong, and Mohamed S. Kamel. 2009. Classification of imbalanced data: A review. Int. J. Pattern Recogn. Artif. Intell. 23, 4 (2009), 687–719.			http://scholar.google.com/scholar?hl=en&q=Yanmin+Sun%2C+Andrew+K.+C.+Wong%2C+and+Mohamed+S.+Kamel.+2009.+Classification+of+imbalanced+data%3A+A+review.+Int.+J.+Pattern+Recogn.+Artif.+Intell.+23%2C+4+%282009%29%2C+687%2D%2D719.
238	Inverse random under sampling for class imbalance problem and its application to multi-label classification	[]	Muhammad Atif Tahir, Josef Kittler, and Fei Yan. 2012. Inverse random under sampling for class imbalance problem and its application to multi-label classification. Pattern Recogn. 45, 10 (2012), 3738–3750.			https://doi.org/10.1016/j.patcog.2012.03.014
239	Multi-class protein fold classification using a new ensemble machine learning approach	['Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'knowledge for imbalance sample sets (eKISS) method <NO>,']	Aik Tan, David Gilbert, and Yves Deville. 2003. Multi-class protein fold classification using a new ensemble machine learning approach. (2003).			http://scholar.google.com/scholar?hl=en&q=Aik+Tan%2C+David+Gilbert%2C+and+Yves+Deville.+2003.+Multi-class+protein+fold+classification+using+a+new+ensemble+machine+learning+approach.+%282003%29.
240	Granular SVM with repetitive undersampling for highly imbalanced protein homology prediction	['We previously presented a preliminary version of the granular SVMs–repetitive undersampling (GSVM-RU) algorithm <NO>.', 'For simplicity and efficiency, in this correspondence, we revise the preliminary GSVM-RU algorithm <NO> and propose running granulation and aggregation in turns.', 'In <NO>, we proposed the “discard” operation for aggregation.', 'The Granular Support Vector Machines—Repetitive Undersampling algorithm (GSVM-RU) was proposed in <NO> to integrate SVM learning with undersampling methods.', 'In the context of imbalanced learning, the GSVM-RU method takes advantage of the GSVM by using an iterative learning procedure that uses the SVM itself for undersampling <NO>.']	Yuchun Tang, and Yan-Qing Zhang. 2006. Granular SVM with repetitive undersampling for highly imbalanced protein homology prediction. 2006 IEEE International Conference on Granular Computing. IEEE, 457–460.			http://scholar.google.com/scholar?hl=en&q=Yuchun+Tang+and+Yan-Qing+Zhang.+2006.+Granular+SVM+with+repetitive+undersampling+for+highly+imbalanced+protein+homology+prediction.+In+2006+IEEE+International+Conference+on+Granular+Computing.+IEEE%2C+457%2D%2D460.
241	SVMs modeling for highly imbalanced classification	[]	Yuchun Tang, Yan-Qing Zhang, Nitesh V. Chawla, and Sven Krasser. 2009. SVMs modeling for highly imbalanced classification. IEEE Trans. Syst. Man Cybernet. B 39, 1 (2009), 281–288.	Traditional classification algorithms can be limited in their performance on highly unbalanced data sets. A popular stream of work for countering the problem of class imbalance has been the application of a sundry of sampling strategies. In this correspondence, we focus on designing modifications to support vector machines (SVMs) to appropriately tackle the problem of class imbalance. We incorporate different “rebalance” heuristics in SVM modeling, including cost-sensitive learning, and overand undersampling. These SVM-based strategies are compared with various state-of-the-art approaches on a variety of data sets by using various metrics, including G-mean, area under the receiver operating characteristic curve, F -measure, and area under the precision/recall curve. We show that we are able to surpass or match the previously known best algorithms on each data set. In particular, of the four SVM variations considered in this correspondence, the novel granular SVMs–repetitive undersampling algorithm (GSVM-RU) is the best in terms of both effectiveness and efficiency. GSVM-RU is effective, as it can minimize the negative effect of information loss while maximizing the positive effect of data cleaning in the undersampling process. GSVM-RU is efficient by extracting much less support vectors and, hence, greatly speeding up SVM prediction.		https://doi.org/10.1109/TSMCB.2008.2002909
242	Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval	[', Asymmetric Bagging <NO> and QuasiBagging <NO>.']	Dacheng Tao, Xiaoou Tang, Xuelong Li, and Xindong Wu. 2006. Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval. IEEE Trans. Pattern Anal. Mach. Intell. 28, 7 (2006), 1088–1099.	Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM’s optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance.	"FEEDBACK IN CONTENT-BASED IMAGE RETRIEVAL
SVM <NO>, <NO> is a very effective binary classification
algorithm. Consider a linearly separable binary classifica-
tion problem (as shown in Fig. 1):
fðxi; yiÞgNi¼1 and yi ¼ þ1; 1f g; ð1Þ
where xi is an n-dimension vector and yi is the label of the class that the vector belongs to. SVM separates the two
classes of points by a hyperplane,
wTxþ b ¼ 0; ð2Þ
where w is an input vector, x is an adaptive weight vector,
and b is a bias. SVM finds the parameters w and b for the
optimal hyperplane to maximize the geometric margin 2= k w k , subject to
yi w Txi þ b
þ1: ð3Þ
The solution can be found through a Wolfe dual problem
with the Lagrangian multiplied by i:
Qð Þ ¼ Xm i¼1 i Xm i;j¼1 i jyiyjðxi xjÞ=2; ð4Þ
subject to i 0 and Pm
i¼1 iyi ¼ 0. In the dual format, data points only appear in the inner
product. To get a potentially better representation of the
data, the data points are mapped into a Hilbert Inner
Product space through a replacement:
xi xj ! ðxiÞ ðxjÞ ¼ Kðxi;xjÞ; ð5Þ
where Kð:Þ is a kernel function. We then get the kernel version of the Wolfe dual problem:
Qð Þ ¼ Xm i¼1 i Xm i;j¼1 i jyiyjKðxi;xjÞ=2: ð6Þ
Thus, for a given kernel function, the SVM classifier is
given by
F xð Þ ¼ sgn f xð Þð Þ; ð7Þ
where f xð Þ ¼ Pl
i¼1 iyiK xi;xð Þ þ b is the output hyperplane decision function of the SVM.
In general, when f xð Þj j for a given pattern is high, the corresponding prediction confidence will be high. Meanwhile, a low f xð Þj j of a given pattern means that the pattern is close to the decision boundary and its corresponding
prediction confidence will be low. Consequently, the output of SVM, f xð Þ, has been used to measure the dissimilarity <NO>, <NO> between a given pattern and the query image, in
traditional SVM-based CBIR RF."	https://doi.org/10.1109/TPAMI.2006.134
243	A new evaluation measure for learning from imbalanced data	[]	Nguyen Thai-Nghe, Zeno Gantner, and Lars Schmidt-Thieme. 2011. A new evaluation measure for learning from imbalanced data. The 2011 International Joint Conference on Neural Networks (IJCNN). IEEE, 537–542.			http://scholar.google.com/scholar?hl=en&q=Nguyen+Thai-Nghe%2C+Zeno+Gantner%2C+and+Lars+Schmidt-Thieme.+2011.+A+new+evaluation+measure+for+learning+from+imbalanced+data.+In+The+2011+International+Joint+Conference+on+Neural+Networks+%28IJCNN%29.+IEEE%2C+537%2D%2D542.
244	Two modifications of CNN	['Some informed undersampling methods also apply data cleaning techniques <NO> to further refine the majority class samples <NO>, <NO>.', 'These methods ally a known over-sampling method, namely Smote <NO>, with two data cleaning methods: Tomek links <NO> and Wilson’s Edited Nearest Neighbor Rule <NO>.', 'Tomek links Tomek links <NO> can be defined as follows: given two examples Ei and Ej belonging to different classes, and d(Ei, Ej) is the distance between Ei and Ej .', 'Generally speaking, Tomek links <NO> can be defined as a pair of minimally distanced nearest neighbors of opposite classes.', 'The borderline examples and examples suffering from the class label noise can be detected using the concept of Tomek links <NO>.', 'Subsequently, Tomek links <NO> are used to remove incorrectly classified instances and borderline instances which lie close to the boundary between the two']	Ivan Tomek. 1976. Two modifications of CNN. IEEE Trans. Syst. Man Cybern. 11 (1976), 769–772.			http://scholar.google.com/scholar?hl=en&q=Ivan+Tomek.+1976.+Two+modifications+of+CNN.+IEEE+Trans.+Syst.+Man+Cybern.+11+%281976%29%2C+769%2D%2D772.
245	Regression error characteristic surfaces	[]	Luı́s Torgo 2005. Regression error characteristic surfaces. In KDD’05: Proc. of the 11th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining. ACM Press,	This paper presents a generalization of Regression Error Characteristic (REC) curves. REC curves describe the cumulative distribution function of the prediction error of models and can be seen as a generalization of ROC curves to regression problems. REC curves provide useful information for analyzing the performance of models, particularly when compared to error statistics like for instance the Mean Squared Error. In this paper we present Regression Error Characteristic (REC) surfaces that introduce a further degree of detail by plotting the cumulative distribution function of the errors across the distribution of the target variable, i.e. the joint cumulative distribution function of the errors and the target variable. This provides a more detailed analysis of the performance of models when compared to REC curves. This extra detail is particularly relevant in applications with non-uniform error costs, where it is important to study the performance of models for specific ranges of the target variable. In this paper we present the notion of REC surfaces, describe how to use them to compare the performance of models, and illustrate their use with an important practical class of applications: the prediction of rare extreme values.	"Bi and Bennet <NO> have presented REC curves. These curves play a role similar to ROC curves (e.g. <NO>) in classification tasks, but for regression problems. They provide a graphical description of the cumulative distribution function of the error of a model, i.e. D( ) = P (ε ≤ ). The authors describe a simple algorithm for plotting these curves based on estimating the probabilities using the observed frequencies of the errors.
REC curves provide a better description of a model predictive performance when compared to prediction error statistics because they illustrate its performance across the range of possible errors. It is thus possible to extract more information by comparing the REC curves of two alternative models than with the two respective error statistics. Moreover, the interpretation of REC curves is quite appealing to non-experts and it is possible to obtain the same quantitative information given by prediction error statistics by calculating the Area Over the Curve (AOC), which Bi and Bennet <NO> have proved to be a biased estimate of the expected error of a model.
Figure 1 shows an example of the REC curves of three models. This example shows a model (model A) clearly
dominating the others over all range of possible errors. On the contrary models B and C have performances that are harder to compare. For smaller errors model C dominates model B, but as we move towards larger errors we see model B overcoming model C. The decision on which of these two is preferable may be domain dependent, provided their area over the curve (i.e. expected error) is similar.
In spite of the above mentioned advantages there are some specific domain requirements that are difficult to check using REC curves. These have to do with domains where the cost of errors is non-uniform, i.e. where the importance of an error with an amplitude of say 1.2, can be different depending on the true target variable value. For this type of applications, it may be important to inspect the distribution of the errors across the distribution of the target variable. In effect, it is possible to have two different models with exactly the same REC curves but still one being preferable to the other just because smaller errors occur for target values that are more relevant (e.g. have higher cost) for the application being studied. Distinguishing these two models and checking that one behaves more favorably than the other is not possible with the information provided by REC curves. This is the objective of our work."	https://doi.org/10.1145/1081870.1081959
246	Predicting outliers. In Knowledge Discovery in Databases: PKDD	[]	Luı́s Torgo, and Rita P. Ribeiro 2003. Predicting outliers. In Knowledge Discovery in Databases: PKDD			http://scholar.google.com/scholar?hl=en&q=Lu%C3%ADs+Torgo+and+Rita+P.+Ribeiro.+2003.+Predicting+outliers.+In+Knowledge+Discovery+in+Databases%3A+PKDD+2003.+Springer%2C+447%2D%2D458.
247	Utility-based regression	[]	Luı́s Torgo, and Rita P. Ribeiro 2007. Utility-based regression. In PKDD’07: Proc. of 11th European Conf. on Principles and Practice of Knowledge Discovery in Databases			http://scholar.google.com/scholar?hl=en&q=Rita+P.+Ribeiro.+2011.+Utility-based+Regression.+Ph.D.+Dissertation.+Dep.+Computer+Science%2C+Faculty+of+Sciences%2C+University+of+Porto.
248	Precision and recall in regression	[]	Luı́s Torgo, and Rita P. Ribeiro 2009. Precision and recall in regression. In DS’09: 12th Int. Conf. on Discovery			https://doi.org/10.1007/978-3-642-04747-3_26
249	SMOTE for regression	[]	Luı́s Torgo, Rita P. Ribeiro, Bernhard Pfahringer, and Paula Branco 2013. SMOTE for regression. In Progress in Artificial Intelligence			http://scholar.google.com/scholar?hl=en&q=Lu%C3%ADs+Torgo%2C+Rita+P.+Ribeiro%2C+Bernhard+Pfahringer%2C+and+Paula+Branco.+2013.+SMOTE+for+regression.+In+Progress+in+Artificial+Intelligence.+Springer%2C+378%2D%2D389.
250	A bias-variance analysis of a real-world learning problem: The coil challenge 2000	[]	Peter Van Der Putten, and Maarten Van Someren. 2004. A bias-variance analysis of a real-world learning problem: The coil challenge 2000. Mach. Learn. 57, 1–2 (2004), 177–195.			https://doi.org/10.1023/B:MACH.0000035476.95130.99
251	Experimental perspectives on learning from imbalanced data	['RUS has been shown to perform very well despite its simplicity <NO>.', ', sensitivity/specificity via ROC analysis, the g-mean) <NO>.', 'For classification, this simple approach works well <NO> and is theoretically motivated <NO>.', 'Data sampling techniques include random undersampling <NO>, onesided selection <NO>, Wilson’s editing <NO>, random oversampling,']	Jason Van Hulse, Taghi M. Khoshgoftaar, and Amri Napolitano. 2007. Experimental perspectives on learning from imbalanced data. Proceedings of the 24th International Conference on Machine Learning. ACM, 935–942.	We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the performance of learners built from imbalanced data? Is the effectiveness of sampling related to the type of learner? Do the results change if the objective is to optimize different performance metrics? We address these and other issues in this work, showing that sampling in many cases will improve classifier performance.	The 35 datasets utilized in our empirical study are listed in Table 1. The percentage of minority examples varies from 1.33% (highly imbalanced) to almost 35% (only slightly imbalanced). The datasets also come from a wide variety of application domains, and 19 are from the UCI repository (Blake & Merz, 1998). The Mammography dataset was generously provided by Dr. Nitesh Chawla (Chawla et al., 2002). Fifteen datasets (some of which are proprietary) are from the domain of software engineering measurements. We have also considered datasets with diversity in the number of attributes, and datasets with both continuous and categorical attributes. The smallest dataset had 214 total examples (Glass-3), while the two largest datasets each contain 20,000 observations. Note that all datasets have, or were transformed to have, a binary class. We only consider binary classification problems in this work.	https://doi.org/10.1145/1273496.1273614
252	A hybrid under-sampling approach for mining unbalanced datasets: Applications to banking and insurance	[]	Madireddi Vasu, and Vadlamani Ravi. 2011. A hybrid under-sampling approach for mining unbalanced datasets: Applications to banking and insurance. Int. J. Data Min. Model. Manag. 3, 1 (2011), 75–105.			http://scholar.google.com/scholar?hl=en&q=Madireddi+Vasu+and+Vadlamani+Ravi.+2011.+A+hybrid+under-sampling+approach+for+mining+unbalanced+datasets%3A+Applications+to+banking+and+insurance.+Int.+J.+Data+Min.+Model.+Manag.+3%2C+1+%282011%29%2C+75%2D%2D105.
253	Improving SMOTE with fuzzy rough prototype selection to detect noise in imbalanced classification data	[]	Nele Verbiest, Enislay Ramentol, Chris Cornelis, and Francisco Herrera. 2012. Improving SMOTE with fuzzy rough prototype selection to detect noise in imbalanced classification data. Advances in Artificial Intelligence–IBERAMIA 2012. Springer, 169–178.			http://scholar.google.com/scholar?hl=en&q=Nele+Verbiest%2C+Enislay+Ramentol%2C+Chris+Cornelis%2C+and+Francisco+Herrera.+2012.+Improving+SMOTE+with+fuzzy+rough+prototype+selection+to+detect+noise+in+imbalanced+classification+data.+In+Advances+in+Artificial+Intelligence%2D%2DIBERAMIA+2012.+Springer%2C+169%2D%2D178.
254	Controlling the sensitivity of support vector machines	['Weighting methods either assign heavier weights to the minority training instances or penalties for misclassifications of minority instances <NO>.', 'However, SVMs are also sensitive to class imbalance <NO><NO>.', 'For training SVMs with imbalanced dataset, there exist algorithmic techniques, such as Different Error Cost (DEC) <NO> and zSVM <NO>.', 'The basic idea is to assign a larger penalty value to false negatives (FNs) than false positives (FPs) <NO>, <NO>, <NO>.', 'Although the SVM algorithm works effectively with balanced datasets, when it comes to imbalanced datasets, it could often produce suboptimal results <NO>–<NO>, i.', 'It has been well-studied that the SVM algorithm can be sensitive to class imbalance <NO>–<NO>, i.', '<NO> has proposed a method called different error costs (DEC), where the SVM objective function has been modified to assign two misclassification cost values C+ and C− as follows:']	Konstantinos Veropoulos, Colin Campbell, and Nello Cristianini. 1999. Controlling the sensitivity of support vector machines. Proceedings of the International Joint Conference on Artificial Intelligence, Vol. 1999. Citeseer, 55–60.			http://scholar.google.com/scholar?hl=en&q=Konstantinos+Veropoulos%2C+Colin+Campbell%2C+and+Nello+Cristianini.+1999.+Controlling+the+sensitivity+of+support+vector+machines.+In+Proceedings+of+the+International+Joint+Conference+on+Artificial+Intelligence%2C+Vol.+1999.+Citeseer%2C+55%2D%2D60.
255	Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion	[]	Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res. 11 (2010), 3371–3408.			https://doi.org/10.5555/1756006.1953039
256	Guiding scientific discovery with explanations using DEMUD	[]	Kiri L. Wagstaff, Nina L. Lanza, David R. Thompson, Thomas G. Dietterich, and Martha S. Gilmore. 2013. Guiding scientific discovery with explanations using DEMUD. AAAI.			https://doi.org/10.5555/2891460.2891586
257	Class probability estimates are unreliable for imbalanced data (and how to fix them)	[]	Byron C. Wallace, and Issa J. Dahabreh. 2012. Class probability estimates are unreliable for imbalanced data (and how to fix them). 2012 IEEE 12th International Conference on Data Mining (ICDM). IEEE, 695–704.	Obtaining good probability estimates is imperative for many applications. The increased uncertainty and typically asymmetric costs surrounding rare events increases this need. Experts (and classification systems) often rely on probabilities to inform decisions. However, we demonstrate that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. To our knowledge, this problem has not previously been explored. Motivated by our exposition of this issue, we propose a simple, effective and theoretically motivated method to mitigate the bias of probability estimates for imbalanced data that bags estimators calibrated over balanced bootstrap samples. This approach drastically improves performance on the minority instances without greatly affecting overall calibration. We show that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates. 	"The standard method for estimating probabilities in the supervised learning framework is to regress measurements correlated with (predicted) class labels output by a trained
1We note that Cieslak and Chawla have investigated the specific case of Probability Estimation Trees (PETs) for imbalanced data <NO>, and that Foster and Stine have considered the related task of variable selection for prediction under imbalance <NO>.
classifier against the true target labels <NO>, <NO>. This process is called calibration. By convention these measurements are denoted by fi, where i indexes instances. This calibration squashes the (arbitrarily scaled) fi’s into the <NO> range permissible for probabilities. When the sigmoid form is used (Equation 1), this method is referred to as Platt scaling <NO>. It is perhaps the most popular way of obtaining probability estimates from classification models.
P (yi|fi) = 1 1 + exp{−β0 − β1fi} (1)
The fi’s may be any scalar that is predictive of class membership. We focus on two specific post-training calibration strategies; Platt calibration with SVMs and with boosted decision trees. We selected these methods because they have been shown to out-perform other supervised learning algorithms with respect to class probability estimation <NO>, <NO>.
In the case of SVMs, the fi is taken as the signed distance from the hyperplane w, i.e., fi = wTxi. This was the method originally proposed by Platt <NO>, and is now widely used <NO>. Niculescu-Mizil and Caruana, meanwhile, have proposed attaining probabilities via calibrated boosted decision trees <NO>. More precisely, recall that in boosting one induces a sequence of learners h0, h1, ... , hk over different distributions of the training set. These are in turn associated with a set of weights α0, α1, ... , αk reflecting the their estimated performance. A prediction is then taken as a function over these, i.e., as sign( ∑ j αjhj(x)). The natural value for fi is then the sum of the weighted class predictions over the ensemble, i.e., ∑ j αjhj(x).
Figure 2 displays the overall and stratified residual errors of probability estimates (obtained via Platt’s method) for the instances comprising a particular imbalanced dataset.2 Specifically, each sub-plot shows histograms of the absolute differences between the true (observed) labels and corresponding probability estimates, i.e., |yi−P̂{yi|xi}|. Density to the left therefore suggests good calibration, as this implies probability estimates largely agree with the observed labels. For example, if yi = 1 and P̂{yi|xi} = .99, the difference would be .01. Were the estimate .01, on the other hand, the difference would be .99.
The left-hand side of Figure 2 shows this histogram for all instances, corresponding to overall calibration. Over 80% of instances are in the left-most bin, implying that the estimator is well-calibrated, i.e., its estimates do not much diverge from the observed labels. But this ostensibly good calibration belies the unreliability of the probability estimates for the instances comprising the rare class. One can see this by looking at the middle plot, which is the same figure but includes only minority instances. In this case, the estimates diverge strikingly from the observed
2The proton beam dataset in Table I.
labels; indeed the model assigned a probability (of belonging to the minority class) of less than 20% to most of the minority instances. In other words, the probability estimates for instances comprising the minority class are completely unreliable (we demonstrate this on 16 datasets in Section V)."	https://doi.org/10.1109/ICDM.2012.115
258	Improving class probability estimates for imbalanced data	[]	Byron C. Wallace, and Issa J. Dahabreh. 2014. Improving class probability estimates for imbalanced data. Knowl. Inform. Syst. 41, 1 (2014), 33–52.			https://doi.org/10.1007/s10115-013-0670-6
259	Class imbalance, redux	[]	Byron C. Wallace, Kevin Small, Carla E. Brodley, and Thomas A. Trikalinos. 2011. Class imbalance, redux. 2011 IEEE 11th International Conference on Data Mining (ICDM). IEEE, 754–763.			http://scholar.google.com/scholar?hl=en&q=Ricardo+Barandela%2C+Jos%C3%A9+Salvador+S%C3%A1nchez%2C+Vicente+Garcia%2C+and+Edgar+Rangel.+2003.+Strategies+for+learning+in+class+imbalance+problems.+Pattern+Recogn.+36%2C+3+%282003%29%2C+849%2D%2D851.
260	Boosting support vector machines for imbalanced data sets	['Some examples include the SMOTE with Different Costs (SDCs) method <NO> and the ensembles of over/undersampled SVMs <NO>, <NO>, <NO>, <NO>.', 'Lastly, Wang and Japkowicz <NO> proposed to modify the SVMs with asymmetric misclassification costs in order to boost performance.']	Benjamin X. Wang, and Nathalie Japkowicz. 2010. Boosting support vector machines for imbalanced data sets. Knowl. Inform. Syst. 25, 1 (2010), 1–20.			https://doi.org/10.1007/s10115-009-0198-y
261	Concept drift detection for imbalanced stream data	[]	Heng Wang, and Zubin Abraham. 2015. Concept drift detection for imbalanced stream data. arXiv Preprint arXiv:1504.01044 (2015).			http://scholar.google.com/scholar?hl=en&q=Heng+Wang+and+Zubin+Abraham.+2015.+Concept+drift+detection+for+imbalanced+stream+data.+arXiv+Preprint+arXiv%3A1504.01044+%282015%29.
262	Combination approach of SMOTE and biased-SVM for imbalanced datasets	[]	He-Yong Wang. 2008. Combination approach of SMOTE and biased-SVM for imbalanced datasets. IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence). IEEE, 228–231.			http://scholar.google.com/scholar?hl=en&q=He-Yong+Wang.+2008.+Combination+approach+of+SMOTE+and+biased-SVM+for+imbalanced+datasets.+In+IEEE+International+Joint+Conference+on+Neural+Networks%2C+2008.+IJCNN+2008.+%28IEEE+World+Congress+on+Computational+Intelligence%29.+IEEE%2C+228%2D%2D231.
263	Diversity analysis on imbalanced data sets by using ensemble models	['On the other hand, with regard to ensemble learning methods, a large number of different approaches have been proposed in the literature, including but not limited to SMOTEBoost <NO>, RUSBoost <NO>, IIVotes <NO>, EasyEnsemble <NO>, or SMOTEBagging <NO>.', 'We distinguish four main algorithms in this family, OverBagging <NO>, UnderBagging <NO>, UnderOverBagging <NO>, and IIVotes <NO>.', 'We distinguish four main algorithms in this family, OverBagging <NO>, UnderBagging <NO>, UnderOverBagging <NO>, and IIVotes <NO>.', 'SMOTEBagging <NO> differs from the use of random oversampling']	Shuo Wang, and Xin Yao. 2009. Diversity analysis on imbalanced data sets by using ensemble models. IEEE Symposium on Computational Intelligence and Data Mining, 2009. CIDM’09. IEEE, 324–331.			http://scholar.google.com/scholar?hl=en&q=Shuo+Wang+and+Xin+Yao.+2009.+Diversity+analysis+on+imbalanced+data+sets+by+using+ensemble+models.+In+IEEE+Symposium+on+Computational+Intelligence+and+Data+Mining%2C+2009.+CIDM%2709.+IEEE%2C+324%2D%2D331.
264	Resampling and cost-sensitive methods for imbalanced multi-instance learning	[]	Xiaoguang Wang, Xuan Liu, Nathalie Japkowicz, and Stan Matwin. 2013. Resampling and cost-sensitive methods for imbalanced multi-instance learning. 2013 IEEE 13th International Conference on Data Mining Workshops (ICDMW). IEEE, 808–816.			http://scholar.google.com/scholar?hl=en&q=Xiaoguang+Wang%2C+Xuan+Liu%2C+Nathalie+Japkowicz%2C+and+Stan+Matwin.+2013a.+Resampling+and+cost-sensitive+methods+for+imbalanced+multi-instance+learning.+In+2013+IEEE+13th+International+Conference+on+Data+Mining+Workshops+%28ICDMW%29.+IEEE%2C+808%2D%2D816.
265	Cost-sensitive boosting algorithms for imbalanced multi-instance datasets	[]	Xiaoguang Wang, Stan Matwin, Nathalie Japkowicz, and Xuan Liu. 2013. Cost-sensitive boosting algorithms for imbalanced multi-instance datasets. Advances in Artificial Intelligence. Springer, 174–186.			http://scholar.google.com/scholar?hl=en&q=Xiaoguang+Wang%2C+Stan+Matwin%2C+Nathalie+Japkowicz%2C+and+Xuan+Liu.+2013b.+Cost-sensitive+boosting+algorithms+for+imbalanced+multi-instance+datasets.+In+Advances+in+Artificial+Intelligence.+Springer%2C+174%2D%2D186.
266	Combating the small sample class imbalance problem using feature selection	[]	Mike Wasikowski, and Xue-wen Chen. 2010. Combating the small sample class imbalance problem using feature selection. IEEE Trans. Knowl. Data Eng. 22, 10 (2010), 1388–1400.	The class imbalance problem is encountered in real-world applications of machine learning and results in a classifier’s suboptimal performance. Researchers have rigorously studied the resampling, algorithms, and feature selection approaches to this problem. No systematic studies have been conducted to understand how well these methods combat the class imbalance problem and which of these methods best manage the different challenges posed by imbalanced data sets. In particular, feature selection has rarely been studied outside of text classification problems. Additionally, no studies have looked at the additional problem of learning from small samples. This paper presents a first systematic comparison of the three types of methods developed for imbalanced data classification problems and of seven feature selection metrics evaluated on small sample data sets from different applications. We evaluated the performance of these metrics using area under the receiver operating characteristic (AUC) and area under the precisionrecall curve (PRC). We compared each metric on the average performance across all problems and on the likelihood of a metric yielding the best performance on a specific problem. We examined the performance of these metrics inside each problem domain. Finally, we evaluated the efficacy of these metrics to see which perform best across algorithms. Our results showed that signal-tonoise correlation coefficient (S2N) and Feature Assessment by Sliding Thresholds (FAST) are great candidates for feature selection in most applications, especially when selecting very small numbers of features.	The two Learning from Imbalanced Data Sets workshops thoroughly explored the approaches to combating the class imbalance problem: sampling, new algorithms, and feature selection. The first was held at the AAAI conference in 2000 <NO>, and the second was held at the ICML conference in 2003 <NO>. Also, Weiss reviewed these approaches in SIGKDD Explorations <NO>.	https://doi.org/10.1109/TKDE.2009.187
267	An improved SVM-KM model for imbalanced datasets	[]	Deng Weiguo, Wang Li, Wang Yiyang, and Qian Zhong. 2012. An improved SVM-KM model for imbalanced datasets. 2012 International Conference on Industrial Control and Electronics Engineering (ICICEE). IEEE, 100–103.			https://doi.org/10.1109/ICICEE.2012.35
268	Mining with rarity: A unifying framework	['Other relevant works are reported in <NO>.', 'The performance measure used to evaluate the classifier performance at the model selection stage would also play a vital role when training classifiers from imbalanced datasets <NO>.', 'It has been well-studied that the most commonly used performance measure, the overall accuracy (Acc = the proportion of the correctly predicted examples) of the model, could not be used to measure the true performance of classification models when the datasets are imbalanced <NO>, <NO>.', 'As a remedy, researchers have used alternative performance measures, such as Geometric-mean (Gm) <NO>,<NO> and F-measure (Fm) <NO>,<NO> for classifier performance evaluation in imbalanced dataset learning.', 'It has been well-studied in the past research that the most widely used performance measure, the Accuracy (Acc), can lead to sub-optimal classification models, if it is used for imbalanced dataset learning <NO>-<NO>.', 'In order to overcome this problem, other performance measures, such as the Gm <NO>,<NO> and Fm <NO>,<NO>, have been used in class imbalance learning research.', 'Fm is also a popular metric applied in imbalanced dataset learning, which also overcomes the problem associated with the Acc metric <NO>,<NO>.', 'We did not consider the use of Acc as a performance measure in these experiments, since its problems associated with the imbalanced dataset learning has been well-studied in the existing research <NO><NO>.', 'However, imbalance learning problems pose a great challenge to the classifier as it becomes very hard to learn the minority class samples <NO>, <NO>, <NO>.', 'Interested readers may refer to <NO> for a good survey.', 'Two such techniques are data sampling and boosting <NO>.', 'Weiss <NO> provides a survey of the class imbalance problem and techniques for reducing the negative impact imbalance that has on classification performance.', 'It is noteworthy that class imbalance is emerging as an important issue in designing classifiers <NO>, <NO>, <NO>.', 'Cost-sensitive learning deals with class imbalance by incurring different costs for the two classes and is considered as an important class of methods to handle class imbalance <NO>.', 'readers to <NO> for a more complete and detailed review.', 'to be helpful in imbalanced problems <NO>, <NO>.', 'The majority of current research in the class-imbalance problem can be grouped into two categories: sampling techniques and algorithmic methods, as discussed in two workshops at the AAAI conference <NO> and the ICML conference <NO>, and later in the sixth issue of SIGKDD Exploration (see, for example, a review by Weiss <NO>).', 'We must note that this not a very uncommon situation, and it known as absolute rarity <NO>.', 'The imbalance learning problem generally manifests itself in two forms: relative imbalances and absolute imbalances <NO>, <NO>.', 'In addition to intrinsic and extrinsic imbalance, it is important to understand the difference between relative imbalance and imbalance due to rare instances (or “absolute rarity”) <NO>, <NO>.', 'In this situation, the lack of representative data will make learning difficult regardless of the between-class imbalance <NO>.', 'Briefly, the problem of small disjuncts can be understood as follows: A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept <NO>, <NO>, <NO>.', ', rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts <NO>, <NO>, <NO>.', 'anced learning problems <NO>, <NO>, <NO>.', 'Many representative works on the ineffectiveness of accuracy in the imbalanced learning scenario exist in the community <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Generally, these methods can be divided into two categories: external methods and internal methods <NO>, <NO>, <NO>.', 'sampling, the majority-class examples are removed randomly, until a particular class ratio is met <NO>.', 'It has been well-studied that when the training dataset is imbalanced, the commonly used performance measure accuracy, which is the proportion of correctly classified instances, could lead to suboptimal models <NO>, <NO>.', 'This evidence supported the fact reported in <NO> that the CIL method giving the best results is dataset-dependent.', ', true positive, true negative, false positive, and false negative <NO>, <NO>.']	Gary M. Weiss. 2004. Mining with rarity: A unifying framework. SIGKDD Explor. Newslett. 6, 1 (2004), 7–19.	Rare objects are often of great interest and great value. Until recently, however, rarity has not received much attention in the context of data mining. Now, as increasingly complex real-world problems are addressed, rarity, and the related problem of imbalanced data, are taking center stage. This article discusses the role that rare classes and rare cases play in data mining. The problems that can result from these two forms of rarity are described in detail, as are methods for addressing these problems. These descriptions utilize examples from existing research, so that this article provides a good survey of the literature on rarity in data mining. This article also demonstrates that rare classes and rare cases are very similar phenomena—both forms of rarity are shown to cause similar problems during data mining and benefit from the same remediation methods.		https://doi.org/10.1145/1007730.1007734
269	Mining with rare cases	[]	Gary M. Weiss. 2005. Mining with rare cases. Data Mining and Knowledge Discovery Handbook. Springer, 765–776.			http://scholar.google.com/scholar?hl=en&q=Gary+M.+Weiss.+2005.+Mining+with+rare+cases.+In+Data+Mining+and+Knowledge+Discovery+Handbook.+Springer%2C+765%2D%2D776.
270	The impact of small disjuncts on classifier learning	[]	Gary M. Weiss. 2010. The impact of small disjuncts on classifier learning. Data Mining. Springer, 193–226.			http://scholar.google.com/scholar?hl=en&q=Gary+M.+Weiss.+2010.+The+impact+of+small+disjuncts+on+classifier+learning.+In+Data+Mining.+Springer%2C+193%2D%2D226.
271	Foundations of imbalanced learning	[]	Gary M. Weiss. 2013. Foundations of imbalanced learning. Imbalanced Learning: Foundations, Algorithms, and Applications, Haibo He and Yunqian Ma (Eds.). John Wiley & Sons.			http://scholar.google.com/scholar?hl=en&q=Gary+M.+Weiss.+2013.+Foundations+of+imbalanced+learning.+In+Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications%2C+Haibo+He+and+Yunqian+Ma+%28Eds.%29.+John+Wiley+%26+Sons.
272	Learning when training data are costly: The effect of class distribution on tree induction	['the minority class is formed of subconcepts <NO>.', 'Other relevant works are reported in <NO>.', 'It has been well-studied that the most commonly used performance measure, the overall accuracy (Acc = the proportion of the correctly predicted examples) of the model, could not be used to measure the true performance of classification models when the datasets are imbalanced <NO>, <NO>.', 'We did not consider the use of Acc as a performance measure in these experiments, since its problems associated with the imbalanced dataset learning has been well-studied in the existing research <NO><NO>.', 'Each dataset and the corresponding class distribution will have its own requirements <NO>.', 'Researchers in the machine learning community have dealt with the problem of class imbalance by using various approaches like over-sampling the minority classes, undersampling the majority classes, assigning different costs for different misclassification errors, learning by recognition as opposed to discrimination, etc <NO>.', 'often considered to be near optimal <NO>.', 'In effect, boosting alters the distribution of the training data (through weighting, not altering the number of examples); thus, it can also be thought of as an advanced data sampling technique <NO>.', 'Our experimental design is inspired by the design used in <NO>.', 'As we previously observed, our experimental design is inspired by the setup used in <NO>.', 'Although the results in <NO> were obtained with the C4.', 'This decision is motivated by the results presented in <NO>, in which it is shown that when AUC is used as performance measure, the best class distribution for learning tends to be near the balanced class distribution.', ', that the test set distribution matches the training set distribution <NO>.', 'It should be noted that allocating half of the training examples to the minority class does not always provide optimal results <NO>.', 'Some studies have shown that the degradation of classification performance attributed to imbalanced data is not necessarily the result of relative imbalances but rather due to the lack of representative examples (absolute imbalances) <NO>, <NO>, <NO>, <NO>, <NO>.', 'This problem of concept data representation within a class is also known as the within-class imbalance problem, <NO>, <NO>, <NO>, and was verified to be more difficult to handle than datasets with only homogeneous concepts for each class <NO>, <NO>.', 'Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>.', 'In this case, imbalanced data sets exploit inadequacies in the splitting criterion at each node of the decision tree <NO>, <NO>, <NO>.', '<NO> and <NO> that directly relate to the aforementioned', 'Weiss and Provost <NO> have analyzed, for a fixed training', 'distribution provides the best classifier” <NO>.', 'An empirical study by Weiss and Provost <NO> supports the conclusion that accuracy leads to poor minority-class performance.', 'Another method that uses this general approach employs a progressive-sampling algorithm to build larger and larger training sets, where the ratio of positive to negative examples added in each iteration is chosen based on the performance of the various class distributions evaluated in the previous iteration <NO>.', 'While existing research <NO> indicates that reducing class imbalance will tend to also reduce within-class imbalances (i.', 'Because rare classes/cases are more error-prone than common classes/cases <NO>, it is reasonable to believe that boosting may improve their classification performance because, overall, it will increase the weights of the examples associated with these rare cases/classes.', '1 to account for the differences between the training distribution and the underlying distribution <NO>.', 'One study <NO> on class distribution shows the advantages of adjusting for this bias.', 'Unfortunately, as shown by Weiss and Provost <NO>, there is no general answer as to which class distribution will perform best, and the answer is surely method and domain dependent.', 'A progressive sampling scheme has been shown to be effective at finding a near-optimal class distribution for learning <NO>, although this does require some additional computation.', 'made up of smaller disjuncts than the majority-class rules <NO>.', 'Weiss and Provost <NO> report that an equal balance between the majority and minority classes is desirable; however, they also state that the optimal ratio between the two groups will vary with different data sets and domains.']	Gary M. Weiss, and Foster J. Provost. 2003. Learning when training data are costly: The effect of class distribution on tree induction. J. Artif. Intell. Res.(JAIR) 19 (2003), 315–354.			https://doi.org/10.5555/1622434.1622445
273	A new evaluation measure for imbalanced datasets	[]	Cheng G. Weng, and Josiah Poon. 2008. A new evaluation measure for imbalanced datasets. Proceedings of the 7th Australasian Data Mining Conference-Volume 87. Australian Computer Society, Inc., 27–32.			https://doi.org/10.5555/2449288.2449295
274	Class-boundary alignment for imbalanced dataset learning	['Weighting methods either assign heavier weights to the minority training instances or penalties for misclassifications of minority instances <NO>.', 'However, SVM still suffer the consequences of severe class imbalance, as noted by <NO>.']	Gang Wu, and Edward Y. Chang. 2003. Class-boundary alignment for imbalanced dataset learning. ICML 2003 Workshop on Learning from Imbalanced Data Sets II, Washington, DC. 49–56.			http://scholar.google.com/scholar?hl=en&q=Gang+Wu+and+Edward+Y.+Chang.+2003.+Class-boundary+alignment+for+imbalanced+dataset+learning.+In+ICML+2003+Workshop+on+Learning+from+Imbalanced+Data+Sets+II%2C+Washington%2C+DC.+49%2D%2D56.
275	KBA: Kernel boundary alignment considering imbalanced data distribution	['The algorithm level (internal) approaches create or modify the algorithms that exist, to take into account the significance of positive examples <NO>–<NO>.', 'It is prone to generating a classifier that has a strong estimation bias toward the majority class, resulting in a large number of false negatives <NO>, <NO>.', 'Wu and Chang <NO> proposed the kernel boundary alignment algorithm (KBA) that adjusts the boundary toward the majority class by modifying the kernel matrix.', 'Wu and Chang <NO> reported that KBA usually takes a longer time for classification than SVM.', 'However, for a highly imbalanced classification, the majority class pushes the ideal decision boundary toward the minority class <NO>, <NO>.', 'For instance, the Kernel-based alignment algorithm was proposed in <NO> and <NO>, in which imbalanced data is used as information prior to adjusting the kernel matrix in order to facilitate SVM learning for improved prediction accuracy.', 'These methods apply boundary alignment techniques to improve SVM classification <NO>, <NO>, <NO>.', 'Additionally, in <NO> and <NO>, the kernel-boundary alignment (KBA) algorithm was proposed which is based on the idea of modifying the kernel matrix generated by a kernel function according to the imbalanced data distribution.', 'In addition to these methods, several kernel-modification methods for training SVMs with imbalanced datasets have been proposed in <NO> and <NO>.']	Gang Wu, and Edward Y. Chang. 2005. KBA: Kernel boundary alignment considering imbalanced data distribution. IEEE Trans. Knowl. Data Eng. 17, 6 (2005), 786–795.	An imbalanced training data set can pose serious problems for many real-world data mining tasks that employ SVMs to conduct supervised learning. In this paper, we propose a kernel-boundary-alignment algorithm, which considers THE training data imbalance as prior information to augment SVMs to improve class-prediction accuracy. Using a simple example, we first show that SVMs can suffer from high incidences of false negatives when the training instances of the target class are heavily outnumbered by the training instances of a nontarget class. The remedy we propose is to adjust the class boundary by modifying the kernel matrix, according to the imbalanced data distribution. Through theoretical analysis backed by empirical study, we show that our kernelboundary-alignment algorithm works effectively on several data sets.		https://doi.org/10.1109/TKDE.2005.95
276	An improved model selection heuristic for AUC	[]	Shaomin Wu, Peter Flach, and César Ferri. 2007. An improved model selection heuristic for AUC. ECML. Springer, 478–489.			https://doi.org/10.1007/978-3-540-74958-5_44
277	Dynamic classifier ensemble model for customer classification with imbalanced class distribution	[]	Jin Xiao, Ling Xie, Changzheng He, and Xiaoyi Jiang. 2012. Dynamic classifier ensemble model for customer classification with imbalanced class distribution. Expert Syst. Appl. 39, 3 (2012), 3668–3675.			https://doi.org/10.1016/j.eswa.2011.09.059
278	Exploring of clustering algorithm on class-imbalanced data	[]	Li Xuan, Chen Zhigang, and Yang Fan. 2013. Exploring of clustering algorithm on class-imbalanced data. 2013 8th International Conference on Computer Science & Education (ICCSE). IEEE, 89–93.			http://scholar.google.com/scholar?hl=en&q=Li+Xuan%2C+Chen+Zhigang%2C+and+Yang+Fan.+2013.+Exploring+of+clustering+algorithm+on+class-imbalanced+data.+In+2013+8th+International+Conference+on+Computer+Science+%26+Education+%28ICCSE%29.+IEEE%2C+89%2D%2D93.
279	An active under-sampling approach for imbalanced data classification	[]	Zeping Yang, and Daqi Gao. 2012. An active under-sampling approach for imbalanced data classification. 2012 Fifth International Symposium on Computational Intelligence and Design (ISCID). Vol. 2. IEEE, 270–273.	An active under-sampling approach is proposed for handling the imbalanced problem in this paper. Traditional classifiers usually assume that training examples are evenly distributed among different classes, so they are often biased to the majority class and tend to ignore the minority class. In this case, it is important to select the suitable training dataset for learning from imbalanced data. The samples of the majority class which are far away from the decision boundary should be got rid of the training dataset automatically in our algorithm, and this process doesn’t change the density distribution of the whole training dataset. As a result, the ratio of majority class is decreased significantly, and the final balance training dataset is more suitable for the traditional classification algorithms. Compared with other under-sampling methods, our approach can effectively improve the classification accuracy of minority classes while maintaining the overall classification performance by the experimental results. 	"V. CONCLUSION Learning from imbalanced dataset is a challenging problem, because traditional classifiers are designed on the assumption that training examples are evenly distributed among different classes. Sampling methods can change data distribution by adding or deleting samples from the original training dataset. An active under-sampling approach has been presented on imbalanced data in this paper. Instead of getting rid of majority class samples randomly, our algorithm
actively selected the samples of majority class near the decision boundary, and, at the same time, maintained the original density distribution. The experimental results show that the proposed algorithm can achieve better performance compared to other methods on imbalanced datasets. For the future, the proposed method and suitable data cleaning technique were combined to handle highly imbalanced and overlapping datasets."	https://doi.org/10.1109/ISCID.2012.219
280	Under-sampling approaches for improving prediction of the minority class in an imbalanced dataset	['However, ignoring the minority class is a disastrous thing with the wide range of application such as identifying fraudulent credit card transactions, risk management and medical diagnosis etc <NO>.', 'The final experimental results showed that the NearMiss-2 method can provide competitive results <NO>.']	Show-Jane Yen, and Yue-Shi Lee. 2006. Under-sampling approaches for improving prediction of the minority class in an imbalanced dataset. Intelligent Control and Automation. Springer, 731–740.			http://scholar.google.com/scholar?hl=en&q=Show-Jane+Yen+and+Yue-Shi+Lee.+2006.+Under-sampling+approaches+for+improving+prediction+of+the+minority+class+in+an+imbalanced+dataset.+In+Intelligent+Control+and+Automation.+Springer%2C+731%2D%2D740.
281	Cluster-based under-sampling approaches for imbalanced data distributions	[]	Show-Jane Yen, and Yue-Shi Lee. 2009. Cluster-based under-sampling approaches for imbalanced data distributions. Expert Syst. Appl. 36, 3 (2009), 5718–5727.			https://doi.org/10.1016/j.eswa.2008.06.108
282	The research of imbalanced data set of sample sampling method based on K-means cluster and genetic algorithm	[]	Yang Yong. 2012. The research of imbalanced data set of sample sampling method based on K-means cluster and genetic algorithm. Energy Procedia 17 (2012), 164–170.			http://scholar.google.com/scholar?hl=en&q=Yang+Yong.+2012.+The+research+of+imbalanced+data+set+of+sample+sampling+method+based+on+K-means+cluster+and+genetic+algorithm.+Energy+Procedia+17+%282012%29%2C+164%2D%2D170.
283	An unsupervised learning approach to resolving the data imbalanced issue in supervised learning problems in functional genomics	[]	Kihoon Yoon, and Stephen Kwek. 2005. An unsupervised learning approach to resolving the data imbalanced issue in supervised learning problems in functional genomics. Fifth International Conference on Hybrid Intelligent Systems, 2005. HIS’05. IEEE, 6–pp.	Learning from imbalanced data occurs very frequently in functional genomic applications. One positive example to thousands of negative instances is common in scientific applications. Unfortunately, traditional machine learning treats the extremely small instances as noise. The standard approach for this difficulty is balancing training data by resampling them. However, this results in high false positive predictions. Hence, we propose preprocessing majority instances by partitioning them into clusters. This greatly reduces the ambiguity between minority instances and instances in each cluster. For moderately high imbalance ratio and low in-class complexity, our technique gives better prediction accuracy than undersampling method. For extreme imbalance ratio like splice site prediction problem, we demonstrate that this technique serves as a good filter with almost perfect recall that reduces the amount of imbalance so that traditional classification techniques can be deployed and yield significant improvements over previous predictor. We also show that the technique works for subcellular localization and post-translational modification site prediction problems.	"Most supervised learning algorithms tend to focus on obtaining high accuracy on the observed labeled training data. To further aggravate this difficulty, almost all algorithms tend to follow the Occam’s razor principle (or related minimum description length MDL principle) where there is a preference toward simple
hypothesis <NO>. Short decision trees and neural networks with small weights are preferred. The underlying assumption here is that events (instances) that occur infrequently are considered as noise. This further discriminates against the minority class so as to achieve high overall prediction accuracy. For highly imbalance data, the classifiers constructed using these algorithms would simply predict negative all the time and achieve almost 100% accuracy! This is nonsensical for applications in functional genomic (and computer security) where the aims are to detect minority instances within a certain reasonable tolerance of false positive mistakes.
Various approaches <NO> have been proposed to tackle the challenge posed by the imbalance ratio problem. These approaches fall into two different categories, namely weighting or resampling based methods. Weighting methods either assign heavier weights to the minority training instances or penalties for misclassifications of minority instances <NO>. The other way is to preprocess training data to minimize discrepancy between the classes. Oversampling <NO> the minority class and undersampling <NO> the majority class are the data level approaches. Ling and Li <NO> combining oversampling and undersampling methods but did not achieve significant improvement in the ""lift index"" metric that they used. Both methods effectively change the training distribution to one that no longer resemble the original (highly imbalance) distribution, resulting in overfitting. Other important related works similar to resampling approaches are to focus on solving small disjuncts problem within each class. Japkowicz <NO> discussed about the cause for lower performance in standard classifiers is actually small disjuncts of within-class. These works agree with what we observed from our experiments."	https://doi.org/10.1109/ICHIS.2005.23
284	Cost-sensitive support vector machine based on weighted attribute	[]	Dai Yuanhong, Chen Hongchang, and Peng Tao. 2009. Cost-sensitive support vector machine based on weighted attribute. International Forum on Information Technology and Applications, 2009. IFITA’09, Vol. 1. IEEE, 690–692.	In practice it is existed a matter in classified problem. The problem can be described as that the different sort has different wrong classified cost. In this paper we propose a Cost-sensitive SVM approach based on weighted attribute. The approach first calculates the weightiness of feature attributes corresponded to the classification attribute, then calculates the corresponding weightiness of attribute for all sample. In the end the samples are used for Cost-sensitive SVM training and testing. The experimental results show that the approach can improve the classification precision of the cost sensitive samples, and also the use of feature attribute increases the integer classified capability of the classifier. The approach has important realistic significance of unbalanced wrongclassification cost in classified problem.		https://doi.org/10.1109/IFITA.2009.125
285	Cost-sensitive learning by cost-proportionate example weighting	['Both of these classes have rich theoretical foundations that justify their approaches, with cost-sensitive dataspace weighting methods building on the translation theorem <NO>, and cost-sensitive Metatechniques building on the Metacost framework <NO>.']	Bianca Zadrozny, John Langford, and Naoki Abe. 2003. Cost-sensitive learning by cost-proportionate example weighting. Third IEEE International Conference on Data Mining, 2003. ICDM 2003. IEEE, 435–442.			https://doi.org/10.5555/951949.952181
286	Bayesian estimation and prediction using asymmetric loss functions	[]	Arnold Zellner. 1986. Bayesian estimation and prediction using asymmetric loss functions. J. Am. Statist. Assoc. 81, 394 (1986), 446–451.			http://scholar.google.com/scholar?hl=en&q=Arnold+Zellner.+1986.+Bayesian+estimation+and+prediction+using+asymmetric+loss+functions.+J.+Am.+Statist.+Assoc.+81%2C+394+%281986%29%2C+446%2D%2D451.
287	A novel improved SMOTE resampling algorithm based on fractal	[]	Dongmei Zhang, Wei Liu, Xiaosheng Gong, and Hui Jin. 2011. A novel improved SMOTE resampling algorithm based on fractal. J. Comput. Inform. Syst. 7, 6 (2011), 2204–2211.			http://scholar.google.com/scholar?hl=en&q=Dongmei+Zhang%2C+Wei+Liu%2C+Xiaosheng+Gong%2C+and+Hui+Jin.+2011.+A+novel+improved+SMOTE+resampling+algorithm+based+on+fractal.+J.+Comput.+Inform.+Syst.+7%2C+6+%282011%29%2C+2204%2D%2D2211.
288	RWO-sampling: A random walk over-sampling approach to imbalanced data classification	[]	Huaxiang Zhang, and Mingfang Li. 2014. RWO-sampling: A random walk over-sampling approach to imbalanced data classification. Inform. Fus. 20 (2014), 99–116.			http://scholar.google.com/scholar?hl=en&q=Huaxiang+Zhang+and+Mingfang+Li.+2014.+RWO-sampling%3A+A+random+walk+over-sampling+approach+to+imbalanced+data+classification.+Inform.+Fus.+20+%282014%29%2C+99%2D%2D116.
289	An extended tuning method for cost-sensitive regression and forecasting	[]	Huimin Zhao, Atish P. Sinha, and Gaurav Bansal. 2011. An extended tuning method for cost-sensitive regression and forecasting. Dec. Support Syst. 51, 3 (2011), 372–383.			https://doi.org/10.1016/j.dss.2011.01.003
290	Feature selection for text categorization on imbalanced data	['Zheng, Wu, and Srihari empirically tested different ratios of features indicating membership in a class versus features indicating lack of membership in a class <NO>.', 'Conversely, if features are chosen based on their absolute value, Zheng, Wu, and Srihari argue that we may not select a ratio of positive to negative features that gives the best results based on the imbalance in the data <NO>.', 'Zheng <NO> used the Naive Bayes classifier and logistic regression methods, and Forman <NO> used the linear SVM and noted its superiority over decision trees, Naive Bayes, and logistic regression.']	Zhaohui Zheng, Xiaoyun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced data. ACM SIGKDD Explor. Newslett. 6, 1 (2004), 80–89.	A number of feature selection metrics have been explored in text categorization, among which information gain (IG), chi-square (CHI), correlation coefficient (CC) and odds ratios (OR) are considered most effective. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and nonmembership (e.g. negative features) by ignoring the signs of features. The former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data. In this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework. Using multinomial näıve Bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.	"In this section, we present six feature selection metrics (four known measures and two proposed variants), which are functions of the following four dependency tuples:
1. (t, ci): presence of t and membership in ci.
2. (t, ci): presence of t and non-membership in ci.
3. (t, ci): absence of t and membership in ci.
4. (t, ci): absence of t and non-membership in ci.
where: t and ci represent a term and a category respectively. The frequencies of the four tuples in the collection are denoted by A, B, C and D respectively. The first and last tuples represent the positive dependency between t and ci, while the other two represent the negative dependency.
Information gain (IG) Information gain <NO> measures the number of bits of information obtained for category prediction by knowing the presence or absence of a term in a document. The information gain of term t and category ci is defined to be:
IG(t, ci) = ∑
c∈{ci,ci}
∑
t′∈{t,t}
P (t′, c) · log P (t ′, c)
P (t′) · P (c)
Information gain is also known as Expected Mutual Information. The Expected Likelihood Estimation (ELE) smoothing technique was used in this paper to handle singularities when estimating those probabilities.
Chi-square (CHI) Chi-square measures the lack of independence between a term t and a category ci and can be compared to the chi-square distribution with one degree of freedom to judge extremeness <NO>. It is defined as:
χ2(t, ci) = N <NO>2
P (t)P (t)P (ci)P (ci)
where: N is the total number of documents.
Correlation coefficient (CC) Correlation coefficient of a word t with a category ci was defined by Ng et al. as <NO>
CC(t, ci) =
√ N <NO>
√
P (t)P (t)P (ci)P (ci)
It is a variant of the CHI metric, where CC2 = χ2. CC can be viewed as a “one-sided” chi-square metric.
Odds ratio (OR) Odds ratio measures the odds of the word occurring in the positive class normalized by that of the negative class. The basic idea is that the distribution of features on the relevant documents is different from the distribution of features on the nonrelevant documents. It has been used by Mladenić for selecting terms in text categorization <NO>. It is defined as follows:
OR(t, ci) = log P (t|ci)<NO> <NO>P (t|ci)
Similar to IG, ELE smoothing was used when estimating those conditional probabilities.
According to the definitions, OR considers the first two dependency tuples, and IG, CHI, and CC consider all the four tuples. CC and OR are one-sided metrics, whose positive and negative values correspond to the positive and negative features respectively. On the other hand, IG and CHI are two-sided, whose values are non-negative. We can easily obtain that the sign for a one-sided metric, e.g. CC or OR, is sign(AD −BC).
A one-sided metric could be converted to its two-sided counterpart by ignoring the sign, while a two-sided metric could be converted to its one-sided counterpart by recovering the sign, e.g. CHI vs. CC.
We propose the two-sided counterpart of OR, namely ORsquare, and the one-sided counterpart of IG, namely signed IG as follows.
OR-square (ORS) and Signed IG (SIG)
ORS(t, ci) = OR 2(t, ci),
SIG(t, ci) = sign(AD −BC) · IG(t, ci)
The overall feature selection procedure is to score each potential feature according to a particular feature selection metric, and then take the best features. Feature selection using one-sided metrics like SIG, CC, and OR pick out the terms most indicative of membership only. The basic idea behind this is the features coming from non-relevant documents are useless. They will never consider negative features unless all the positive features have already been selected. Feature selection using two-sided metrics like IG, CHI, and ORS, however, do not differentiate between the positive and negative features. They implicitly combine the two."	https://doi.org/10.1145/1007730.1007741
291	Training cost-sensitive neural networks with methods addressing the class imbalance problem	['Other examples of cost-sensitive learning include the MetaCost framework <NO>, cost-sensitive neural network <NO>, costsensitive support vector machines (SVMs) <NO>, and others.', 'Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'domains <NO>, <NO>, <NO>, cost-sensitive learning is superior to', 'However, we note that a more significant performance increase can be achieved by applying this estimate to ensemble methods by using cross-validation techniques on a given set; a similar approach is considered in <NO>, however using a slightly different estimate.', 'cost-sensitive neural networks <NO>, <NO>, the ensemble', 'For instance, in cost-sensitive learning, it is natural to use misclassification costs for performance evaluation for multiclass imbalanced problems <NO>, <NO>, <NO>.', 'Algorithmic techniques have been developed for the different classification algorithms, such as neural networks <NO>, decision trees <NO>, fuzzy systems <NO>, <NO> etc.']	Zhi-Hua Zhou, and Xu-Ying Liu. 2006. Training cost-sensitive neural networks with methods addressing the class imbalance problem. IEEE Trans. Knowl. Data Eng. 18, 1 (2006), 63–77.	This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and softensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that costsensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training costsensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.	"Suppose there areC classes and the ith class hasNi number of training examples. LetCost½i; c ði; c 2 f1::CgÞdenote the cost of misclassifying an example of the ith class to the cth class (Cost½i; i ¼ 0) and Cost½i ði 2 f1::CgÞ denote the cost of the ith class. Moreover, suppose the classes are ordered such that, for the ith class and the jth class, if i < j, then ðCost½i < Cost½j Þor ðCost½i ¼ Cost½j and Ni NjÞ.Cost½i is usually derived from Cost½i; c . There are many possible rules for the derivation, among which a popular one is Cost½i ¼PC
c¼1 Cost½i; c <NO>, <NO>."	https://doi.org/10.1109/TKDE.2006.17
292	Active learning for word sense disambiguation with methods for addressing the class imbalance problem	[', in text classification and natural language problems <NO> and medical applications <NO>.', 'In <NO>, the stopping condition for active learning applications in word sense disambiguation (WSD) domains was investigated.', 'For instance, Zhu and Hovy <NO> analyzed the effect of undersampling and oversampling techniques with active learning for the word sense disambiguation (WSD) imbalanced learning problem.', 'Additionally, two stopping mechanisms based on maximum confidence and minimal error were investigated in <NO>.']	Jingbo Zhu, and Eduard H. Hovy. 2007. Active learning for word sense disambiguation with methods for addressing the class imbalance problem. EMNLP-CoNLL, Vol. 7. 783–790.			http://scholar.google.com/scholar?hl=en&q=Jingbo+Zhu+and+Eduard+H.+Hovy.+2007.+Active+learning+for+word+sense+disambiguation+with+methods+for+addressing+the+class+imbalance+problem.+In+EMNLP-CoNLL%2C+Vol.+7.+783%2D%2D790.
293	Parameter estimation of one-class SVM on imbalance text classification	['Representative works in this area include the one-class SVMs <NO>, <NO>, <NO>, <NO>, <NO>, <NO> and the autoassociator (or autoencoder) method <NO>, <NO>, <NO>, <NO>.']	Ling Zhuang, and Honghua Dai. 2006. Parameter estimation of one-class SVM on imbalance text classification. Advances in Artificial Intelligence. Springer, 538–549.			https://doi.org/10.1007/11766247_46
294	Parameter optimization of kernel-based one-class classifier on imbalance learning	[]	Ling Zhuang, and Honghua Dai. 2006. Parameter optimization of kernel-based one-class classifier on imbalance learning. J. Comput. 1, 7 (2006), 32–40.			http://scholar.google.com/scholar?hl=en&q=Ling+Zhuang+and+Honghua+Dai.+2006b.+Parameter+optimization+of+kernel-based+one-class+classifier+on+imbalance+learning.+J.+Comput.+1%2C+7+%282006%29%2C+32%2D%2D40.
