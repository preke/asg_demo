ref_title	ref_context	ref_entry	abstract	intro	ref_link	label	topic_bigram	topic_word	topic_trigram	description
Cache index-aware memory allocation	[]	Y. Afek, D. Dice, and A. Morrison. 2011. Cache index-aware memory allocation. Proc. of the ISMM. ACM, 55–64.	Poor placement of data blocks in memory may negatively impact application performance because of an increase in the cache conflict miss rate [18]. For dynamically allocated structures this placement is typically determined by the memory allocator. Cache index-oblivious allocators may inadvertently place blocks on a restricted fraction of the available cache indexes, artificially and needlessly increasing the conflict miss rate. While some allocators are less vulnerable to this phenomena, no general-purpose malloc allocator is index-aware and methodologically addresses this concern. We demonstrate that many existing state-of-the-art allocators are index-oblivious, admitting performance pathologies for certain block sizes. We show that a simple adjustment within the allocator to control the spacing of blocks can provide better index coverage, which in turn reduces the superfluous conflict miss rate in various applications, improving performance with no observed negative consequences. The result is an index-aware allocator. Our technique is general and can easily be applied to most memory allocators and to various processor architectures. Furthermore, we can reduce inter-thread and inter-process conflict misses for processors where threads concurrently share the level-1 cache such as the Sun UltraSPARC-T2TMand Intel “Nehalem” by coloring the placement of blocks so that allocations for different threads and processes start on different cache indexes.	"The default SolarisTMlibc allocator uses a single global heap protected by one mutex. Memory is allocated from the operating system by means of the sbrk system call. The global free list is organized as a splay tree <NO> ordered by size and allocation requests are serviced via a best-fit policy. The heap is augmented by a small set of segregated free lists of bounded capacity, allowing many common requests to operate in constant-time. This results in an allocator with excellent heap density, reasonable single-threaded latency, but poor scalability. Furthermore, applications using the libc allocator may be subject to excessive allocator-induced false sharing, where blocks allocated to different threads happen to abut in the midst of a cache line.
Modern state-of-the-art allocators include Hoard <NO>, CLFMalloc <NO>, LFMalloc <NO>, libumem <NO>, jemalloc <NO> and tcmalloc <NO>. They are broadly categorized as segregated free-list <NO> allocators as they maintain distinct free lists based on block size. Such allocators round requested allocation sizes up to the nearest size-class where a size-class is simply an interval of block sizes and without ambiguity we can refer to a size-class by its upper bound. The set of size-classes forms a partition on the set of possible allocation sizes. The choice of size-classes is largely arbitrary and defined at the whim of the implementor, although a step size of 1.2x between adjacent size-classes is common <NO> as the worst-case internal fragmentation is constrained to 20%.
We will use Hoard as a representative example of modern allocator design. Hoard uses multiple heaps to reduce contention. Specifically, Hoard attempts to diffuse contention and improve scalability by satisfying potentially concurrent malloc requests from multiple local heaps – this strategy also mitigates the allocatedinduced false sharing problem. Each heap consists of an array of references to superblocks, with one slot for each possible size-class. A superblock is simply an array of blocks of a certain size class. Superblocks are all the same size, a multiple of the system page size, and are allocated from the system via the mmap interface which allocates virtual address pages and associates physical pages to those addresses. Mmap is used instead of the more traditional sbrk operator as pages allocated through mmap may later be returned to the system, if desired, through munmap. The superblock is the fundamental unit of allocator for Hoard. Each superblock has a local singly-linked free list threaded through the free blocks and maintained in LIFO order to promote TLB and data cache locality. A small superblock header at the base of the array contains the head of the superblock-local free list. Superblocks and heaps are opaque to the application that uses the allocator. The Hoard implementation
places superblocks on highly aligned addresses. The free operator then uses address arithmetic – simple masking – on the block address to locate the header of the enclosing superblock, which in turn allows the operator to quickly push the block onto the superblock’s free list. As such, in-use blocks do not require a header field. If a superblock becomes depleted it can be detached from a heap and moved to a global heap. The local heap can be reprovisioned from either the global heap, assuming a superblock with sufficient free space is available, or by allocating a new superblock from the system. Superblocks can circulate between various local heaps and the global heap, but will be associated with at most one local heap at any one time. Allocator metadata is minimal, consisting of the heap structures and superblock headers. The implementation associates a malloc request with a heap by hashing the identity of the current thread. To reduce collisions Hoard overprovisions the number of heaps to be twice the number of processors. Concurrency control is provided by per-heap locks. Hoard’s malloc operator first quantizes the requested size to an appropriate size-class, identifies a heap, locks the heap, locates a superblock of the appropriate size-class in that heap, unlinks a block from that superblock’s free list, unlocks the heap, and finally returns the address of the block’s data area. As Hoard employs segregated free lists (segregated by size), in the common case finding a free block of a given size is a simple constant-time operation. Given this allocation policy the returned addresses for a given size-class may be regular in a manner that results in interblock cache index conflicts and excessive conflict misses if a group of blocks of a size-class are accessed frequently by the application. More generally, array-based superblock allocators coupled with inopportune index-oblivious block sizes can easily result in patterns of block addresses that map to only a few cache indices.
Superblock-based allocators of this design allow for good scaling although their footprint is often somewhat larger than that of libc as they attempt to diffuse contention by distributing requests over multiple heaps. Latency varies but usually reflects path length through malloc and free and metadata access costs, which are properties of the implementation and not fundamental to the category of segregated free list allocators. CLFMalloc is structurally similar to Hoard, differing mostly in the policy by which it associates malloc requests with heap instances and in that CLFMalloc is lock-free. Libumem and tcmalloc use a central heap but diffuse contention via multiple local free lists. In the case of tcmalloc the central heap uses segregated free lists which are populated by allocating runs of pages and then splitting those pages into contiguous arrays of the desired size-class."	https://doi.org/10.1145/1993478.1993486	3	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking', 'locked', 'core']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Afek, et al. [NO] demonstrates that many existing state-of-the-art allocators are index-oblivious, admitting performance pathologies for certain block sizes. 
Performance evaluation of cache replacement policies for the SPEC CPU2000 benchmark suite	[]	Hussein Al-Zoubi, Aleksandar Milenkovic, and Milena Milenkovic. 2004. Performance evaluation of cache replacement policies for the SPEC CPU2000 benchmark suite. Proc. of the 42nd ACM-SE. ACM, New York, NY, 267–272.	Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set. In our quest for replacement policy as close to optimal as possible, we thoroughly explored the design space of existing replacement mechanisms using SimpleScalar toolset and SPEC CPU2000 benchmark suite, across wide range of cache sizes and organizations. In order to better understand the behavior of different policies, we introduced new measures, such as cumulative distribution of cache hits in the LRU stack. We also dynamically monitored the number of cache misses, per each 100000 instructions. Our results show that the PLRU techniques can approximate and even outperform LRU with much lower complexity, for a wide range of cache organizations. However, a relatively large gap between LRU and optimal replacement policy, of up to 50%, indicates that new research aimed to close the gap is necessary. The cumulative distribution of cache hits in the LRU stack indicates a very good potential for way prediction using LRU information, since the percentage of hits to the bottom of the LRU stack is relatively high.	"The LRU mechanism uses a program’s memory access patterns to guess that the cache line which has been accessed most recently will, most likely, be accessed again in the near future, and the cache line that has been “least recently used” should be replaced by the cache controller. An example of how the LRU stack is maintained is shown in Figure 1. Although the LRU replacement heuristic is relatively efficient, it does require a number of bits to track when each block is accessed, and relatively a complex logic. Another problem with the LRU heuristic is that each time the cache hit or miss occurs the block comparison and LRU stack shift operations require time and power.
To reduce the cost and complexity of the LRU heuristic, Random policy can be used, but potentially at the expense of performance. Random replacement policy chooses its victim randomly from all the cache lines in the set. An obvious way to implement it is with a simple Linear Feedback Shift Register (LFSR). Round Robin (or FIFO) replacement heuristic simply replaces the cache lines in a sequential order, replacing the oldest block in the set. Each cache memory set is accompanied with a circular counter which points to the next cache block to be replaced; the counter is updated on every cache miss.
PLRU schemes employ approximations of the LRU mechanism to speed up operations and reduce the complexity of implementation <NO>, <NO>. Due to the approximations, the least recently accessed cache memory is not always the location to be replaced. Here, we will discuss two implementations, a treebased and a MRU-based. In the tree-based PLRU replacement heuristic nway-1 bits are used to track the accesses to the cache blocks, where nway represents the number of cache blocks (ways) in a set. Figure 2 illustrates tree-based PLRU (PLRUt) using a 4-way cache memory as an example. The track bits B0, B1, B2 form a decision binary tree. The track bit B1 indicates whether two lower cache blocks CL0 and CL1 (B1=1), or 2 higher cache blocks CL2 and CL3 (B1 = 0) have been recently used. The track bit B0 determines further which one of two blocks CL0 (B0=1) or CL1 (B0=0) has been recently used; bit B2 keeps the access track between cache lines CL2 and CL3. On a cache miss, bit B1 determines where to look for the least recently block (2 lower cache lines or 2 higher cache lines). Bit
B0 or B2 determines the least recently used block. On a cache hit, the tree bits are set according to this policy.
The other implementation of the PLRU heuristic is based on using the most recently used (MRU) bits (PLRUm). In this case each cache block is assigned an MRU bit, stored in the tag table. The MRU bit for each cache block is set to a “1” each time a cache hit occurs on the cache block, indicating that the cache block has recently been used. When the cache controller is forced to replace a cache block, it examines the MRU bit for each cache block looking for a “0”. When it finds a “0”, the cache controller replaces that cache block and then sets the MRU bit to a “1”. A problem could occur if the MRU bits for all cache memory blocks are set to a “1”. If this happens, all the blocks are unavailable for replacement causing a deadlock. To prevent this type of deadlock, all the MRU bits in the set are cleared except the MRU bit being accessed when a potential overflow situation is detected. An example in Figure 3 illustrates the MRU-based PLRU.
Table 1 gives storage requirements and corresponding actions taken on a cache hit and a cache miss for all replacement
policies discussed. Random policy guarantees the minimal hardware cost, while the LRU hardware cost increases dramatically for caches with associativity larger than 8. In 2-way cache organizations PLRUt policy requires only one track bit, and shows the same performance as other LRU-based polices, hence it is an obvious choice for 2-way caches. For caches with higher associativity, PLRUt and PLRUm have roughly the same complexity. If we take the number of transitions in track bits as a qualitative measure of power consumption and assume the same number of misses, it is clear that PLRUm shows slightly better characteristics than PLRUt, and both are much better than LRU."	https://doi.org/10.1145/986537.986601	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Al-Zoubi, et al. [NO] thoroughly explores the design space of existing replacement mechanisms using simplescalar toolset and spec cpu2000 benchmark suite, across wide range of cache sizes and organizations. 
Efficient shared cache management through sharing-aware replacement and streaming-aware insertion policy	[]	Yu Chen, Wenlong Li, Changkyu Kim, and Zhizhong Tang. 2009. Efficient shared cache management through sharing-aware replacement and streaming-aware insertion policy. Proc. of the 24th IEEE IPDPS’09. 1–11.	Multi-core processors with shared caches are now commonplace. However, prior works on shared cache management primarily focused on multi-programmed workloads. These schemes consider how to partition the cache space given that simultaneously-running applications may have different cache behaviors. In this paper, we examine policies for managing shared caches for running single multi-threaded applications. First, we show that the shared-cache miss rate can be significantly reduced by reserving a certain amount of space for shared data. Therefore, we modify the replacement policy to dynamically partition each set between shared and private data. Second, we modify the insertion policy to prevent streaming data (data not reused before eviction) from promoting to the MRU position. Finally, we use a low-overhead sampling mechanism to dynamically select the optimal policy. Compared to LRU policy, our scheme reduces the miss rate on average by 8.7% on 8MB caches and 20.1% on 16MB caches respectively.	In this section, we show that the shared cache blocks in multi-threaded applications can affect the cache behavior significantly, thus rendering the new opportunity to improve cache performance over prior shareoblivious cache management policies.	https://doi.org/10.1109/IPDPS.2009.5161016	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Chen, et al. [NO] examines policies for managing shared caches for running single multi-threaded applications. 
Fully associative cache partitioning with don’t care bits for real-time applications	[]	Ali Chousein, and Rabi N. Mahapatra. 2005. Fully associative cache partitioning with don’t care bits for real-time applications. SIGBED Review 2, 2 (April 2005), 35–38.	The usage of cache memories in time-critical applications has been limited as caches introduce unpredictable execution behavior. Cache partitioning techniques have been developed to reduce the impact of unpredictability owing to context switch effects. However, partitioning reduces the cache size available for each task resulting in capacity related cache misses. This paper introduces a fully associative cache architecture for multi-tasking applications where effective partition sizes are increased by tag compression in the cache. The proposed scheme uses a few don’t care cells in its least significant bits of the tag to aggregate multiple tag entries into a single entry. The experimental results indicate that the proposed scheme is context switch resilient when eight different real-time benchmarks use the cache concurrently. Further, this cache architecture requires less time and less energy to perform tag table search compared to contemporary fully associative caches of the same size.	"The proposed fully associative cache architecture employs TCAM cells in the last significant L bits of the tag entries to compact tag table. Each TCAM cell can store don’t care state (x) in addition to the regular 0 and 1 binary states. This don’t care state is used as a wild bit to aggregate multiple entries in the tag table to single entry to achieve tag table compaction. This scheme is different from traditional tag compression schemes <NO>. As an example, the compaction can be effective up to eight times in reducing the tag table size with three don’t care bits in the tag when program locality behaves favorably. Due to tag table compaction, it is possible to build large sized caches with fewer number of tag entries. This in turn provides larger working cache area for each task when partitioning takes place and improves the miss ratio performance.
The overhead to maintain the above compaction comes with a price of additional hardware but no additional time overhead. The aggregation process that is responsible for tag compaction is neither on the critical path of cache access nor needs extensive hardware. The details of the aggregation module are not discussed in this paper. The decoding of the compacted tag entry is done concurrently with the tag table search and does not affect the critical path of the cache access. The decoder is implemented using offthe-shelf de-multiplexers."	https://doi.org/10.1145/1121788.1121799	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Chousein, et al. [NO] introduces a fully associative cache architecture for multi-tasking applications where effective partition sizes are increased by tag compression in the cache. 
A survey of hard real-time scheduling for multiprocessor systems	[]	Robert I. Davis, and Alan Burns. 2011. A survey of hard real-time scheduling for multiprocessor systems. Computing Surveys 43, 4, Article 35 (Oct. 2011), 44 pages.	This survey covers hard real-time scheduling algorithms and schedulability analysis techniques for homogeneous multiprocessor systems. It reviews the key results in this field from its origins in the late 1960s to the latest research published in late 2009. The survey outlines fundamental results about multiprocessor real-time scheduling that hold independent of the scheduling algorithms employed. It provides a taxonomy of the different scheduling methods, and considers the various performance metrics that can be used for comparison purposes. A detailed review is provided covering partitioned, global, and hybrid scheduling algorithms, approaches to resource sharing, and the latest results from empirical investigations. The survey identifies open issues, key research challenges, and likely productive research directions.	"Systems are referred to as real-time when their correct behavior depends not only on the operations they perform being logically correct, but also on the time at which they are performed. For example in avionics, flight control software must execute within a fixed time interval in order to accurately control the aircraft. In automotive electronics there are tight time constraints on engine management and transmission control systems that derive from the mechanical systems that they control.
Guaranteeing real-time performance while making the most effective use of the available processing capacity requires the use of efficient scheduling policies or algorithms supported by accurate schedulability analysis techniques. These analysis techniques need to be capable of analyzing the worst-case behavior of the application under a given scheduling policy, thus providing proof, subject to a set of assumptions about application behavior, that timing constraints will always be met during operation of the system.
Research into uniprocessor real-time scheduling can trace its origins back to the late 1960s and early 1970s with significant research effort and advances made in the 1980s and 1990s. The interested reader is referred to Audsley et al. <NO> and Sha et al. <NO>, which provide historical accounts of the most important advances in the field of uniprocessor scheduling during those decades. Today, although there is still significant scope for further research, uniprocessor real-time scheduling theory can be viewed as reasonably mature, with a large number of key results documented in textbooks such as those by Burns and Wellings <NO>, Buttazzo <NO>, and Liu <NO>, and successfully transferred into industrial practice.
Multiprocessor real-time scheduling theory also has it origins in the late 1960s and early 1970s. Liu <NO> noted that multiprocessor real-time scheduling is intrinsically a much more difficult problem than uniprocessor scheduling:
“Few of the results obtained for a single processor generalize directly to the multiple processor case; bringing in additional processors adds a new dimension to the scheduling problem. The simple fact that a task can use only one processor even when several processors are free at the same time adds a surprising amount of difficulty to the scheduling of multiple processors.”
The seminal paper of Dhall and Liu <NO> heavily influenced the course of research in this area for two decades. During the 1980s and 1990s, conventional wisdom was that global approaches to multiprocessor scheduling, where tasks may migrate from one processor to another, suffered from the so-called “Dhall effect,” and were therefore inferior to partitioned approaches, with a fixed allocation of tasks to processors. Research efforts therefore focused almost exclusively on partitioned approaches. It was not until Phillips et al. <NO> showed that the Dhall effect was more of a problem with high-utilization tasks than it was with global scheduling algorithms that there was renewed interest in global scheduling algorithms.
In the late 1990s silicon vendors such as IBM and AMD began research into the development of multicore processors, with IBM releasing the first nonembedded dual-core processor, the POWER4, in 2001. This trend away from increasing processing capacity via ever higher clock speeds toward increasing performance via multiple processor cores became evident to the real-time systems research community. This resulted in significant research effort being focused on the problem of real-time multiprocessor scheduling. While markedly more articles have been published in this area since 2000 than before, and significant progress has been made, there are still many open questions and research challenges that remain.
This article presents a survey of multiprocessor real-time scheduling algorithms and schedulability analysis techniques, from the origins of the field in the late 1960s up to the latest research published at the end of 2009. The aim of the survey is to provide a classification of existing research, both providing a perspective on the area and identifying significant open issues and future research directions."	https://doi.org/10.1145/1978802.1978814	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	I. et al. [NO] covers hard real-time scheduling algorithms and schedulability analysis techniques for homogeneous multiprocessor systems. 
Analysis of probabilistic cache related pre-emption delays	['For reasons of space and clarity, here we only present results for the first two experiments for the simple FAC benchmark, results for the other experiments and for the BS, FDCT, FIBCALL, FIR, JFDCTINT, and INSERTSORT benchmarks can be found in the technical report <NO>.', 'Detailed results can be found in the technical report <NO>.', 'The appendix of the technical report <NO> contains the results of the four experiments for the BS, FDCT, FIBCALL, FIR, JFDCTINT, and INSERTSORT benchmarks.', 'Meanwhile, Probabilistic Timing Analysis (PTA) techniques <NO> rely on caches with a time-randomised behaviour in which hit and miss events have an associated probability for every cache access.', 'Probabilistic Timing Analysis (PTA) <NO> provides WCET estimates with an associated exceedance probability, called probabilistic WCET (pWCET) estimates.']	R.I. Davis, L. Santinelli, S. Altmeyer, C. Maiza, and L. Cucu-Grosjean. 2013. Analysis of probabilistic cache related pre-emption delays. Proc. of the 25th ECRTS. IEEE, 168–179.	This paper integrates analysis of probabilistic cache related pre-emption delays (pCRPD) and static probabilistic timing analysis (SPTA) for multipath programs running on a hardware platform that uses an evict-on-miss random cache replacement policy. The SPTA computes an upper bound on the probabilistic worst-case execution time (pWCET) of the program, which is an exceedance function giving the probability that the execution time of the program will exceed any given value on any particular run. The pCRPD analysis determines the maximum effect of a pre-emption on the pWCET. The integration between SPTA and pCRPD updates the pWCET to account for the effects of one or more pre-emptions at any arbitrary points in the program. This integration is a necessary step enabling effective schedulability analysis for probabilistic hard real-time systems that use pre-emptive or co-operative scheduling. The analysis is illustrated via a number of benchmark programs.	"Temporal analysis of probabilistic real-time systems where at least one parameter, e.g. execution time, is described by a random variable, was first investigated by Lehoczky in 1990 <NO> who extended queuing theory under real-time hypotheses. This work was improved upon in 2002 by Zhu et al. <NO>; however, the main limitation remained the use of the same probability law for the execution times of all tasks, which is not always realistic. Gardner et al. in 1999 <NO> and Tia et al. in 1995 <NO> also considered execution times as random variables with special assumptions made about the critical instant. Schedulability analysis for real-time systems with probabilistic execution times was given by Diaz et al. in 2002 <NO> and refined by Lopez et al. in 2008 <NO>; however, the analysis was difficult to use in practice for computational reasons. Improvements based on re-sampling of random variables were proposed by Maxim et al. in 2012 <NO>.
In 2009, Quinones et al. <NO> investigated the use of random cache replacement policies as a means of obtaining real-time performance less dependent on execution history. In 2012, Cucu-Grosjean et al. <NO> and Cazorla et al. <NO> introduced SPTA for single-path programs, assuming an evict-on-access random cache replacement policy.
For deterministic systems, the integration of cache related pre-emption delays into schedulability analysis for fixed priority pre-emptive scheduling has been considered by (i) analysing the effect of the pre-empting task (Busquets-Mataix et al. in 1996 <NO>), (ii) analysing the effect on the pre-empted
1In this paper, we use ’program’ and ’task’ interchangeably.
1068-3070/13 $26.00 © 2013 IEEE DOI 10.1109/ECRTS.2013.27
168
task (Lee et al. in 1998 <NO>), or (iii) a combination of both (Altmeyer et al. in 2011 <NO> and 2012 <NO>).
In this paper, we build on the idea of using random cache replacement policies in hard real-time systems proposed in <NO>, and the SPTA for the evict-on-access random cache replacement policy introduced in <NO> and <NO>; however, we assume an evict-on-miss policy because its performance dominates that of evict-on-access in terms of the pWCET distributions (exceedance functions) obtained. We extend previous work on SPTA for single path programs given in <NO> and <NO>, both integrating analysis of pCRPD, and introducing for the first time a method of analysing multipath programs.
Section II presents our system model, terminology and notation. In Section III we provide SPTA for single-path programs assuming an evict-on-miss random cache replacement policy. In Section IV we derive analysis of pCRPD, based on the effect on the pre-empted program. In section V we extend our SPTA and pCRPD analysis to multi-path programs. Section VI applies our analysis to a number of benchmarks, while Section VII concludes with a summary and discussion of future work."	https://doi.org/10.1109/ECRTS.2013.27	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Davis, et al. [NO] integrates analysis of probabilistic cache related pre-emption delays (pcrpd) and static probabilistic timing analysis (spta) for multipath programs running on a hardware platform that uses an evict-on-miss random cache replacement policy. 
WCET-directed dynamic scratchpad memory allocation of data	['Scratchpads <NO> can be used to store data as well as instructions.', 'This subset may be static <NO> or dynamically updated during execution <NO>, but the forms of data and programs that can be supported are restricted.', 'scratchpad allocation algorithms only support statically allocated or stack data <NO>, while non time-predictable algorithms can only support dynamic data if whole-program pointer analysis can identify every memory operation that could access each variable <NO>.', 'Thirdly, whole-program pointer analysis is not required: previous work required such analysis <NO> or eliminated pointers entirely <NO>.', '<NO> or Deverge and Puaut <NO> Latency/ CPU Bus System CPU clock freq/ freq/ cycles MHz MHz', 'While straightforward, this approach will not be taken by time-predictable scratchpad allocation algorithms as proposed by Suhendra <NO> or Deverge <NO> because the objects are dynamically allocated.', 'Sometimes, this can be determined by pointer analysis at compile time, and this is part of the purpose of the restrictions applied by <NO>.', 'Dynamic allocation algorithms <NO> make better use of scratchpad space because the set of objects stored in scratchpad can change during execution, matching the requirements of the current function.', 'The restrictions applied by <NO> make it easy to determine the size of each object used by the program offline, because there is a 1-1 relationship between the variables in the source code and the objects they represent.', 'Scratchpad allocation algorithms demonstrate that a subset of the data used by a program can be allocated to the scratchpad in order to minimize the estimated WCET <NO>.', 'ti can be specified by the programmer, but generating this value is really a task for a scratchpad allocation algorithm: existing algorithms <NO> already specify methods for doing this.', 'For example, the algorithm specified by Deverge <NO> may be extended by:', 'This idea has been successfully applied within tasks for SPM <NO>, <NO>.', '<NO>, <NO>) could have done some of the required work.']	J.-F. Deverge, and I. Puaut. 2007. WCET-directed dynamic scratchpad memory allocation of data. Proc. of the 19th Euromicro Conference on Real-Time Systems (ECRTS’07). 179–190.	Many embedded systems feature processors coupled with a small and fast scratchpad memory. To the difference with caches, allocation of data to scratchpad memory must be handled by software. The major gain is to enhance the predictability of memory accesses latencies. A compile-time dynamic allocation approach enables eviction and placement of data to the scratchpad memory at runtime. Previous dynamic scratchpad memory allocation approaches aimed to reduce average-case program execution time or the energy consumption due to memory accesses. For real-time systems, worst-case execution time is the main metric to optimize. In this paper, we propose a WCET-directed algorithm to dynamically allocate static data and stack data of a program to scratchpad memory. The granularity of placement of memory transfers (e.g. on function, basic block boundaries) is discussed from the perspective of its computation complexity and the quality of allocation.	"On many programs, a large amount of data accesses are dynamic; the target address of load-store instructions may change for each execution. Table 1 gives a twodimensional classification of data storage and load-store accesses from <NO>. The storage type defines the location of a given data. Static data, stack data and heap data are respectively stored in global, heap and stack sections of the program memory space layout. Literals are compiler-generated constants stored in the code section; these data are used to reduce the size of the program code.
19th Euromicro Conference on Real-Time Systems (ECRTS'07) 0-7695-2914-3/07 $20.00 © 2007
The access type defines the way a data is accessed. Scalar access types are accesses to a unique data address for statics and to a relative address to stack frame base addresses for stack data. The access type of a load-store instruction is regular if this instruction is accessing to multiple elements of a unique array with a constant stride (classified as linear address sequence accesses in <NO>). Irregular accesses include accesses to (possibly multiple) data through pointers and are still independent to the input data. Lastly, input dependent accesses include any accesses with addresses computed at runtime from unknown input data (mentioned as indirect address sequence accesses in <NO>).
In the next section, we will motivate the need for a method to analyze any data memory accesses of programs."	https://doi.org/10.1109/ECRTS.2007.37	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Deverge, et al. [NO] proposes a wcet-directed algorithm to dynamically allocate static data and stack data of a program to scratchpad memory. 
WCET-Centric partial instruction cache locking	[]	Huping Ding, Yun Liang, and T. Mitra. 2012. WCET-Centric partial instruction cache locking. Proc. of the 49th ACM/IEEE DAC. 412–420.	Caches play an important role in embedded systems by bridging the performance gap between high speed processors and slow memory. At the same time, caches introduce imprecision in Worst-case Execution Time (WCET) estimation due to unpredictable access latencies. Modern embedded processors often include cache locking mechanism for better timing predictability. As the cache contents are statically known, memory access latencies are predictable, leading to precise WCET estimate. Moreover, by carefully selecting the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling without locking. Existing static instruction cache locking techniques strive to lock the entire cache to minimize the WCET. We observe that such aggressive locking mechanisms may have negative impact on the overall WCET as some memory blocks with predictable access behavior get excluded from the cache. We introduce a partial cache locking mechanism that has the flexibility to lock only a fraction of the cache. We judiciously select the memory blocks for locking through accurate cache modeling that determines the impact of the decision on the program WCET. Our synergistic cache modeling and locking mechanism achieves substantial reduction in WCET for a large number of embedded benchmark applications.	"Cache locking is used to improve timing predictability in realtime systems. Puaut and Decotigny <NO> explore static cache locking in multitasking real-time systems. Two content selection algorithms have been proposed in their work to minimize the utilization and inter-task interferences. Campoy et al. <NO> employ genetic al-
gorithm to perform instruction cache locking. However, both <NO> and <NO> do not model the change in worst-case path after locking.
Falk et al. <NO> perform cache locking by taking into account the change of worst-case path and achieve better WCET reduction. Their greedy algorithm computes the worst-case path and selects the procedure with maximum WCET reduction for locking. This process continues until the cache is fully locked. Liu et al. <NO> present an optimal solution to minimize WCET via cache locking. However, their approach is optimal on the premise that the cache is fully locked. It may not be optimal towards minimizing WCET as shown in our motivating example. More importantly, they do not consider the cache mapping function at all in the locking algorithm. They simply assume that any memory block can be locked in any cache set (as if the cache is a scratchpad memory). After locking decisions are taken, they have to use code placement/layout technique <NO> that force the locked memory blocks to be mapped to the appropriate cache sets. This can lead to serious code size blowup, which has not been addressed.
Vera et al. <NO> combine compile-time cache analysis and data cache locking in order to estimate a safe and tight worst-case memory performance. This work also assume full cache locking. Arnaud and Puaut <NO> propose dynamic instruction cache locking for hard real-time systems. In their approach, the program is partitioned into regions, and static cache locking is performed for each region. In <NO>, cache locking is explored for predictable shared caches on multi-core systems. Cache locking is also shown to be quite effective for improving average-case execution time <NO>. Finally, optimal on-chip scratchpad memory allocation to improve the WCET has been explored in <NO>."	https://doi.org/10.1145/2228360.2228434	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Ding, et al. [NO] observes that such aggressive locking mechanisms may have negative impact on the overall wcet as some memory blocks with predictable access behavior get excluded from the cache. 
Coordinated control of multiple prefetchers in multi-core systems	[]	Eiman Ebrahimi, Onur Mutlu, Chang Joo Lee, and Yale N. Patt. 2009. Coordinated control of multiple prefetchers in multi-core systems. Proc. of the 42Nd MICRO. ACM, 316–326.	Aggressive prefetching is very beneficial for memory latency tolerance of many applications. However, it faces significant challenges in multi-core systems. Prefetchers of different cores on a chip multiprocessor (CMP) can cause significant interference with prefetch and demand accesses of other cores. Because existing prefetcher throttling techniques do not address this prefetcher-caused inter-core interference, aggressive prefetching in multi-core systems can lead to significant performance degradation and wasted bandwidth consumption. To make prefetching effective in CMPs, this paper proposes a low-cost mechanism to control prefetcher-caused inter-core interference by dynamically adjusting the aggressiveness of multiple cores’ prefetchers in a coordinated fashion. Our solution consists of a hierarchy of prefetcher aggressiveness control structures that combine per-core (local) and prefetcher-caused intercore (global) interference feedback to maximize the benefits of prefetching on each core while optimizing overall system performance. These structures improve system performance by 23% while reducing bus traffic by 17% compared to employing aggressive prefetching and improve system performance by 14% compared to a state-of-the-art prefetcher aggressiveness control technique on an eight-core system.	We briefly describe relevant previous research on prefetcher aggressiveness control, since our proposal builds on this prior work. We also describe the shortcomings of these prefetcher control mechanisms and provide insight into the potential benefits of reducing prefetcher-caused inter-core interference using coordinated control of multiple prefetchers.	https://doi.org/10.1145/1669112.1669154	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Ebrahimi, et al. [NO] proposes a low-cost mechanism to control prefetcher-caused inter-core interference by dynamically adjusting the aggressiveness of multiple cores ’ prefetchers in a coordinated fashion. 
Compile-time decided instruction cache locking using worst-case execution paths	['The algorithms in <NO> and <NO> are compared with the polynomial time optimal algorithms proposed in this paper.', 'The heuristic algorithm proposed in <NO> performs close to our optimal solutions.', 'The algorithms in <NO> and <NO> are compared with the optimal algorithms proposed in this paper.', 'EFT is similar to the EFG proposed by <NO>.', 'This EFG is different from EFG in <NO>, where there are values associated with edges.', '<NO> obtains the same results as the optimal solutions calculated by our algorithms.', 'That is because the heuristic algorithm <NO> considers the cost saving and function size at the same time and it can achieve optimal solutions for some applications.', '<NO> take the changing of worse-case execution path into', 'The algorithms under comparison are the static I-Cache locking algorithm in <NO>, Algorithm SLEFGS and Algorithm DLEFGS.', '4 and 5 that for some multi-task sets, the static algorithm in <NO> obtains close results as the', 'That is because the heuristic algorithm in <NO> considers the cost saving and node size at the same time and it can achieve approximate solutions for some applications.', 'More importantly, by carefully choosing the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling techniques without locking <NO>.', 'Recently, a heuristic <NO> and an optimal solution <NO> have been proposed to minimize the WCET via static instruction cache locking.', 'By employing full cache locking, <NO> can completely bypass cache modeling in WCET analysis phase and thereby achieve tight WCET estimation.', 'In this paper, we argue (and experimentally validate) that aggressive full cache locking as proposed in <NO> may have substantial negative impact on WCET reduction.', 'Full cache locking: Existing cache locking techniques <NO> first build the worst case path (e.', 'Both cache locking techniques <NO> model the fact that the WCET path may change after locking some memory locks.', 'For this example, the heuristic <NO> and the optimal <NO> approach return the same solution.', 'From the example above, we first observe that full locking techniques <NO> are not guaranteed to perform better than cache modeling (with no locking) specially when some memory accesses can be easily classified as cache hits (m3, m4, m5 in our example).', '<NO> perform cache locking by taking into account the change of worst-case path and achieve better WCET reduction.', '3 Partial versus Full Cache Locking There exist two full cache locking techniques as mentioned in Section 2 <NO>.', '<NO> show that their approach can achieve better WCET reduction compared to <NO>, it has several limitations.', '’s method <NO> as both approaches do not require any subsequent code placement technique.', 'This choice of granularity does not change the greedy heuristic algorithm proposed in <NO>.', 'Instruction cache locking has been primarily employed in hard real time systems for better timing predictability <NO>.']	Heiko Falk, Sascha Plazar, and Henrik Theiling. 2007. Compile-time decided instruction cache locking using worst-case execution paths. Proc. of the 5th IEEE/ACM CODES+ISSS. ACM, 143–148.	Caches are notorious for their unpredictability. It is difficult or even impossible to predict if a memory access results in a definite cache hit or miss. This unpredictability is highly undesired for real-time systems. The Worst-Case Execution Time (WCET) of a software running on an embedded processor is one of the most important metrics during real-time system design. The WCET depends to a large extent on the total amount of time spent for memory accesses. In the presence of caches, WCET analysis must always assume a memory access to be a cache miss if it can not be guaranteed that it is a hit. Hence, WCETs for cached systems are imprecise due to the overestimation caused by the caches. Modern caches can be controlled by software. The software can load parts of its code or of its data into the cache and lock the cache afterwards. Cache locking prevents the cache’s contents from being flushed by deactivating the replacement. A locked cache is highly predictable and leads to very precise WCET estimates, because the uncertainty caused by the replacement strategy is eliminated completely. This paper presents techniques exploring the lockdown of instruction caches at compile-time to minimize WCETs. In contrast to the current state of the art in the area of cache locking, our techniques explicitly take the worst-case execution path into account during each step of the optimization procedure. This way, we can make sure that always those parts of the code are locked in the I-cache that lead to the highest WCET reduction. The results demonstrate that WCET reductions from 54% up to 73% can be achieved with an acceptable amount of CPU seconds required for the optimization and WCET analyses themselves.	"The papers <NO> present different algorithms for static I-cache lockdown. These publications are very close to the work presented in this paper. In <NO>, the authors present two heuristics for cache contents selection. The first one tries to minimize CPU utilization by locking an I-cache. The second algorithm minimizes interference between tasks. In <NO>, an additional genetic algorithm for cache locking is proposed. All three algorithms have in common that they do not consider changing WC-paths at all. Instead, the WC-path is determined only once before the optimization process takes place. After that, optimization is done along this single WC-path. The authors admit that their approach is nonoptimal. Due to the consideration of only one WC-path, we call such techniques “single-path analyses” in the following.
In <NO>, the techniques of <NO> for I-cache locking are extended to deal with changing WC-paths. However, the way how WC-paths are recomputed is not detailed. The authors use a parameter N trading off accuracy of WC-path recomputation with runtime consumption. Since runtimes for WC-path recomputation are still very high, the authors are unable to provide results for some of their benchmarks. In contrast, the techniques presented here scale much better so that we can present results for very large benchmarks.
The work of <NO> is complementary to this paper since it presents a D-cache locking algorithm for WCET minimization. Using an extended version of reuse vectors, a system of cache miss equations is set up describing where data reuse translates to locality. These equations can be solved statically and define a set of data to be locked in the D-cache. Data dependencies in the CFG which can not be analyzed statically are handled by a heuristic that locks data which is likely to be accessed. In this paper on D-cache locking, the WC-path is not considered – neither implicitly nor explicitly.
In terms of predictability, locked caches behave similar to software-controlled scratchpad memories. In the past, sev-
eral papers were published exploiting scratchpads for energy dissipation minimization. In <NO>, the influence of scratchpads on WCET prediction is studied. Even though WCET is targeted in that work, the selection algorithm deciding which objects to be placed in the scratchpad is not WCETaware. Instead, a selection algorithm for energy reduction is employed, and the effect of this energy reduction strategy on WCET is evaluated afterwards. Hence, that work is not a true WCET-aware optimization and does not consider WC-paths at all."	https://doi.org/10.1145/1289816.1289853	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Falk, et al. [NO] presents techniques exploring the lockdown of instruction caches at compile-time to minimize wcets. 
Using bypass to tighten WCET estimates for multi-core processors with shared instruction caches	['We also assume that loads to the lines that have not been locked in the L1 cache bypass the L1 cache (as in a previous research work <NO>).']	Damien Hardy, Thomas Piquet, and Isabelle Puaut. 2009. Using bypass to tighten WCET estimates for multi-core processors with shared instruction caches. Proc. of the 30th RTSS. IEEE, 68–77.	Multi-core chips have been increasingly adopted by the microprocessor industry. For real-time systems to exploit multicore architectures, it is required to obtain both tight and safe estimates of worst-case execution times (WCETs). Estimating WCETs for multi-core platforms is very challenging because of the possible interferences between cores due to shared hardware resources such as shared caches, memory bus, etc. This paper proposes a compile-time approach to reduce shared instruction cache interferences between cores to tighten WCET estimations. Unlike [28], which accounts for all possible conflicts caused by tasks running on the other cores when estimating the WCET of a task, our approach drastically reduces the amount of inter-core interferences. This is done by controlling the contents of the shared instruction cache(s), by caching only blocks statically known as reused. Experimental results demonstrate the practicality of our approach.	"A multi-core architecture is assumed. Each core has a private first-level (L1) instruction cache, followed by instruction cache levels with at least one shared cache. The caches are setassociative. Each level of the cache hierarchy is non-inclusive:
− A piece of information is searched for in the cache of level if and only if a cache miss occurred when searching it in the cache of level −1. Cache of level 1 is always accessed. − Except if the bypass mechanism presented in Section 4 is used, every time a cache miss occurs at cache level , the entire cache block containing the missing piece of information is always loaded into the cache of level . − There are no actions on the cache contents (i.e. invalidations, lookups/modifications) other than the ones mentioned above.
Our study concentrates on instruction caches; it is assumed that the shared caches do not contain data. This study can be seen as a first step towards a general solution for shared caches. It can also push the use of separate shared instruction and data caches instead of unified ones1.
Our method assumes a LRU (Least Recently Used) cache replacement policy. The access time variability to main memory and shared caches, due to bus contention, is supposed to
1Unified caches could be partitioned at boot time for instance in a A-way instruction cache and a B-way data cache.
be bounded and known, by using for instance Time Division Multiple Access (TDMA) like in <NO> or other predictable bus arbitration policies <NO>.
Figure 1 illustrates two different supported architectures.
Regarding scheduling, it is assumed that a job does not migrate between cores at run-time. Migrations are allowed between job instances only. No further assumption is made on task scheduling, implying that any part of an interfering task may be executed simultaneously with the analysed task and may thus pollute the shared cache(s). This assumption was made in a first approach to keep WCET estimation and schedulability independent activities, as traditionally done when temporally validating real-time software. We do not attempt to explore joint WCET estimation and scheduling, which is left for future work. Tasks are independent (i.e. do not synchronize with each other), but might share code, such as libraries (see paragraph 3.2).
The focus in this paper is to estimate the WCET of a hardreal time task running on a core, in isolation from the tasks running on the same core, but suffering indirect interferences because of cache sharing from tasks running on the other cores. The computation of cache-related preemption delay due to intra-core interferences is considered out of the scope of this paper."	https://doi.org/10.1109/RTSS.2009.34	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Hardy, et al. [NO] proposes a compile-time approach to reduce shared instruction cache interferences between cores to tighten wcet estimations. 
CAMA: A predictable cache-aware memory allocator	['Similar techniques can be applied in software <NO> or in hardware <NO> with different granularities.']	J. Herter, P. Backes, F. Haupenthal, and J. Reineke. 2011. CAMA: A predictable cache-aware memory allocator. Proc. of the 2011 ECRTS. 23–32.	General-purpose dynamic memory allocation algorithms strive for small memory fragmentation and good average-case response times. Hard real-time settings, in contrast, place different demands on dynamic memory allocators: worst-case response times are more important than averagecase response times. Furthermore, predictable cache behavior is a prerequisite for timing analysis to derive tight bounds on a program’s execution time. This paper proposes a novel algorithm that meets these demands. It guarantees constant response times, does not cause unpredictable cache pollution, and allocations are cache-set directed, i.e., allocated memory is guaranteed to be mapped to a given cache set. The latter two are necessary to enable a subsequent precise static cache analysis.	"The remainder of this paper is organized as follows. We elaborate on the problems that dynamic memory allocation imposes on the determination of WCET bounds in Section II and present related work in Section III. In Section IV, we
2011 23rd Euromicro Conference on Real-Time Systems
1068-3070/11 $26.00 © 2011 IEEE DOI 10.1109/ECRTS.2011.11
23
describe our cache-aware memory allocation algorithm and its implementation. For a set of real-time applications, we compare worst-case execution time bounds and memory consumption of CAMA to that of TLSF, and, in the case of memory consumption, to that of Doug Lea’s allocator in Section V."	https://doi.org/10.1109/ECRTS.2011.11	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Herter, et al. [NO] proposes a novel algorithm that meets these demands. 
CQoS: A framework for enabling QoS in shared caches of CMP platforms	['This is a key advantage of our technique over some other cache partitioning schemes <NO> since cache allocation is not limited in the absence of contention.', '<NO> proposed a scheme for QoS support in shared CMP caches.', 'As expected, recent studies <NO><NO><NO><NO><NO><NO> have indicated that contention for critical platform resources (e.', 'RELATED WORK Previous research on partitioning for cache/memory architectures can be found in <NO><NO><NO><NO><NO><NO><NO> The primary optimization goal for the techniques presented in most of these papers has been either fairness <NO><NO><NO> or improving overall throughput <NO>.', 'Although QoS has been studied for a long time in real-time environments <NO>, networking <NO> and multimedia <NO>, it has only been recently introduced to the CMP architecture<NO>.', 'Iyer <NO> described the need for a priority-based framework to introduce QoS in CMP caches.', 'Similar cache partitioning mechanisms have been presented before <NO><NO><NO><NO>.', 'Iyer <NO> in a recent publication presented a framework for enabling QoS in shared caches.', 'The work on heterogeneous cache regions by Iyer <NO> is based on the use of multiple caches each with different cache parameters and the optimal assignment of applications to appropriate caches with suitable cache parameters.', 'Similar techniques can be applied in software <NO> or in hardware <NO> with different granularities.', 'There are many prior studies <NO><NO><NO><NO><NO><NO> that investigate the optimal shared cache partitioning for multiprogrammed workloads.', '<NO> pursue cache fairness and quality of service as the main goal.', 'The majority of work required modifications to hardware and falls into one of two camps: performance aware cache modification (most commonly cache-partitioning) <NO> or performance-aware DRAM controller memory scheduling [Loh 2008; Suh et al.', 'The effect of simultaneously executing threads competing for space in the LLC has been explored extensively by many different researchers <NO>.', 'The fact that a thread’s instruction retirement rate depends on the other threads in the workload makes performance on CMPs unpredictable which can be an even bigger problem than reduced throughput <NO>.', 'To incorporate the decisions made by the partitioning algorithm, the baseline LRU policy is augmented to enable way partitioning <NO><NO><NO>.', 'Mechanisms to facilitate static and dynamic partitioning of cache is described in detail in <NO>.']	Ravi Iyer. 2004. CQoS: A framework for enabling QoS in shared caches of CMP platforms. Proc. of the 18th ICS. ACM, 257–266.	Cache hierarchies have been traditionally designed for usage by a single application, thread or core. As multi-threaded (MT) and multi-core (CMP) platform architectures emerge and their workloads range from single-threaded and multithreaded applications to complex virtual machines (VMs), a shared cache resource will be consumed by these different entities generating heterogeneous memory access streams exhibiting different locality properties and varying memory sensitivity. As a result, conventional cache management approaches that treat all memory accesses equally are bound to result in inefficient space utilization and poor performance even for applications with good locality properties. To address this problem, this paper presents a new cache management framework (CQoS) that (1) recognizes the heterogeneity in memory access streams, (2) introduces the notion of QoS to handle the varying degrees of locality and latency sensitivity and (3) assigns and enforces priorities to streams based on latency sensitivity, locality degree and application performance needs. To achieve this, we propose CQoS options for priority classification, priority assignment and priority enforcement. We briefly describe CQoS priority classification and assignment options -ranging from user-driven and developer-driven to compiler-detected and flow-based approaches. Our focus in this paper is on CQoS mechanisms for priority enforcement -these include (1) selective cache allocation, (2) static/dynamic set partitioning and (3) heterogeneous cache regions. We discuss the architectural design and implementation complexity of these CQoS options. To evaluate the performance trade-offs for these options, we have modeled these CQoS options in a cache simulator and evaluated their performance in CMP platforms running network-intensive server workloads. Our simulation results show the effectiveness of our proposed options and make the case for CQoS in future multi-threaded/multi-core platforms since it improves shared cache efficiency and increases overall system performance as a result.	"The typical architecture of a processor shown in Figure 1. As shown in the figure typically consists of a number of compu L1 (and perhaps L2 caches). While the las be made up of private caches per proce shown that shared caching is more desira and design point of view. Our focus is on last-level cache as it is critical shared reso keep the compute cores busy executing an against the memory wall.
Conventional cache management relies o by only one memory access stream at a However, as processors, systems and a more complex, we need to consider the di access streams that allocate data into th consider the potential for different memo be broadly classified into the following ca
(a) Multi-Threaded Applications: The simplest case is where the last-level cache is being used by multiple threads of the same application. In this scenario, if the threads are sharing and communicating a lot of data between each other, the conventional management of shared caches will work reasonably well. However, if the threads perform entirely different types of transactions concurrently (for instance – HTTP transactions in a web server application), then the performance of the concurrent transactions is dependent not only on its own locality, but also the nature of memory accesses generated by the other transactions. In such scenarios, the transactions that are of higher importance (e.g. secure payment transactions) should be prioritized higher than those of lower importance (e.g. browsing transactions).
(b) Multiple Heterogeneous Applications: When multi-tasking multiple applications in a CMP platform, it is likely that threads of one application and another get scheduled on to the same microprocessor, thereby sharing the last-level cache. This computing model is particularly gaining relevance/importance as virtual machines <NO> start to proliferate in data centers as a mechanism to reduce server sprawl. In such scenarios, different applications will definitely exhibit different memory access properties and therefore should be handled differently in terms of cache space allocated. The notion of cache space prioritization between is important here for high efficiency.
(c) Specialized Cores in CMPs: As application and network processing tends to frequently execute some common communication layers (TCP/IP, SSL for instance) or computing components (data encryption, compression, CRC, XML parsing, etc), architects are considering replacing one or two of the CPU d Many Different Applications, VMs Threads, Cores, Specialized Engines, Devices, Assists, Data Prefetch/Forwar
P Processors
CHES in a CMP platform is
, a CMP microprocessor te cores with individual t level cache (LLC) can ssor, studies <NO> have ble from a performance the performance of this urce that is intended to d the last line of defense
n the cache being used ny given point in time. pplications become far fferent types of memory e shared LLC. Let us ry access streams as can tegories:
cores with specialized cores for such components. In such scenarios, the processing and memory access flows generated by the cores on the CMP will definitely have different properties and can be best handled with that knowledge.
(d) Sharing Caches between Cores and Devices: With the imminent potential of computing appearing in I/O devices (and possibly management controllers) in the system and the integration of I/O links into the CPU, researchers and architects are evaluating the benefits of using cache space to speed up the processing on the device. In such scenarios, it becomes important to perhaps partition the cache space dynamically between the cores and the devices.
(e) Memory Latency Helpers: In current processors, prefetching is employed to overlap computation with memory access. Excessive prefetching <NO> is known to cause problems such as cache pollution and deteriorate application performance. In addition to prefetching <NO> initiated by the CPU, researchers have considered memory-side prefetching, data forwarding <NO> and direct placement of network data into CPU cache <NO>. To reduce the amount of pollution caused by these memory latency helpers, it is important to prioritize cache space usage between demand activity and prefetching/forwarding activity.
Several other considerations need to be kept in mind when considering prioritizing cache space utilization. Other than the basic priority of the application, it also needs to be kept in mind that providing higher cache space to a higher priority application does not always guarantee higher cache performance. This depends heavily on the application’s memory access characteristics (i.e. locality properties). As a result, locality and
“user-defined” application priority need to be considered in unison to form the cache priority of the memory access stream. Other aspects to consider are the dynamic changes to the priority of an application or memory access stream. Since processing tends to go through several phases, it is important that the priority assignment mechanism be designed to allow dynamic changes. These issues motivated us to develop a framework that introduced the notion of quality of service in caches. In the next section, we will discuss the basic ideas behind this and present several potential design and implementation options to enable this in future systems."	https://doi.org/10.1145/1006209.1006246	2	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['workload', 'priority', 'platform']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Iyer. et al. [NO] presents a new cache management framework (cqos) that (1) recognizes the heterogeneity in memory access streams, (2) introduces the notion of qos to handle the varying degrees of locality and latency sensitivity and (3) assigns and enforces priorities to streams based on latency sensitivity, locality degree and application performance needs. 
QoS policies and architecture for cache/memory in CMP platforms	['The fact that a thread’s instruction retirement rate depends on the other threads in the workload makes performance on CMPs unpredictable which can be an even bigger problem than reduced throughput <NO>.', 'We note that HPAC is orthogonal to techniques that provide fairness in shared resources <NO>.']	Ravi Iyer, Li Zhao, Fei Guo, Ramesh Illikkal, Srihari Makineni, Don Newell, Yan Solihin, Lisa Hsu, and Steve Reinhardt. 2007. QoS policies and architecture for cache/memory in CMP platforms. Proc. of the 2007 ACM SIGMETRICS. ACM, 25–36.	As we enter the era of CMP platforms with multiple threads/cores on the die, the diversity of the simultaneous workloads running on them is expected to increase. The rapid deployment of virtualization as a means to consolidate workloads on to a single platform is a prime example of this trend. In such scenarios, the quality of service (QoS) that each individual workload gets from the platform can widely vary depending on the behavior of the simultaneously running workloads. While the number of cores assigned to each workload can be controlled, there is no hardware or software support in today’s platforms to control allocation of platform resources such as cache space and memory bandwidth to individual workloads. In this paper, we propose a QoS-enabled memory architecture for CMP platforms that addresses this problem. The QoS-enabled memory architecture enables more cache resources (i.e. space) and memory resources (i.e. bandwidth) for high priority applications based on guidance from the operating environment. The architecture also allows dynamic resource reassignment during run-time to further optimize the performance of the high priority application with minimal degradation to low priority. To achieve these goals, we will describe the hardware/software support required in the platform as well as the operating environment (O/S and virtual machine monitor). Our evaluation framework consists of detailed platform simulation models and a QoS-enabled version of Linux. Based on evaluation experiments, we show the effectiveness of a QoSenabled architecture and summarize key findings/trade-offs.	In this section, we motivate QoS needs by describing disparate threads and the shared resource problem.	https://doi.org/10.1145/1254882.1254886	2	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['workload', 'priority', 'platform']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Iyer, et al. [NO] enters the era of cmp platforms with multiple threads/cores on the die, the diversity of the simultaneous workloads running on them is expected to increase. 
A coordinated approach for practical OS-level cache management in multi-core real-time systems	['While LLC offers high potential for average performance improvement, it challenges worst-case execution time (WCET) estimation, which has made LLC to be studied in the last years by the real-time community <NO>.', 'Software cache partitioning <NO> and hardware cache partitioning <NO> have been used so far to control inter-task interaction in the LLC.', 'Software cache partitioning is done through memory coloring <NO>.']	Hyoseung Kim, Arvind Kandhalu, and Ragunathan Rajkumar. 2013. A coordinated approach for practical OS-level cache management in multi-core real-time systems. Proc. of the 25th ECRTS. 80–89.	Many modern multi-core processors sport a large shared cache with the primary goal of enhancing the statistic performance of computing workloads. However, due to resulting cache interference among tasks, the uncontrolled use of such a shared cache can significantly hamper the predictability and analyzability of multi-core real-time systems. Software cache partitioning has been considered as an attractive approach to address this issue because it does not require any hardware support beyond that available on many modern processors. However, the state-of-the-art software cache partitioning techniques face two challenges: (1) the memory co-partitioning problem, which results in page swapping or waste of memory, and (2) the availability of a limited number of cache partitions, which causes degraded performance. These are major impediments to the practical adoption of software cache partitioning. In this paper, we propose a practical OS-level cache management scheme for multi-core real-time systems. Our scheme provides predictable cache performance, addresses the aforementioned problems of existing software cache partitioning, and efficiently allocates cache partitions to schedule a given taskset. We have implemented and evaluated our scheme in Linux/RK running on the Intel Core i7 quad-core processor. Experimental results indicate that, compared to the traditional approaches, our scheme is up to 39% more memory space efficient and consumes up to 25% less cache partitions while maintaining cache predictability. Our scheme also yields a significant utilization benefit that increases with the number of tasks.	"We discuss related work on cache interference and describe the assumptions and notation used in our paper.
1Intel Core i7 processors are used in not only desktop/server machines but also aerospace and defense embedded systems <NO>. In fact, Intel’s Embedded Systems Division is rather big inside Intel."	https://doi.org/10.1109/ECRTS.2013.19	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Kim, et al. [NO] proposes a practical os-level cache management scheme for multi-core real-time systems. 
Multi-level unified caches for probabilistically time analysable real-time systems	['A similar two-level cache hierarchy has been shown to be MBPTA compliant <NO> though it was deployed in a single-core architecture with no inter-task interferences.']	L. Kosmidis, J. Abella, R. Quinones, and F.J. Cazorla. 2013. Multi-level unified caches for probabilistically time analysable real-time systems. Proc. of the 34th RTSS. IEEE, 360–371.	Caches are key resources in high-end processor architectures to increase performance. In fact, most highperformance processors come equipped with a multi-level cache hierarchy. In terms of guaranteed performance, however, cache hierarchies severely challenge the computation of tight worstcase execution time (WCET) estimates. On the one hand, the analysis of the timing behaviour of a single level of cache is already challenging, particularly for data accesses. On the other hand, unifying data and instructions in each level, makes the problem of cache analysis significantly more complex requiring tracking simultaneously data and instruction accesses to cache. In this paper we prove that multi-level cache hierarchies can be used in the context of Probabilistic Timing Analysis and tight WCET estimates can be obtained. Our detailed analysis (1) covers unified data and instruction caches, (2) covers different cache-write policies (write-through and write back), write allocation policies (write-allocate and non-write-allocate) and several inclusion mechanisms (inclusive, non-inclusive and exclusive caches), and (3) scales to an arbitrary number of cache levels. Our results show that the probabilistic WCET (pWCET) estimates provided by our analysis technique effectively benefit from having multi-level caches. For a two-level cache configuration and for EEMBC benchmarks, pWCET reductions are 55% on average (and up to 90%) with respect to a processor with a single level of cache.	This section provides some background on PTA as well as the cache characteristics and assumptions in this paper.	https://doi.org/10.1109/RTSS.2013.43	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Kosmidis, et al. [NO] of a single level of cache is already challenging, particularly for data accesses. 
Instruction cache locking using temporal reuse profile	['Cache locking is also shown to be quite effective for improving average-case execution time <NO>.']	Yun Liang, and T. Mitra. 2010. Instruction cache locking using temporal reuse profile. Proc. of the 47th ACM/IEEE DAC. 344–349.	The performance of most embedded systems is critically dependent on the average memory access latency. Improving the cache hit rate can have significant positive impact on the performance of an application. Modern embedded processors often feature cache locking mechanisms that allow memory blocks to be locked in the cache under software control. Cache locking was primarily designed to offer timing predictability for hard real-time applications. Hence, the compiler optimization techniques focus on employing cache locking to improve worst-case execution time. However, cache locking can be quite effective in improving the average-case execution time of general embedded applications as well. In this paper, we explore static instruction cache locking to improve average-case program performance. We introduce temporal reuse profile to accurately and efficiently model the cost and benefit of locking memory blocks in the cache. We propose an optimal algorithm and a heuristic approach that use the temporal reuse profile to determine the most beneficial memory blocks to be locked in the cache. Experimental results show that locking heuristic achieves close to optimal results and can improve the cache miss rate by up to 24% across a suite of real-world benchmarks. Moreover, our heuristic provides significant improvement compared to the state-of-the-art locking algorithm both in terms of performance and efficiency.	"Instruction cache locking has been primarily employed in hard real time systems for better timing predictability <NO>. In hard real time systems, worst case execution time (WCET) is an essential input to the schedulability analysis of mutli-tasking real time systems. It is difficult to estimate a safe but tight WCET in the presence of complex micro-architectural features such as caches. By statically locking instructions in the cache, WCET becomes more predictable. Locking has also been applied to shared caches in multi-cores in <NO>.
In this paper, our aim is to improve average-case execution time for general embedded systems through locking. Data cache locking mechanism based on the length of the reference window for each data-access instruction is proposed in <NO>. However, they do not model the cost/benefit of locking and there is no guarantee of performance improvement. Recently, Anand and Barua proposed an instruction cache locking algorithm for improving average-case execution time in <NO>. This work is most related to ours. However, our approach differs in two important aspects. First, Anand and Barua’s approach relies on simulation, while we use an accurate cache modeling. More concretely, in Anand and Barua’s method, two detailed trace simulations are employed in each iteration where an iteration locks one memory block in the cache. In contrast, we require only one profiling step. Secondly, in our approach, the cost/benefit of cache locking is modeled precisely using temporal reuse profiles of memory blocks. However, in their method, cache locking benefit is approximated by locking dummy blocks to keep the number of simulations reasonable. Thus our work improves over <NO> both in terms of performance and efficiency.
In this work, we introduce temporal reuse profile to model cache behavior. Previously, reuse distance has been proposed for the same purpose <NO>. Reuse distance is defined as the number of distinct data accesses between two consecutive references to the same address and it accurately models cache behavior of a fully associative cache. However, to precisely model the effect of cache locking, we need the content instead of the number (size) of the distinct data accesses between two consecutive references. TRP records both the reuse content and their frequencies."	https://doi.org/10.1145/1837274.1837362	3	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking', 'locked', 'core']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Liang, et al. [NO] explores static instruction cache locking to improve average-case program performance. 
Instruction cache locking for real-time embedded systems with multi-tasks	[]	Tiantian Liu, Minming Li, and C.J. Xue. 2009. Instruction cache locking for real-time embedded systems with multi-tasks. Proc. of the 15th IEEE RTCSA. 494–499.	Modern processors often provide cache locking capability which can be applied statically and dynamically to manage cache in a predictable manner. The selection of instructions to be locked in the instruction cache (I-Cache) has dramatic influence on the performance of multi-task real-time embedded systems. This paper focuses on using cache locking techniques on a shared I-Cache in a real-time embedded system with multi-tasks to minimize its worst-case utilization (WCU) which is one of the most important criteria for designing realtime embedded systems. We analyze the static and dynamic strategies to perform I-Cache locking and propose different algorithms which utilize the foreknowing information of the real-time embedded applications. Experiments show that the proposed algorithms can reduce WCU further compared to previous techniques. Design suggestions on which strategy should be utilized under different situations are also induced from the experimental results.	"There is N periodic tasks Ti, 1 ≤ i ≤ N with each period peri. The tasks are scheduled in the processor using some scheduling methods. WCU <NO> presents the busy proportion of a processor used in task computation. The following equation gives this ratio:
WCU = ∑
i
WCET (Ti) peri
(1)
In this paper, an Execution Flow Tree (EFT ) is used for modelling a real-time program.
Definition 1: An EFT is a weighted tree EFT = (V, E), where V represents the set of nodes and E represents the set of edges. Node v ∈ V represents a code block of the program and has three attributes: w(v) is the processing time when v is not in the cache, w′(v) is the processing time when v is in the cache, and s(v) is the size of v. Edge e(v, u) denotes a program control flow from node v to node u. It can be a sequential flow, a loop flow or a branch flow.
Algorithm EFT CON in <NO> constructs an EFT from an executable program code. An example of EFT is shown in Fig. 1. Fig. 1(a) is a code segment of the benchmark “Audio beam former” <NO> and Fig. 1(b) is its corresponding EFT . For simplicity, the EFT ’s call functions are not presented recursively in this example. Algorithm EFT CON does recursively process the subroutines. There is a procedure Duplicate() in Algorithm EFT CON. If a node v has an indegree(v) at least 2, Duplicate() instantiates the structure starting from v by indegree(v) times, which ensures the output to be a tree. For example, node 31, 32 and 33 in Fig. 1 (b) are the duplicated nodes introduced by Duplicate().
To calculate the WCU, we first need to obtain the WCET of each task. For an EFT , WCET is the length of its longest rootleaf path. For a root-leaf path Px = (px0px1 . . . px num(Px)) in an EFT where px0, px1, . . . , px num(Px) ∈ V and num(Px) is the number of edges on this path, Len(Px) represents the execution time needed if this path is chosen to execute in runtime. It can be calculated as: Len(Px) = ∑num(Px) y=0 (1 − δ(pxy)) ·w(pxy)+ δ(pxy) ·w′(pxy), where δ(pxy) = 1 if node
pxy is put in the I-Cache and δ(pxy) = 0 otherwise. Then the WCET (Ti) can be calculated by the following equation:
WCET (Ti) = max px∈Ti num(Px)∑ y=0 ((1 − δ(pxy)) · w(pxy) +
δ(pxy) · w′(pxy)) (2)"	https://doi.org/10.1109/RTCSA.2009.59	3	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking', 'locked', 'core']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Liu, et al. [NO] focuses on using cache locking techniques on a shared i-cache in a real-time embedded system with multi-tasks to minimize its worst-case utilization (wcu) which is one of the most important criteria for designing realtime embedded systems. 
Minimizing WCET for real-time embedded systems via static instruction cache locking	['More importantly, by carefully choosing the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling techniques without locking <NO>.', 'Recently, a heuristic <NO> and an optimal solution <NO> have been proposed to minimize the WCET via static instruction cache locking.', 'By employing full cache locking, <NO> can completely bypass cache modeling in WCET analysis phase and thereby achieve tight WCET estimation.', 'In this paper, we argue (and experimentally validate) that aggressive full cache locking as proposed in <NO> may have substantial negative impact on WCET reduction.', 'Full cache locking: Existing cache locking techniques <NO> first build the worst case path (e.', 'Both cache locking techniques <NO> model the fact that the WCET path may change after locking some memory locks.', 'For this example, the heuristic <NO> and the optimal <NO> approach return the same solution.', 'From the example above, we first observe that full locking techniques <NO> are not guaranteed to perform better than cache modeling (with no locking) specially when some memory accesses can be easily classified as cache hits (m3, m4, m5 in our example).', '<NO> present an optimal solution to minimize WCET via cache locking.', '3 Partial versus Full Cache Locking There exist two full cache locking techniques as mentioned in Section 2 <NO>.', '<NO> show that their approach can achieve better WCET reduction compared to <NO>, it has several limitations.']	Tiantian Liu, Minming Li, and C.J. Xue. 2009. Minimizing WCET for real-time embedded systems via static instruction cache locking. Proc. of the 15th IEEE RTAS. 35–44.	Cache is effective in bridging the gap between processor and memory speed. It is also a source of unpredictability because of its dynamic and adaptive behavior. Worst-case execution time (WCET) of an application is one of the most important criteria for real-time embedded system design. The unpredictability of instruction miss/hit behavior in the instruction cache (I-Cache) leads to an unnecessary overestimation of the real-time application’s WCET. A lot of modern processors provide cache locking capability. Static ICache locking locks function/instruction blocks of a program into the I-Cache before program execution. In this way, a more precise estimation of WCET can be achieved. The selection of functions/instructions to be locked in the I-Cache has dramatic influence on the performance of the real-time application. This paper focuses on the static I-Cache locking problem to minimize WCET for real-time embedded systems. We formulate the problem using an Execution Flow Tree (EFT ) and a linear programming model. For a subset of the problems with certain properties, corresponding polynomial time optimal algorithms are proposed. We prove that the general problem is an NP-Hard problem. We also show that for a subset of the general problem with certain patterns, optimal solutions can be achieved in polynomial time. Experimental results show that our algorithms can reduce the WCET of applications further compared to current best known techniques.	"A lot of works have been done regarding the predictability and performance issues of caches in real-time embedded systems. Some of them consider the I-Cache locking problem.
Puaut et al. have done a series of studies about I-Cache locking. In <NO>, they propose two polynomial greedy algorithms to select instruction locking contents. These two algorithms have different metrics in selecting tasks and program lines. One aims at minimizing the worst-case CPU utilization, while the other aims at minimizing the interferences between tasks. Experiments show that their algorithms improve these two metrics compared with a static cache analysis method <NO>. Their candidates are selected in a single set of tasks, which are the tasks along the worst-cast execution path without using cache. In <NO>, they compared their reference-based algorithm in <NO> with a genetic algorithm for cache contents selection in <NO>. Experiments show that both algorithms perform closely with respect to worst-case system utilization. The genetic algorithm performs slightly better than the reference-based algorithm with respect to the average slack time of tasks, while it has a higher time complexity.
The genetic algorithm mentioned above is proposed by Campoy et al. in <NO>. This genetic idea is inspired by Darwin’s
theory of evolution and is adopted by many researchers to solve optimization problems in a variety of research fields. They initially operate on a population of potential solutions, which are individual function/instruction blocks in the ICache locking problem. At each generation, a new set of approximations is created by selecting individuals according to their levels of fitness in the problem domain and breeding them together using cross-over operators borrowed from natural genetics. In the I-Cache locking selection problem, the fitness level is weighted by the average response time of all tasks and the cross-over operators are done by randomly selecting/reducing/increasing/modifying the locked program lines. This process leads to evolutions of individuals that are better suited to their environment than their ancestors. Finally an approximately optimal solution is achieved. This method is noted for its high time complexity. Also the cross-over operators need to be further discussed.
Falk et al. point out in <NO> that Puaut et al.’s algorithms do not consider the changing of worst-case path (WC-Path) after a function node is selected to be locked in I-Cache. Falk et al. take the changing of WC-Path into account during each step of the optimization procedure. They use an Execution Flow Graph (EFG) to model a program, where its nodes represent the function blocks and edges represent the control flows. They propose an algorithm for WC-Path construction with a complexity of O((|V |+ |E|) log |V |) and adopt greedy strategy to choose a node x with a maximal g(x) in each step. Here, g(x) is defined as g(x) = wx−w ′ x
sx ∗w(∗,x), where wx is
the WCET of x if x is placed in memory, w′x is the WCET of x if x is in the I-Cache, sx is the size of x in bytes, and w(∗,x) is the execution frequency of x over all contexts on the current WC-Path. This greedy algorithm can not guarantee the optimal solutions.
Asaduzzaman et al. <NO> also aim at minimizing the WCET. They believe WCET has direct connection with cache miss rate. So they propose a greedy algorithm which picks the function node with the maximal miss rate in each selecting step. However, the relation between WCET and miss rate is vague and has not been proved by persuasive statement.
Previous works have not given a formal formulation of I-Cache locking selection problem. No previous work has analyzed whether there exist polynomial algorithms to obtain the optimal solutions. Most of the previous proposed heuristic algorithms use greedy strategy. Also, they ignore the problem that the selected functions may conflict when they are filled into cache according to the cache mapping strategies. In this paper, we explicitly formulate the I-Cache locking selection problem and propose polynomial algorithms to obtain optimal solutions for a subset of the programs with certain properties. We prove the general problem is NP-Hard and also identify that a subset of the general problems which exhibit special patterns can be solved optimally in polynomial time. The algorithms in <NO> and <NO> are compared with the optimal algorithms proposed in this paper. The experimental results show that the proposed algorithms can reduce WCETs further for real-time applications effectively under different cache sizes."	https://doi.org/10.1109/RTAS.2009.11	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Liu, et al. [NO] focuses on the static i-cache locking problem to minimize wcet for real-time embedded systems. 
Soft-OLP: Improving hardware cache performance through software-controlled object-level partitioning	[]	Qingda Lu, Jiang Lin, Xiaoning Ding, Zhao Zhang, Xiaodong Zhang, and P. Sadayappan. 2009. Soft-OLP: Improving hardware cache performance through software-controlled object-level partitioning. Proc. of the 18th PACT. 246–257.	Performance degradation of memory-intensive programs caused by the LRU policy’s inability to handle weaklocality data accesses in the last level cache is increasingly serious for two reasons. First, the last-level cache remains in the CPU’s critical path, where only simple management mechanisms, such as LRU, can be used, precluding some sophisticated hardware mechanisms to address the problem. Second, the commonly used shared cache structure of multi-core processors has made this critical path even more performance-sensitive due to intensive inter-thread contention for shared cache resources. Researchers have recently made efforts to address the problem with the LRU policy by partitioning the cache using hardware or OS facilities guided by run-time locality information. Such approaches often rely on special hardware support or lack enough accuracy. In contrast, for a large class of programs, the locality information can be accurately predicted if access patterns are recognized through small training runs at the data object level. To achieve this goal, we present a system-software framework referred to as Soft-OLP (Software-based Object-Level cache Partitioning). We first collect per-object reuse distance histograms and inter-object interference histograms via memory-trace sampling. With several low-cost training runs, we are able to determine the locality patterns of data objects. For the actual runs, we categorize data objects into different locality types and partition the cache space among data objects with a heuristic algorithm, in order to reduce cache misses through segregation of contending objects. The object-level cache partitioning framework has been implemented with a modified Linux kernel, and tested on a commodity multi-core processor. Experimental results show that in comparison with a standard L2 cache managed by LRU, Soft-OLP significantly reduces the execution time by reducing L2 cache misses across inputs for a set of single- and multi-threaded programs from the SPEC CPU2000 benchmark suite, NAS benchmarks and a computational kernel set.	"Here we use the conjugate gradient (CG) program from the NAS benchmarks as a motivating example to illustrate the problem. As shown in Fig. 1, CG spends most of its running time on a sparse matrix-vector multiplication w = a ·p,
where a is a sparse matrix, rowstr and colidx are row and column index arrays and w and p are dense vectors. There are also code pieces not shown in Fig. 1 due to the space limitation for this paper. These code pieces access arrays iv,v,acol,arow, x,r,q,z,aelt in addition to the arrays shown in Fig. 1. In CG, the majority of the memory accesses are on arrays a, p and colidx. Although vector p has high temporal reuse in the matrix-vector multiplication code, depending on its size, its elements may get repeatedly evicted from cache before their next uses, due to the streaming accesses on arrays a and colidx. As the result of this thrashing effect caused by accessing arrays a and colidx, CG often reveals a streaming access pattern and has a very high miss rate in cache. Without special code/data treatment based on domain knowledge, general compiler optimizations, such as loop tiling, cannot be applied in this case because of the irregular nature of this program — there is indirection in array accesses.
The caching inefficiency problem occurs in CG because the conventional LRU cache replacement policy does not distinguish strong- and weak-locality accesses and thus is unable to treat them differently. Since cache replacement decisions are made at the whole-system level, any data reuse with a reuse distance greater than the cache size cannot be hit in the cache. The CG case is an example of variable locality strengths among different data objects, which can not be distinguished and handled properly by LRU. If we allow the cache space to be partitioned between data objects, we would be able to allocate variable cache sizes to different objects based on their locality strengths, well utilizing the limited cache space and minimizing cache misses. With CG, there are different ways to reduce and even completely eliminate capacity misses on strong-locality array p without increasing cache misses on the other objects. One approach is to protect p in an exclusive cache space and leave the remaining cache capacity for the remaining data objects. Alternatively, we can divide the cache such that the minimum cache quota is given to weak-locality arrays colidx and a. This optimization is not limited to single-thread performance. Even when the code is augmented with OpenMP directives, with a shared cache the object-level partitioning decisions should still reduce capacity misses, since memory accesses from different processor cores collectively reveal the same pattern as with sequential execution. If we allocate a very small cache quota for arrays colidx and a and co-schedule CG with other programs, it no longer exhibits a streaming access pattern that significantly interferes with its co-runners, so
that high throughput can be achieved with judicious interthread cache partitioning. In this paper, we focus on objectlevel cache partitioning and defer the combination of interobject and inter-thread cache partitioning to future work.
III. OVERVIEW OF THE APPROACH
The CG example in Fig. 1 demonstrates the benefits of partitioning the cache space at the object level. In this paper the term object is defined as an allocated region of data storage and used interchangeably with variable. Note that this definition is not equivalent to its usage in objectoriented programming. We partition the last-level cache space among global and heap objects for high-performance scientific applications. There are two reasons for this decision. First, high-performance scientific applications often have relatively regular memory access patterns and high data reuse ratios, which makes object-level cache partitioning possible and profitable. Second, in these programs, the majority of the memory accesses and cache misses are on a limited number of global and heap objects. In order to partition the last-level cache space among data objects, we need to answer the following questions: (1) How can we capture data reuse patterns at the object level, across cache configurations and program inputs? (2) How can we capture the interference among the data objects that share and compete for cache space? (3) How can we identify critical objects as partitioning candidates? (4) How can we make quick object-level partitioning decisions with a different program input? (5) What system support is needed to enforce cache partitioning decisions?
To answer the above questions, we propose a system framework called Soft-OLP that detects a program’s data reuse patterns at the object level, through memory profiling and pattern recognition, and enforces partitioning decisions at run time with operating system support. This proposed framework consists of the following steps and is summarized in Fig. 2.
1) Profile Generation. For a given program and several small training inputs, we capture memory accesses in an object-relative form through binary instrumentation. We obtain per-object reuse distance histograms and inter-object interference histograms for data objects. These histograms are program profiles with
training inputs that are to be used to predict the program’s data access and reuse patterns. 2) Profile Analysis. Based on program profiles from training runs, we detect the patterns of the program’s perobject data reuse, object sizes and access frequencies as polynomial functions, using a pattern recognition algorithm based on the work in <NO>. 3) Cache Partitioning Decision Making and Enforcement. When the program is scheduled to run with an actual input, we predict its per-object reuse distance histograms and inter-object interference information with detected access patterns. We then categorize data objects as being “hot”, “hog”, “cold” or “other”. Using this classification, we follow a heuristic algorithm to make an object-level cache partitioning decision so that “hog” objects do not prevent us from exploiting the locality of “hot” objects and the contention between “hot” objects is alleviated. Such a partitioning decision is finally enforced on commodity CMPs with an OS kernel that supports page coloring <NO>, <NO> at the object level.
IV. OBJECT-LEVEL PROGRAM LOCALITY PROFILE
With a given input, we model a program’s data locality at the object level with a locality profile. An object-level program locality profile has two components: an objectrelative locality profile consisting of per-object reuse distance histograms and an inter-object interference profile including inter-object interference histograms."	https://doi.org/10.1109/PACT.2009.35	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Lu, et al. [NO] presents a system-software framework referred to as soft-olp (software-based object-level cache partitioning). 
Real-time cache management framework for multi-core architectures	[]	R. Mancuso, R. Dudko, E. Betti, M. Cesati, M. Caccamo, and R. Pellizzoni. 2013. Real-time cache management framework for multi-core architectures. Proc. of the 19th IEEE RTAS. 45–54.	Multi-core architectures are shaking the fundamental assumption that in real-time systems the WCET, used to analyze the schedulability of the complete system, is calculated on individual tasks. This is not even true in an approximate sense in a modern multi-core chip, due to interference caused by hardware resource sharing. In this work we propose (1) a complete framework to analyze and profile task memory access patterns and (2) a novel kernel-level cache management technique to enforce an efficient and deterministic cache allocation of the most frequently accessed memory areas. In this way, we provide a powerful tool to address one of the main sources of interference in a system where the last level of cache is shared among two or more CPUs. The technique has been implemented on commercial hardware and our evaluations show that it can be used to significantly improve the predictability of a given set of critical tasks.	In this section we will briefly discuss existing profiling and cache management techniques and we will explain how our cache allocation strategy differs from existing related work.	https://doi.org/10.1109/RTAS.2013.6531078	2	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['workload', 'priority', 'platform']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Mancuso, et al. [NO] proposes (1) a complete framework to analyze and profile task memory access patterns and (2) a novel kernel-level cache management technique to enforce an efficient and deterministic cache allocation of the most frequently accessed memory areas. 
A comparison of memory allocators for real-time applications	[]	Miguel Masmano, Ismael Ripoll, and Alfons Crespo. 2006. A comparison of memory allocators for real-time applications. Proc. of the 4th JTRES. ACM, 68–76.	Real-Time applications can require dynamic storage management. However this feature has been sistematically avoided due to the general belief about the poor performance of allocation and deallocation operations in time and space. Actually, the use of Java technologies in real-time require to analyse in detail the performance of this feature due to its intensive use. In a previous paper, the authors proposed a new dynamic storage allocator that perform malloc and free operations in constant time (O(1)) with a very high efficiency. In this paper, we compare the behaviour of several allocators under ”real-time” loads measuring the temporal cost and the fragmentation incurred by each allocator. In order to compare the temporal cost of the allocators, two parameters have been considered: number of instructions and processor cycles. To measure the fragmentation, we have calculated the relation between the maximum memory used by the each allocator relative to the point of the maximum amount of memory used by the load. Additionally, we have measured the impact of delayed deallocation in a similar way a periodic garbage collector server will do. The results of this paper show that TLSF allocator obtains the best resuts when both aspects, temporal and spatial are considered.	"This section presents a brief descrition of the allocators used in the evaluation. Allocators can be classified attending to the policy used (first fit, best fit, good fit, etc.) and the mechanism implemented (doubly linked lists, segregated lists, bitmaps, etc.) based on the work of Wilson et al.<NO>
In order to perform the evaluation presented in this paper, we have selected some of the allocatores more representatives taking into account considerations like:
Representatives of very well known policies. First-fit and Best-fit are two of the most representative sequential fit allocators. First-fit allocator is used in all comparisons. It does not provide good results in terms of time and fragmentation but it is a reference. Best-fit provides very good results on fragmentation but bad results in time. Both of them are usually implemented with a doubly linked list. The pointers which implement the list are emb]dded inside the header of each free block. First-fit allocator searches the free list and selects the first block whose size is equal or greater than the requested size, whereas Best-fit goes further to select the block which best fits the request.
Widely used in several environments. Doug Lea’s allocator <NO> is the most representative of hybrid allocator and it is used in Linux systems and several environments. It is a combination of several mechanisms. This allocator uses a single array of lists, where the first 48 indexes are lists of blocks of an exact size (16 to 64 bytes) called “fast bins” . The remaining part
of the array contains lists of segregated lists, called “bins”. Each of these segregated lists are sorted by block size. A mapping function is used to quickly locate a suitable list. DLmalloc uses the delayed coalescing strategy, that is, the deallocation operation does not coalesce blocks. Instead a massive coalescing is done when the allocator can not serve a request.
Labelled as ”real-time” allocators. Binary-Buddy and Half-fit are good-fit allocators that provide excellent results in time reponse. However, the fragmentation produced by these allocators is known to be non negligible.
Buddy systems <NO> are a particular case of Segregated free lists. Being H the heap size, there are only log2(H) lists since the heap can only be split in powers of two. This restriction yields efficient splitting and merging operations, but it also causes a high memory fragmentation. There exist several variants of this method <NO> such as Binary-buddy, Fibonacci-buddy, Weighted buddy and Double-buddy.
The Binary-buddy <NO> allocator is the most representative of the Buddy Systems allocators, which besides has always been considered as a real-time allocator. The initial heap size has to be a power of two. If a smaller block is needed, then any available block can only be split into two blocks of the same size, which are called buddies. When both buddies are again free, they are coalesced back into a single block. Only buddies are allowed to be coalesced. When a small block is requested and no free block of the requested size is available, a bigger free block is split one or more times until one of a suitable size is obtained.
Half-fit <NO> uses bitmaps to find free blocks rapidly without having to perform an exhaustive search. Halffit groups free blocks in the range [2i, 2i+1[ in a list indexed by i. Bitmaps to keep track of empty lists jointly with bitmap processor instructions are used to speed-up search operations. When a block of size r is required, the search for a suitable free block starts on i, where i = ⌊log2(r−1)⌋+1 (or 0 if r = 1). Note that the list i always holds blocks whose sizes are equal to or larger than the requested size. If this list is empty, then the next non-empty free list is used instead. If the size of the selected free block is larger than the requested one, the block is split in two blocks of sizes r and r′. The remainder block of size r′ is re-inserted in the list indexed by i′ = ⌊log2(r ′)⌋.
New real-time allocator. TLSF (Two-Level Segregated Fit) <NO> is a bounded-time, good-fit allocator. TLSF implements a combination of segregated and bitmap fits mechanisms. The use of bitmaps allow to implement fast, bounded-time mapping and searching functions. TLSF data structure can be represented as a two-dimension array. The first dimension splits free blocks in size-ranges a power of two apart from each other, so that first-level index i refers to free blocks of sizes in the range [2i,2i+1[. The second dimension splits each first-level range linearly in a number of ranges of an equal width. The number of such ranges, 2L, should not exceed the number of bits of the underlying architecture, so that a one-word bitmap
can represent the availability of free blocks in all the ranges. TLSF uses word-size bitmaps and processor bit instructions to find a suitable list in constant time. The range of sizes of the segregated lists has been chosen so that a mapping function can be used to locate the position of the segregated list given the block size, with no sequential or binary search. Also, ranges have been spread along the whole range of possible sizes in such a way that the relative width (the length of the range) of the range is similar for small blocks than for large blocks. In other words, there are more lists used for smaller blocks than for larger blocks.
One important aspect is the theoretical temporal cost (complexity) of each allocator. Table 1 summarises these costs for each allocator.
In <NO>, the worst-case or bad-case2 scenario of each allocator has been analysed and detailed. For each allocator, a sinthetic load was generated to conduct it to its worst-case allocating and deallocating scenarios. Once these scenarios were reached, we measured the number of instructions performed by the allocation or deallocation operations. Table 2 shows a summary of these results.
These results can slightly change depending on the compiler version and the optimisation options used."	https://doi.org/10.1145/1167999.1168012	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Masmano, et al. [NO] compares the behaviour of several allocators under ” real-time ” loads measuring the temporal cost and the fragmentation incurred by each allocator. 
Hardware support for WCET analysis of hard real-time multicore systems	['<NO> have proposed TDMA-based bus and L2 cache access to improve predictability on multi-core architectures.', 'Such hybrid execution of application tasks has been considered in recent research <NO>.', 'While LLC offers high potential for average performance improvement, it challenges worst-case execution time (WCET) estimation, which has made LLC to be studied in the last years by the real-time community <NO>.', 'Software cache partitioning <NO> and hardware cache partitioning <NO> have been used so far to control inter-task interaction in the LLC.', 'Hardware cache partitioning assigns different cache ways or cache banks to the different co-running tasks such in a way that contention is prevented <NO>.', 'hardware cache (way) partitioning <NO>.', 'However, we believe the scheduling algorithm and analysis techniques in this paper is a necessary step towards completely avoiding interference between tasks running on multicores, and can be integrated with techniques of performance isolation on other shared resources, for instance, the work in <NO> to avoid interference caused by the shared on-chip bus.', '<NO> formulated cache allocation as a MILP problem to minimize the total CPU utilization of Paolieri’s new multi-core architecture <NO>.', 'be bounded and known, by using for instance Time Division Multiple Access (TDMA) like in <NO> or other predictable bus arbitration policies <NO>.', 'DL1 and IL1 caches are connected to the UL2 through fully-dedicated bidirectional buses, whose access latency can be bounded using the technique presented in <NO>.']	Marco Paolieri, Eduardo Quiñones, Francisco J Cazorla, Guillem Bernat, and Mateo Valero. 2009. Hardware support for WCET analysis of hard real-time multicore systems. ACM SIGARCH Computer Architecture News, Vol. 37. ACM, 57–68.	The increasing demand for new functionalities in current and future hard real-time embedded systems like automotive, avionics and space industries is driving an increase in the performance required in embedded processors. Multicore processors represent a good design solution for such systems due to their high performance, low cost and power consumption characteristics. However, hard real-time embedded systems require time analyzability and current multicore processors are less analyzable than single-core processors due to the interferences between different tasks when accessing shared hardware resources. In this paper we propose a multicore architecture with shared resources that allows the execution of applications with hard real-time and non hard real-time constraints at the same time, providing time analizability for the hard real-time tasks so that they can meet their deadlines. Moreover our architecture proposal provides high-performance for the non hard real-time tasks.	"One of the design goals of our architecture is that it can be easily analyzed by current measurement based WCET tools with no modifications. In this section we provide some background on real-time scheduling and the analysis tool we use in this paper.
In real-time systems, for each task, the scheduler knows three main parameters: The period, the deadline and the Worst-Case Execution Time (WCET). If the task is periodic, its period is the interval at which new instances of that task are ready for execution. The deadline is the time before a task instance must be complete. For simplicity, the deadline is often set equal to the period. This means that a task has to be executed before its next instance arrives into the system. The WCET is a safe estimation of the upper bound time required to execute any instance of the task. In single-core systems the WCET of a task is computed assuming that the task has full access to processor resources.
One of the current approaches to analyze WCET in singlecore processors is measurement-based WCET analysis<NO>. In this paper we use RapiTime<NO>, a commercial tool developed by Rapita Systems Ltd.3, that estimates the WCET using a measurement-based technique. This tool is widely used in the avionics, telecommunications, space, and automotive industries. RapiTime uses on-line testing to measure the execution time of sub-paths between instrumentation points in the code. Moreover, by contrast, offline static analysis is the best way to determine the overall structure of the code and the paths through it. RapiTime therefore uses path
3 www.rapitasystems.com
analysis techniques to build up a precise model of the overall code structure and determine which combinations of subpaths form complete and feasible paths through the code. Finally RapiTime combines the measurement and control flow analysis information to compute measurement based worst-case execution time estimations in a way that captures accurately the execution time variation on individual paths due to hardware effects<NO>."	https://doi.org/10.1145/1555815.1555764	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Paolieri, et al. [NO] proposes a multicore architecture with shared resources that allows the execution of applications with hard real-time and non hard real-time constraints at the same time, providing time analizability for the hard real-time tasks so that they can meet their deadlines. 
Architectural support for operating system-driven CMP cache management	['that its blocks are not evicted by demand based replacement policies like LRU (the code for the hog program is available in the technical report <NO>).', 'Rafique (23) proposes an OS scheme that consists of a hardware cache quota management mechanism, an OS interface and a set of OS level quota orchestration policies for greater flexibility in policies.', 'The majority of work required modifications to hardware and falls into one of two camps: performance aware cache modification (most commonly cache-partitioning) <NO> or performance-aware DRAM controller memory scheduling [Loh 2008; Suh et al.', 'The effect of simultaneously executing threads competing for space in the LLC has been explored extensively by many different researchers <NO>.']	Nauman Rafique, Won-Taek Lim, and Mithuna Thottethodi. 2006. Architectural support for operating system-driven CMP cache management. Proc. of the 15th PACT. ACM, 2–12.	The role of the operating system (OS) in managing shared resources such as CPU time, memory, peripherals, and even energy is well motivated and understood [23]. Unfortunately, one key resource—lower-level shared cache in chip multi-processors—is commonly managed purely in hardware by rudimentary replacement policies such as least-recentlyused (LRU). The rigid nature of the hardware cache management policy poses a serious problem since there is no single best cache management policy across all sharing scenarios. For example, the cache management policy for a scenario where applications from a single organization are running under “best effort” performance expectation is likely to be different from the policy for a scenario where applications from competing business entities (say, at a third party data center) are running under a minimum service level expectation. When it comes to managing shared caches, there is an inherent tension between flexibility and performance. On one hand, managing the shared cache in the OS offers immense policy flexibility since it may be implemented in software. Unfortunately, it is prohibitively expensive in terms of performance for the OS to be involved in managing temporally fine-grain events such as cache allocation. On the other hand, sophisticated hardware-only cache management techniques to achieve fair sharing or throughput maximization have been proposed. But they offer no policy flexibility. This paper addresses this problem by designing architectural support for OS to efficiently manage shared caches with a wide variety of policies. Our scheme consists of a hardware cache quota management mechanism, an OS interface and a set of OS level quota orchestration policies. The hardware mechanism guarantees that OS-specified quotas are enforced in shared caches, thus eliminating the need for (and the performance penalty of) temporally fine-grained OS intervention. The OS retains policy flexibility since it can tune the quotas during regularly scheduled OS interventions. We demonstrate that our scheme can support a wide range of policies including policies that provide (a) passive performance differentiation, (b) reactive fairness by miss-rate equalization and (c) reactive performance differentiation.	Our cache management scheme consists of three essential components: a hardware quota enforcement mechanism, an interface between hardware and OS, and a set of OS level policies. We first describe two hardware quota enforcement mechanisms in Section 2.1, their advantages and limitations. Section 2.2 describes the OS interface of our scheme and finally Section 2.3 explains how the OS can use this interface to implement different policies.	https://doi.org/10.1145/1152154.1152160	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Rafique, et al. [NO] addresses this problem by designing architectural support for os to efficiently manage shared caches with a wide variety of policies. 
Reconfigurable caches and their application to media processing	['The other kinds of work <NO> on this topic dynamically adjust the cache for performance benefits.']	Parthasarathy Ranganathan, Sarita Adve, and Norman P. Jouppi. 2000. Reconfigurable caches and their application to media processing. Proc. of the 27th ISCA. ACM, 214–224.	High performance general-purpose processors are increasingly being used for a variety of application domains { scienti c, engineering, databases, and more recently, media processing. It is therefore important to ensure that architectural features that use a signi cant fraction of the on-chip transistors are applicable across these di erent domains. For example, current processor designs often devote the largest fraction of on-chip transistors (up to 80%) to caches. Many workloads, however, do not make e ective use of large caches; e.g., media processing workloads which often have streaming data access patterns and large working sets. This paper proposes a new recon gurable cache design. This design enables the cache SRAM arrays to be dynamically divided into multiple partitions that can be used for di erent processor activities. These activities can bene t applications that would otherwise not use the storage allocated to large conventional caches. Our design involves relatively few modi cations to conventional cache design, and analysis using a modi cation of the CACTI analytical model shows a small impact on cache access time. We evaluate one representative use of recon gurable caches { instruction reuse for media processing. We nd this use gives IPC improvements ranging from 1.04X to 1.20X in simulation across eight media processing benchmarks. 	"Some of the possible applications for recon gurable caches are discussed below. We speci cally discuss how these applications are relevant to the domain of media processing; however, codes from other domains can bene t from these as well.
Hardware optimizations using lookup tables or bu ers. Several hardware optimizations have been proposed that re-
quire maintaining lookup tables or bu ers, where the effectiveness of the optimization improves signi cantly with larger table sizes. For example, value prediction, memoization, and instruction reuse have recently been studied to exploit redundancy in computation in the SPEC benchmarks <NO>. Other optimizations that require large lookup tables or bu ers include coherence prediction, memory disambiguation prediction, compression-based branch prediction, hardware prefetching (where lookup tables are used to store information for address prediction), and dynamic optimizations triggered by performance information collected and stored in tables at runtime. Several of these techniques have been reported to have the capacity to perform better with larger lookup table spaces <NO>. The lookup tables and bu ers for these optimizations could be implemented in a partition of a recon gurable cache instead of using other valuable chip area. Section 5 studies one such technique, instruction reuse, with recon gurable caches to address the computation bottleneck in media processing workloads.
Software and hardware prefetched data. Software and hardware prefetching are widely used techniques to hide memory latency. However, if the prefetched data is fetched too far in advance, it can pollute the cache replacing other useful data or be replaced before use by a demand access, eliminating any performance bene ts. On the other hand, prefetches that occur too late do not fully hide the latency. Therefore, prefetching techniques need to strike a careful balance when scheduling the prefetches, but are often unsuccessful in doing so. With recon gurable caches, a separate partition can be used to prefetch data early while avoiding the problem of cache pollution or replacement of prefetched data. Such an application of recon gurable caches could be particularly useful with media processing benchmarks which often have streaming behavior <NO>.
Compiler or application controlled memory. A partition of a recon gurable cache could be con gured as compiler or application controlled memory. As discussed in <NO>, the compiler could use such memory as a scratch area for spill code to achieve performance bene ts. Alternatively, this area can be used by system code or device drivers as a separately addressable bu er area. Such a use may also be bene cial in ensuring real-time requirements of media applications with general-purpose processors. Many DSP processors hardwire their on-chip SRAM to be used as memory (as opposed to caches) to ensure predictability of memory latencies <NO>. Cache line locking (as in the Cyrix MediaGX processor <NO>) or controlled cache line replacement (as with malleable caches <NO>) can provide the same functionality."	https://doi.org/10.1145/339647.339685	2	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['workload', 'priority', 'platform']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Ranganathan, et al. [NO] proposes a new recon gurable cache design. 
Predictable task migration for locked caches in multi-core systems	[]	Abhik Sarkar, Frank Mueller, and Harini Ramaprasad. 2011. Predictable task migration for locked caches in multi-core systems. Proc. of the LCTES’11. ACM, 131–140.	Locking cache lines in hard real-time systems is a common means of achieving predictability of cache access behavior and tightening as well as reducing worst case execution time, especially in a multitasking environment. However, cache locking poses a challenge for multi-core hard real-time systems since theoretically optimal scheduling techniques on multi-core architectures assume zero cost for task migration. Tasks with locked cache lines need to proactively migrate these lines before the next invocation of the task. Otherwise, cache locking on multi-core architectures becomes useless as predictability is compromised. This paper proposes hardware-based push-assisted cache migration as a means to retain locks on cache lines across migrations. We extend the push-assisted migration model with several cache migration techniques to efficiently retain locked cache lines on a bus-based chip multi-processor architecture. We also provide deterministic migration delay bounds that help the scheduler decide which migration technique(s) to utilize to relocate a single or multiple tasks. This information also allows the scheduler to determine feasibility of task migrations, which is critical for the safety of any hard real-time system. Such proactive migration of locked cache lines in multi-cores is unprecedented to our knowledge.	"In the past decade, there has been considerable research on cache line locks in the context of multi-tasking real-time systems. Static and dynamic cache locking algorithms for instruction caches have been proposed to improve system utilization in <NO>. Data cache locking techniques that pin data when cache behavior is hard to analyze statically have been proposed <NO>. Past work presented techniques for cache locking that provides comparable performance to scratchpad allocation <NO>. Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>. Of course, cache locking can also be used in conjunction with private L2 caches. Multi-cores certainly make cache locking even more attractive in terms of real-time predictability.
Choffnes et al. propose migration policies for multi-core fairshare scheduling <NO>. Their technique strives to minimize migration costs while ensuring fairness among the tasks by maintaining balanced scheduling queues as new tasks are activated. The work is in the context of soft real-time systems while ours focuses on hard real-time. Calandrino et al. propose scheduling techniques that account for co-schedulability of tasks with respect to cache behavior <NO>. Their approach is based on organizing tasks with the same period into groups of cooperating tasks. While their method improves cache performance in soft real-time systems, they do not specifically address issues related to task migration. Li et al. discuss migration policies that facilitate efficient operating system scheduling in asymmetric multi-core architectures <NO>. Their work focuses on fault-and-migrate techniques to handle resource-related faults in heterogeneous cores and does not consider real-time constraints. Eisler et al. <NO> develop a cache capacity increasing scheme for multi-cores that scavenges unused neighboring cache lines. They consider “migration” of cache lines amounting to distribution of data in caches while we focus on task migration combined with data migration mechanisms that keep data local to the target core and retains the locks in caches across migration.
Acquaviva et al. <NO> assess the cost of task migration for soft real-time systems. They assume private memory and different operating system instances per core on a low-end processor. In contrast, we assume private caches with a single operating system instance, which more accurately reflects contemporary embedded multi-cores <NO>. Their focus is on task replication and re-creation across different memory spaces while our work focuses on task migration within part shared, part private memory spaces. Hardy et al. have recently proposed static cache analysis techniques to quantify cache-related migration delay cost on multi-cores by estimating re-use of cache lines that cause cache misses <NO>. Our methodology focuses upon eliminating migration delay to support cache line locking in multi-cores and providing support for deterministic migration delay for locked cache lines."	https://doi.org/10.1145/1967677.1967696	3	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking', 'locked', 'core']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Sarkar, et al. [NO] proposes hardware-based push-assisted cache migration as a means to retain locks on cache lines across migrations. 
Static task partitioning for locked caches in multi-core real-time systems	[]	Abhik Sarkar, Frank Mueller, and Harini Ramaprasad. 2012. Static task partitioning for locked caches in multi-core real-time systems. Proc. of the CASES’12. ACM, 161–170.	Locking cache lines in hard real-time systems is a common means to ensure timing predictability of data references and to lower bounds on worst-case execution time, especially in a multi-tasking environment. Growing processing demand on multi-tasking real-time systems can be met by employing scalable multi-core architectures, like the recently introduced tile-based architectures. This paper studies the use of cache locking on massive multi-core architectures with private caches in the context of hard real-time systems. In shared cache architectures, a single resource is shared among all the tasks. However, in scalable cache architectures with private caches, conflicts exist only among the tasks scheduled on one core. This calls for a cache-aware allocation of tasks onto cores. Our work extends the cache-unaware First Fit Decreasing (FFD) algorithm with a Naive locked First Fit Decreasing (NFFD) policy. We further propose two cache-aware static scheduling schemes: (1) Greedy First Fit Decreasing (GFFD) and (2) Colored First Fit Decreasing (CoFFD). This work contributes an adaptation of these algorithms for conflict resolution of partially locked regions. Experiments indicate that NFFD is capable of scheduling high utilization task sets that FFD cannot schedule. Experiments also show that CoFFD consistently outperforms GFFD resulting in lower number of cores and lower system utilization. CoFFD reduces the number of core requirements from 30% to 60% compared to NFFD. With partial locking, the number of cores in some cases is reduced by almost 50% with an increase in system utilization of 10%. Overall, this work is unique in considering the challenges of future multicore architectures for real-time systems and provides key insights into task partitioning with locked caches for architectures with private caches.	"In the past decade, there has been considerable research promoting locked caches in the context of multi-tasking realtime systems. Static and dynamic cache locking algorithms for instruction caches have been proposed to improve system utilization in <NO>. Several methods have been developed to lock program data that is hard to analyze statically <NO>. Further techniques have been developed for cache locking that provide performance comparable to that obtained with scratchpad allocation <NO>. Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>. This trend is a strong proponent of cache locking as a viable solution in future real-time system designs on multi-cores.
Choffnes et al. have proposed migration policies for multicore fair-share scheduling <NO>. Their technique strives to minimize migration costs while ensuring fairness among the tasks by maintaining balanced scheduling queues as new tasks are activated. Calandrino et al. propose scheduling techniques that account for co-schedulability of tasks with respect to cache behavior <NO>. Their approach is based on organizing tasks with the same period into groups of cooperating tasks. All these methods improve cache performance in soft real-time systems. Li et al. discuss migration policies that facilitate efficient operating system scheduling in asymmetric multicore architectures <NO>. Their work focuses on fault-and-migrate techniques to handle resource-related faults in heterogeneous cores and does not operate in the context of real-time systems. Eisler et al. <NO> develop a cache capacity increasing scheme for multicores that scavenges unused neighboring cache lines.
Paolieri et al. <NO> have proposed TDMA-based bus and L2 cache access to improve predictability on multi-core architectures. Their work focuses on supporting hard real-time applications on multi-cores but assumes shared L2 caches with contention due to accesses by different tasks. Ouyang et al. <NO> have proposed extending Quality of Service support to mesh-based interconnects but their study is limited to the on-chip network traffic."	https://doi.org/10.1145/2380403.2380434	3	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking', 'locked', 'core']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Sarkar, et al. [NO] studies the use of cache locking on massive multi-core architectures with private caches in the context of hard real-time systems. 
Adaptive set pinning: Managing shared caches in chip multiprocessors	['<NO> propose adaptive set pinning scheme.', 'The effect of simultaneously executing threads competing for space in the LLC has been explored extensively by many different researchers <NO>.']	S. Srikantaiah, M. Kandemir, and M.J. Irwin. 2008. Adaptive set pinning: Managing shared caches in chip multiprocessors. Proc. of the 13th ASPLOS. ACM, 135–144.	As part of the trend towards Chip Multiprocessors (CMPs) for the next leap in computing performance, many architectures have explored sharing the last level of cache among different processors for better performance–cost ratio and improved resource allocation. Shared cache management is a crucial CMP design aspect for the performance of the system. This paper first presents a new classification of cache misses – CII: Compulsory, Inter-processor and Intra-processor misses – for CMPs with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a CMP. We then propose a novel approach, called set pinning, for eliminating inter-processor misses and reducing intra-processor misses in a shared cache. Furthermore, we show that an adaptive set pinning scheme improves over the benefits obtained by the set pinning scheme by significantly reducing the number of off–chip accesses. Extensive analysis of these approaches with SPEComp 2001 benchmarks is performed using a full system simulator. Our experiments indicate that the set pinning scheme achieves an average improvement of 22.18% in the L2 miss rate while the adaptive set pinning scheme reduces the miss rates by an average of 47.94% as compared to the traditional shared cache scheme. They also improve the performance by 7.24% and 17.88% respectively.	"The motivation for our classification springs from the example transactions depicted in Figure 1. Consider a CMP with two processors, P1 and P2 and a fully associative shared L2 cache. Exam-
Replaced (P1)Referenced (P1)
Inter Processor MissNever Referenced
Referenced (P2)
Cache Hit
Replaced (P2)
Cache Hit Intra Processor Miss
Intra Processor Miss
Cold Miss
Cold Miss
P1 Ref P2 Ref P1 Replacement P2 Replacement Any
Figure 2. State diagram representing a memory element’s life cycle in the shared cache. Non–compulsory misses are classified based on the processor responsible for evicting the referenced block. A non–compulsory miss is classified as Intra-processor miss if it was evicted by the same processor that brought it into the cache and Inter-processor miss if it was evicted by other processors.
ples (a) and (b) in Figure 1 show two possible types of transactions that could result in a miss in a shared cache. Example (a) depicts a traditional capacity miss where the same processor P1 is responsible for both first reference and eviction of the memory element X. Example (b) also depicts a miss by P1, that occurs to a memory element X. The difference here is that X was brought into the cache by an earlier reference by P1, but was evicted only because of a reference to a different memory element Y that mapped to the same cache block as X by P2.
Clearly, we would fail to understand the inherent differences in the cause for such misses, by classifying both of these misses as “capacity misses” (as in the 3C miss classification). The same is true with conflict misses. We classify the misses similar to that shown in (a) of Figure 1 as Intra-processor misses and ones similar to that shown in (b) as Inter-processor misses. Thus our classification, CII, classifies the cache misses in a CMP with a shared cache
into compulsory misses, intra-processor misses and inter-processor misses.
In order to present a more formal understanding of the CII classification, we can represent the life cycle of a memory element as shown in the state diagram in Figure 2. This diagram depicts the life cycle of a memory element in the shared cache during the execution of a program accessing it, assuming the program is executing on a dual core CMP. The same idea is easily extensible to any number of processors. As seen from Figure 2, the memory element under consideration is initially in the Never Referenced state. The first access by P1 or P2 causes a compulsory (cold) miss and the memory element enters the Referenced state for the first time in the life cycle. Any subsequent references (by any processor) to a memory element in the Referenced state leads to a cache hit. Further, a replacement of the cache block takes the memory element into the Replaced state. We tag the memory element with the id of the processor which replaced the element. For instance, a memory element which is evicted from the cache as a result of a reference from P1 is in Replaced(P1) state. It is evident that all non-compulsory cache misses to a memory element occur when it is in the Replaced state. Our classification of the non-compulsory misses is based on whether the cache miss is occurring because of the block being replaced (at an earlier point in time) by the same processor or a different processor. This is deciphered by comparing the processor facing the miss with the tag of the memory element in the Replaced state.
It is important to note that the classification of non-compulsory misses into intra-processor misses and inter-processor misses in the CII classification is orthogonal to the classification of the same as capacity and conflict misses. For instance, the examples discussed with reference to Figure 1, in case of a fully associative cache, represent (a) capacity miss that is also an intra-processor miss and (b) capacity miss that is also an inter-processor miss. Conflict misses can also be classified as intra-processor misses and interprocessor misses by the CII classification. Our CII classification is more expressive; and more importantly, it is able to model the interactions between transactions of multiple processors at the level of the shared cache.
We measured the distribution of various classes of misses in the CII classification. Figure 3 plots the distribution of compulsory, inter-processor and intra-processor misses in our base system configuration (see Section 5 for a detailed description of our baseline configuration). The black portion of the stacked bars represents the inter-processor misses, the spotted portion (in the middle) represents intra-processor misses and the striped portion represents the compulsory misses. On an average, 40.3% of the misses are interprocessor misses, 24.6% of the misses are intra-processor misses and the remaining 35.1% are compulsory misses.
Characterization of CII classification. We vary L2 cache size, L2 cache associativity and the number of processors individually from the baseline configuration in order to measure their impact on the distribution of various classes of CII cache misses. Graphs (a) and (b) of Figure 4 plot the normalized miss rates of various benchmarks by varying the size and associativity of the shared L2 cache. The miss rates are normalized to the total L2 cache miss rates of the respective benchmarks in the base configuration. Although the number of inter-processor and intra-processor misses in the L2 cache tend to decrease with increasing associativity and the size of the L2 cache, the contribution of inter-processor misses to the non–compulsory misses (as a percentage of non–compulsory misses) increases and that of intra-processor misses decreases as associativity and size increase. We also studied the impact of the number of processors on the CII classification. In order to have a fair comparison (the private L1 caches of a different number
of processors lead to a different number of total L2 accesses) after varying the number of processors, we plot the variation in distribution of CII misses with the number of processors in Graph (c) of Figure 4. The contribution of inter-processor misses also increases with increasing number of processors. This is clearly due to the increased interaction between memory transactions of a higher number of processors in the shared L2 cache.
Reducing off–chip accesses is the key to a successful shared cache management scheme in a CMP with large shared L2/L3 cache (16). The effect of compulsory misses can be reduced by hiding their latency. This can be achieved by prefetching data into the cache before it is accessed. There have been many recent studies for reducing memory bandwidth and the number of off–chip accesses through hardware/software data prefetching (17; 24; 27; 34). The focus of this paper is on developing techniques to reduce inter-processor and intra-processor misses."	https://doi.org/10.1145/1346281.1346299	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Srikantaiah, et al. [NO] first presents a new classification of cache misses – cii : compulsory, inter-processor and intra-processor misses – for cmps with shared caches to provide a better understanding of the interactions between memory transactions of different processors at the level of shared cache in a cmp. 
Exploring locking & partitioning for predictable shared caches on multi-cores	['Another study on cache locking for shared caches has assumed locking individual cache lines <NO>.', 'Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>.', 'They can be grouped in three main categories: cache partitioning, cache locking, and page coloring <NO>.', 'In <NO>, cache locking is explored for predictable shared caches on multi-core systems.', 'Locking has also been applied to shared caches in multi-cores in <NO>.', 'Recently, cache locking has also been proposed for multi-core systems that use shared L2 caches <NO>.', 'A very different approach for multi-cores with shared instruction caches is proposed in <NO> and is based on the combined use of cache locking, i.', 'In contrast to <NO>, our approach does neither lock nor partition the shared instruction cache(s).', 'Experiments would be required to assess the respective merits of <NO> compared with our approach.']	Vivy Suhendra, and Tulika Mitra. 2008. Exploring locking & partitioning for predictable shared caches on multi-cores. Proc. of the 45th DAC. ACM, 300–303.	Multi-core architectures consisting of multiple processing cores on a chip have become increasingly prevalent. Synthesizing hard realtime applications onto these platforms is quite challenging, as the contention among the cores for various shared resources leads to inherent timing unpredictability. This paper proposes the use of shared cache in a predictable manner through a combination of locking and partitioning mechanisms. We explore possible design choices and evaluate their effects on the worst-case application performance. Our study reveals certain design principles that strongly dictate the performance of a predictable memory hierarchy.	"We consider a multi-core architecture consisting of identical cores. The on-chip memory is configured as two-level caches with a shared L2 cache, which is the focus of this work. We assume that the cache coherence is implemented in hardware, and that the caches support locking and set-based partitioning <NO> that allocates a number of sets (rows) to each task. This paper focuses on instruction cache, though our technique is equally applicable to data caches.
We adopt the classic real-time system model where a set of independent tasks {T1, T2, . . . , TK} execute periodically. Each task Ti is associated with a period pi, which defines its deadline, and a worst-case execution time ci. We choose partitioning <NO> strategy for homogeneous multiprocessor scheduling. In a partitioning strategy, once a task is allocated to a processor, it is executed exclusively on that processor. Any uniprocessor scheduling algorithm can then be applied on each processor. Partitioning strategy has the advantage of lower overhead compared to global strategy that allows migration of a task to a different processor at runtime.
López et al. <NO> show that the earliest deadline first (EDF) scheduling policy with First Fit (FF) allocation is an optimal partitioning approach with respect to utilization bounds. Our framework applies this policy. FF assigns a task to the first processor that can accept it. A task set is EDF-schedulable on uniprocessor if U ≤ 1, where U is the utilization of a task set {T1, T2, . . . , TK} given by U = ∑K i=1 ci pi
. The system utilization of a Q-core multiprocessor is Usystem = UQ . We measure the performance of a task set on a multiprocessor by the system utilization: the lower, the better.
We separate the treatment of the private L1 caches and the shared L2 cache, in order to observe the shared cache behavior while abstracting out the effects of the L1 caches. As our focus is on the shared cache, we choose a simple static locking scheme for L1. The private L1 cache attached to a core is utilized only by the tasks executing on that core; for each, we adopt the cache content selection algorithm for multitasking systems <NO>. The chosen blocks for L1 will be excluded during content selection for the L2 cache.
The shared L2 cache opens up the opportunity to combine different locking and partitioning schemes as shown in Figure 1(e). For cache locking, we can choose a static scheme (cache content remains unchanged throughout execution) or a dynamic scheme (cache content can be reloaded at runtime) For cache partitioning, we have the choice of (1) no partition, where a cache block may be occupied by any task, scheduled on any core; (2) task-based partition, where each task is assigned a portion of the cache; or (3) core-based partition, where each core is assigned a portion of the cache, and each task scheduled on that core may occupy the whole portion while it is executing. From these, the {dynamic locking, no partition} combination must be ruled out, because dynamic locking strictly requires a dedicated partition. Further, both the {static locking, no partition} (SN) and the {static locking, task-based partition} (ST) schemes lock the cache contents chosen from all tasks in the application throughout execution, but SN offers more flexibility by not enforcing a concrete boundary. Thus ST is either inferior or at most as good as SN; we eliminate ST from our evaluation.
Figure 1(a–d) illustrates the four eligible possibilities, applied on a multi-core with 2 processing elements (PE1, PE2) and 4 independent tasks (T1, . . . T4). The scheduler assigns T1, T2 to PE1 and T3, T4 to PE2. T1 and T4 are each divided into two regions for dynamic cache locking. We assume a 2-way set-associative shared L2 cache with 8 sets.
Static Locking, No Partition (SN). This is the simplest scheme where the cache content is kept unchanged throughout application runtime (Figure 1(a)). A cache block can be assigned to any task irrespective of the processor it is scheduled on. This scheme offers maximum flexibility; however, its performance is restricted if the code size of all the tasks together far exceeds the L2 cache size. For static locking, we apply the cache content selection algorithm presented in <NO>, which minimizes the system utilization.
Static Locking, Core-based Partition (SC). On a system with preemptive scheduling, only memory blocks belonging to the “active"" tasks are useful at any time. When a task Ti is preempted by Ti′ on one of the cores, we can replace Ti’s memory blocks in the cache with those of Ti′ ’s. This comes at the cost of reloading the cache at every preemption. This scheme requires the cache to be partitioned among the cores, as each core has an active task at any point of time and the cores invoke preemptions independently. However, when a task runs on a core, it can occupy the entire partition for that core. In Figure 1(b), PE1 gets the first four sets; PE2 gets the rest. Initially T1 occupies PE1’s partition and T4 occupies PE2’s partition. When T2 preempts T1, it loads and locks PE1’s partition with its own content. We adapt a dynamic programming based optimal partitioning algorithm <NO> here.
Dynamic Locking, Task-based Partition (DT). Dynamic locking allows more memory blocks to fit in the cache via runtime load and lock. The overhead is the cache reload cost every time the execution of a task moves from one region to another. As different tasks have different region formations, the cache is first partitioned among the tasks. Each task then performs dynamic locking within its partition. Figure 1(c) shows the scheme at work for T1 and T4. In contrast to SC, reloading is performed intra-task. No intertask reloading is required as the partitioning prevents interference among the tasks, thus preemptions incur no cache reload overhead. However, if the application comprises a large number of tasks, such rigid partitioning might not be effective. DT also suffers from the same drawback as SN: tasks occupy cache blocks even when they are inactive (preempted). We employ the dynamic locking algorithm in <NO> here.
Dynamic Locking, Core-based Partition (DC). In this most complex scheme, reloading is supported within a task in addition to reloading at preemption (see Figure 1(d)). Initially, the cache is loaded with region 1 of T1 and region 1 of T4. As time progresses, T1’s execution (on PE1) moves to region 2, which then replaces the content of PE1’s portion of the cache. Later on, a preemption brings into the cache the content associated with T2. However, when T1 resumes, it again brings in region 2 into the cache."	https://doi.org/10.1145/1391469.1391545	3	['conflict_miss', 'first_fit', 'fit_decreasing', 'cache_line', 'architectures_private']	['locking', 'locked', 'core']	['locking_cache_lines', 'first_fit_decreasing', 'architectures_private_caches', 'conflict_miss_rate', 'multi_core_architectures']	Suhendra, et al. [NO] proposes the use of shared cache in a predictable manner through a combination of locking and partitioning mechanisms. 
Dynamic allocation for scratch-pad memory using compile-time decisions	['Approaches for static <NO> and dynamic <NO> allocations have been designed to automatically place code and data on scratchpad memories.', 'Previous approaches for dynamic scratchpad allocation <NO> have focused on the optimization of the average case.', '<NO> proposes to restrict memory transfer operations to interesting program points, such as functions, conditionals or loops entries/exits with high execution frequencies in a flexible way.', 'Moreover, <NO> associates execution timestamps to program points in order to capture program execution context.', '<NO> has shown that such a mechanism has a minor impact on program performance.', '<NO> also addresses major implementations issues on static data and stack data relocation for dynamic scratchpad memory allocation.', 'This idea has been successfully applied within tasks for SPM <NO>, <NO>.', '<NO>, <NO>) could have done some of the required work.']	S. Udayakumaran, A. Dominguez, and R. Barua. 2006. Dynamic allocation for scratch-pad memory using compile-time decisions. ACM Transactions on Embedded Computing Systems (TECS) 5, 2 (May 2006).	In this research, we propose a highly predictable, low overhead, and, yet, dynamic, memoryallocation strategy for embedded systems with scratch pad memory. A scratch pad is a fast compilermanaged SRAM memory that replaces the hardware-managed cache. It is motivated by its better real-time guarantees versus cache and by its significantly lower overheads in energy consumption, area, and overall runtime, even with a simple allocation scheme. Primarily scratch pad allocation methods are of two types. First, software-caching schemes emulate the workings of a hardware cache in software. Instructions are inserted before each load/store to check the software-maintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption, and SRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of algorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation schemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that never changes at runtime cannot achieve the full locality benefits of a cache. We propose a dynamic allocation methodology for global and stack data and program code that; (i) accounts for changing program requirements at runtime, (ii) has no software-caching tags, (iii) requires no runtime checks, (iv) has extremely low overheads, and (v) yields 100% predictable memory access times. In this method, data that is about to be accessed frequently is copied into the scratch pad using compiler-inserted code at fixed and infrequent points in the program. Earlier data is evicted if necessary. When compared to a provably optimal static allocation, results show that our scheme reduces runtime by up to 39.8% and energy by up to 31.3%, on average, for our benchmarks, depending on the SRAM size used. The actual gain depends on the SRAM size, but our results show that close to the maximum benefit in runtime and energy is achieved for a substantial range of small SRAM sizes commonly found in embedded systems. Our comparison with a direct mapped cache shows that our method performs roughly as well as a cached architecture.	"Memory systems generally are organized using a variety of devices, which serve different purposes. Devices like SRAM and ROM are fast, but expensive. On the other hand, devices like DRAM and tape drives are slower, but, being cheaper, can be used to provide capacity. Designing a memory system, therefore, involves using small amounts of faster devices like SRAM along with slower devices like DRAM to obtain satisfactory performance, while keeping a check on the overall dollar cost.
In desktops, the usual approach to adding SRAM is to configure it as a hardware cache. The cache dynamically stores a subset of the frequently used data. Caches have been a big success for desktops, a trend that is likely to continue in the future. Using noncached SRAM or scratch-pads is usually not feasible for desktops; one reason is the lack of binary code portability. Scratch-pad code is not portable across different sizes of scratch-pad because all existing compiletime methods for data1 allocation to scratch-pad (including the method in this paper) require that the scratch-pad size be known; otherwise, they cannot reason about what variables will fit in the scratch-pad. This contrasts with cache allocation, which is decided only at runtime and, hence, does not require compiletime knowledge of the size of cache. Binary portability is valuable for desktops, where independently distributed binaries must work on any cache size. However, in embedded systems, software is configured along with the hardware in the factory and rarely changes thereafter. Thus embedded system designers can afford to customize the SRAM to a particular size to reap the additional cost savings from customization.
For embedded systems, the serious overheads of caches are less defensible. An alternative that is, instead, prevalent is to use compiler-managed SRAM or scratch-pad. Studies <NO> have shown that scratch-pad’s use 34% lesser area and consume 40% lower power than a cache of the same capacity. These savings are significant, since the on-chip cache typically consumes 25–50% of the processor’s area and energy consumption, a fraction that is increasing with time <NO>. Given the power, cost, performance, and real-time advantages of scratch-pad, it is not surprising that scratch-pads are the most common form of SRAM in embedded CPUs today. Some examples of processors with scratch-pad memory are Intel IXP network processor, ARMv6, IBM 440 and 405, Motorola’s MCORE and 6812, and TI TMS-370. Trends in recent embedded designs indicate that the dominance of scratch-pad will likely consolidate further in the future <NO>.
Although many embedded processors with a scratch-pad exist, using the scratch-pad effectively has been a challenge. Central to the effectiveness of caches is their ability to maintain, at each time during program execution, the subset of data that is frequently used at that time in fast memory. The contents of cache constantly change during runtime to reflect the changing working set of data across time. Unfortunately, two of the existing allocation
1We use the terms data and program objects to broadly refer to both program code and program data. approaches for scratch-pad—program annotations and the recent compilerdriven approaches <NO>—are static allocators, i.e., they do not change the contents of scratch-pad at runtime. This is a serious limitation. For example, consider the following thought experiment. Let a program consist of three successive loops, the first of which makes repeated references to array A; the second to B; and the third to C. If only one of the three arrays can fit within the scratch-pad, then any static allocation suffers DRAM accesses for two out of three loops. In contrast, a dynamic strategy can fit all three arrays in the scratch-pad at different times. Although this example is oversimplified, it intuitively illustrates the benefits of dynamic allocation.
This research presents a new compiler method for allocating three types of program objects—global variables, stack variables, and program code—to scratch-pad that is able to change the allocation at runtime and avoid the overheads of runtime methods. A preliminary version of our method published in Udayakumaran and Barua <NO> was the first such method to allocate global and stack data using whole program analysis. Our method (i) accounts for changing program requirements at runtime, (ii) has no tags like that used by runtime methods, (iii) requires no runtime checks per load/store, (iv) has extremely low overheads, and (v) yields 100% predictable memory access times.
Our method is outlined as follows. The compiler analyzes the program to identify locations we call program points where it may be beneficial to insert code to copy a variable from DRAM into the scratch-pad. It is beneficial to copy a variable into scratch-pad if the latency gain from having it in scratchpad rather than DRAM is greater than the cost of its transfer. A profile-driven cost model estimates these benefits and costs. The compiler ensures that the program data allocated to scratch-pad fits at all times by occasionally evicting existing variables in scratch-pad to make space for incoming variables. In other words, just like in a cache, data is moved back and forth between DRAM and scratch-pad, but under compiler control, and with no additional overhead.
Key components of our method are as follows: (i) to reason about the contents of scratch-pad across time, it helps to attach a concept of time to the abovedefined program points. Toward this end, we introduce a new data structure called the Data-Program Relationship Graph (DPRG), which associates a timestamp with each program point. As far as we know, this is the first time that a static data structure to represent program execution time has been defined. (ii) A detailed cost model is presented to estimate the runtime cost of any proposed data transfer at a program point. (iii) A compile-time heuristic is presented that uses the cost model to decide which transfers minimize the runtime. The well-known data-flow concept of liveness analysis <NO> is used to eliminate unnecessary transfers—probably dead variables2 are not
2In compiler terminology, a variable is dead at a point in the program if the value in it is not used beyond this point, although the space could be. A dead variable becomes live later if it is written to with subsequently used data. As a special case it is worth noting that every uninitialized variable is dead at the beginning of the program. It becomes live only when written to first. Further, a variable may have more than one live range separated by times when it is dead. copied back to DRAM, nor are newly alive variables in this region copied in from DRAM to SRAM.3 In programs, where the final results (only global) need to be left in the memory itself, this optimization can be turned off, in which case the benefits would be reduced.4 This optimizations also needs to be turned off for segments shared between tasks.
We observe three desirable features of our algorithm: (i) No additional transfers beyond those required by a caching strategy are done; (ii) data that is accessed only once is not brought into the scratch-pad, unlike in caches, where the data is cached and potentially useful data evicted. This is particularly beneficial for streaming multimedia codes where use-once data is common. (iii) Data that the compiler knows to be dead is not written out to DRAM upon eviction, unlike in a cache, where the caching mechanism writes out all evicted data.
Our method is clearly profile-dependent; that is, its improvements are dependent upon how representative the profile data set really is. Indeed, all existing scratch-pad allocation methods, whether compiler-derived or programmerspecified, are inherently profile-dependent. This cannot be avoided since they all need to predict which data will be frequently used. Further our method does not require the profile data to be like the actual data in all respects—so long as the relative reuse trends between variables are similar in the profile and actual data, good allocation decisions will be made, even if the reuse factors are not identical. A regions gain may even be higher with nonprofile data if its data reuse is more than in the profile data."	https://doi.org/10.1145/1151074.1151085	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Udayakumaran, et al. [NO] proposes a highly predictable, low overhead, and, yet, dynamic, memoryallocation strategy for embedded systems with scratch pad memory. 
Molecular Caches: A caching structure for dynamic creation of application-specific Heterogeneous cache regions	[]	K. Varadarajan, S.K. Nandy, V. Sharda, A. Bharadwaj, R. Iyer, S. Makineni, and D. Newell. 2006. Molecular Caches: A caching structure for dynamic creation of application-specific Heterogeneous cache regions. Proc. of the 39th MICRO. 433–442.	CMPs enable simultaneous execution of multiple applications on the same platforms that share cache resources. Diversity in the cache access patterns of these simultaneously executing applications can potentially trigger inter-application interference, leading to cache pollution. Whereas a large cache can ameliorate this problem, the issues of larger power consumption with increasing cache size, amplified at sub-100nm technologies, makes this solution prohibitive. In this paper, in order to address the issues relating to power-aware performance of caches, we propose a caching structure that addresses the following: 1. Definition of application-specific cache partitions as an aggregation of caching units (molecules). The parameters of each molecule namely size, associativity and line size are chosen so that the power consumed by it and access time are optimal for the given technology. 2. Application-Specific resizing of cache partitions with variable and adaptive associativity per cache line, way size and variable line size. 3. A replacement policy that is transparent to the partition in terms of size, heterogeneity in associativity and line size. Through simulation studies we establish the superiority of molecular cache (caches built as aggregations of molecules) that offers a 29% power advantage over that of an equivalently performing traditional cache.	"This section explains in brief some of the related work that has been carried out in the context of QoS in caches and Cache Partitioning.
Iyer <NO> in a recent publication presented a framework for enabling QoS in shared caches. The paper makes a key observation that the current design of caches is more suited towards single application memory access and is not well suited for newer computing paradigms such as CMPs, Simultaneous Multi-Threading (SMT) and Specialized Cores. The paper then proposes a framework for Cache QoS management that includes three aspects namely Priority Classification, Priority Assignment and Priority Enforcement. The Priority Enforcement is of particular interest to us. For priority enforcement three different schemes are proposed. These include static and dynamic partitioning of caches, selective cache allocation and heterogeneous cache regions. 
We only elaborate the technique for heterogeneous cache regions in this paper. In this technique, caches are composed of multiple caching units with different organizational structures and policies. The problem is that of assigning the right application to the appropriate cache structure, keeping in mind the priority of the application and its memory access pattern. The results from the paper indicates that heterogeneous cache regions help reduce dedicated cache size.
Cache Partitioning has been greatly researched, even before the arrival of CMPs. Of the several flavours of cache partitioning, multi-application dynamic partitioning techniques are of particular interest to us. Single application partitioning for low power in the context of direct mapped caches is reported in <NO>. In this section we discuss examples of both the hardware controlled and the software controlled variants. Several solutions have been proposed that address the issue of cache pollution through the use of NUCA architectures. Techniques such as Victim Replication <NO>, NuRAPID <NO>, CMP-NuRAPID<NO> address several different aspects of these. However none of these use application QoS parameters as the guiding principle. They attempt to reduce the miss rate experienced by a processor.
Suh et al <NO> propose the use of two different techniques for implementing partitioned caches namely Modified LRU and Column Caching. In Modified LRU scheme, the replacement decision depends on the number of cache blocks already allocated to a particular process. If the process has not exceeded its predefined space threshold, a global replacement is performed, else a local replacement is performed. The column caching approach restricts some processes to place data in some ’columns’ (i.e. ways) of a multi-way associative cache. Their results indicate that for a cache of size 1-2MB the hit rate improves by 40% as compared to standard LRU for a time quantum of 200,000 memory references and weighting factor of 0.5 (which is used to give preference to recent measurements as opposed to older ones). Compared with the standard LRU, relative IPC improvements of 13.29% for 2 processes on a 2MB cache and 14.21% for 4 processes on 2 MB cache are recorded. Kim et al <NO> propose a variant of the Modified LRU technique where fairness is taken as a measure of cache allocation. As indicated in <NO>, cache pollution violates operating system expectation that all scheduled entities get an equal chance to utilize the processor, hence fairness of allocation is required. We take this argument forward, since fairness weighted by application’s priorities is far more important. Yet another technique that takes fairness into consideration was proposed by Yeh et al <NO>. The approach used in this technique is very similar to our approach. The authors propose a phase-based approach for cache partition resizing.
Kim et al <NO> proposed a partitioning technique in which different banks of a multi-banked cache are allocated to
different processes. First a search of the ’home’ bank is performed, failing which the entire cache is searched in a set-associative manner. Called Process Ownership-based Cache Architecture (POCA), this cache contains four components viz. Current Set Number, Set Assignment Table, Data Enable Controller and the Victim Process Table. The authors <NO> used ATUM traces to evaluate the cache. Simulation performed on cache of size from 8KB-1MB with 4 banks indicate that the hit rate of the cache is higher than a direct mapped cache of equal size and is either as good or better than a 4 way associative cache of equal size. The cache access time of POCA is 10% better than an equivalent associative cache.
Is there a need for something better than the state of the art? Suh et al’s <NO> proposed cache partitioning solution does not look into the dimension of heterogeneous cache regions that can potentially improve the efficiency of the cache usage. A major drawback of their cache architecture is the reliance on multi-way associative caches. Multiway associative caches tend to consume a lot of power and have longer access time. Hence associativity cannot be increased beyond a certain value, since the power consumption increases exponentially with increase in associativity (refer <NO>). Kim et al’s <NO> work is a software based resizing solution and relies on operating system based information for determining the victim sets. Such a scheme would not be practically realizable due to the non-standard ways of obtaining the required information from the large variety of operating systems available today.
The work on heterogeneous cache regions by Iyer <NO> is based on the use of multiple caches each with different cache parameters and the optimal assignment of applications to appropriate caches with suitable cache parameters. This technique would only enable some applications to run efficiently. We extend this work by marrying the ideas of dynamic partitioning and heterogeneous cache regions to create more effective dynamic partitions that are dynamically customized to the application’s needs. We create caching structures that allow application dependent dynamic creation and reconfiguration of heterogeneous cache regions with different associativities and line sizes. The capacity of these regions too can be modified. The cache regions are created from homogeneous building blocks that are direct mapped. These cache regions also support nonuniform line associativity."	https://doi.org/10.1109/MICRO.2006.38	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Varadarajan, et al. [NO] proposes a caching structure that addresses the following : 1. definition of application-specific cache partitions as an aggregation of caching units (molecules). 
Data Cache locking for higher program predictability	['In the context of data caches, <NO> tries to balance the performance and predictability tradeoff introduced due to locking, by applying it only on parts of the program that are difficult to analyze statically.', '<NO> combine compile-time cache analysis and data cache locking in order to estimate a safe and tight worst-case memory performance.', 'Instruction cache locking has been primarily employed in hard real time systems for better timing predictability <NO>.', 'The work of <NO> is complementary to this paper since it presents a D-cache locking algorithm for WCET minimization.']	Xavier Vera, Björn Lisper, and Jingling Xue. 2003. Data Cache locking for higher program predictability. Proc. of ACM SIGMETRICS. ACM, New York, NY, 272–282.	Caches have become increasingly important with the widening gap between main memory and processor speeds. However, they are a source of unpredictability due to their characteristics, resulting in programs behaving in a different way than expected. Cache locking mechanisms adapt caches to the needs of real-time systems. Locking the cache is a solution that trades performance for predictability: at a cost of generally lower performance, the time of accessing the memory becomes predictable. This paper combines compile-time cache analysis with data cache locking to estimate the worst-case memory performance (WCMP) in a safe, tight and fast way. In order to get predictable cache behavior, we first lock the cache for those parts of the code where the static analysis fails. To minimize the performance degradation, our method loads the cache, if necessary, with data likely to be accessed. Experimental results show that this scheme is fully predictable, without compromising the performance of the transformed program. When compared to an algorithm that assumes compulsory misses when the state of the cache is unknown, our approach eliminates all overestimation for the set of benchmarks, giving an exact WCMP of the transformed program without any significant decrease in performance.	"Real-time systems rely on the assumption that tasks’ worstcase execution times (WCETs) are known. In order to get an accurate WCET, a tight worst-case memory performance (WCMP) is needed. However, cache behavior is very hard to predict, which leads to an overestimation of the WCMP, and thus for the WCET as well. For this reason, many safety-critical systems such as antilock brake systems do not use caches: it is very hard to prove that the system is reliable under all circumstances. For instance, the ARM966E-S processor does not have a cache in order to have predictable memory timings.
When using caches in hard real-time systems, there is an unacceptable possibility that a high cache miss penalty combined with a high miss ratio might cause a missed deadline, jeopardizing the safety of the controlled system. A system with disabled caches will however waste a lot of resources; not only will the CPU be underutilized but the power consumption will be higher. Memory accesses that fall into the cache are faster and consume less power than accesses to larger or off-chip memories.
Frameworks of WCET prediction are used to ensure that deadlines of tasks can be met. While the computation of WCET in the presence of instruction caches has progressed in such a way that makes it possible to obtain an accurate estimate of the WCET <NO>, there has not been much progress with the presence of data caches. The main problem when dealing with data caches is that each load/store instruction may access multiple memory locations (such as those that implement array or pointer accesses).
Cache locking allows some or all of the contents of the cache to be locked in place. Disabling the normal replacement mechanism, provided that the cache contents are known, makes the time required for a memory access predictable. This ability to lock cache contents is available on several commercial processors (PowerPC 604e <NO>, 405 and 440 families <NO>, Intel-960, some Intel x86, Motorola MPC7400 and others). Each processor implements cache locking in several ways, allowing in all cases static locking (the cache is loaded and locked at system start) and dynamic locking (the state of the cache is allowed to change during the system execution).
Whereas loading and locking the cache offers predictability, it does not guarantee good response time of tasks (thus, we are trading performance for predictability). On the other hand, static cache analysis allows us to predict the WCMP and does not affect the performance. However, static analyses only apply to codes free of data-dependent constructs.
We introduce a method that combines static cache analysis and cache locking in order to achieve both predictability and good performance. Furthermore, it allows computing a WCMP estimate of tasks in a fast and tight way. Our approach first transforms the original program issuing lock/unlock instructions to ensure a tight analysis of the WCMP at static time. In order to keep a high performance, load instructions are added when necessary. Later, the actual computation of the WCMP estimate is performed. We present results for a collection of programs drawn from several related papers in the real-time area <NO>. This collection includes kernels operating on both arrays and scalars, such as SQRT or FIBONACCI. We have also used FFT to show the feasibility of our approach for typical DSP codes. For the sake of concreteness, we present results for a directmapped and a set-associative cache with different cache line sizes. We have chosen the memory hierarchies of two modern processors widely used in the real-time area: microSPARCIIep <NO> and PowerPC 604e <NO>."	https://doi.org/10.1145/781027.781062	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Vera, et al. [NO] combines compile-time cache analysis with data cache locking to estimate the worst-case memory performance (wcmp) in a safe, tight and fast way. 
Making shared caches more predictable on multicore platforms	['While LLC offers high potential for average performance improvement, it challenges worst-case execution time (WCET) estimation, which has made LLC to be studied in the last years by the real-time community <NO>.', 'Software cache partitioning <NO> and hardware cache partitioning <NO> have been used so far to control inter-task interaction in the LLC.', 'Software cache partitioning is done through memory coloring <NO>.']	B.C. Ward, J.L. Herman, C.J. Kenna, and J.H. Anderson. 2013. Making shared caches more predictable on multicore platforms. Proc. of the 25th ECRTS. 157–167.	In safety-critical cyber-physical systems, the usage of multicore platforms has been hampered by problems due to interactions across cores through shared hardware. The inability to precisely characterize such interactions can lead to worst-case execution time pessimism that is so great, the extra processing capacity of additional cores is entirely negated. In this paper, several techniques are proposed and analyzed for dealing with such interactions in the context of shared caches. These techniques are applied in a mixedcriticality scheduling framework motivated by the needs of next-generation unmanned air vehicles.	"In this section, we provide background on scheduling, synchronization, and page coloring that is relevant to our work.
Task model. We consider real-time workloads that can be defined using the implicit-deadline periodic task model and
1Enabling even a quad-core machine to be used in an avionics setting would be a significant innovation.
we assume familiarity with this model. We specifically consider a task system τ = {T1, . . . , Tn}, which is to be scheduled on m processors,2 where task Ti’s period and worstcase execution time (WCET) are denoted pi and ei, respectively. We denote the jobs released by Ti as Ji,1, Ji,2, . . .. (We sometimes omit the job index and let Ji denote an arbitrary job of Ti.) We denote Ti’s utilization by ui = ei/pi. Algorithms for scheduling such a task system may follow a partitioned approach (tasks are statically assigned to processors), a global scheduling approach (any task may execute on any processor), or some hybrid of the two.
MC2. We consider periodic task systems scheduled under the MC2 mixed-criticality framework <NO>. Under mixed-criticality schedulability analysis <NO>, different methods for determining task execution times are assumed to be applied at different criticality levels, with greater pessimism at higher levels. For example, provably correct upper bounds on execution times from timing analysis tools might be assumed at the highest criticality level, while observed worst-case times from profiling might be sufficient at lower levels. When checking real-time correctness, L variants of a system with L criticality levels must be analyzed: in the level-l variant, level-l execution times are assumed for all tasks.
In MC2, four criticality levels exist, denoted A (highest) through D (lowest), as shown in Fig. 1. Higher criticality tasks are statically prioritized over lower criticality ones. Level-A tasks are partitioned and scheduled on each processor using a table-driven cyclic executive. Level-B tasks are also partitioned but are scheduled using a rate-monotonic (RM) scheduler on each processor.3 Level-A and -B tasks are required to be simply periodic (all tasks commence execution at time 0 and periods are harmonic). Level-C tasks are scheduled via a global earliest-deadline-first (G-EDF) scheduler. Level-D tasks are scheduled with no real-time guarantees on a best-effort basis (so we do not consider them further). A task’s execution time at its own criticality level is treated as an operating-system (OS) enforced execution budget: if a job of a task Ti has an execution time exceeding Ti’s budget, then more than one budget allocation will be required to service it. Level-A and -B tasks are HRT, while level-C tasks are SRT (under the “bounded deadline tardiness” definition of SRT <NO>). Most interesting cacherelated issues are exposed by focusing only on levels B and
2We use the terms “processor,” “core,” and “CPU” interchangeably. 3An EDF scheduler can be optionally used at level B.
C (one HRT level and one SRT level). Thus, due to space constraints, we hereafter focus on systems in which only levels B and C are present. We note that systems with two criticality levels have been the predominate focus of prior work on mixed-criticality scheduling (see, e.g., <NO>). Page coloring. The ARM platform used in our experiments has four cores that share an L2 cache. In this paper, we consider page coloring with respect to this cache. The L2 cache on this platform is a 1 MB 8-way set associative cache: it stores contents of physical memory in 32 B units called “lines,” each line of physical memory maps to a particular cache “set,” each such set can store 8 lines (equivalently, there are eight “ways” per set), and in total there are 212 sets. The physical memory of this platform is subdivided into 4 KB pages. To envision the coloring process, consider each page in sequence. For the first page in memory, assign the color “0” to it, and assign the same color to the cache sets to which its contents map. Then, since each page consists of 4KB/(32B/line) = 128 lines, sets 1 − 128 are assigned color 0. Repeat this process, assigning color 1 (mapping to sets 129− 256) to the second page in memory, color 2 (sets 257− 384) to the third page, and so on. Then, after the 32nd page, all 212 sets will have been used and color assignments will “wrap,” i.e., the 33rd page will map to the same cache sets as the first, so we reuse color 0 for it. Continuing this process, each page will be assigned to one of 32 colors. Moreover, two pages that are assigned different colors will map to different cache sets and thus cannot conflict with each other in the cache.
In Sec. 3, we consider techniques that exploit page coloring to eliminate or control cache conflicts. In discussing these techniques, we limit attention to non-shared task data pages, as only these pages are managed in the initial prototype system described in Sec. 4. We define the working set size (WSS) of a task to be the size (in bytes) of the set of data pages it may access in one job, i.e., the size of its perjob working set (WS). We assume that each task’s WSS is at most the size of the shared cache. Multiprocessor real-time locking. Some of the cache management schemes we consider utilize multiprocessor real-time locking protocols. In the protocols we consider, tasks wait by suspending execution. Locking protocols must ensure that priority inversion blocking (pi-blocking) can be analytically bounded. Pi-blocking is the duration of time a job is blocked while a lower-priority job is running. Per-task bounds on pi-blocking are required when analyzing schedulability. We let bi denote the pi-blocking bound for task Ti.
On a multiprocessor system, the actual definition of pi-blocking depends on how schedulability analysis is done <NO>. For some schedulers, suspensions are notoriously difficult to analyze, so suspension-oblivious (s-oblivious) analysis is applied: jobs may suspend, but each ei must be analytically inflated by bi prior to applying a schedulability test to account for lock-related delays. We utilize s-oblivious analysis in this paper. Some of the nuances of such analysis can best be explained by comparing it to suspension-aware (s-aware) analysis, which explicitly ac-
counts for bi and is available for some schedulers. Since suspended jobs are counted as demand under soblivious analysis, the mere presence of m higher-priority jobs rules out a priority inversion, whereas only ready higher-priority jobs can nullify a priority inversion under s-aware analysis.4 Accordingly, under s-oblivious (resp., saware) schedulability analysis, a job Ji incurs s-oblivious (resp., s-aware) pi-blocking at time t if Ji is pending but not scheduled and fewer than m higher-priority jobs are pending (resp., ready). This is illustrated in Fig. 2. Prior research has shown that s-aware and s-oblivious analysis are comparable in terms of schedulability achievable in practice <NO>. Cache-related locking problem. We now describe the basic synchronization problem that arises when using locking protocols for cache management (protocol-specific details are discussed in Sec. 3). When using such protocols, each color is viewed as a shared resource that has a number of “replicas” as given by the number of cache ways, as illustrated in Fig. 3. Before a job commences execution, it must first lock a replica of each color that it requires (as given by the pages it will access). If the job accesses r pages with the same color, then it must lock r replicas of that color. The needed synchronization protocol must enable a set of shared resources to be managed, where each resource has multiple replicas, and jobs may need to lock several replicas simultaneously. In actuality, such a protocol is utilized by the OS when making scheduling decisions, i.e., the jobs themselves do not acquire and release color-related locks. This means that the OS must know the pages a job will access prior to making a scheduling decision."	https://doi.org/10.1109/ECRTS.2013.26	2	['cmp_platform', 'media_processing', 'conventional_cache', 'cqos_options', 'qos_enabled']	['workload', 'priority', 'platform']	['enabled_memory_architecture', 'fraction_chip_transistors', 'hardware_software_support', 'heterogeneous_memory_access', 'high_priority_applications']	Ward, et al. [NO], several techniques are proposed and analyzed for dealing with such interactions in the context of shared caches. 
Implementing time-predictable load and store operations	['The issues are tricky, but they have already been investigated during our earlier work on the scratchpad memory management unit (SMMU) <NO>.', 'This table is based on data published in <NO>.']	J. Whitham, and N. Audsley. 2009. Implementing time-predictable load and store operations. Proc. EMSOFT. 265–274.	Scratchpads have been widely proposed as an alternative to caches for embedded systems. Advantages of scratchpads include reduced energy consumption in comparison to a cache and access latencies that are independent of the preceding memory access pattern. The latter property makes memory accesses time-predictable, which is useful for hard real-time tasks as the worst-case execution time (WCET) must be safely estimated in order to check that the system will meet timing requirements. However, data must be explicitly moved between scratchpad and external memory as a task executes in order to make best use of the limited scratchpad space. When dynamic data is moved, issues such as pointer aliasing and pointer invalidation become problematic. Previous work has proposed solutions that are not suitable for hard real-time tasks because memory accesses are not time-predictable. This paper proposes the scratchpad memory management unit (SMMU) as an enhancement to scratchpad technology. The SMMU implements an alternative solution to the pointer aliasing and pointer invalidation problems which (1) does not require whole-program pointer analysis and (2) makes every memory access operation time-predictable. This allows WCET analysis to be applied to hard-real time tasks which use a scratchpad and dynamic data, but results are also applicable in the wider context of minimizing energy consumption or average execution time. Experiments using C software show that the combination of an SMMU and scratchpad compares favorably with the best and worst case performance of a conventional data cache.	"Figure 1 shows the contents of the ycc rgb convert function from libjpeg <NO>. This code forms part of the process of decoding a JPEG file into RGB data for display on a screen. It has been chosen as an example because previous techniques would force a time-predictable implementation to use external memory, with a high latency for each access.
Firstly, none of the data used by this function would be suitable for data scratchpad allocation using techniques described by Suhendra et al. <NO> or Deverge and Puaut <NO>
because each variable is accessed using a pointer from dynamically allocated memory. The surrounding library code would change significantly to accommodate static allocation.
Secondly, a time-predictable implementation could not use a data cache. The pointer values are unknown and some of the effective addresses are dependent on input data (e.g. Crrtab<NO>, since cr := inptr2<NO>). Current WCET analysis techniques for data caches would force the majority of memory accesses in the function to use external memory <NO>.
Future WCET analysis techniques for data caches might be able to support code of this kind. However, the results of tight WCET analysis would still be disappointing, as Table 2 illustrates. In Table 2, the approximate “best” and “worst” numbers of cache misses (and related data) are shown for Figure 1 and two cache sizes. These were computed using a genetic algorithm which searched the space of possible offsets for each array used in Figure 12. The algorithm used the measured execution time as a fitness value and attempted to minimize (or maximize) it, finding an estimate for the best (or worst) case. The huge difference between the “best” and “worst” cases observed here is entirely due to conflict misses <NO>. A conflict miss occurs in any data cache whenever two or more items of data are competing for a single cache line. The combination of a data cache and full support for pointers forces WCET analysis to account for all possible conflict miss scenarios (or use some other safe upper bound). Although the figures in Table 2 are specific to this example, a similar disparity between the best and worst case will be found whenever more than n unknown addresses are being accessed within a loop given an n-way associative cache.
The remainder of this paper shows how the SMMU is able to support code such as Figure 1 and approach the “best” case of Table 2 for any input data and any pointer values."	https://doi.org/10.1145/1629335.1629371	1	['cache_partitioning', 'software_cache', 'energy_consumption', 'data_object', 'scratch_pad']	['data', 'size', 'partitioning']	['software_cache_partitioning', 'memory_access_times', 'accesses_time_predictable', 'aliasing_pointer_invalidation', 'associativity_line_size']	Whitham, et al. [NO] proposes the scratchpad memory management unit (smmu) as an enhancement to scratchpad technology. 
Explicit reservation of local memory in a predictable, preemptive multitasking real-time system	[]	Jack Whitham, and Neil C. Audsley. 2012. Explicit reservation of local memory in a predictable, preemptive multitasking real-time system. Proc. of the 2012 IEEE 18th Real Time and Embedded Technology and Applications Symposium. IEEE, 3–12.	This paper proposes Carousel, a mechanism to manage local memory space, i.e. cache or scratchpad memory (SPM), such that inter-task interference is completely eliminated. The cost of saving and restoring the local memory state across context switches is explicitly handled by the preempting task, rather than being imposed implicitly on preempted tasks. Unlike earlier attempts to eliminate inter-task interference, Carousel allows each task to use as much local memory space as it requires, permitting the approach to scale to large numbers of tasks. Carousel is experimentally evaluated using a simulator. We demonstrate that preemption has no effect on task execution times, and that the Carousel technique compares well to the conventional approach to handling interference, where worst-case interference costs are simply added to the worst-case execution times (WCETs) of lower-priority tasks.	"Inter-task interference may occur whenever tasks share a stateful resource such as a cache <NO> (e.g. Figure 1).
The problem of intra-task interference due to cache state is now quite well-known and has been studied thoroughly. Cache WCET analyses model the state of a cache at each point within a task in order to estimate the worst-case miss count, and hence the maximum execution time <NO>, <NO>, <NO>. Each cache state is dependent on earlier cache states. Earlier task activity may result in subsequent activity producing either a hit or a miss. This dependence is a form of interference, but intra-task interference, since it occurs between one part of a task and another.
Inter-task interference has also been examined <NO>, <NO>. This occurs between two or more tasks in a multitasking system. When two or more tasks share a cache, activity in one task can disturb data used by the other, producing hits or misses at unpredictable times. Inter-task analysis is pessimistic because the exact set of evicted and/or useful cache blocks of tasks cannot usually be computed offline.
Earlier work has prevented inter-task interference entirely by static partitioning: reserving local RAM space for each task <NO>, <NO>. Each task is only permitted to update its own partition (Figure 3). The remainder is locked <NO>. While the size of the partitions can vary between tasks, and non realtime tasks can share a single partition, the assignment is static. Tasks cannot use more than their fixed share of local RAM, not even temporarily. This is a problem, because tasks may have a suboptimal local RAM allocation <NO>. This situation becomes a near-certainty as the number of tasks increases.
Consequently, some researchers have suggested allowing inter-task interference, but bounding its impact. A number of approaches are evaluated in <NO>. They are suitable for timing-compositional systems where interference does not
cause any timing anomalies <NO>. They incorporate the cost of reloading evicted cache blocks into the worst-case response time equation <NO>.
A third approach involves explicitly saving and restoring the state of local RAM. This idea has been successfully applied within tasks for SPM <NO>, <NO>. It avoids intra-task interference by ensuring that the state of local RAM is known at each point within the task. But it is not trivial to expand the idea to multitasking. The large size of local RAM (several kilobytes or more) prevents saving and restoring the entire state on each context switch."	https://doi.org/10.1109/RTAS.2012.19	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Whitham, et al. [NO] proposes carousel, a mechanism to manage local memory space, i.e. 
WCET analysis for multi-core processors with shared L2 instruction caches	['To our best knowledge, the only known work on WCET analysis for multicores with shared cache is <NO>, which is only applicable to a special scenario and very simple hardware architecture (we will discuss its limitation in Section 2).', 'Yan and Zhang <NO> is the only known work to studied the WCET analysis problem for multicore systems with shared L2 cache.', 'Unlike <NO>, which accounts for all possible conflicts caused by tasks running on the other cores when estimating the WCET of a task, our approach drastically reduces the amount of inter-core interferences.', 'The method described in <NO> estimates the WCET in presence of shared caches on multi-cores by analysing inter-thread cache conflicts.', 'More generally, the method described in <NO> is expected to lack scalability with respect to the task size and number of tasks, because every conflict with every other task is considered.', 'Our proposed method, like <NO> identifies all inter-task interferences due to cache sharing for the sake of safety.', 'Our method is more general than <NO> in the sense that it supports multiple levels of shared caches, set-associative caches and an arbitrary number of realtime tasks and cores competing for the shared caches.', 'Note that the cache analysis method presented in this paragraph can be seen as a safe extension of the method presented in <NO> to set-associative caches, multiple levels of shared caches and an arbitrary number of tasks/cores competing for the shared caches.', 'This demonstrates the pessimism of methods such as the base method presented in Section 3 and the approach described in <NO>, that consider all interferences between cores without any mechanism to decrease inter-core interference.']	Jun Yan, and Wei Zhang. 2008. WCET analysis for multi-core processors with shared L2 instruction caches. Proc. of the 14th IEEE RTAS. 80–89.	Multi-core chips have been increasingly adopted by microprocessor industry. For real-time systems to safely harness the potential of multi-core computing, designers must be able to accurately obtain the worstcase execution time (WCET) of applications running on multi-core platforms, which is very challenging due to the possible runtime inter-core interferences in using shared resources such as the shared L2 caches. As the first step toward time-predictable multi-core computing, this paper presents a novel approach to bounding the worst-case performance for threads running on multi-core processors with shared L2 instruction caches. The idea of our approach is to compute the worst-case instruction access interferences between different threads based on the program control flow information of each thread, which can be statically analyzed. Our experiments indicate that the proposed approach can reasonably estimate the worstcase shared L2 instruction cache misses by considering inter-thread instruction conflicts. Also, the WCET of applications running on multi-core processors estimated by our approach is much better than the estimation by simply assuming all L2 instruction accesses are misses.	In a multi-core processor, each core typically has private L1 instruction and data caches. The L2 (and/or L3) caches can be either private or shared. While private L2 caches are more time-predictable in the sense that there are no inter-core L2 cache conflicts, each core can only exploit limited cache space. Due to the great impact of the L2 cache hit rate on the performance of multi-core processors <NO>, private L2 caches may have worse performance than shared L2 caches with the same total size, because each core with shared L2 cache can make use of the aggregate L2 cache space more efficiently. Moreover, shared L2 cache architecture makes it easier for multiple cooperative threads to share instructions, data and the precious memory bandwidth to maximize performance. Therefore, in this paper, we focus on studying WCET analysis of multi-core processors with shared L2 caches (by contrast, the WCET analysis for multicore chips with private L2 caches is a less challenging problem).	https://doi.org/10.1109/RTAS.2008.6	0	['wcet_estimation', 'managing_shared', 'set_pinning', 'core_interference', 'instruction_cache']	['shared', 'wcet', 'policy']	['inter_core_interference', 'managing_shared_caches', 'set_pinning_scheme', 'execution_time_wcet', 'cache_management_policy']	Yan, et al. [NO] presents a novel approach to bounding the worst-case performance for threads running on multi-core processors with shared l2 instruction caches. 
