ref_title	ref_context	ref_entry	abstract	intro	ref_link	label	topic_word	topic_bigram	topic_trigram	description	top_n_words
Wrapper-based computation and evaluation of sampling methods for imbalanced datasets	[]	Nitesh V. Chawla, Lawrence O. Hall, and Ajay Joshi. 2005. Wrapper-based computation and evaluation of sampling methods for imbalanced datasets. Proceedings of the 1st International Workshop on UtilityBased Data Mining. ACM, New York, NY, 24–33.	Learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. When the imbalance is large, classification accuracy on the smaller class(es) tends to be lower. In particular, when a class is of great interest but occurs relatively rarely such as cases of fraud, instances of disease, and regions of interest in largescale simulations, it is important to accurately identify it. It then becomes more costly to misclassify the interesting class. In this paper, we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (SMOTE) to improve minority class accuracy. The f-value serves as the evaluation function. Experimental results show the wrapper approach is effective in optimization of the composite f-value, and reduces the average cost per test example for the datasets considered. We report both average cost per test example and the cost curves in the paper. The true positive rate of the minority class increases significantly without causing a significant change in the f-value. We also obtain the lowest cost per test example, compared to any result we are aware of for the KDD Cup-99 intrusion detection data set.	"Researchers in the machine learning community have dealt with the problem of class imbalance by using various approaches like over-sampling the minority classes, undersampling the majority classes, assigning different costs for different misclassification errors, learning by recognition as opposed to discrimination, etc <NO>. There is a significant body of research comparing the various sampling methods <NO>. Sampling strategies have almost become the de facto standard for countering the imbalance in datasets <NO>. With all this there is still no answer on how to do the sampling required for obtaining good classifier accuracies on minority classes.
There are a number of different approaches that can be applied to build classifiers on imbalanced data sets. In this work, we examined under sampling and over-sampling by creating synthetic examples of minority classes. Under-
sampling the majority class can reduce the bias of the learned classifier towards it and thus improve the accuracy on the minority classes.
Some studies <NO> have been done which combined under-sampling of majority classes with over sampling by replication of minority classes. While Japkowicz <NO> found this approach very effective, Ling and Li <NO> were not able to get significant improvement in their performance measures. Japkowicz experimented with only one-dimensional artificial data of varying complexity whereas Ling and Li used real data from a Direct Marketing problem. This might have been the reason for the discrepancy between their results. On the whole, from the body of literature, it was found that under-sampling of majority classes was better than over-sampling with replication of minority classes <NO> and that the combination of the two did not significantly improve the performance over under sampling alone.
Chawla et al. <NO> introduced a new over-sampling approach for two class problems that over-sampled the minority class by creating synthetic examples rather than replicating examples. They pointed out the limitation of oversampling with replication in terms of the decision regions in feature space for decision trees. They showed that as the minority class was over sampled by increasing amounts, for decision trees, the result was to identify similar but more specific regions in the feature space. A preferable approach is to build generalized regions around minority class examples.
The synthetic minority over-sampling technique (SMOTE) was introduced to provide synthetic minority class examples which were not identical but came from the same region in feature space. The over-sampling was done by selecting each minority class example and creating a synthetic example along the line segment joining the selected example and any/all of the k minority class nearest neighbors. In the calculations of the nearest neighbors for the minority class examples a Euclidean distance for continuous features and the value Distance Metric (with the Euclidean assumption) for nominal features was used. For examples with continuous features, the synthetic examples are generated by taking the difference between the feature vectors of selected examples under consideration and their nearest neighbors. The difference between the feature vectors is multiplied by a random number between 0 and 1 and then added to the feature vector of the example under consideration to get a new synthetic example. For nominal valued features, a majority vote for the feature value is taken between the example under consideration and its k nearest neighbors. This approach effectively selects a random point along the line segment between the two feature vectors. This strategy forces the decision regions of the minority class learned by the classifier to become more general and effectively provides better generalization performance on unseen data.
However, an investigation into how to choose the number of examples to be added was not done. In addition, the amount of under-sampling also needs to be determined. Given the various costs of making errors, it is important to identify potentially optimal values for both SMOTE and under-sampling. This is equivalent to discovering the operating point in the ROC space giving the best trade-off between True Positives and False Positives. In this paper, we develop an approach to automatically set the parameters. We discuss a wrapper framework using cross-validation that
performs a step-wise and greedy search for the parameters. Note that while the computational aspects of the automated approach induces certain costs, we do not incorporate that into our framework. We optimize based on the different types of errors made. However, we do try to restrict our search space. We show that this approach works on three highly skewed datasets. We also utilized a cost-matrix to indicate the costs per test example based on the different kinds of errors."	https://doi.org/10.1145/1089827.1089830	2	['cost']	['cost_test', 'f_value', 'test_example', 'threshold_moving', 'average_cost']	['cost_test_example', 'average_cost_test', '99_intrusion_detection', 'accurately_identify_it', 'accuracy_smaller_class']	V. et al. [NO] implements a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (smote) to improve minority class accuracy. 	test f interesting wrapper cost 
Training cost-sensitive neural networks with methods addressing the class imbalance problem	['Other examples of cost-sensitive learning include the MetaCost framework <NO>, cost-sensitive neural network <NO>, costsensitive support vector machines (SVMs) <NO>, and others.', 'Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'domains <NO>, <NO>, <NO>, cost-sensitive learning is superior to', 'However, we note that a more significant performance increase can be achieved by applying this estimate to ensemble methods by using cross-validation techniques on a given set; a similar approach is considered in <NO>, however using a slightly different estimate.', 'cost-sensitive neural networks <NO>, <NO>, the ensemble', 'For instance, in cost-sensitive learning, it is natural to use misclassification costs for performance evaluation for multiclass imbalanced problems <NO>, <NO>, <NO>.', 'Algorithmic techniques have been developed for the different classification algorithms, such as neural networks <NO>, decision trees <NO>, fuzzy systems <NO>, <NO> etc.']	Zhi-Hua Zhou, and Xu-Ying Liu. 2006. Training cost-sensitive neural networks with methods addressing the class imbalance problem. IEEE Trans. Knowl. Data Eng. 18, 1 (2006), 63–77.	This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both oversampling and undersampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and softensemble, i.e., the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that costsensitive learning with multiclass tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multiclass tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training costsensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may, in fact, only be effective on learning with imbalanced two-class data sets.	"Suppose there areC classes and the ith class hasNi number of training examples. LetCost½i; c ði; c 2 f1::CgÞdenote the cost of misclassifying an example of the ith class to the cth class (Cost½i; i ¼ 0) and Cost½i ði 2 f1::CgÞ denote the cost of the ith class. Moreover, suppose the classes are ordered such that, for the ith class and the jth class, if i < j, then ðCost½i < Cost½j Þor ðCost½i ¼ Cost½j and Ni NjÞ.Cost½i is usually derived from Cost½i; c . There are many possible rules for the derivation, among which a popular one is Cost½i ¼PC
c¼1 Cost½i; c <NO>, <NO>."	https://doi.org/10.1109/TKDE.2006.17	2	['cost']	['cost_test', 'f_value', 'test_example', 'threshold_moving', 'average_cost']	['cost_test_example', 'average_cost_test', '99_intrusion_detection', 'accurately_identify_it', 'accuracy_smaller_class']	Zhou, et al. [NO] studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. 	threshold task cost moving study 
