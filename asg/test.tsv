ref_title	ref_context	ref_entry	abstract	intro	ref_link	label	topic_word	topic_bigram	topic_trigram	description	top_n_words
Active learning for class imbalance problem	['For example, an SVM-based active learning approach for imbalanced datasets was proposed in <NO> and <NO>.', 'This algorithm locates the “most informative” sample by evaluating a small fixed number of randomly selected examples instead of the entire dataset <NO>.', 'Recently, however, various issues on active learning from imbalanced data sets have been discussed in literature <NO>, <NO>, <NO>, <NO>.', 'SVM-based active learning aims to select the most informative instances from the unseen training data in order to retrain the kernel-based model <NO>, i.', '<NO> and <NO> proposed an efficient SVM-based active learning method which queries a small pool of data at each iterative step of active learning instead of querying the entire data set.', '<NO> and <NO> also point out that the search process for the most informative instances can be computationally expensive because, for each instance of unseen data, the algorithm needs to recalculate the distance between each instance and the current hyperplane.', 'To solve this problem, they proposed a method to effectively select such informative instances from a random set of training populations to reduce the computational cost for large-scale imbalanced data sets <NO>, <NO>.']	Şeyda Ertekin, Jian Huang, and C. Lee Giles. 2007. Active learning for class imbalance problem. Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New York, NY, 823–824.	The class imbalance problem has been known to hinder the learning performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem.	"The basic SVM based active learning selects the closest instance to the current hyperplane from the unseen training data and adds it to the training set to retrain the model. In classical active learning <NO>, the search for the most informative (closest) instance is done through the entire unseen dataset. Each iteration of active learning involves the recomputation of the distances of each instance to the new hyperplane. Thus, for large datasets, searching the entire training set is very time-consuming and computationally expensive.
We propose a selection method which will not necessitate a full search through the entire dataset but locates an approximate most informative sample by examining a small constant number of randomly chosen samples. The method picks L (L # training instances) random training samples in each iteration and selects the best (closest to the hyperplane) among them. Suppose, instead of picking the closest instance among all the training samples XN = (x1, x2, · · · , xN) at each iteration, we first pick a random subset XL, L N and select the closest sample xi from XL based on the condition that xi is among the top p% closest instances in XN with probability (1− η). Any numerical modification to these constraints can be met by varying the size of L, and is independent of N . To demonstrate, the probability that at least one of the L instances is among the closest p% is 1 − (1 − p%)L. Due to the requirement of (1 − η) probability, we have
1 − (1 − p%)L = 1 − η (1) which follows the solution of L in terms of η and p
L = log η / log(1 − p%) (2) For example, the active learner will pick one instance, with 95% probability, that is among the top 5% closest instances to the hyperplane, by randomly sampling only log(.05)/ log(.95) = 59 instances regardless of the training set size. This approach scales well since the size of the subset L is independent of the training set size N , requires significantly less training time and does not have an adverse effect on the classification performance of the learner. In our experiments, we set L = 59.
Early Stopping: In SVM learning the classification boundary (hyperplane) is only determined by support vectors. This means that there is no point of adding new instances to the model after the
number of support vectors saturates. A practical implementation of this idea is to count the number of support vectors during the active learning training process. If the number of the support vectors stabilizes, it implies that all possible support vectors have been selected by the active learning method and the rest of the training instances are redundant. Therefore, we choose our stopping point where the number of support vectors saturates."	https://doi.org/10.1145/1277741.1277927	2	[]	['probability_estimates', 'imbalanced_data', 'combined_bagging', 'conventional_decision', 'knowledge_discovery']	['conventional_decision_trees', 'imbalanced_learning_problem', 'learning_imbalanced_data', '20_66_datasets', '5_hellinger_distance']	Ertekin, et al. [NO] demonstrates that active learning is capable of solving the problem. 	capable phenomenon solving various hinder 
Theoretical analysis of a performance measure for imbalanced data	[]	Vicente Garcı́a, Ramón Alberto Mollineda, and José Salvador Sánchez 2010. Theoretical analysis of a performance measure for imbalanced data. In 2010 20th International Conference on Pattern Recognition (ICPR)	This paper analyzes a generalization of a new metric to evaluate the classification performance in imbalanced domains, combining some estimate of the overall accuracy with a plain index about how dominant the class with the highest individual accuracy is. A theoretical analysis shows the merits of this metric when compared to other well-known measures.	"Traditionally, classification accuracy (Acc) and/or error rates have been the standard metrics used to estimate the performance of learning systems. For a twoclass problem, they can be easily derived from a 2 × 2 confusion matrix as that given in Table 1.
However, empirical and theoretical evidences show that these measures are biased with respect to data imbalance and proportions of correct and incorrect classifications. These shortcomings have motivated a search for new metrics based on simple indexes, such as the true positive rate (TPrate) and the true negative rate (TNrate). The TPrate (TNrate) is the percentage of positive (negative) examples correctly classified.
One of the most widely-used evaluation methods in the context of class imbalance is the ROC curve, which is a tool for visualizing and selecting classifiers based on their trade-offs between benefits (true positives) and costs (false positives). A quantitative representation of a ROC curve is the area under it (AUC) <NO>. For just one run of a classifier, the AUC can be computed as <NO> AUC = (TPrate+ TNrate)/2.
1051-4651/10 $26.00 © 2010 IEEE DOI 10.1109/ICPR.2010.156
62117
Kubat et al. <NO> use the geometric mean of accuracies measured separately on each class, with the aim of maximizing the accuracies of both classes while keeping them balanced, Gmean = √ TPrate · TNrate.
Both AUC and Gmean minimize the negative influence of skewed distributions of classes, but they do not show up the contribution of each class to the overall performance, nor which is the prevalent class. This means that different combinations of TPrate and TNrate may produce the same result for those metrics.
Recently, Ranawana and Palade <NO> introduced the optimized precision, which can be computed as,
OP = Acc− |TNrate− TPrate| TNrate+ TPrate
(1)
This represents the difference between the global accuracy and a second term that computes how balanced both class accuracies are. High OP values require high global accuracy and well-balanced class accuracies. However, OP can be strongly affected by the biased influence of the global accuracy."	https://doi.org/10.1109/ICPR.2010.156	2	[]	['probability_estimates', 'imbalanced_data', 'combined_bagging', 'conventional_decision', 'knowledge_discovery']	['conventional_decision_trees', 'imbalanced_learning_problem', 'learning_imbalanced_data', '20_66_datasets', '5_hellinger_distance']	Garcı́a, et al. [NO] analyzes a generalization of a new metric to evaluate the classification performance in imbalanced domains, combining some estimate of the overall accuracy with a plain index about how dominant the class with the highest individual accuracy is. 	dominant highest index plain show 
Learning from imbalanced data	['itself <NO>, <NO>, the issue is that usually a series of difficulties related to this problem turn up.', 'Most of these works fall under four different categories: sampling-based methods, cost-based methods, kernel-based methods, and active learning-based methods <NO>.', 'Details of work performed on the other categories can be found in <NO>.', 'The main problem associates with the accuracy measure is its dependence on the distribution of positive class and negative class samples in the data set, thus not suitable for imbalanced learning problems <NO>.', 'They are precision, recall, geometric-mean (G-mean), and F-measure <NO>.', 'For several base classifiers, studies have shown that a balanced training dataset provides high prediction accuracy and good generalization capability compared to an imbalanced dataset <NO> <NO>.', 'However, the drawbacks of over-sampling approaches are enlarging the size of the original training dataset, and leading to overfitting<NO><NO>.', 'Traditionally, metrics which are used by the most standard algorithm are accuracy and error rate <NO><NO><NO><NO>.', 'Indeed, there are at least three surveys of methods for improving classification performance on imbalanced datasets <NO>, <NO>, <NO>.', 'In the case this assumption does not hold, the literature has several comprehensible surveys, we recommend <NO>.', 'LEARNING from imbalanced data (imbalanced learning) <NO>, <NO> has become a critical and significant research issue in many of today’s data-intensive applications, such as financial engineering, anomaly detection, biomedical data analysis, and many others.', 'The imbalance learning problem generally manifests itself in two forms: relative imbalances and absolute imbalances <NO>, <NO>.', 'Some studies have shown that the degradation of classification performance attributed to imbalanced data is not necessarily the result of relative imbalances but rather due to the lack of representative examples (absolute imbalances) <NO>, <NO>, <NO>, <NO>, <NO>.', 'In particular, for a given dataset that contains several sub-concepts, the distribution of minority examples over the minority class concepts may yield clusters with insufficient representative examples to form a classification rule <NO>.', 'A comprehensive review of the development of research in learning from imbalanced data, including the nature of the problem, the state-of-the-art approaches, the assessment metrics, and the major opportunities and challenges, has been presented in <NO>.', 'These techniques have shown great success when applied to imbalanced learning problems <NO>.', 'Kernel-based methods have recently become very popular across various fields including imbalanced learning <NO>.', 'Recently, active learning methods have found increased use in imbalanced learning applications <NO>.', 'accuracy (OA), may not be able to provide a comprehensive assessment of the learning algorithm <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'The detailed discussions on these metrics and their applications for imbalanced learning can be found in <NO>.', 'The interested reader can refer to <NO> for further details on H -measure and <NO> for a critical review of the assessment metrics for imbalanced learning.', 'Hence, several specific approaches have been proposed <NO>.', 'Although the SVM algorithm works effectively with balanced datasets, when it comes to imbalanced datasets, it could often produce suboptimal results <NO>–<NO>, i.', 'It has been well-studied that the SVM algorithm can be sensitive to class imbalance <NO>–<NO>, i.', 'Generally, these methods can be divided into two categories: external methods and internal methods <NO>, <NO>, <NO>.', 'A comprehensive review of different CIL methods can be found in <NO>.', 'until a particular class ratio is met <NO>.', 'Therefore, we used the geometric mean of sensitivity (SE = proportion of the positives correctly recognized) and specificity (SP = proportion of the negatives correctly recognized), given by Gm = √ SE × SP, for the classifier performance evaluation in this study, as commonly used in class imbalanced-learning research <NO>, <NO>, <NO>.']	Haibo He, and Edwardo A. Garcia. 2009. Learning from imbalanced data. IEEE Knowl. Data Eng. 21, 9 (2009), 1263–1284.	With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.	"Technically speaking, any data set that exhibits an unequal distribution between its classes can be considered imbalanced. However, the common understanding in the community is that imbalanced data correspond to data sets exhibiting significant, and in some cases extreme, imbalances. Specifically, this form of imbalance is referred to as a between-class imbalance; not uncommon are betweenclass imbalances on the order of 100:1, 1,000:1, and 10,000:1, where in each case, one class severely outrepresents another <NO>, <NO>, <NO>. Although this description would seem to imply that all between-class imbalances are innately binary (or two-class), we note that there are multiclass data in which imbalances exist between the various classes <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. In this paper, we only briefly touch upon the multiclass imbalanced learning problem, focusing instead on the two-class imbalanced learning problem for space considerations.
In order to highlight the implications of the imbalanced learning problem in the real world, we present an example from biomedical applications. Consider the “Mammography Data Set,” a collection of images acquired from a series of mammography exams performed on a set of distinct patients, which has been widely used in the analysis of algorithms addressing the imbalanced learning problem <NO>, <NO>, <NO>. Analyzing the images in a binary sense, the natural classes (labels) that arise are “Positive” or “Negative” for an image representative of a “cancerous” or “healthy” patient, respectively. From experience, one would expect the number of noncancerous patients to exceed greatly the number of cancerous patients; indeed, this data
set contains 10,923 “Negative” (majority class) samples and 260 “Positive” (minority class) samples. Preferably, we require a classifier that provides a balanced degree of predictive accuracy (ideally 100 percent) for both the minority and majority classes on the data set. In reality, we find that classifiers tend to provide a severely imbalanced degree of accuracy, with the majority class having close to 100 percent accuracy and the minority class having accuracies of 0-10 percent, for instance <NO>, <NO>. Suppose a classifier achieves 10 percent accuracy on the minority class of the mammography data set. Analytically, this would suggest that 234 minority samples are misclassified as majority samples. The consequence of this is equivalent to 234 cancerous patients classified (diagnosed) as noncancerous. In the medical industry, the ramifications of such a consequence can be overwhelmingly costly, more so than classifying a noncancerous patient as cancerous <NO>. Therefore, it is evident that for this domain, we require a classifier that will provide high accuracy for the minority class without severely jeopardizing the accuracy of the majority class. Furthermore, this also suggests that the conventional evaluation practice of using singular assessment criteria, such as the overall accuracy or error rate, does not provide adequate information in the case of imbalanced learning. Therefore, more informative assessment metrics, such as the receiver operating characteristics curves, precision-recall curves, and cost curves, are necessary for conclusive evaluations of performance in the presence of imbalanced data. These topics will be discussed in detail in Section 4 of this paper. In addition to biomedical applications, further speculation will yield similar consequences for domains such as fraud detection, network intrusion, and oil-spill detection, to name a few <NO>, <NO>, <NO>, <NO>, <NO>.
Imbalances of this form are commonly referred to as intrinsic, i.e., the imbalance is a direct result of the nature of the dataspace. However, imbalanced data are not solely restricted to the intrinsic variety. Variable factors such as time and storage also give rise to data sets that are imbalanced. Imbalances of this type are considered extrinsic, i.e., the imbalance is not directly related to the nature of the dataspace. Extrinsic imbalances are equally as interesting as their intrinsic counterparts since it may very well occur that the dataspace from which an extrinsic imbalanced data set is attained may not be imbalanced at all. For instance, suppose a data set is procured from a continuous data stream of balanced data over a specific interval of time, and if during this interval, the transmission has sporadic interruptions where data are not transmitted, then it is possible that the acquired data set can be imbalanced in which case the data set would be an extrinsic imbalanced data set attained from a balanced dataspace.
In addition to intrinsic and extrinsic imbalance, it is important to understand the difference between relative imbalance and imbalance due to rare instances (or “absolute rarity”) <NO>, <NO>. Consider a mammography data set with 100,000 examples and a 100:1 between-class imbalance. We would expect this data set to contain 1,000 minority class examples; clearly, the majority class dominates the minority class. Suppose we then double the sample space by testing more patients, and suppose further that the distribution
does not change, i.e., the minority class now contains 2,000 examples. Clearly, the minority class is still outnumbered; however, with 2,000 examples, the minority class is not necessarily rare in its own right but rather relative to the majority class. This example is representative of a relative imbalance. Relative imbalances arise frequently in real-world applications and are often the focus of many knowledge discovery and data engineering research efforts. Some studies have shown that for certain relative imbalanced data sets, the minority concept is accurately learned with little disturbance from the imbalance <NO>, <NO>, <NO>. These results are particularly suggestive because they show that the degree of imbalance is not the only factor that hinders learning. As it turns out, data set complexity is the primary determining factor of classification deterioration, which, in turn, is amplified by the addition of a relative imbalance.
Data complexity is a broad term that comprises issues such as overlapping, lack of representative data, small disjuncts, and others. In a simple example, consider the depicted distributions in Fig. 2. In this figure, the stars and circles represent the minority and majority classes, respectively. By inspection, we see that both distributions in Figs. 2a and 2b exhibit relative imbalances. However, notice how Fig. 2a has no overlapping examples between its classes and has only one concept pertaining to each class, whereas Fig. 2b has both multiple concepts and severe overlapping. Also of interest is subconcept C in the distribution of Fig. 2b. This concept might go unlearned by some inducers due to its lack of representative data; this issue embodies imbalances due to rare instances, which we proceed to explore.
Imbalance due to rare instances is representative of domains where minority class examples are very limited, i.e., where the target concept is rare. In this situation, the lack of representative data will make learning difficult regardless of the between-class imbalance <NO>. Furthermore, the minority concept may additionally contain a subconcept with limited instances, amounting to diverging degrees of classification difficulty <NO>, <NO>. This, in fact, is the result of another form of imbalance, a within-class imbalance, which concerns itself with the distribution of representative data for subconcepts within a class <NO>, <NO>, <NO>. These ideas are again highlighted in our simplified example in Fig. 2. In Fig. 2b, cluster B represents the dominant minority class concept and cluster C represents a subconcept of the minority class. Cluster D represents two subconcepts of the majority class and cluster A (anything
not enclosed) represents the dominant majority class concept. For both classes, the number of examples in the dominant clusters significantly outnumber the examples in their respective subconcept clusters, so that this dataspace exhibits both within-class and between-class imbalances. Moreover, if we completely remove the examples in cluster B, the dataspace would then have a homogeneous minority class concept that is easily identified (cluster C), but can go unlearned due to its severe underrepresentation.
The existence of within-class imbalances is closely intertwined with the problem of small disjuncts, which has been shown to greatly depreciate classification performance <NO>, <NO>, <NO>, <NO>. Briefly, the problem of small disjuncts can be understood as follows: A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept <NO>, <NO>, <NO>. In the case of homogeneous concepts, the classifier will generally create large disjuncts, i.e., rules that cover a large portion (cluster) of examples pertaining to the main concept. However, in the case of heterogeneous concepts, small disjuncts, i.e., rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts <NO>, <NO>, <NO>. Moreover, since classifiers attempt to learn both majority and minority concepts, the problem of small disjuncts is not only restricted to the minority concept. On the contrary, small disjuncts of the majority class can arise from noisy misclassified minority class examples or underrepresented subconcepts. However, because of the vast representation of majority class data, this occurrence is infrequent. A more common scenario is that noise may influence disjuncts in the minority class. In this case, the validity of the clusters corresponding to the small disjuncts becomes an important issue, i.e., whether these examples represent an actual subconcept or are merely attributed to noise. For example, in Fig. 2b, suppose a classifier generates disjuncts for each of the two noisy minority samples in cluster A, then these would be illegitimate disjuncts attributed to noise compared to cluster C, for example, which is a legitimate cluster formed from a severely underrepresented subconcept.
The last issue we would like to discuss is the combination of imbalanced data and the small sample size problem <NO>, <NO>. In many of today’s data analysis and knowledge discovery applications, it is often unavoidable to have data with high dimensionality and small sample size; some specific examples include face recognition and gene expression data analysis, among others. Traditionally, the small sample size problem has been studied extensively in the pattern recognition community <NO>. Dimensionality reduction methods have been widely adopted to handle this issue, e.g., principal component analysis (PCA) and various extension methods <NO>. However, when the representative data sets’ concepts exhibit imbalances of the forms described earlier, the combination of imbalanced data and small sample size presents a new challenge to the community <NO>. In this situation, there are two critical issues that arise simultaneously <NO>. First, since the sample size is small, all of the issues related to absolute rarity and within-class imbalances are applicable. Second and more importantly, learning algorithms often fail to
Fig. 2. (a) A data set with a between-class imbalance. (b) A highcomplexity data set with both between-class and within-class imbalances, multiple concepts, overlapping, noise, and lack of representative data.
generalize inductive rules over the sample space when presented with this form of imbalance. In this case, the combination of small sample size and high dimensionality hinders learning because of difficultly involved in forming conjunctions over the high degree of features with limited samples. If the sample space is sufficiently large enough, a set of general (albeit complex) inductive rules can be defined for the dataspace. However, when samples are limited, the rules formed can become too specific, leading to overfitting. In regards to learning from such data sets, this is a relatively new research topic that requires much needed attention in the community. As a result, we will touch upon this topic again later in our discussions."	https://doi.org/10.1145/1007730.1007733	2	[]	['probability_estimates', 'imbalanced_data', 'combined_bagging', 'conventional_decision', 'knowledge_discovery']	['conventional_decision_trees', 'imbalanced_learning_problem', 'learning_imbalanced_data', '20_66_datasets', '5_hellinger_distance']	He, et al. [NO] provides a comprehensive review of the development of research in learning from imbalanced data. 	knowledge discovery raw understanding research 
Disturbing neighbors ensembles of trees for imbalanced data	[]	Juan J. Rodrı́guez, José-Francisco Dı́ez-Pastor, Jesús Maudes, and César Garcı́a-Osorio 2012. Disturbing neighbors ensembles of trees for imbalanced data. In 2012 11th International Conference on Machine Learning and Applications (ICMLA),	Disturbing Neighbors (DN ) is a method for generating classifier ensembles. Moreover, it can be combined with any other ensemble method, generally improving the results. This paper considers the application of these ensembles to imbalanced data: classification problems where the class proportions are significantly different. DN ensembles are compared and combined with Bagging, using three tree methods as base classifiers: conventional decision trees (C4.5), Hellinger distance decision trees (HDDT) —a method designed for imbalance data— and model trees (M5P) —trees with linear models at the leaves—. The methods are compared using two collections of imbalanced datasets, with 20 and 66 datasets, respectively. The best results are obtained combining Bagging and DN , using conventional decision trees. 	"As many other ensemble methods (e.g., Bagging, Random Subspaces) the Disturbing Neighbors (DN ) method is based on training each base classifier with a random transformation of the dataset.
In this method, the transformation adds new attributes. These attributes are obtained from a small sample of randomly selected training examples. These examples are called the Disturbing Neighbors.
The additional attributes are based on which one of the Disturbing Neighbors is the nearest. The first added attribute is nominal, the possible values are the class labels. For a binary class data set, the attribute will be binary. For a given example, the value of this attribute is the class label of the nearest Disturbing Neighbor.
For each Disturbing Neighbor an additional boolean attribute is included. The value of the attribute is true for a given example if the corresponding Disturbing Neighbor is the nearest one.
As an additional source of diversity (a necessary ingredient of successful ensembles) for each base classifier the distances are calculated in a random subspace. This random subspace is only used for determining the nearest Disturbing Neighbor, the transformed dataset contains all the original attributes.
It is possible to obtain ensembles using only the DN method, but usually is possible to obtain better results combining DN with other ensemble methods. The combination generally is better than the other ensemble method and the DN ensemble <NO>.
In Figure 1 the tree at the right was obtained with the same method and from the same dataset (glass4) as the other trees but with the additional attributes from the Disturbing Neighbors. One of this attributes, Nearest 8, was selected as the root of the tree. This attribute indicates if the nearest DN is the eighth.
From the considered sources of diversity, in <NO> it was determined that for non particularly imbalanced data sets the most important source was the use of the boolean features, the impact of the nominal feature and the calculation of the distances in a random subspace were less important."	https://doi.org/10.1109/ICMLA.2012.181	2	[]	['probability_estimates', 'imbalanced_data', 'combined_bagging', 'conventional_decision', 'knowledge_discovery']	['conventional_decision_trees', 'imbalanced_learning_problem', 'learning_imbalanced_data', '20_66_datasets', '5_hellinger_distance']	J. et al. [NO] considers the application of these ensembles to imbalanced data : classification problems where the class proportions are significantly different. 	tree dn ensemble combined conventional 
Class probability estimates are unreliable for imbalanced data (and how to fix them)	[]	Byron C. Wallace, and Issa J. Dahabreh. 2012. Class probability estimates are unreliable for imbalanced data (and how to fix them). 2012 IEEE 12th International Conference on Data Mining (ICDM). IEEE, 695–704.	Obtaining good probability estimates is imperative for many applications. The increased uncertainty and typically asymmetric costs surrounding rare events increases this need. Experts (and classification systems) often rely on probabilities to inform decisions. However, we demonstrate that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. To our knowledge, this problem has not previously been explored. Motivated by our exposition of this issue, we propose a simple, effective and theoretically motivated method to mitigate the bias of probability estimates for imbalanced data that bags estimators calibrated over balanced bootstrap samples. This approach drastically improves performance on the minority instances without greatly affecting overall calibration. We show that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates. 	"The standard method for estimating probabilities in the supervised learning framework is to regress measurements correlated with (predicted) class labels output by a trained
1We note that Cieslak and Chawla have investigated the specific case of Probability Estimation Trees (PETs) for imbalanced data <NO>, and that Foster and Stine have considered the related task of variable selection for prediction under imbalance <NO>.
classifier against the true target labels <NO>, <NO>. This process is called calibration. By convention these measurements are denoted by fi, where i indexes instances. This calibration squashes the (arbitrarily scaled) fi’s into the <NO> range permissible for probabilities. When the sigmoid form is used (Equation 1), this method is referred to as Platt scaling <NO>. It is perhaps the most popular way of obtaining probability estimates from classification models.
P (yi|fi) = 1 1 + exp{−β0 − β1fi} (1)
The fi’s may be any scalar that is predictive of class membership. We focus on two specific post-training calibration strategies; Platt calibration with SVMs and with boosted decision trees. We selected these methods because they have been shown to out-perform other supervised learning algorithms with respect to class probability estimation <NO>, <NO>.
In the case of SVMs, the fi is taken as the signed distance from the hyperplane w, i.e., fi = wTxi. This was the method originally proposed by Platt <NO>, and is now widely used <NO>. Niculescu-Mizil and Caruana, meanwhile, have proposed attaining probabilities via calibrated boosted decision trees <NO>. More precisely, recall that in boosting one induces a sequence of learners h0, h1, ... , hk over different distributions of the training set. These are in turn associated with a set of weights α0, α1, ... , αk reflecting the their estimated performance. A prediction is then taken as a function over these, i.e., as sign( ∑ j αjhj(x)). The natural value for fi is then the sum of the weighted class predictions over the ensemble, i.e., ∑ j αjhj(x).
Figure 2 displays the overall and stratified residual errors of probability estimates (obtained via Platt’s method) for the instances comprising a particular imbalanced dataset.2 Specifically, each sub-plot shows histograms of the absolute differences between the true (observed) labels and corresponding probability estimates, i.e., |yi−P̂{yi|xi}|. Density to the left therefore suggests good calibration, as this implies probability estimates largely agree with the observed labels. For example, if yi = 1 and P̂{yi|xi} = .99, the difference would be .01. Were the estimate .01, on the other hand, the difference would be .99.
The left-hand side of Figure 2 shows this histogram for all instances, corresponding to overall calibration. Over 80% of instances are in the left-most bin, implying that the estimator is well-calibrated, i.e., its estimates do not much diverge from the observed labels. But this ostensibly good calibration belies the unreliability of the probability estimates for the instances comprising the rare class. One can see this by looking at the middle plot, which is the same figure but includes only minority instances. In this case, the estimates diverge strikingly from the observed
2The proton beam dataset in Table I.
labels; indeed the model assigned a probability (of belonging to the minority class) of less than 20% to most of the minority instances. In other words, the probability estimates for instances comprising the minority class are completely unreliable (we demonstrate this on 16 datasets in Section V)."	https://doi.org/10.1109/ICDM.2012.115	2	[]	['probability_estimates', 'imbalanced_data', 'combined_bagging', 'conventional_decision', 'knowledge_discovery']	['conventional_decision_trees', 'imbalanced_learning_problem', 'learning_imbalanced_data', '20_66_datasets', '5_hellinger_distance']	C. et al. [NO] demonstrates that class probability estimates attained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. 	probability estimate calibration motivated uncertainty 
