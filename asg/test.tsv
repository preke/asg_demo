ref_title	ref_context	ref_entry	abstract	intro	ref_link	label	topic_word	topic_bigram	topic_trigram	description	top_n_words
Web tap: Detecting covert web traffic	[]	Kevin Borders, and Atul Prakash. 2004. Web tap: Detecting covert web traffic. Proceedings of the 11th ACM Conference on Computer and Communications Security.	As network security is a growing concern, system administrators lock down their networks by closing inbound ports and only allowing outbound communication over selected protocols such as HTTP. Hackers, in turn, are forced to find ways to communicate with compromised workstations by tunneling through web requests. While several tools attempt to analyze inbound traffic for denial-of-service and other attacks on web servers, Web Tap’s focus is on detecting attempts to send significant amounts of information out via HTTP tunnels to rogue Web servers from within an otherwise firewalled network. A related goal of Web Tap is to help detect spyware programs, which often send out personal data to servers using HTTP transactions and may open up security holes in the network. Based on the analysis of HTTP traffic over a training period, we designed filters to help detect anomalies in outbound HTTP traffic using metrics such as request regularity, bandwidth usage, interrequest delay time, and transaction size. Subsequently, Web Tap was evaluated on several available HTTP covert tunneling programs as well as a test backdoor program, which creates a remote shell from outside the network to a protected machine using only outbound HTTP transactions. Web Tap’s filters detected all the tunneling programs tested after modest use. Web Tap also analyzed the activity of approximately thirty faculty and students who agreed to use it as a proxy server over a 40 day period. It successfully detected a significant number of spyware and aware programs. This paper presents the design of Web Tap, results from its evaluation, as well as potential limits to Web Tap’s capabilities.	"Network security has been an increasing concern for network administrators and executives alike. Consequently, Firewalls and proxy servers have become prevalent among high-security networks (and even private homes). Many networks require all traffic to the internet to go through an HTTP proxy server or mail server, allowing no direct access to the internal network. This makes the job of a hacker much more difficult than before, where direct access to network machines was available.
When a hacker attacks a network with no direct access to the internet, the first step is getting a user to access a malicious file or web site. This can be done effectively by e-mailing a Trojan horse program or a link to a page which exploits the browser <NO>. Once the machine is compromised, the next step is to establish a path of communication. Traditionally, this would be done by installing a backdoor program such as BackOrifice <NO>. The problem with using such programs on firewalled networks is that they listen for an incoming connection on a specific port. All incoming traffic, however, is blocked. This means that the only way to communicate with a compromised machine is to have it make a callback (outbound connection). Often, the only two ways out of the network are through a mail server or through a proxy server. Since e-mail is often more closely logged and filtered, the hacker may find outbound HTTP transactions to be the best avenue for communication with a compromised workstation.
Spyware is also a huge problem for both system administrators and users alike <NO>. Besides annoying users by popping up advertisements, spyware can leak information about a user’s behavior or even send data on the machine to outside servers. Spyware programs can also degrade system performance and take valuable time and effort to remove. In addition to these lesser threats, Security holes have been found in Gator and eZula (two popular spyware programs) that would allow a hacker to execute arbitrary code on a target machine <NO>.
Web Tap is a network-level anomaly detection system that takes advantage of legitimate web request patterns to detect covert communication, backdoors, and spyware activity that is tunneled through outbound HTTP connections. We note that, unlike the previous work on securing web servers (e.g., <NO>), Web Tap’s focus is on analyzing outbound HTTP traffic from protected network machines to outside web servers, rather than guarding web servers against hostile attacks. The goal is to make it more difficult for hackers or malicious users to run Trojan and HTTP tunnel programs within an organization that leak information to the outside. Web Tap is designed for deployment at an organization’s HTTP proxy server (either passively or actively) to help detect anomalies in outbound traffic.
To evaluate Web Tap, we used it to look at web traffic from 30 clients over a 40-day period as well as traffic from known Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CCS’04, October 25-29, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-961-6/04/0010…$5.00.
HTTP tunneling programs. We were successful at detecting different types of spyware and adware as well as many data mining and advertisement servers. During the 40 days of observation, Web Tap generated alerts for adware clients such as Weatherbug, Kazaa, Lycos search bar, Google search bar, and Gator. It also was able to find programs which may be unwanted in a workplace environment such as iTunes, BitTorrent, and AIM Express. In addition to non-browser clients, Web Tap detected data mining and advertisement sites such as coremetrics.com, ru4.com abetterinternet.com, and doubleclick.net. Two of the three known HTTP tunneling programs tested, Wsh <NO> and Firepass <NO>, immediately caused Web Tap to raise an alert. The third HTTP tunnel, Hopster <NO>, was detected an hour and twenty minutes after it began running. A custom HTTP tunnel that we designed, which does a better job of mimicking legitimate browser requests, was detected within seven hours. When the backdoor was actively used to transfer files, it was detected almost immediately.
The rest of the paper is presented as follows. Section 2 discusses related work. Section 3 gives a threat model. Section 4 presents the design of filtering methods, based on measurements during the one week training phase. Section 5 provides an evaluation of Web Tap for an extended period after the evaluation phase. Section 6 talks about vulnerabilities in Web Tap filters. Section 7 outlines future work and Section 8 concludes."	https://doi.org/10.1145/1030083.1030100	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Borders, et al. [NO] designs filters to help detect anomalies in outbound http traffic using metrics such as request regularity, bandwidth usage, interrequest delay time, and transaction size. 	web http tap server outbound 
Polyglot: Automatic extraction of protocol message format using dynamic binary analysis	[', Polyglot <NO> and Panorama <NO>), Siren <NO> and oth-']	Juan Caballero, Heng Yin, Zhenkai Liang, and Dawn Song. 2007. Polyglot: Automatic extraction of protocol message format using dynamic binary analysis. Proceedings of the 14th ACM Conference on Computer and Communications Security (CCS).	Protocol reverse engineering, the process of extracting the application-level protocol used by an implementation, without access to the protocol specification, is important for many network security applications. Recent work [17] has proposed protocol reverse engineering by using clustering on network traces. That kind of approach is limited by the lack of semantic information on network traces. In this paper we propose a new approach using program binaries. Our approach, shadowing, uses dynamic analysis and is based on a unique intuition—the way that an implementation of the protocol processes the received application data reveals a wealth of information about the protocol message format. We have implemented our approach in a system called Polyglot and evaluated it extensively using real-world implementations of five different protocols: DNS, HTTP, IRC, Samba and ICQ. We compare our results with the manually crafted message format, included in Wireshark, one of the state-ofthe-art protocol analyzers. The differences we find are small and usually due to different implementations handling fields in different ways. Finding such differences between implementations is an added benefit, as they are important for problems such as fingerprint generation, fuzzing, and error detection.	Security	https://doi.org/10.1145/1315245.1315286	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Caballero, et al. [NO] proposes a new approach using program binaries. 	protocol implementation different difference format 
On deriving unknown vulnerabilities from zero-day polymorphic and metamorphic worm exploits	['Although signature detection techniques are widely used, they are not effective against zero-day attacks (new malicious code), polymorphic attacks (different encryptions of the same binary), or metamorphic attacks (different code for the same functionality) <NO>.']	Jedidiah R. Crandall, Zhendong Su, S. Felix Wu, and Frederic T. Chong. 2005. On deriving unknown vulnerabilities from zero-day polymorphic and metamorphic worm exploits. Proceedings of the 12th ACM Conference on Computer and Communications Security (CCS).	Vulnerabilities that allow worms to hijack the control flow of each host that they spread to are typically discovered months before the worm outbreak, but are also typically discovered by third party researchers. A determined attacker could discover vulnerabilities as easily and create zero-day worms for vulnerabilities unknown to network defenses. It is important for an analysis tool to be able to generalize from a new exploit observed and derive protection for the vulnerability. Many researchers have observed that certain predicates of the exploit vector must be present for the exploit to work and that therefore these predicates place a limit on the amount of polymorphism and metamorphism available to the attacker. We formalize this idea and subject it to quantitative analysis with a symbolic execution tool called DACODA. Using DACODA we provide an empirical analysis of 14 exploits (seven of them actual worms or attacks from the Internet, caught by Minos with no prior knowledge of the vulnerabilities and no false positives observed over a period of six months) for four operating systems. Evaluation of our results in the light of these two models leads us to conclude that 1) single contiguous byte string signatures are not effective for content filtering, and tokenbased byte string signatures composed of smaller substrings are only semantically rich enough to be effective for content filtering if the vulnerability lies in a part of a protocol that is not commonly used, and that 2) practical exploit analysis must account for multiple processes, multithreading, and kernel processing of network data necessitating a focus on primitives instead of vulnerabilities. 	Zero-day worms that exploit unknown vulnerabilities are a very real threat. Typically vulnerabilities are discovered by “white hat” hackers using fuzz testing <NO>, reverse engineering, or source code analysis and then the software vendors are notified. The same techniques for discovering these vulnerabilities could be as easily employed by “black hat” hackers, especially now that computer criminals are increasingly seeking profit rather than mischief. None of the 14 exploits analyzed in this paper are for vulnerabilities discovered by the vendors of the software being attacked. A vulnerability gives the attacker an important primitive (a primitive is an ability the attacker has, such as the ability to write an arbitrary value to an arbitrary location in a process’ address space), and then the attacker can build different exploits using this primitive. The host contains information about the vulnerability and primitive that cannot be determined from network traffic alone. It is impossible to generalize how the attack might morph in the future without this information. In order to respond effectively during an incipient worm outbreak, an automated analysis tool must be able to generalize one instance of an exploit and derive protection for the exploited vulnerability, since a worm can build multiple exploits for the same vulnerability from primitives.	https://doi.org/10.1145/1102120.1102152	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	R. et al. [NO] formalizes this idea and subject it to quantitative analysis with a symbolic execution tool called dacoda. 	vulnerability exploit observed worm dacoda 
A feature selection and evaluation scheme for computer virus detection	['<NO> used intra-family and inter-family support for n-grams for feature selection.', 'N-grams Static Hybrid <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO> Misuse <NO>']	Olivier Henchiri, and Nathalie Japkowicz. 2006. A feature selection and evaluation scheme for computer virus detection. Proceedings of the 6th International Conference on Data Mining.	Anti-virus systems traditionally use signatures to detect malicious executables, but signatures are overfitted features that are of little use in machine learning. Other more heuristic methods seek to utilize more general features, with some degree of success. In this paper, we present a data mining approach that conducts an exhaustive feature search on a set of computer viruses and strives to obviate over-fitting. We also evaluate the predictive power of a classifier by taking into account dependence relationships that exist between viruses, and we show that our classifier yields high detection rates and can be expected to perform as well in real-world conditions.	"Current research applying data mining to virus detection strives to automate the search for features used in classification. This process has been tackled from two different angles: extracting optimal signatures from a dataset of viruses, and discovering more general features for use in a complex classification scheme.
Extracting virus signatures is not a new problem. Kephart et al. <NO> developed a popular extraction method for virus signatures, by infecting a large number of files with a given virus and then harvesting for constant regions of 12 to 36 bytes. Then, from the considerable number of signatures collected, the ones with lowest predicted false positive rates were selected. While this method make it possible to extract signatures quickly and without the help of an expert, the authors concede that the algorithm fails for viruses that are moderately polymorphic.
Some detection methods utilize a variety of features, such as Win32 dll file calls, ASCII strings and byte sequences contained in the binary files. In an early heuristic approach <NO>, features such as duplicated UNIX system calls and files targeted by the program for writing purposes were used to detect malicious executables. In a machine learning method developed by Matthew Schultz et al. <NO>, ASCII strings and bytes sequences yielded good results. However, despite the byte sequences having a fixed length of 16, the feature space was very large, such that their dataset had to be split into partitions and different classifiers trained separately on each of them. Research in non signature-based heuristics has shown that sequences as short as 4 bytes can be used to detect unseen virus instances successfully <NO>. However, as was found in <NO>, the list of candidate features extracted from a small dataset can contain tens of thousands of sequences.
Finally, many viruses are considered to belong to common virus families, based on the similarities in structure, code or method of infection that they share <NO>. This classification is crucial to properly evaluating the effectiveness of a virus detection system. The first occurrence of a new kind of virus is typically the most devastating, as virus scanners are often incapable of detecting it. Then a host of variants typically emerge soon after the initial outbreak, albeit with less damaging consequences. Our method uses a priori knowledge of virus families, and evaluates the ability of our classifier to detect instances of a family without having been trained on any other instance from that same family."	https://doi.org/10.1109/ICDM.2006.4	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Henchiri, et al. [NO] presents a data mining approach that conducts an exhaustive feature search on a set of computer viruses and strives to obviate over-fitting. 	virus conduct degree dependence exhaustive 
Learning to detect malicious executables in the wild	['A similar approach was used by <NO>, where they trained Instance-Based Learner, TFIDF, Naive-Bayes, support vector machines, decision tree, boosted Naive-Bayes, boosted SVMs and boosted decision tree on n-grams.', 'N-grams Static Hybrid <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO> Misuse <NO>', 'Hence, malware detection is one of the internet security topics that are of great interest <NO>.', 'Recently, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'In the first step, various features such as Application Programming Interface (API) calls <NO> and program strings <NO> are extracted to capture the characteristics of the file samples.', 'In the second step, intelligent classification techniques such as decision trees <NO>, Naive Bayes, and associative classifiers <NO> are used to automatically classify the file samples into different classes based on computational analysis of the feature representations.', 'Classification: For classification, over the last couple of years, many data mining and machine learning approaches have been adopted for malware detection <NO>.', 'Naive Bayes method, Support Vector Machine(SVM), decision tree and associative classification methods are applied to detect new malicious executables in previous studies <NO>.', 'So far, several data mining and machine-learning approaches have been used in malware detection <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'the previous studies <NO>, <NO>, <NO> and the results show that our IMDS applying associative classification method outperforms other classification approaches in both detection rate', 'tion <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Naive Bayes method, SVM, and decision tree classifiers are used to detect new malicious executables in previous studies <NO>, <NO>, <NO>.', 'The collected data in our work is significantly larger than those used in previous studies on data mining for malware detection <NO>.', '<NO> gathered 1971 benign executables and 1651 malicious executables in Windows PE format, and examined the performance of different classifiers such as Naive Bayes, support vector machine (SVM) and Decision Tree using 10-fold cross validation and plotting ROC curves <NO>.', 'Over the last few years, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'Both classifiers have been successfully used in malware detection <NO> and have distinct properties.', 'In order to overcome the disadvantages of the widely-used signature-based malware detection method, data mining and machine learning approaches are proposed for malware detection <NO>.', 'Naive Bayes, Support Vector Machine(SVM) and Decision Tree classifiers are used to detect new malicious executables based on small data collection in the previous studies <NO>.', '2) Both associative classification and SVM have been successfully applied in malware detection <NO>.', 'The problem of detecting malware using data mining <NO> involves classifying each executable as either benign or malicious.']	Jeremy Z. Kolter, and Marcus A. Maloof. 2004. Learning to detect malicious executables in the wild. Proceedings of the International Conference on Knowledge Discovery and Data Mining (SIGKDD).	In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.	"Malicious code is “any code added, changed, or removed from a software system to intentionally cause harm or subvert the system’s intended function” <NO>. Such software has been used to compromise computer systems, to destroy their information, and to render them useless. It has also been used to gather information, such as passwords and credit card numbers, and to distribute information, such as pornography, all without the knowledge of the system’s users. As more novice users obtain sophisticated computers
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD’04, August 22–25, 2004, Seattle, Washington, USA. Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
with high-speed connections to the Internet, the potential for further abuse is great.
Malicious executables generally fall into three categories based on their transport mechanism: viruses, worms, and Trojan horses. Viruses inject malicious code into existing programs, which become “infected” and, in turn, propagate the virus to other programs when executed. Viruses come in two forms, either as an infected executable or as a virus loader, a small program that only inserts viral code. Worms, in contrast, are self-contained programs that spread over a network, usually by exploiting vulnerabilities in the software running on the networked computers. Finally, Trojan horses masquerade as benign programs, but perform malicious functions. Malicious executables do not always fit neatly into these categories and can exhibit combinations of behaviors.
Excellent technology exists for detecting known malicious executables. Software for virus detection has been quite successful, and programs such as McAfee Virus Scan and Norton AntiVirus are ubiquitous. Indeed, Dell recommends Norton AntiVirus for all of its new systems. Although these products use the word virus in their names, they also detect worms and Trojan horses.
These programs search executable code for known patterns, and this method is problematic. One shortcoming is that we must obtain a copy of a malicious program before extracting the pattern necessary for its detection. Obtaining copies of new or unknown malicious programs usually entails them infecting or attacking a computer system.
To complicate matters, writing malicious programs has become easier: There are virus kits freely available on the Internet. Individuals who write viruses have become more sophisticated, often using mechanisms to change or obfuscate their code to produce so-called polymorphic viruses <NO>. Indeed, researchers have recently discovered that simple obfuscation techniques foil commercial programs for virus detection <NO>. These challenges have prompted some researchers to investigate learning methods for detecting new or unknown viruses, and more generally, malicious code.
Our efforts to address this problem have resulted in a fielded application, built using techniques from machine learning <NO> and data mining <NO>. The Malicious Executable Classification System (mecs) currently detects unknown malicious executables “in the wild”, that is, without removing any obfuscation. To date, we have gathered 1971 system and non-system executables, which we will refer to as “benign” executables, and 1651 malicious executables with a variety of transport mechanisms and payloads (e.g., key-
loggers and backdoors). Although all were for the Windows operating system, it is important to note that our approach is not restricted to this operating system.
We extracted byte sequences from the executables, converted these into n-grams, and constructed several classifiers: ibk, tfidf, naive Bayes, support vector machines (svms), decision trees, boosted naive Bayes, boosted svms, and boosted decision trees. In this domain, there is an issue of unequal but unknown costs of misclassification error, so we evaluated the methods using receiver operating characteristic (roc) analysis <NO>, using area under the roc curve as the performance metric. Ultimately, boosted decision trees outperformed all other methods with an area under the curve of 0.996.
We delivered mecs to the mitre Corporation, the sponsors of this project, as a research prototype. Users interact with mecs through a command line. They can add new executables to the collection, update learned models, display roc curves, and produce a single classifier at a specific operating point on a selected roc curve.
With this paper, we make three main contributions. We show how established methods for text classification apply to executables. We present empirical results from an extensive study of inductive methods for detecting malicious executables in the wild. We report on a fielded application developed using machine learning and data mining.
In the three sections that follow, we describe related work, our data collection, and the methods we applied. Then, in Section 6, we present empirical results, and in Section 7, we discuss these results and other approaches."	https://doi.org/10.1145/1014052.1014105	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Z. et al. [NO] describes the development of a fielded application for detecting malicious executables in the wild. 	gram n fielded executables decision 
Valgrind: A framework for heavyweight dynamic binary instrumentation	[]	Nicholas Nethercote, and Julian Seward. 2007. Valgrind: A framework for heavyweight dynamic binary instrumentation. Proceedings of ACM SIGPLAN 2007 Conference on Programming Language Design and Implementation.	Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values—a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.	"Many programmers use program analysis tools, such as error checkers and profilers, to improve the quality of their software. Dynamic binary analysis (DBA) tools are one such class of tools; they analyse programs at run-time at the level of machine code.
DBA tools are often implemented using dynamic binary instrumentation (DBI), whereby the analysis code is added to the original code of the client program at run-time. This is convenient for users,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
PLDI’07 June 11–13, 2007, San Diego, California, USA. Copyright c© 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00
as no preparation (such as recompiling or relinking) is needed. Also, it gives 100% instrumentation coverage of user-mode code, without requiring source code. Several generic DBI frameworks exist, such as Pin <NO>, DynamoRIO <NO>, and Valgrind <NO>. They provide a base system that can instrument and run code, plus an environment for writing tools that plug into the base system.
The performance of DBI frameworks has been studied closely <NO>. Less attention has been paid to their instrumentation capabilities, and the tools built with them. This is a shame, as it is the tools that make DBI frameworks useful, and complex tools are more interesting than simple tools. As a result, we believe the potential of DBI has not been fully exploited."	https://doi.org/10.1145/1250734.1250746	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Nethercote, et al. [NO] believes the potential of dbi has not been fully exploited. 	dbi valgrind dba framework build 
Graph-based malware distributors detection	[]	Andrei Venzhega, Polina Zhinalieva, and Nikolay Suboch. 2013. Graph-based malware distributors detection. Proceedings of the 22nd International Conference on World Wide Web Companion (WWW).	Search engines are currently facing a problem of websites that distribute malware. In this paper we present a novel efficient algorithm that learns to detect such kind of spam. We have used a bipartite graph with two types of nodes, each representing a layer in the graph: web-sites and file hostings (FH), connected with edges representing the fact that a file can be downloaded from the hosting via a link on the web-site. The performance of this spam detection method was verified using two set of ground truth labels: manual assessments of antivirus analysts and automatically generated assessments obtained from antivirus companies. We demonstrate that the proposed method is able to detect new types of malware even before the best known antivirus solutions are able to detect them.	"Due to Internet propagation malware has been rapidly spreading and infecting computers around the world at an unprecedented rate <NO> and malware detection became one of the top internet security topics <NO>. Security software developers reported that the release rate of malicious code and other unwanted programs may be exceeding that of legitimate software applications <NO>.
Search engines (SE) have become one of the principal boosters of malware distribution. Users are looking for software with SE, but sometimes instead of sites of software developers or legal distributors, they get fake websites or malware distributors (MD).
Recently, SEs began to realize their unintentional contribution in malware distribution. To protect users from malware search results they made agreements on cooperation between SE and antivirus companies. Web services enable the identification of malware with a huge partners data about viruses collected, e.g. virustotal.com1. But even a huge malware database does not guarantee detection of new ones. Most of anti-malware software products, such as Kaspersky, Symantec, MacAfee typically use the signaturebased method to recognize threats2. But malware writers successfully invent counter-measures against proposed malware analysis techniques. Today’s malware samples are created at a rate of thousands per day. According to Symantec’s annual report <NO>: 5,5 billion malware attacks were blocked in 2011, 81% more than in 2010. More than 403 million new types of malicious software were detected in 2011, 41% more than in 2011. Symantec reports huge amount of blocked malware, but they estimate that new malware techniques are able to generate an almost unique version of their malware for each potential victim. This suggests traditional signature-based malware detection solutions will likely be outpaced by the number of innovative threats being created by malware authors. A new radically different approach to the problem is currently needed.
SE companies are the first who face a threat from newmalwares. That is why early detection of new malware and in particular their distributors is the principle objective of ensuring safe and high-quality web search. Some websites even if they are not MDs, but closely related to the distributors, for example, linked with hyperlinks, can also be dangerous. We can even suspect them of intentional cooperation with distributors of viral software. Therefore to find suspicious websites, we propose an approach that consists in spreading information about MD via connections between neighbours, which is similar to the idea of homophily. We used a bipartite graph with two types of nodes: website and FH. An edge represents the fact that a file hosted on FH and can be downloaded from the website."	https://doi.org/10.1145/2487788.2488136	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Venzhega, et al. [NO] presents a novel efficient algorithm that learns to detect such kind of spam. 	assessment representing site antivirus spam 
Survey of clustering algorithms	['partitioning clustering are two common type of clustering methods and each of them has its own traits <NO>.']	Rui Xu, and Donald Wunsch. 2005. Survey of clustering algorithms. IEEE Transactions on Neural Networks 16, 3 (2005), 645–678.	Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.	"Different starting points and criteria usually lead to different taxonomies of clustering algorithms <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. A rough but widely agreed frame is to classify clustering techniques as hierarchical clustering and partitional clustering, based on the properties of clusters generated <NO>, <NO>. Hierarchical clustering groups data objects with a sequence of partitions, either from singleton clusters to a cluster including all individuals or vice versa, while partitional clustering directly divides data objects into some prespecified number of clusters without the hierarchical structure. We follow this frame in surveying the clustering algorithms in the literature. Beginning with the discussion on proximity measure, which is the basis for most clustering algorithms, we focus on hierarchical clustering and classical partitional clustering algorithms in Section II-B–D. Starting from part E, we introduce and analyze clustering algorithms based on a wide variety of theories and techniques, including graph theory, combinatorial search techniques, fuzzy set theory, neural networks, and kernels techniques. Compared with graph theory and fuzzy set
theory, which had already been widely used in cluster analysis before the 1980s, the other techniques have been finding their applications in clustering just in the recent decades. In spite of the short history, much progress has been achieved. Note that these techniques can be used for both hierarchical and partitional clustering. Considering the more frequent requirement of tackling sequential data sets, large-scale, and high-dimensional data sets in many current applications, we review clustering algorithms for them in the following three parts. We focus particular attention on clustering algorithms applied in bioinformatics. We offer more detailed discussion on how to identify appropriate number of clusters, which is particularly important in cluster validity, in the last part of the section."	https://doi.org/10.1109/TNN.2005.845141	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Xu, et al. [NO] survey clusters algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. 	cluster hand appearing attracting benchmark 
IMDS: Intelligent malware detection system	"['<NO> developed Intelligent Malware Detection System (IMDS) that used Objective-Oriented Association (OOA) mining based classification.', 'API/System Calls Static Hybrid <NO>, <NO> Dynamic Anomaly <NO>', '<NO> proposed Intelligent Malware Detection System (IMDS) using Object Oriented Association (OOA) mining based classification.', 'API <NO> Detects polymorphic and unknown malware.', 'Hence, malware detection is one of the internet security topics that are of great interest <NO>.', 'Recently, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'In the first step, various features such as Application Programming Interface (API) calls <NO> and program strings <NO> are extracted to capture the characteristics of the file samples.', 'In the second step, intelligent classification techniques such as decision trees <NO>, Naive Bayes, and associative classifiers <NO> are used to automatically classify the file samples into different classes based on computational analysis of the feature representations.', 'For example, IMDS <NO> performs association classification on Windows API calls extracted from executable files, while Naive Bayes methods on the extracted strings and byte sequences are applied in <NO>.', 'PE is designed as a common file format for all flavor of Windows operating system, and PE malware are in the majority of the malware rising in recent years <NO>.', 'We extract the Application Programming Interface (API) calls from the Import Tables <NO> of collected malicious and benign PE files, convert them to a group of 32-bit global IDs (for example, the API ""MAPI32.', 'Compared with dynamic feature extraction methods, static feature extraction methods are easier and less expensive <NO>.', 'Classification: For classification, over the last couple of years, many data mining and machine learning approaches have been adopted for malware detection <NO>.', 'Naive Bayes method, Support Vector Machine(SVM), decision tree and associative classification methods are applied to detect new malicious executables in previous studies <NO>.', 'So far, several data mining and machine-learning approaches have been used in malware detection <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'In our previous paper <NO>, <NO>, since', 'tion <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Due to the fact that frequent itemsets (sets of API calls) discovered by association mining can well represent the underlying semantics (profiles) of malware and benign file datasets, associative classification has been successfully used in the IMDS system developed in <NO>', 'It can be effectively used in malware detection <NO>, <NO>, since frequent itemsets are typically of statistical significance and classifiers based on frequent pattern analysis are', 'Based on the system architecture of our previous malware detection system IMDS <NO>, we extract the API calls as the features of the file samples and store them in the signature database.', 'For rule generation, we use the OOA_Fast_FP-Growth algorithm proposed in <NO> to derive the complete set of the rules with certain support and confidence thresholds, since it is much faster than Apriori for mining frequent itemsets.', 'In our paper, based on the complete set of the rules generated by the malware detection rule generator, IMDS applied the technique of CBACB <NO> to build a classifier as the malware detection module to predict new file samples <NO>.', 'By using the OOA_Fast_FP-Growth algorithm <NO>, <NO>, we generate 31', 'Over the last few years, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'For example, IMDS <NO> performs association classification on Windows API calls extracted from executable files while Naive Bayes methods on the extracted strings and byte sequences are applied in <NO>.', 'Both classifiers have been successfully used in malware detection <NO> and have distinct properties.', 'Recently, associative classification <NO>, with its ability to utilize relationships among attributes, has been also applied in <NO>.', '2) Both associative classification and SVM have been successfully applied in malware detection <NO>.', 'Various classification approaches including association classifiers, support vector machines, and Naive Bayes have been applied in malware detection <NO>.']"	Yanfang Ye, Dingding Wang, Tao Li, and Dongyi Ye. 2007. IMDS: Intelligent malware detection system. Proccedings of ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD).	The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based antivirus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using ObjectiveOriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA Fast FPGrowth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of KingSoft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system outperform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.	"Besides the traditional signature-based malware detection methods, there is some work to improve the signature-based detection <NO> and also a few attempts to apply data mining and machine learning techniques to detect new malicious executables.
Sung et al. <NO> developed a signature based malware detection system called SAVE (Static Analyzer of Vicious Executables) which emphasized on detecting polymorphic malware. The basic idea of this approach is to extract the signatures from the original malware with the hypothesis that all versions of the same malware
share a common core signature. Schultz et al. <NO> applied Naive Bayes method to detect previously unknown malicious code. Decision Tree was studied in <NO>. Kolter et al. <NO> gathered 1971 benign executables and 1651 malicious executables in Windows PE format, and examined the performance of different classifiers such as Naive Bayes, support vector machine (SVM) and Decision Tree using 10-fold cross validation and plotting ROC curves <NO>. Their results also showed that the ROC curve of the Decision Tree method dominated all others.
Different from earlier studies, our work is based on a large collection of malicious executables collected at KingSoft Anti-Virus Laboratory. In addition, we apply OOA mining technique to extract the characterizing frequent patterns to achieve accurate malware detection since frequent patterns found by association mining carry the underlying semantics of the data."	https://doi.org/10.1145/1281192.1281308	2	[]	['dbi_frameworks', 'web_tap', 'decision_trees', 'web_tap’s', 'able_detect']	['byte_string_signatures', 'detect_malicious_executables', 'effective_content_filtering', 'protocol_reverse_engineering', 'tunneling_programs_test']	Ye, et al. [NO] develops the intelligent malware detection system (imds) using objectiveoriented association (ooa) mining based classification. 	ooa imds rule pe system 
