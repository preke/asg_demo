ref_title	ref_context	ref_entry	abstract	intro	ref_link	label	topic_word	topic_bigram	topic_trigram	description	top_n_words
Web tap: Detecting covert web traffic	[]	Kevin Borders, and Atul Prakash. 2004. Web tap: Detecting covert web traffic. Proceedings of the 11th ACM Conference on Computer and Communications Security.	As network security is a growing concern, system administrators lock down their networks by closing inbound ports and only allowing outbound communication over selected protocols such as HTTP. Hackers, in turn, are forced to find ways to communicate with compromised workstations by tunneling through web requests. While several tools attempt to analyze inbound traffic for denial-of-service and other attacks on web servers, Web Tap’s focus is on detecting attempts to send significant amounts of information out via HTTP tunnels to rogue Web servers from within an otherwise firewalled network. A related goal of Web Tap is to help detect spyware programs, which often send out personal data to servers using HTTP transactions and may open up security holes in the network. Based on the analysis of HTTP traffic over a training period, we designed filters to help detect anomalies in outbound HTTP traffic using metrics such as request regularity, bandwidth usage, interrequest delay time, and transaction size. Subsequently, Web Tap was evaluated on several available HTTP covert tunneling programs as well as a test backdoor program, which creates a remote shell from outside the network to a protected machine using only outbound HTTP transactions. Web Tap’s filters detected all the tunneling programs tested after modest use. Web Tap also analyzed the activity of approximately thirty faculty and students who agreed to use it as a proxy server over a 40 day period. It successfully detected a significant number of spyware and aware programs. This paper presents the design of Web Tap, results from its evaluation, as well as potential limits to Web Tap’s capabilities.	"Network security has been an increasing concern for network administrators and executives alike. Consequently, Firewalls and proxy servers have become prevalent among high-security networks (and even private homes). Many networks require all traffic to the internet to go through an HTTP proxy server or mail server, allowing no direct access to the internal network. This makes the job of a hacker much more difficult than before, where direct access to network machines was available.
When a hacker attacks a network with no direct access to the internet, the first step is getting a user to access a malicious file or web site. This can be done effectively by e-mailing a Trojan horse program or a link to a page which exploits the browser <NO>. Once the machine is compromised, the next step is to establish a path of communication. Traditionally, this would be done by installing a backdoor program such as BackOrifice <NO>. The problem with using such programs on firewalled networks is that they listen for an incoming connection on a specific port. All incoming traffic, however, is blocked. This means that the only way to communicate with a compromised machine is to have it make a callback (outbound connection). Often, the only two ways out of the network are through a mail server or through a proxy server. Since e-mail is often more closely logged and filtered, the hacker may find outbound HTTP transactions to be the best avenue for communication with a compromised workstation.
Spyware is also a huge problem for both system administrators and users alike <NO>. Besides annoying users by popping up advertisements, spyware can leak information about a user’s behavior or even send data on the machine to outside servers. Spyware programs can also degrade system performance and take valuable time and effort to remove. In addition to these lesser threats, Security holes have been found in Gator and eZula (two popular spyware programs) that would allow a hacker to execute arbitrary code on a target machine <NO>.
Web Tap is a network-level anomaly detection system that takes advantage of legitimate web request patterns to detect covert communication, backdoors, and spyware activity that is tunneled through outbound HTTP connections. We note that, unlike the previous work on securing web servers (e.g., <NO>), Web Tap’s focus is on analyzing outbound HTTP traffic from protected network machines to outside web servers, rather than guarding web servers against hostile attacks. The goal is to make it more difficult for hackers or malicious users to run Trojan and HTTP tunnel programs within an organization that leak information to the outside. Web Tap is designed for deployment at an organization’s HTTP proxy server (either passively or actively) to help detect anomalies in outbound traffic.
To evaluate Web Tap, we used it to look at web traffic from 30 clients over a 40-day period as well as traffic from known Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CCS’04, October 25-29, 2004, Washington, DC, USA. Copyright 2004 ACM 1-58113-961-6/04/0010…$5.00.
HTTP tunneling programs. We were successful at detecting different types of spyware and adware as well as many data mining and advertisement servers. During the 40 days of observation, Web Tap generated alerts for adware clients such as Weatherbug, Kazaa, Lycos search bar, Google search bar, and Gator. It also was able to find programs which may be unwanted in a workplace environment such as iTunes, BitTorrent, and AIM Express. In addition to non-browser clients, Web Tap detected data mining and advertisement sites such as coremetrics.com, ru4.com abetterinternet.com, and doubleclick.net. Two of the three known HTTP tunneling programs tested, Wsh <NO> and Firepass <NO>, immediately caused Web Tap to raise an alert. The third HTTP tunnel, Hopster <NO>, was detected an hour and twenty minutes after it began running. A custom HTTP tunnel that we designed, which does a better job of mimicking legitimate browser requests, was detected within seven hours. When the backdoor was actively used to transfer files, it was detected almost immediately.
The rest of the paper is presented as follows. Section 2 discusses related work. Section 3 gives a threat model. Section 4 presents the design of filtering methods, based on measurements during the one week training phase. Section 5 provides an evaluation of Web Tap for an extended period after the evaluation phase. Section 6 talks about vulnerabilities in Web Tap filters. Section 7 outlines future work and Section 8 concludes."	https://doi.org/10.1145/1030083.1030100	2	[]	['web_tap', 'web_tap’s', 'able_detect', 'access_processing', 'based_classification']	['access_processing_behavior', 'information_access_processing', 'tunneling_programs_test', '000_recent_malware', '25_000_recent']	Borders, et al. [NO] designs filters to help detect anomalies in outbound http traffic using metrics such as request regularity, bandwidth usage, interrequest delay time, and transaction size. 	web http tap server outbound 
Ether: Malware analysis via hardware virtualization extensions	['We choose the Intel Pin program <NO> because it allowed us to collect both instructions and system calls simultaneously, but it does not make an effort to be a transparent tracing tool like the Ether framework <NO>.', 'introspection <NO>, information flow tracking <NO>, <NO>, instruction trace monitoring <NO>–<NO>.']	Artem Dinaburg, Paul Royal, Monirul Sharif, and Wenke Lee. 2008. Ether: Malware analysis via hardware virtualization extensions. Proceedings of the 15th ACM Conference on Computer and Communications Security (CCS).	Malware has become the centerpiece of most security threats on the Internet. Malware analysis is an essential technology that extracts the runtime behavior of malware, and supplies signatures to detection systems and provides evidence for recovery and cleanup. The focal point in the malware analysis battle is how to detect versus how to hide a malware analyzer from malware during runtime. State-of-the-art analyzers reside in or emulate part of the guest operating system and its underlying hardware, making them easy to detect and evade. In this paper, we propose a transparent and external approach to malware analysis, which is motivated by the intuition that for a malware analyzer to be transparent, it must not induce any side-effects that are unconditionally detectable by malware. Our analyzer, Ether, is based on a novel application of hardware virtualization extensions such as Intel VT, and resides completely outside of the target OS environment. Thus, there are no in-guest software components vulnerable to detection, and there are no shortcomings that arise from incomplete or inaccurate system emulation. Our experiments are based on our study of obfuscation techniques used to create 25,000 recent malware samples. The results show that Ether remains transparent and defeats the obfuscation tools that evade existing approaches.	"Malware–the increasingly common vehicle by which criminal organizations facilitate online crime–has become an artifact whose use intersects multiple major security threats (e.g., botnets) faced by information security practitioners. Given the financially motivated nature of these threats, methods of recovery now mandate more than just remediation: knowing what occurred after an asset became compromised is as valuable as knowing it was compromised. Concisely, independent of simple detection, there exists a pronounced need to understand the intentions or runtime behavior of modern malware.
Recent advances in malware analysis <NO> show promise in understanding modern malware, but before these and other approaches can be used to determine what a malware instance does or might do, the runtime behavior of that instance and/or an unobstructed view of its code must be obtained. However, malware authors are incentivized to complicate attempts at understanding the internal workings of their creations. Therefore, modern malware contain a myriad of anti-debugging, anti-instrumentation, and anti-VM techniques to stymie attempts at runtime observation <NO>. Similarly, techniques that use a malware instance’s static code model are challenged by runtime-generated code, which often requires execution to discover.
In the obfuscation/deobfuscation game played between attackers and defenders, numerous anti-evasion techniques have been applied in the creation of robust in-guest API call tracers and automated deobfuscation tools <NO>. More recent frameworks <NO> and their discrete components <NO> attempt to offer or mimic a level of transparency analogous to that of a non-instrumented OS running on physical hardware. However, given that nearly all of these approaches reside in or emulate part of the guest OS or its underlying hardware, little effort is required by a knowledgeable adversary to detect their existence and evade <NO>.
In this paper we present a transparent, external approach to malware analysis. Our approach is motivated by the intuition that for a malware analyzer to be transparent, it must not induce any side-effects that are unconditionally detectable by its observation target. In formalizing this intuition, we model the structural properties and execution semantics of modern programs to derive the requirements for transparent malware analysis. An analyzer that satisfies these transparency requirements can obtain an execution trace of a program identical to that if it were run in an environment with no analyzer present. Approaches unable to fulfill these requirements are vulnerable to one or more de-
tection attacks–categorical, formal abstractions of detection techniques employed by modern malware.
Creating a transparent malware analyzer required us to diverge from existing approaches that employ in-guest components, API virtualization or partial or full system emulation, because none of these implementations satisfy all the transparency requirements. Based on novel application of hardware virtualization extensions such as Intel VT <NO>, our analyzer–called Ether–resides completely outside of the target OS environment– there are no in-guest software components vulnerable to detection or attack. Additionally, in contrast to other external approaches, the hardware-assisted nature of our approach implicitly avoids many shortcomings that arise from incomplete or inaccurate system emulation.
To demonstrate the efficacy of our approach we tested Ether with other academic and commercial approaches. Our testing included the analysis of specific in-the-wild malware instances that attempt to detect instrumentation and/or a virtual environment. In addition, we also surveyed over 25,000 recent malware samples to identify the distribution of obfuscation tools used in their creation; this knowledge was then used to create a synthetic sample set that represents the majority of the original corpus. The results of testing (presented in Section 5) show that Ether is able to remain transparent and defeat a large percentage of the obfuscation tools that evade existing approaches.
Our work represents the following contributions:
• A formal framework for describing program execution and analyzing the requirements for transparent malware analysis.
• Implementation of Ether, an external, transparent malware analyzer that operates using hardware virtualization extensions to offer both fine- (single instruction) and coarse- (system call) granularity tracing. To motivate the use of our approach by the information security community, the GPL’ed source code for Ether is available for download at http://ether.gtisc.gatech.edu.
• Broad-scale evaluation of current approaches using a proxy set of samples representing the majority of a recent, large malware corpus. Copies of discrete samples referenced in this paper and the 25,000 malware sample corpus used for our survey are available to any academic or industry professional at an accredited organization.
The remainder of this paper is organized as follows. Section 2 describes related work. Section 3 presents our model for programs and their execution, formal requirements for transparency, and abstract representations of failures in transparency that lead to detection attacks. Section 4 describes Ether’s design and implementation, including an in-depth explanation of how hardware virtualization extensions are leveraged. Section 5 details the experiment selection process and how experimentation was performed, and provides an analysis of the results. Finally, Section 6 briefly describes future work and provides some concluding remarks."	https://doi.org/10.1145/1455770.1455779	2	[]	['web_tap', 'web_tap’s', 'able_detect', 'access_processing', 'based_classification']	['access_processing_behavior', 'information_access_processing', 'tunneling_programs_test', '000_recent_malware', '25_000_recent']	Dinaburg, et al. [NO] proposes a transparent and external approach to malware analysis, which is motivated by the intuition that for a malware analyzer to be transparent, it must not induce any side-effects that are unconditionally detectable by malware. 	transparent analyzer ether guest evade 
Graph-based malware distributors detection	[]	Andrei Venzhega, Polina Zhinalieva, and Nikolay Suboch. 2013. Graph-based malware distributors detection. Proceedings of the 22nd International Conference on World Wide Web Companion (WWW).	Search engines are currently facing a problem of websites that distribute malware. In this paper we present a novel efficient algorithm that learns to detect such kind of spam. We have used a bipartite graph with two types of nodes, each representing a layer in the graph: web-sites and file hostings (FH), connected with edges representing the fact that a file can be downloaded from the hosting via a link on the web-site. The performance of this spam detection method was verified using two set of ground truth labels: manual assessments of antivirus analysts and automatically generated assessments obtained from antivirus companies. We demonstrate that the proposed method is able to detect new types of malware even before the best known antivirus solutions are able to detect them.	"Due to Internet propagation malware has been rapidly spreading and infecting computers around the world at an unprecedented rate <NO> and malware detection became one of the top internet security topics <NO>. Security software developers reported that the release rate of malicious code and other unwanted programs may be exceeding that of legitimate software applications <NO>.
Search engines (SE) have become one of the principal boosters of malware distribution. Users are looking for software with SE, but sometimes instead of sites of software developers or legal distributors, they get fake websites or malware distributors (MD).
Recently, SEs began to realize their unintentional contribution in malware distribution. To protect users from malware search results they made agreements on cooperation between SE and antivirus companies. Web services enable the identification of malware with a huge partners data about viruses collected, e.g. virustotal.com1. But even a huge malware database does not guarantee detection of new ones. Most of anti-malware software products, such as Kaspersky, Symantec, MacAfee typically use the signaturebased method to recognize threats2. But malware writers successfully invent counter-measures against proposed malware analysis techniques. Today’s malware samples are created at a rate of thousands per day. According to Symantec’s annual report <NO>: 5,5 billion malware attacks were blocked in 2011, 81% more than in 2010. More than 403 million new types of malicious software were detected in 2011, 41% more than in 2011. Symantec reports huge amount of blocked malware, but they estimate that new malware techniques are able to generate an almost unique version of their malware for each potential victim. This suggests traditional signature-based malware detection solutions will likely be outpaced by the number of innovative threats being created by malware authors. A new radically different approach to the problem is currently needed.
SE companies are the first who face a threat from newmalwares. That is why early detection of new malware and in particular their distributors is the principle objective of ensuring safe and high-quality web search. Some websites even if they are not MDs, but closely related to the distributors, for example, linked with hyperlinks, can also be dangerous. We can even suspect them of intentional cooperation with distributors of viral software. Therefore to find suspicious websites, we propose an approach that consists in spreading information about MD via connections between neighbours, which is similar to the idea of homophily. We used a bipartite graph with two types of nodes: website and FH. An edge represents the fact that a file hosted on FH and can be downloaded from the website."	https://doi.org/10.1145/2487788.2488136	2	[]	['web_tap', 'web_tap’s', 'able_detect', 'access_processing', 'based_classification']	['access_processing_behavior', 'information_access_processing', 'tunneling_programs_test', '000_recent_malware', '25_000_recent']	Venzhega, et al. [NO] presents a novel efficient algorithm that learns to detect such kind of spam. 	assessment representing site antivirus spam 
Survey of clustering algorithms	['partitioning clustering are two common type of clustering methods and each of them has its own traits <NO>.']	Rui Xu, and Donald Wunsch. 2005. Survey of clustering algorithms. IEEE Transactions on Neural Networks 16, 3 (2005), 645–678.	Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.	"Different starting points and criteria usually lead to different taxonomies of clustering algorithms <NO>, <NO>, <NO>, <NO>, <NO>, <NO>. A rough but widely agreed frame is to classify clustering techniques as hierarchical clustering and partitional clustering, based on the properties of clusters generated <NO>, <NO>. Hierarchical clustering groups data objects with a sequence of partitions, either from singleton clusters to a cluster including all individuals or vice versa, while partitional clustering directly divides data objects into some prespecified number of clusters without the hierarchical structure. We follow this frame in surveying the clustering algorithms in the literature. Beginning with the discussion on proximity measure, which is the basis for most clustering algorithms, we focus on hierarchical clustering and classical partitional clustering algorithms in Section II-B–D. Starting from part E, we introduce and analyze clustering algorithms based on a wide variety of theories and techniques, including graph theory, combinatorial search techniques, fuzzy set theory, neural networks, and kernels techniques. Compared with graph theory and fuzzy set
theory, which had already been widely used in cluster analysis before the 1980s, the other techniques have been finding their applications in clustering just in the recent decades. In spite of the short history, much progress has been achieved. Note that these techniques can be used for both hierarchical and partitional clustering. Considering the more frequent requirement of tackling sequential data sets, large-scale, and high-dimensional data sets in many current applications, we review clustering algorithms for them in the following three parts. We focus particular attention on clustering algorithms applied in bioinformatics. We offer more detailed discussion on how to identify appropriate number of clusters, which is particularly important in cluster validity, in the last part of the section."	https://doi.org/10.1109/TNN.2005.845141	2	[]	['web_tap', 'web_tap’s', 'able_detect', 'access_processing', 'based_classification']	['access_processing_behavior', 'information_access_processing', 'tunneling_programs_test', '000_recent_malware', '25_000_recent']	Xu, et al. [NO] survey clusters algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. 	cluster hand appearing attracting benchmark 
IMDS: Intelligent malware detection system	"['<NO> developed Intelligent Malware Detection System (IMDS) that used Objective-Oriented Association (OOA) mining based classification.', 'API/System Calls Static Hybrid <NO>, <NO> Dynamic Anomaly <NO>', '<NO> proposed Intelligent Malware Detection System (IMDS) using Object Oriented Association (OOA) mining based classification.', 'API <NO> Detects polymorphic and unknown malware.', 'Hence, malware detection is one of the internet security topics that are of great interest <NO>.', 'Recently, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'In the first step, various features such as Application Programming Interface (API) calls <NO> and program strings <NO> are extracted to capture the characteristics of the file samples.', 'In the second step, intelligent classification techniques such as decision trees <NO>, Naive Bayes, and associative classifiers <NO> are used to automatically classify the file samples into different classes based on computational analysis of the feature representations.', 'For example, IMDS <NO> performs association classification on Windows API calls extracted from executable files, while Naive Bayes methods on the extracted strings and byte sequences are applied in <NO>.', 'PE is designed as a common file format for all flavor of Windows operating system, and PE malware are in the majority of the malware rising in recent years <NO>.', 'We extract the Application Programming Interface (API) calls from the Import Tables <NO> of collected malicious and benign PE files, convert them to a group of 32-bit global IDs (for example, the API ""MAPI32.', 'Compared with dynamic feature extraction methods, static feature extraction methods are easier and less expensive <NO>.', 'Classification: For classification, over the last couple of years, many data mining and machine learning approaches have been adopted for malware detection <NO>.', 'Naive Bayes method, Support Vector Machine(SVM), decision tree and associative classification methods are applied to detect new malicious executables in previous studies <NO>.', 'So far, several data mining and machine-learning approaches have been used in malware detection <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'In our previous paper <NO>, <NO>, since', 'tion <NO>, <NO>, <NO>, <NO>, <NO>, <NO>, <NO>.', 'Due to the fact that frequent itemsets (sets of API calls) discovered by association mining can well represent the underlying semantics (profiles) of malware and benign file datasets, associative classification has been successfully used in the IMDS system developed in <NO>', 'It can be effectively used in malware detection <NO>, <NO>, since frequent itemsets are typically of statistical significance and classifiers based on frequent pattern analysis are', 'Based on the system architecture of our previous malware detection system IMDS <NO>, we extract the API calls as the features of the file samples and store them in the signature database.', 'For rule generation, we use the OOA_Fast_FP-Growth algorithm proposed in <NO> to derive the complete set of the rules with certain support and confidence thresholds, since it is much faster than Apriori for mining frequent itemsets.', 'In our paper, based on the complete set of the rules generated by the malware detection rule generator, IMDS applied the technique of CBACB <NO> to build a classifier as the malware detection module to predict new file samples <NO>.', 'By using the OOA_Fast_FP-Growth algorithm <NO>, <NO>, we generate 31', 'Over the last few years, many research efforts have been conducted on developing intelligent malware detection systems <NO>.', 'For example, IMDS <NO> performs association classification on Windows API calls extracted from executable files while Naive Bayes methods on the extracted strings and byte sequences are applied in <NO>.', 'Both classifiers have been successfully used in malware detection <NO> and have distinct properties.', 'Recently, associative classification <NO>, with its ability to utilize relationships among attributes, has been also applied in <NO>.', '2) Both associative classification and SVM have been successfully applied in malware detection <NO>.', 'Various classification approaches including association classifiers, support vector machines, and Naive Bayes have been applied in malware detection <NO>.']"	Yanfang Ye, Dingding Wang, Tao Li, and Dongyi Ye. 2007. IMDS: Intelligent malware detection system. Proccedings of ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD).	The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based antivirus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using ObjectiveOriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA Fast FPGrowth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of KingSoft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system outperform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.	"Besides the traditional signature-based malware detection methods, there is some work to improve the signature-based detection <NO> and also a few attempts to apply data mining and machine learning techniques to detect new malicious executables.
Sung et al. <NO> developed a signature based malware detection system called SAVE (Static Analyzer of Vicious Executables) which emphasized on detecting polymorphic malware. The basic idea of this approach is to extract the signatures from the original malware with the hypothesis that all versions of the same malware
share a common core signature. Schultz et al. <NO> applied Naive Bayes method to detect previously unknown malicious code. Decision Tree was studied in <NO>. Kolter et al. <NO> gathered 1971 benign executables and 1651 malicious executables in Windows PE format, and examined the performance of different classifiers such as Naive Bayes, support vector machine (SVM) and Decision Tree using 10-fold cross validation and plotting ROC curves <NO>. Their results also showed that the ROC curve of the Decision Tree method dominated all others.
Different from earlier studies, our work is based on a large collection of malicious executables collected at KingSoft Anti-Virus Laboratory. In addition, we apply OOA mining technique to extract the characterizing frequent patterns to achieve accurate malware detection since frequent patterns found by association mining carry the underlying semantics of the data."	https://doi.org/10.1145/1281192.1281308	2	[]	['web_tap', 'web_tap’s', 'able_detect', 'access_processing', 'based_classification']	['access_processing_behavior', 'information_access_processing', 'tunneling_programs_test', '000_recent_malware', '25_000_recent']	Ye, et al. [NO] develops the intelligent malware detection system (imds) using objectiveoriented association (ooa) mining based classification. 	ooa imds rule pe system 
Panorama: Capturing system-wide information flow for malware detection and analysis	[', Polyglot <NO> and Panorama <NO>), Siren <NO> and oth-', 'BitBlaze has been shown to provide reliable data that has been used to produce very accurate malware classification results <NO>.', 'The first phase is implemented by the execution monitor <NO>.', 'Capturing information flow in dependence graphs: Existing techniques for constructing dependence graphs from programs provide only data-flow (and sometimes controlflow) dependencies between operations <NO>, <NO>, <NO>.', 'Previous work has shown that using data flows to describe malicious behavior is a powerful approach <NO>, <NO>, <NO> and that the system-call interface is the right abstraction for characterizing user-space malware <NO>,', 'HOLMES builds on existing work for creating the behavior graph and will benefit from more powerful tools that use, for example, dynamic taint tracing <NO>, <NO> and multipath analysis <NO>.', 'describe a behavioral specification of browser-based spyware based on taint-tracking <NO>, and Panorama uses whole-system taint analysis in a similar vein to detect more general classes of spyware <NO>.']	Heng Yin, Dawn Song, Manuel Egele, Christopher Kruegel, and Engin Kirda. 2007. Panorama: Capturing system-wide information flow for malware detection and analysis. Proceedings of the 14th ACM Conference on Computer and Communications Security (CCS).	Malicious programs spy on users’ behavior and compromise their privacy. Even software from reputable vendors, such as Google Desktop and Sony DRM media player, may perform undesirable actions. Unfortunately, existing techniques for detecting malware and analyzing unknown code samples are insufficient and have significant shortcomings. We observe that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users’ privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. We propose a system, Panorama, to detect and analyze malware by capturing this fundamental trait. In our extensive experiments, Panorama successfully detected all the malware samples and had very few false positives. Furthermore, by using Google Desktop as a case study, we show that our system can accurately capture its information access and processing behavior, and we can confirm that it does send back sensitive information to remote servers in certain settings. We believe that a system such as Panorama will offer indispensable assistance to code analysts and malware researchers by enabling them to quickly comprehend the behavior and inner-workings of an unknown sample.	D.4.6 <NO>: Security and Protection—Invasive software	https://doi.org/10.5555/1025118.1025582	2	[]	['web_tap', 'web_tap’s', 'able_detect', 'access_processing', 'based_classification']	['access_processing_behavior', 'information_access_processing', 'tunneling_programs_test', '000_recent_malware', '25_000_recent']	Yin, et al. [NO] observes that malicious information access and processing behavior is the fundamental trait of numerous malware categories breaching users ’ privacy (including keyloggers, password thieves, network sniffers, stealth backdoors, spyware and rootkits), which separates these malicious applications from benign software. 	panorama desktop fundamental google privacy 
